Error response from daemon: Get "https://registry.baidubce.com/v2/": proxyconnect tcp: dial tcp: lookup http on 172.18.100.1:53: no such host
tipc-xpu-sjx
+ unset http_proxy
+ unset https_proxy
+ export repo=PaddleNLP
+ repo=PaddleNLP
+ python -m pip install --retries 50 --upgrade pip -i https://mirror.baidu.com/pypi/simple
Looking in indexes: https://mirror.baidu.com/pypi/simple
Requirement already satisfied: pip in /opt/py39/lib/python3.9/site-packages (23.2.1)
Collecting pip
  Downloading https://mirror.baidu.com/pypi/packages/15/aa/3f4c7bcee2057a76562a5b33ecbd199be08cdb4443a02e26bd2c3cf6fc39/pip-23.3.2-py3-none-any.whl (2.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 8.8 MB/s eta 0:00:00
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 23.2.1
    Uninstalling pip-23.2.1:
      Successfully uninstalled pip-23.2.1
Successfully installed pip-23.3.2
+ python -m pip config set global.index-url https://mirror.baidu.com/pypi/simple
Writing to /root/.config/pip/pip.conf
+ python -m pip config set global.extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple
Writing to /root/.config/pip/pip.conf
+ python -m pip config set install.trusted-host mirror.baidu.com,pypi.tuna.tsinghua.edu.cn
Writing to /root/.config/pip/pip.conf
+ rm -rf paddlepaddle_xpu-0.0.0-cp39-cp39-linux_x86_64.whl
+ wget -q https://paddle-device.bj.bcebos.com/develop/xpu/paddlepaddle_xpu-0.0.0-cp39-cp39-linux_x86_64.whl
+ python -m pip install paddlepaddle_xpu-0.0.0-cp39-cp39-linux_x86_64.whl --force-reinstall
Looking in indexes: https://mirror.baidu.com/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Processing ./paddlepaddle_xpu-0.0.0-cp39-cp39-linux_x86_64.whl
Collecting httpx (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl (75 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.9/75.9 kB 1.5 MB/s eta 0:00:00
Collecting numpy<2.0,>=1.13 (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2f/75/f007cc0e6a373207818bef17f463d3305e9dd380a70db0e523e7660bf21f/numpy-1.26.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 15.8 MB/s eta 0:00:00
Collecting Pillow (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/3a/ada56d489446dbb7679d242bfd7bb159cee8a7989c34dd34045103d5280d/Pillow-10.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 30.0 MB/s eta 0:00:00
Collecting decorator (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/50/83c593b07763e1161326b3b8c6686f0f4b0f24d5526546bee538c89837d6/decorator-5.1.1-py3-none-any.whl (9.1 kB)
Collecting astor (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)
Collecting opt-einsum==3.3.0 (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 15.8 MB/s eta 0:00:00
Collecting protobuf>=3.20.2 (from paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ae/5b/7ed02a9b8e752c8f7bca8661779c0275b9e3e6a903a3045e6da51f796dda/protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 54.8 MB/s eta 0:00:00
Collecting anyio (from httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl (85 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.5/85.5 kB 20.3 MB/s eta 0:00:00
Collecting certifi (from httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl (162 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 34.1 MB/s eta 0:00:00
Collecting httpcore==1.* (from httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl (76 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 18.8 MB/s eta 0:00:00
Collecting idna (from httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl (61 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 13.2 MB/s eta 0:00:00
Collecting sniffio (from httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/a0/5dba8ed157b0136607c7f2151db695885606968d1fae123dc3391e0cfdbf/sniffio-1.3.0-py3-none-any.whl (10 kB)
Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 14.6 MB/s eta 0:00:00
Collecting exceptiongroup>=1.0.2 (from anyio->httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b8/9a/5028fd52db10e600f1c4674441b968cf2ea4959085bfb5b99fb1250e5f68/exceptiongroup-1.2.0-py3-none-any.whl (16 kB)
Collecting typing-extensions>=4.1 (from anyio->httpx->paddlepaddle-xpu==0.0.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl (32 kB)
Installing collected packages: typing-extensions, sniffio, protobuf, Pillow, numpy, idna, h11, exceptiongroup, decorator, certifi, astor, opt-einsum, httpcore, anyio, httpx, paddlepaddle-xpu
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.7.1
    Uninstalling typing_extensions-4.7.1:
      Successfully uninstalled typing_extensions-4.7.1
  Attempting uninstall: sniffio
    Found existing installation: sniffio 1.3.0
    Uninstalling sniffio-1.3.0:
      Successfully uninstalled sniffio-1.3.0
  Attempting uninstall: protobuf
    Found existing installation: protobuf 3.20.3
    Uninstalling protobuf-3.20.3:
      Successfully uninstalled protobuf-3.20.3
  Attempting uninstall: Pillow
    Found existing installation: Pillow 10.0.0
    Uninstalling Pillow-10.0.0:
      Successfully uninstalled Pillow-10.0.0
  Attempting uninstall: numpy
    Found existing installation: numpy 1.24.4
    Uninstalling numpy-1.24.4:
      Successfully uninstalled numpy-1.24.4
  Attempting uninstall: idna
    Found existing installation: idna 3.4
    Uninstalling idna-3.4:
      Successfully uninstalled idna-3.4
  Attempting uninstall: h11
    Found existing installation: h11 0.14.0
    Uninstalling h11-0.14.0:
      Successfully uninstalled h11-0.14.0
  Attempting uninstall: exceptiongroup
    Found existing installation: exceptiongroup 1.1.2
    Uninstalling exceptiongroup-1.1.2:
      Successfully uninstalled exceptiongroup-1.1.2
  Attempting uninstall: decorator
    Found existing installation: decorator 5.1.1
    Uninstalling decorator-5.1.1:
      Successfully uninstalled decorator-5.1.1
  Attempting uninstall: certifi
    Found existing installation: certifi 2023.7.22
    Uninstalling certifi-2023.7.22:
      Successfully uninstalled certifi-2023.7.22
  Attempting uninstall: astor
    Found existing installation: astor 0.8.1
    Uninstalling astor-0.8.1:
      Successfully uninstalled astor-0.8.1
  Attempting uninstall: opt-einsum
    Found existing installation: opt-einsum 3.3.0
    Uninstalling opt-einsum-3.3.0:
      Successfully uninstalled opt-einsum-3.3.0
  Attempting uninstall: httpcore
    Found existing installation: httpcore 0.17.3
    Uninstalling httpcore-0.17.3:
      Successfully uninstalled httpcore-0.17.3
  Attempting uninstall: anyio
    Found existing installation: anyio 3.7.1
    Uninstalling anyio-3.7.1:
      Successfully uninstalled anyio-3.7.1
  Attempting uninstall: httpx
    Found existing installation: httpx 0.24.1
    Uninstalling httpx-0.24.1:
      Successfully uninstalled httpx-0.24.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.2 which is incompatible.
Successfully installed Pillow-10.1.0 anyio-4.2.0 astor-0.8.1 certifi-2023.11.17 decorator-5.1.1 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 numpy-1.26.2 opt-einsum-3.3.0 paddlepaddle-xpu-0.0.0 protobuf-4.25.1 sniffio-1.3.0 typing-extensions-4.9.0
+ python -c 'import paddle; print(paddle.version.commit)'
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
35f4501b1fe91b94fc869b63213b0e833dba8ac9
No XPU Memory Leak
+ python -m pip install ./AutoLog/dist/auto_log-1.2.0-py3-none-any.whl
Looking in indexes: https://mirror.baidu.com/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Processing ./AutoLog/dist/auto_log-1.2.0-py3-none-any.whl
Collecting GPUtil (from auto-log==1.2.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz (5.5 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: psutil in /opt/py39/lib/python3.9/site-packages (from auto-log==1.2.0) (5.9.5)
Collecting pynvml (from auto-log==1.2.0)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5b/9c/adb8070059caaa15d5a572b66bccd95900d8c1b9fa54d6ecea6ae97448d1/pynvml-11.5.0-py3-none-any.whl (53 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 327.7 kB/s eta 0:00:00
Requirement already satisfied: distro in /opt/py39/lib/python3.9/site-packages (from auto-log==1.2.0) (1.8.0)
Requirement already satisfied: wheel in /opt/py39/lib/python3.9/site-packages (from auto-log==1.2.0) (0.41.1)
Building wheels for collected packages: GPUtil
  Building wheel for GPUtil (setup.py): started
  Building wheel for GPUtil (setup.py): finished with status 'done'
  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=8c81341c513f97e538fa2bdbc02d82648b683909b006fdf2c226ebaf0217d586
  Stored in directory: /root/.cache/pip/wheels/45/e5/d8/15d0e5d4d4ae3abb8f40ece7918477ac1b0a7ef38aa16ae70c
Successfully built GPUtil
Installing collected packages: GPUtil, pynvml, auto-log
Successfully installed GPUtil-1.4.0 auto-log-1.2.0 pynvml-11.5.0
+ echo PaddleNLP
+ bash run.sh PaddleNLP
PaddleNLP
+ repo=PaddleNLP
++ pwd
+ ROOT_PATH=/workspace
+ [[ PaddleNLP == \P\a\d\d\l\e\V\i\d\e\o ]]
+ cd /workspace
+ [[ PaddleNLP == \P\a\d\d\l\e\O\C\R ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\R\e\c ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\S\e\g ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\V\i\d\e\o ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\D\e\t\e\c\t\i\o\n ]]
+ touch full_chain_list_all
+ rm -rf PaddleNLP-develop.tar.gz
+ rm -rf PaddleNLP
+ rm -rf PaddleNLP-develop
+ wget -nv https://xly-devops.bj.bcebos.com/PaddleTest/PaddleNLP/PaddleNLP-develop.tar.gz
2023-12-21 10:15:32 URL:https://xly-devops.bj.bcebos.com/PaddleTest/PaddleNLP/PaddleNLP-develop.tar.gz [127814007/127814007] -> "PaddleNLP-develop.tar.gz" [1]
+ tar -zxf PaddleNLP-develop.tar.gz
+ mv PaddleNLP-develop PaddleNLP
+ cd PaddleNLP
+ [[ PaddleNLP == \P\a\d\d\l\e\R\e\c ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\D\e\t\e\c\t\i\o\n ]]
+ pip install --upgrade -r requirements.txt
Looking in indexes: https://mirror.baidu.com/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Ignoring protobuf: markers 'platform_system == "Windows"' don't match your environment
Collecting jieba (from -r requirements.txt (line 1))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 6.2 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting colorlog (from -r requirements.txt (line 2))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/df/520663eb7f7a329f7c585834b754bcc3cbcc03957d85fcbba4a2a723ad9d/colorlog-6.8.0-py3-none-any.whl (11 kB)
Collecting colorama (from -r requirements.txt (line 3))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Collecting seqeval (from -r requirements.txt (line 4))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 8.9 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting dill<0.3.5 (from -r requirements.txt (line 5))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 4.4 MB/s eta 0:00:00
Collecting multiprocess<=0.70.12.2 (from -r requirements.txt (line 6))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e3/12/c1c7b5574a574a5bc898c8656b0ab8514d0609356b3ca18180e2ae94c2f7/multiprocess-0.70.12.2-py39-none-any.whl (128 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.7/128.7 kB 5.8 MB/s eta 0:00:00
Collecting datasets>=2.0.0 (from -r requirements.txt (line 7))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl (521 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 521.2/521.2 kB 3.2 MB/s eta 0:00:00
Requirement already satisfied: tqdm in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.65.2)
Collecting tqdm (from -r requirements.txt (line 8))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl (78 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 3.1 MB/s eta 0:00:00
Collecting paddlefsl (from -r requirements.txt (line 9))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.0/101.0 kB 3.2 MB/s eta 0:00:00
Collecting sentencepiece (from -r requirements.txt (line 10))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6b/22/4157918b2112d47014fb1e79b0dd6d5a141b8d1b049bae695d405150ebaf/sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 4.2 MB/s eta 0:00:00
Requirement already satisfied: huggingface_hub>=0.11.1 in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (0.16.4)
Collecting huggingface_hub>=0.11.1 (from -r requirements.txt (line 11))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a0/0a/02ac0ae1047d97769003ff4fb8e6717024f3f174a5d13257415aa09e13d9/huggingface_hub-0.20.1-py3-none-any.whl (330 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.1/330.1 kB 6.4 MB/s eta 0:00:00
Requirement already satisfied: onnx>=1.10.0 in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.14.0)
Collecting onnx>=1.10.0 (from -r requirements.txt (line 12))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/be/758ff7735e35c0ce84725c250a48b69c7cf38ddedcb598848b79c3038f3c/onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 14.3 MB/s eta 0:00:00
Requirement already satisfied: protobuf>=3.20.2 in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (4.25.1)
Requirement already satisfied: paddle2onnx in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (1.0.8)
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError("HTTPSConnectionPool(host='mirror.baidu.com', port=443): Read timed out. (read timeout=15)")': /pypi/simple/paddle2onnx/
Collecting paddle2onnx (from -r requirements.txt (line 15))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/80/3c/a68c16f62cb90548f62787942ee87377fac01fe1a26aab4bd29068993ecb/paddle2onnx-1.1.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 29.4 MB/s eta 0:00:00
Requirement already satisfied: Flask-Babel in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (3.1.0)
Collecting Flask-Babel (from -r requirements.txt (line 16))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/14/c2/e0ab5abe37882e118482884f2ec660cd06da644ddfbceccf5f88f546b574/flask_babel-4.0.0-py3-none-any.whl (9.6 kB)
Requirement already satisfied: visualdl in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (2.5.0)
Collecting visualdl (from -r requirements.txt (line 17))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ea/b5/37726c750a4f4598660998327c3566b2d2ed5a1a5f44e9f0dde875602447/visualdl-2.5.3-py3-none-any.whl (6.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 47.1 MB/s eta 0:00:00
Requirement already satisfied: fastapi in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.101.0)
Collecting fastapi (from -r requirements.txt (line 18))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ad/b9/b7ea33663daffa9db94119ea2a3df8f97bdca297024145fe79a5a13d37af/fastapi-0.105.0-py3-none-any.whl (93 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.1/93.1 kB 21.3 MB/s eta 0:00:00
Requirement already satisfied: uvicorn in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (0.23.2)
Collecting uvicorn (from -r requirements.txt (line 19))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7e/17/4b7a76fffa7babf397481040d8aef2725b2b81ae19f1a31b5ca0c17d49e6/uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.7/59.7 kB 15.3 MB/s eta 0:00:00
Collecting typer (from -r requirements.txt (line 20))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bf/0e/c68adf10adda05f28a6ed7b9f4cd7b8e07f641b44af88ba72d9c89e4de7a/typer-0.9.0-py3-none-any.whl (45 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.9/45.9 kB 8.8 MB/s eta 0:00:00
Collecting rich (from -r requirements.txt (line 21))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl (240 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.6/240.6 kB 43.8 MB/s eta 0:00:00
Collecting safetensors (from -r requirements.txt (line 22))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/5f/deffd66027e4652521d4cd4dd390a1482b4342f5aa99d86d466122300d6b/safetensors-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 53.4 MB/s eta 0:00:00
Collecting tool_helpers (from -r requirements.txt (line 23))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/eb/19/137aa2604f55a86c9265f7e312d9ae57ab7e7e146e3057817bbf8858c983/tool_helpers-0.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.7/107.7 kB 29.5 MB/s eta 0:00:00
Collecting aistudio-sdk>=0.1.3 (from -r requirements.txt (line 24))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/a3/fbba1a0daf6739e9457d6ed14f4623a61052b4d87226a769d25adb73dd4e/aistudio_sdk-0.1.5-py3-none-any.whl (17 kB)
Requirement already satisfied: jinja2 in /opt/py39/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (3.1.2)
Requirement already satisfied: numpy>=1.14.0 in /opt/py39/lib/python3.9/site-packages (from seqeval->-r requirements.txt (line 4)) (1.26.2)
Requirement already satisfied: scikit-learn>=0.21.3 in /opt/py39/lib/python3.9/site-packages (from seqeval->-r requirements.txt (line 4)) (1.3.0)
Collecting pyarrow>=8.0.0 (from datasets>=2.0.0->-r requirements.txt (line 7))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b0/84/d2b6d658112332813834ca50a98c9422a1bf66c6c16028020e153b7bc193/pyarrow-14.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 12.5 MB/s eta 0:00:00
Collecting pyarrow-hotfix (from datasets>=2.0.0->-r requirements.txt (line 7))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)
Requirement already satisfied: pandas in /opt/py39/lib/python3.9/site-packages (from datasets>=2.0.0->-r requirements.txt (line 7)) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in /opt/py39/lib/python3.9/site-packages (from datasets>=2.0.0->-r requirements.txt (line 7)) (2.31.0)
Collecting xxhash (from datasets>=2.0.0->-r requirements.txt (line 7))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 37.9 MB/s eta 0:00:00
Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/py39/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.0.0->-r requirements.txt (line 7)) (2023.6.0)
Requirement already satisfied: aiohttp in /opt/py39/lib/python3.9/site-packages (from datasets>=2.0.0->-r requirements.txt (line 7)) (3.8.5)
Requirement already satisfied: packaging in /opt/py39/lib/python3.9/site-packages (from datasets>=2.0.0->-r requirements.txt (line 7)) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /opt/py39/lib/python3.9/site-packages (from datasets>=2.0.0->-r requirements.txt (line 7)) (6.0.1)
Requirement already satisfied: filelock in /opt/py39/lib/python3.9/site-packages (from huggingface_hub>=0.11.1->-r requirements.txt (line 11)) (3.12.2)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/py39/lib/python3.9/site-packages (from huggingface_hub>=0.11.1->-r requirements.txt (line 11)) (4.9.0)
Requirement already satisfied: Babel>=2.12 in /opt/py39/lib/python3.9/site-packages (from Flask-Babel->-r requirements.txt (line 16)) (2.12.1)
Requirement already satisfied: Flask>=2.0 in /opt/py39/lib/python3.9/site-packages (from Flask-Babel->-r requirements.txt (line 16)) (2.3.2)
Requirement already satisfied: pytz>=2022.7 in /opt/py39/lib/python3.9/site-packages (from Flask-Babel->-r requirements.txt (line 16)) (2023.3)
Requirement already satisfied: bce-python-sdk in /opt/py39/lib/python3.9/site-packages (from visualdl->-r requirements.txt (line 17)) (0.8.87)
Requirement already satisfied: Pillow>=7.0.0 in /opt/py39/lib/python3.9/site-packages (from visualdl->-r requirements.txt (line 17)) (10.1.0)
Requirement already satisfied: six>=1.14.0 in /opt/py39/lib/python3.9/site-packages (from visualdl->-r requirements.txt (line 17)) (1.16.0)
Requirement already satisfied: matplotlib in /opt/py39/lib/python3.9/site-packages (from visualdl->-r requirements.txt (line 17)) (3.7.2)
Requirement already satisfied: rarfile in /opt/py39/lib/python3.9/site-packages (from visualdl->-r requirements.txt (line 17)) (4.0)
Requirement already satisfied: psutil in /opt/py39/lib/python3.9/site-packages (from visualdl->-r requirements.txt (line 17)) (5.9.5)
Collecting anyio<4.0.0,>=3.7.1 (from fastapi->-r requirements.txt (line 18))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl (80 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 19.3 MB/s eta 0:00:00
Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/py39/lib/python3.9/site-packages (from fastapi->-r requirements.txt (line 18)) (2.1.1)
Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/py39/lib/python3.9/site-packages (from fastapi->-r requirements.txt (line 18)) (0.27.0)
Requirement already satisfied: click>=7.0 in /opt/py39/lib/python3.9/site-packages (from uvicorn->-r requirements.txt (line 19)) (8.1.6)
Requirement already satisfied: h11>=0.8 in /opt/py39/lib/python3.9/site-packages (from uvicorn->-r requirements.txt (line 19)) (0.14.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/py39/lib/python3.9/site-packages (from rich->-r requirements.txt (line 21)) (2.2.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/py39/lib/python3.9/site-packages (from rich->-r requirements.txt (line 21)) (2.16.1)
Collecting pybind11 (from tool_helpers->-r requirements.txt (line 23))
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/06/55/9f73c32dda93fa4f539fafa268f9504e83c489f460c380371d94296126cd/pybind11-2.11.1-py3-none-any.whl (227 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.7/227.7 kB 43.9 MB/s eta 0:00:00
Requirement already satisfied: MarkupSafe>=2.0 in /opt/py39/lib/python3.9/site-packages (from jinja2->-r requirements.txt (line 25)) (2.1.3)
Requirement already satisfied: idna>=2.8 in /opt/py39/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 18)) (3.6)
Requirement already satisfied: sniffio>=1.1 in /opt/py39/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 18)) (1.3.0)
Requirement already satisfied: exceptiongroup in /opt/py39/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 18)) (1.2.0)
Requirement already satisfied: Werkzeug>=2.3.3 in /opt/py39/lib/python3.9/site-packages (from Flask>=2.0->Flask-Babel->-r requirements.txt (line 16)) (2.3.6)
Requirement already satisfied: itsdangerous>=2.1.2 in /opt/py39/lib/python3.9/site-packages (from Flask>=2.0->Flask-Babel->-r requirements.txt (line 16)) (2.1.2)
Requirement already satisfied: blinker>=1.6.2 in /opt/py39/lib/python3.9/site-packages (from Flask>=2.0->Flask-Babel->-r requirements.txt (line 16)) (1.6.2)
Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/py39/lib/python3.9/site-packages (from Flask>=2.0->Flask-Babel->-r requirements.txt (line 16)) (6.8.0)
Requirement already satisfied: attrs>=17.3.0 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (3.2.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (1.4.0)
Requirement already satisfied: aiosignal>=1.1.2 in /opt/py39/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 7)) (1.3.1)
Requirement already satisfied: mdurl~=0.1 in /opt/py39/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 21)) (0.1.2)
Requirement already satisfied: annotated-types>=0.4.0 in /opt/py39/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 18)) (0.5.0)
Requirement already satisfied: pydantic-core==2.4.0 in /opt/py39/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 18)) (2.4.0)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/py39/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.0.0->-r requirements.txt (line 7)) (2.0.4)
Requirement already satisfied: certifi>=2017.4.17 in /opt/py39/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.0.0->-r requirements.txt (line 7)) (2023.11.17)
Requirement already satisfied: scipy>=1.5.0 in /opt/py39/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 4)) (1.11.1)
Requirement already satisfied: joblib>=1.1.1 in /opt/py39/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 4)) (1.3.1)
Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/py39/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 4)) (3.2.0)
Requirement already satisfied: pycryptodome>=3.8.0 in /opt/py39/lib/python3.9/site-packages (from bce-python-sdk->visualdl->-r requirements.txt (line 17)) (3.18.0)
Requirement already satisfied: future>=0.6.0 in /opt/py39/lib/python3.9/site-packages (from bce-python-sdk->visualdl->-r requirements.txt (line 17)) (0.18.3)
Requirement already satisfied: contourpy>=1.0.1 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (1.1.0)
Requirement already satisfied: cycler>=0.10 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (4.42.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (1.4.4)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (3.0.9)
Requirement already satisfied: python-dateutil>=2.7 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (2.8.2)
Requirement already satisfied: importlib-resources>=3.2.0 in /opt/py39/lib/python3.9/site-packages (from matplotlib->visualdl->-r requirements.txt (line 17)) (6.0.1)
Requirement already satisfied: tzdata>=2022.1 in /opt/py39/lib/python3.9/site-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 7)) (2023.3)
Requirement already satisfied: zipp>=0.5 in /opt/py39/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->Flask>=2.0->Flask-Babel->-r requirements.txt (line 16)) (3.16.2)
Building wheels for collected packages: jieba, seqeval
  Building wheel for jieba (setup.py): started
  Building wheel for jieba (setup.py): finished with status 'done'
  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314459 sha256=40d1dab08ee2f57c70678d9b640b4aae7e36e4285d8e0f492cb0ac6019672ee1
  Stored in directory: /root/.cache/pip/wheels/1a/76/68/b6d79c4db704bb18d54f6a73ab551185f4711f9730c0c15d97
  Building wheel for seqeval (setup.py): started
  Building wheel for seqeval (setup.py): finished with status 'done'
  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=3e5914318b13cd96fc29bf9b383629a709bf8cd011b37ff70aaa03fa1a468582
  Stored in directory: /root/.cache/pip/wheels/f3/bc/ba/35bdfc58cc87820ac8efd93be97285f5dd887c1c72a89f5275
Successfully built jieba seqeval
Installing collected packages: sentencepiece, paddle2onnx, jieba, xxhash, uvicorn, typer, tqdm, safetensors, pybind11, pyarrow-hotfix, pyarrow, onnx, dill, colorlog, colorama, anyio, tool_helpers, rich, paddlefsl, multiprocess, huggingface_hub, aistudio-sdk, seqeval, Flask-Babel, fastapi, visualdl, datasets
  Attempting uninstall: paddle2onnx
    Found existing installation: paddle2onnx 1.0.8
    Uninstalling paddle2onnx-1.0.8:
      Successfully uninstalled paddle2onnx-1.0.8
  Attempting uninstall: uvicorn
    Found existing installation: uvicorn 0.23.2
    Uninstalling uvicorn-0.23.2:
      Successfully uninstalled uvicorn-0.23.2
  Attempting uninstall: tqdm
    Found existing installation: tqdm 4.65.2
    Uninstalling tqdm-4.65.2:
      Successfully uninstalled tqdm-4.65.2
  Attempting uninstall: onnx
    Found existing installation: onnx 1.14.0
    Uninstalling onnx-1.14.0:
      Successfully uninstalled onnx-1.14.0
  Attempting uninstall: dill
    Found existing installation: dill 0.3.7
    Uninstalling dill-0.3.7:
      Successfully uninstalled dill-0.3.7
  Attempting uninstall: anyio
    Found existing installation: anyio 4.2.0
    Uninstalling anyio-4.2.0:
      Successfully uninstalled anyio-4.2.0
  Attempting uninstall: multiprocess
    Found existing installation: multiprocess 0.70.15
    Uninstalling multiprocess-0.70.15:
      Successfully uninstalled multiprocess-0.70.15
  Attempting uninstall: huggingface_hub
    Found existing installation: huggingface-hub 0.16.4
    Uninstalling huggingface-hub-0.16.4:
      Successfully uninstalled huggingface-hub-0.16.4
  Attempting uninstall: Flask-Babel
    Found existing installation: flask-babel 3.1.0
    Uninstalling flask-babel-3.1.0:
      Successfully uninstalled flask-babel-3.1.0
  Attempting uninstall: fastapi
    Found existing installation: fastapi 0.101.0
    Uninstalling fastapi-0.101.0:
      Successfully uninstalled fastapi-0.101.0
  Attempting uninstall: visualdl
    Found existing installation: visualdl 2.5.0
    Uninstalling visualdl-2.5.0:
      Successfully uninstalled visualdl-2.5.0
Successfully installed Flask-Babel-4.0.0 aistudio-sdk-0.1.5 anyio-3.7.1 colorama-0.4.6 colorlog-6.8.0 datasets-2.15.0 dill-0.3.4 fastapi-0.105.0 huggingface_hub-0.20.1 jieba-0.42.1 multiprocess-0.70.12.2 onnx-1.15.0 paddle2onnx-1.1.0 paddlefsl-1.1.0 pyarrow-14.0.2 pyarrow-hotfix-0.6 pybind11-2.11.1 rich-13.7.0 safetensors-0.4.1 sentencepiece-0.1.99 seqeval-1.2.2 tool_helpers-0.1.1 tqdm-4.66.1 typer-0.9.0 uvicorn-0.24.0.post1 visualdl-2.5.3 xxhash-3.4.1
+ python setup.py install
/opt/py39/lib/python3.9/site-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.
!!

        ********************************************************************************
        Requirements should be satisfied by a PEP 517 installer.
        If you are using pip, you can try `pip install --use-pep517`.
        ********************************************************************************

!!
  dist.fetch_build_eggs(dist.setup_requires)
/opt/py39/lib/python3.9/site-packages/setuptools/dist.py:509: InformationOnly: Normalizing '2.6.1.post' to '2.6.1.post0'
  self.metadata.version = self._normalize_version(
running install
/opt/py39/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  self.initialize_options()
/opt/py39/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` and ``easy_install``.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://github.com/pypa/setuptools/issues/917 for details.
        ********************************************************************************

!!
  self.initialize_options()
running bdist_egg
running egg_info
creating paddlenlp.egg-info
writing paddlenlp.egg-info/PKG-INFO
writing dependency_links to paddlenlp.egg-info/dependency_links.txt
writing entry points to paddlenlp.egg-info/entry_points.txt
writing requirements to paddlenlp.egg-info/requires.txt
writing top-level names to paddlenlp.egg-info/top_level.txt
writing manifest file 'paddlenlp.egg-info/SOURCES.txt'
reading manifest file 'paddlenlp.egg-info/SOURCES.txt'
adding license file 'LICENSE'
writing manifest file 'paddlenlp.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
creating build
creating build/lib
creating build/lib/paddlenlp
copying paddlenlp/__init__.py -> build/lib/paddlenlp
creating build/lib/paddlenlp/trainer
copying paddlenlp/trainer/trainer_utils.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/__init__.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/compression_args.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/trainer_compress.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/training_args_seq2seq.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/trainer.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/training_args.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/integrations.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/trainer_seq2seq.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/argparser.py -> build/lib/paddlenlp/trainer
copying paddlenlp/trainer/trainer_callback.py -> build/lib/paddlenlp/trainer
creating build/lib/paddlenlp/quantization
copying paddlenlp/quantization/qlora.py -> build/lib/paddlenlp/quantization
copying paddlenlp/quantization/quantization_config.py -> build/lib/paddlenlp/quantization
copying paddlenlp/quantization/__init__.py -> build/lib/paddlenlp/quantization
copying paddlenlp/quantization/quantization_linear.py -> build/lib/paddlenlp/quantization
copying paddlenlp/quantization/quantization_utils.py -> build/lib/paddlenlp/quantization
creating build/lib/paddlenlp/metrics
copying paddlenlp/metrics/span.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/sighan.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/__init__.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/rouge.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/utils.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/dureader.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/mrr.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/perplexity.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/chunk.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/glue.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/bleu.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/squad.py -> build/lib/paddlenlp/metrics
copying paddlenlp/metrics/distinct.py -> build/lib/paddlenlp/metrics
creating build/lib/paddlenlp/generation
copying paddlenlp/generation/__init__.py -> build/lib/paddlenlp/generation
copying paddlenlp/generation/configuration_utils.py -> build/lib/paddlenlp/generation
copying paddlenlp/generation/utils.py -> build/lib/paddlenlp/generation
copying paddlenlp/generation/stopping_criteria.py -> build/lib/paddlenlp/generation
copying paddlenlp/generation/streamers.py -> build/lib/paddlenlp/generation
copying paddlenlp/generation/logits_process.py -> build/lib/paddlenlp/generation
creating build/lib/paddlenlp/losses
copying paddlenlp/losses/__init__.py -> build/lib/paddlenlp/losses
copying paddlenlp/losses/rdrop.py -> build/lib/paddlenlp/losses
creating build/lib/paddlenlp/utils
copying paddlenlp/utils/profiler.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/initializer.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/__init__.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/distributed.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/tools.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/log.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/converter.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/doc_parser.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/downloader.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/import_utils.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/env.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/image_utils.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/batch_sampler.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/serialization.py -> build/lib/paddlenlp/utils
copying paddlenlp/utils/ie_utils.py -> build/lib/paddlenlp/utils
creating build/lib/paddlenlp/version
copying paddlenlp/version/__init__.py -> build/lib/paddlenlp/version
copying paddlenlp/version/git.py -> build/lib/paddlenlp/version
creating build/lib/paddlenlp/layers
copying paddlenlp/layers/__init__.py -> build/lib/paddlenlp/layers
copying paddlenlp/layers/crf.py -> build/lib/paddlenlp/layers
copying paddlenlp/layers/linear.py -> build/lib/paddlenlp/layers
copying paddlenlp/layers/sequence.py -> build/lib/paddlenlp/layers
copying paddlenlp/layers/globalpointer.py -> build/lib/paddlenlp/layers
copying paddlenlp/layers/tcn.py -> build/lib/paddlenlp/layers
creating build/lib/paddlenlp/embeddings
copying paddlenlp/embeddings/__init__.py -> build/lib/paddlenlp/embeddings
copying paddlenlp/embeddings/token_embedding.py -> build/lib/paddlenlp/embeddings
copying paddlenlp/embeddings/constant.py -> build/lib/paddlenlp/embeddings
creating build/lib/paddlenlp/ops
copying paddlenlp/ops/__init__.py -> build/lib/paddlenlp/ops
copying paddlenlp/ops/einsum.py -> build/lib/paddlenlp/ops
copying paddlenlp/ops/ext_utils.py -> build/lib/paddlenlp/ops
creating build/lib/paddlenlp/server
copying paddlenlp/server/predictor.py -> build/lib/paddlenlp/server
copying paddlenlp/server/__init__.py -> build/lib/paddlenlp/server
copying paddlenlp/server/server.py -> build/lib/paddlenlp/server
copying paddlenlp/server/base_router.py -> build/lib/paddlenlp/server
copying paddlenlp/server/utils.py -> build/lib/paddlenlp/server
copying paddlenlp/server/model_manager.py -> build/lib/paddlenlp/server
copying paddlenlp/server/taskflow_manager.py -> build/lib/paddlenlp/server
creating build/lib/paddlenlp/prompt
copying paddlenlp/prompt/verbalizer.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/prompt_trainer.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/prompt_tokenizer.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/__init__.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/prompt_model.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/prompt_utils.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/prompt_args.py -> build/lib/paddlenlp/prompt
copying paddlenlp/prompt/template.py -> build/lib/paddlenlp/prompt
creating build/lib/paddlenlp/datasets
copying paddlenlp/datasets/bellegroup.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/dureader_robust.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/cblue.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/__init__.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/advertisegen.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/nlpcc13_evsam05_thu.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/imdb.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/nlpcc_dbqa.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/cail2018_small.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/cote.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/dataset.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/lcsts_new.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/xnli_cn.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/ptb.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/wmt14ende.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/wos.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/chnsenticorp_v2.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/thucnews.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/clue.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/yahoo_answer_100k.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/poetry.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/fewclue.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/nlpcc14_sc.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/lcqmc_v2.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/bq_corpus.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/nlpcc13_evsam05_hit.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/couplet.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/cmrc2018.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/iwslt15.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/paws-x.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/msra_ner.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/intokens_dataset.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/drcd.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/dureader_qg.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/dureader_yesno.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/hyp.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/drcd_cn.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/bstc.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/dureader_checklist.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/triviaqa.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/chnsenticorp.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/lcqmc.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/seabsa16.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/glue.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/peoples_daily_ner.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/c3.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/xnli.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/cail2019_scm.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/duconv.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/cnn_dailymail.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/squad.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/sighan-cn.py -> build/lib/paddlenlp/datasets
copying paddlenlp/datasets/conll2002.py -> build/lib/paddlenlp/datasets
creating build/lib/paddlenlp/data
copying paddlenlp/data/iterator.py -> build/lib/paddlenlp/data
copying paddlenlp/data/vocab.py -> build/lib/paddlenlp/data
copying paddlenlp/data/__init__.py -> build/lib/paddlenlp/data
copying paddlenlp/data/data_collator.py -> build/lib/paddlenlp/data
copying paddlenlp/data/blendable_dataset.py -> build/lib/paddlenlp/data
copying paddlenlp/data/collate.py -> build/lib/paddlenlp/data
copying paddlenlp/data/indexed_dataset.py -> build/lib/paddlenlp/data
copying paddlenlp/data/sampler.py -> build/lib/paddlenlp/data
copying paddlenlp/data/causal_dataset.py -> build/lib/paddlenlp/data
copying paddlenlp/data/dist_dataloader.py -> build/lib/paddlenlp/data
copying paddlenlp/data/tokenizer.py -> build/lib/paddlenlp/data
creating build/lib/paddlenlp/transformers
copying paddlenlp/transformers/sentencepiece_model_pb2.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/image_transforms.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/processing_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/__init__.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/sequence_parallel_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/configuration_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/image_processing_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/attention_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/optimization.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/tokenizer_utils_fast.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/convert_slow_tokenizer.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/segment_parallel_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/model_outputs.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/tokenizer_utils_base.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/model_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/conversion_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/tokenizer_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/activations.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/feature_extraction_sequence_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/audio_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/export.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/feature_extraction_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/distill_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/aistudio_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/image_utils.py -> build/lib/paddlenlp/transformers
copying paddlenlp/transformers/ofa_utils.py -> build/lib/paddlenlp/transformers
creating build/lib/paddlenlp/experimental
copying paddlenlp/experimental/__init__.py -> build/lib/paddlenlp/experimental
copying paddlenlp/experimental/ernie_model.py -> build/lib/paddlenlp/experimental
copying paddlenlp/experimental/model_utils.py -> build/lib/paddlenlp/experimental
copying paddlenlp/experimental/faster_tokenizer.py -> build/lib/paddlenlp/experimental
creating build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text_summarization.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/dialogue.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text_classification.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text_generation.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/__init__.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/fill_mask.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text_feature_extraction.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/zero_shot_text_classification.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/lexical_analysis.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/utils.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/taskflow.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text_similarity.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text_correction.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/task.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/sentiment_analysis.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/word_segmentation.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/named_entity_recognition.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/question_answering.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/information_extraction.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/knowledge_mining.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/pos_tagging.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/question_generation.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/code_generation.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/document_intelligence.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/multimodal_feature_extraction.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/dependency_parsing.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/text2text_generation.py -> build/lib/paddlenlp/taskflow
copying paddlenlp/taskflow/poetry_generation.py -> build/lib/paddlenlp/taskflow
creating build/lib/paddlenlp/dataaug
copying paddlenlp/dataaug/__init__.py -> build/lib/paddlenlp/dataaug
copying paddlenlp/dataaug/char.py -> build/lib/paddlenlp/dataaug
copying paddlenlp/dataaug/base_augment.py -> build/lib/paddlenlp/dataaug
copying paddlenlp/dataaug/sentence.py -> build/lib/paddlenlp/dataaug
copying paddlenlp/dataaug/word.py -> build/lib/paddlenlp/dataaug
creating build/lib/paddlenlp/seq2vec
copying paddlenlp/seq2vec/__init__.py -> build/lib/paddlenlp/seq2vec
copying paddlenlp/seq2vec/encoder.py -> build/lib/paddlenlp/seq2vec
creating build/lib/paddlenlp/cli
copying paddlenlp/cli/__init__.py -> build/lib/paddlenlp/cli
copying paddlenlp/cli/server.py -> build/lib/paddlenlp/cli
copying paddlenlp/cli/bos_community.py -> build/lib/paddlenlp/cli
copying paddlenlp/cli/main.py -> build/lib/paddlenlp/cli
copying paddlenlp/cli/install.py -> build/lib/paddlenlp/cli
copying paddlenlp/cli/download.py -> build/lib/paddlenlp/cli
creating build/lib/paddlenlp/peft
copying paddlenlp/peft/__init__.py -> build/lib/paddlenlp/peft
creating build/lib/paddlenlp/trainer/utils
copying paddlenlp/trainer/utils/__init__.py -> build/lib/paddlenlp/trainer/utils
copying paddlenlp/trainer/utils/sharding_io.py -> build/lib/paddlenlp/trainer/utils
copying paddlenlp/trainer/utils/doc.py -> build/lib/paddlenlp/trainer/utils
copying paddlenlp/trainer/utils/helper.py -> build/lib/paddlenlp/trainer/utils
creating build/lib/paddlenlp/trainer/plugins
copying paddlenlp/trainer/plugins/__init__.py -> build/lib/paddlenlp/trainer/plugins
copying paddlenlp/trainer/plugins/npu_plugin.py -> build/lib/paddlenlp/trainer/plugins
copying paddlenlp/trainer/plugins/timer.py -> build/lib/paddlenlp/trainer/plugins
copying paddlenlp/trainer/plugins/unified_checkpoint.py -> build/lib/paddlenlp/trainer/plugins
creating build/lib/paddlenlp/trainer/utils/reshard
copying paddlenlp/trainer/utils/reshard/__init__.py -> build/lib/paddlenlp/trainer/utils/reshard
copying paddlenlp/trainer/utils/reshard/sharding_v2.py -> build/lib/paddlenlp/trainer/utils/reshard
copying paddlenlp/trainer/utils/reshard/common.py -> build/lib/paddlenlp/trainer/utils/reshard
copying paddlenlp/trainer/utils/reshard/sharding_v1.py -> build/lib/paddlenlp/trainer/utils/reshard
creating build/lib/paddlenlp/ops/distributed
copying paddlenlp/ops/distributed/__init__.py -> build/lib/paddlenlp/ops/distributed
copying paddlenlp/ops/distributed/parallel.py -> build/lib/paddlenlp/ops/distributed
creating build/lib/paddlenlp/ops/optimizer
copying paddlenlp/ops/optimizer/__init__.py -> build/lib/paddlenlp/ops/optimizer
copying paddlenlp/ops/optimizer/lr.py -> build/lib/paddlenlp/ops/optimizer
copying paddlenlp/ops/optimizer/ema.py -> build/lib/paddlenlp/ops/optimizer
copying paddlenlp/ops/optimizer/adamwdl.py -> build/lib/paddlenlp/ops/optimizer
creating build/lib/paddlenlp/ops/fast_transformer
copying paddlenlp/ops/fast_transformer/__init__.py -> build/lib/paddlenlp/ops/fast_transformer
creating build/lib/paddlenlp/ops/distributed/utils
copying paddlenlp/ops/distributed/utils/topo.py -> build/lib/paddlenlp/ops/distributed/utils
copying paddlenlp/ops/distributed/utils/__init__.py -> build/lib/paddlenlp/ops/distributed/utils
copying paddlenlp/ops/distributed/utils/random.py -> build/lib/paddlenlp/ops/distributed/utils
creating build/lib/paddlenlp/ops/fast_transformer/transformer
copying paddlenlp/ops/fast_transformer/transformer/decoding.py -> build/lib/paddlenlp/ops/fast_transformer/transformer
copying paddlenlp/ops/fast_transformer/transformer/__init__.py -> build/lib/paddlenlp/ops/fast_transformer/transformer
copying paddlenlp/ops/fast_transformer/transformer/fast_transformer.py -> build/lib/paddlenlp/ops/fast_transformer/transformer
copying paddlenlp/ops/fast_transformer/transformer/encoder.py -> build/lib/paddlenlp/ops/fast_transformer/transformer
copying paddlenlp/ops/fast_transformer/transformer/decoder.py -> build/lib/paddlenlp/ops/fast_transformer/transformer
creating build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/token_model_handler.py -> build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/__init__.py -> build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/custom_model_handler.py -> build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/taskflow_handler.py -> build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/qa_model_handler.py -> build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/cls_post_handler.py -> build/lib/paddlenlp/server/handlers
copying paddlenlp/server/handlers/base_handler.py -> build/lib/paddlenlp/server/handlers
creating build/lib/paddlenlp/server/http_router
copying paddlenlp/server/http_router/__init__.py -> build/lib/paddlenlp/server/http_router
copying paddlenlp/server/http_router/router.py -> build/lib/paddlenlp/server/http_router
creating build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/dureader_robust.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/xfund_zh.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/__init__.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/imdb.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/mt_eng_vietnamese.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/cote.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/ptb_text_only.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/clue.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/cmrc2018.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/msra_ner.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/docvqa_zh.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/funsd.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/squad_v2.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/chnsenticorp.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/language_pair.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/seabsa16.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/glue.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/xnli.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/duconv.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/cnn_dailymail.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/rvl_cdip_sampled.py -> build/lib/paddlenlp/datasets/hf_datasets
copying paddlenlp/datasets/hf_datasets/squad.py -> build/lib/paddlenlp/datasets/hf_datasets
creating build/lib/paddlenlp/transformers/rw
copying paddlenlp/transformers/rw/__init__.py -> build/lib/paddlenlp/transformers/rw
copying paddlenlp/transformers/rw/configuration.py -> build/lib/paddlenlp/transformers/rw
copying paddlenlp/transformers/rw/modeling.py -> build/lib/paddlenlp/transformers/rw
copying paddlenlp/transformers/rw/tokenizer.py -> build/lib/paddlenlp/transformers/rw
creating build/lib/paddlenlp/transformers/gau_alpha
copying paddlenlp/transformers/gau_alpha/__init__.py -> build/lib/paddlenlp/transformers/gau_alpha
copying paddlenlp/transformers/gau_alpha/configuration.py -> build/lib/paddlenlp/transformers/gau_alpha
copying paddlenlp/transformers/gau_alpha/modeling.py -> build/lib/paddlenlp/transformers/gau_alpha
copying paddlenlp/transformers/gau_alpha/tokenizer.py -> build/lib/paddlenlp/transformers/gau_alpha
creating build/lib/paddlenlp/transformers/artist
copying paddlenlp/transformers/artist/__init__.py -> build/lib/paddlenlp/transformers/artist
copying paddlenlp/transformers/artist/configuration.py -> build/lib/paddlenlp/transformers/artist
copying paddlenlp/transformers/artist/modeling.py -> build/lib/paddlenlp/transformers/artist
copying paddlenlp/transformers/artist/tokenizer.py -> build/lib/paddlenlp/transformers/artist
creating build/lib/paddlenlp/transformers/fnet
copying paddlenlp/transformers/fnet/__init__.py -> build/lib/paddlenlp/transformers/fnet
copying paddlenlp/transformers/fnet/configuration.py -> build/lib/paddlenlp/transformers/fnet
copying paddlenlp/transformers/fnet/modeling.py -> build/lib/paddlenlp/transformers/fnet
copying paddlenlp/transformers/fnet/tokenizer.py -> build/lib/paddlenlp/transformers/fnet
creating build/lib/paddlenlp/transformers/convbert
copying paddlenlp/transformers/convbert/__init__.py -> build/lib/paddlenlp/transformers/convbert
copying paddlenlp/transformers/convbert/configuration.py -> build/lib/paddlenlp/transformers/convbert
copying paddlenlp/transformers/convbert/modeling.py -> build/lib/paddlenlp/transformers/convbert
copying paddlenlp/transformers/convbert/tokenizer.py -> build/lib/paddlenlp/transformers/convbert
creating build/lib/paddlenlp/transformers/visualglm
copying paddlenlp/transformers/visualglm/__init__.py -> build/lib/paddlenlp/transformers/visualglm
copying paddlenlp/transformers/visualglm/processing.py -> build/lib/paddlenlp/transformers/visualglm
copying paddlenlp/transformers/visualglm/configuration.py -> build/lib/paddlenlp/transformers/visualglm
copying paddlenlp/transformers/visualglm/image_processing.py -> build/lib/paddlenlp/transformers/visualglm
copying paddlenlp/transformers/visualglm/modeling.py -> build/lib/paddlenlp/transformers/visualglm
creating build/lib/paddlenlp/transformers/roformerv2
copying paddlenlp/transformers/roformerv2/__init__.py -> build/lib/paddlenlp/transformers/roformerv2
copying paddlenlp/transformers/roformerv2/configuration.py -> build/lib/paddlenlp/transformers/roformerv2
copying paddlenlp/transformers/roformerv2/modeling.py -> build/lib/paddlenlp/transformers/roformerv2
copying paddlenlp/transformers/roformerv2/tokenizer.py -> build/lib/paddlenlp/transformers/roformerv2
creating build/lib/paddlenlp/transformers/blip
copying paddlenlp/transformers/blip/__init__.py -> build/lib/paddlenlp/transformers/blip
copying paddlenlp/transformers/blip/processing.py -> build/lib/paddlenlp/transformers/blip
copying paddlenlp/transformers/blip/configuration.py -> build/lib/paddlenlp/transformers/blip
copying paddlenlp/transformers/blip/image_processing.py -> build/lib/paddlenlp/transformers/blip
copying paddlenlp/transformers/blip/modeling_text.py -> build/lib/paddlenlp/transformers/blip
copying paddlenlp/transformers/blip/modeling.py -> build/lib/paddlenlp/transformers/blip
creating build/lib/paddlenlp/transformers/ctrl
copying paddlenlp/transformers/ctrl/__init__.py -> build/lib/paddlenlp/transformers/ctrl
copying paddlenlp/transformers/ctrl/configuration.py -> build/lib/paddlenlp/transformers/ctrl
copying paddlenlp/transformers/ctrl/modeling.py -> build/lib/paddlenlp/transformers/ctrl
copying paddlenlp/transformers/ctrl/tokenizer.py -> build/lib/paddlenlp/transformers/ctrl
creating build/lib/paddlenlp/transformers/blip_2
copying paddlenlp/transformers/blip_2/__init__.py -> build/lib/paddlenlp/transformers/blip_2
copying paddlenlp/transformers/blip_2/processing.py -> build/lib/paddlenlp/transformers/blip_2
copying paddlenlp/transformers/blip_2/configuration.py -> build/lib/paddlenlp/transformers/blip_2
copying paddlenlp/transformers/blip_2/modeling.py -> build/lib/paddlenlp/transformers/blip_2
creating build/lib/paddlenlp/transformers/layoutlmv2
copying paddlenlp/transformers/layoutlmv2/__init__.py -> build/lib/paddlenlp/transformers/layoutlmv2
copying paddlenlp/transformers/layoutlmv2/configuration.py -> build/lib/paddlenlp/transformers/layoutlmv2
copying paddlenlp/transformers/layoutlmv2/modeling.py -> build/lib/paddlenlp/transformers/layoutlmv2
copying paddlenlp/transformers/layoutlmv2/tokenizer.py -> build/lib/paddlenlp/transformers/layoutlmv2
creating build/lib/paddlenlp/transformers/blenderbot_small
copying paddlenlp/transformers/blenderbot_small/__init__.py -> build/lib/paddlenlp/transformers/blenderbot_small
copying paddlenlp/transformers/blenderbot_small/configuration.py -> build/lib/paddlenlp/transformers/blenderbot_small
copying paddlenlp/transformers/blenderbot_small/modeling.py -> build/lib/paddlenlp/transformers/blenderbot_small
copying paddlenlp/transformers/blenderbot_small/tokenizer.py -> build/lib/paddlenlp/transformers/blenderbot_small
creating build/lib/paddlenlp/transformers/layoutxlm
copying paddlenlp/transformers/layoutxlm/__init__.py -> build/lib/paddlenlp/transformers/layoutxlm
copying paddlenlp/transformers/layoutxlm/configuration.py -> build/lib/paddlenlp/transformers/layoutxlm
copying paddlenlp/transformers/layoutxlm/visual_backbone.py -> build/lib/paddlenlp/transformers/layoutxlm
copying paddlenlp/transformers/layoutxlm/modeling.py -> build/lib/paddlenlp/transformers/layoutxlm
copying paddlenlp/transformers/layoutxlm/tokenizer.py -> build/lib/paddlenlp/transformers/layoutxlm
creating build/lib/paddlenlp/transformers/transformer
copying paddlenlp/transformers/transformer/__init__.py -> build/lib/paddlenlp/transformers/transformer
copying paddlenlp/transformers/transformer/modeling.py -> build/lib/paddlenlp/transformers/transformer
creating build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/__init__.py -> build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/processing.py -> build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/configuration.py -> build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/image_processing.py -> build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/feature_extraction.py -> build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/modeling.py -> build/lib/paddlenlp/transformers/ernie_vil
copying paddlenlp/transformers/ernie_vil/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_vil
creating build/lib/paddlenlp/transformers/bigbird
copying paddlenlp/transformers/bigbird/__init__.py -> build/lib/paddlenlp/transformers/bigbird
copying paddlenlp/transformers/bigbird/configuration.py -> build/lib/paddlenlp/transformers/bigbird
copying paddlenlp/transformers/bigbird/modeling.py -> build/lib/paddlenlp/transformers/bigbird
copying paddlenlp/transformers/bigbird/tokenizer.py -> build/lib/paddlenlp/transformers/bigbird
creating build/lib/paddlenlp/transformers/llama
copying paddlenlp/transformers/llama/__init__.py -> build/lib/paddlenlp/transformers/llama
copying paddlenlp/transformers/llama/modeling_auto.py -> build/lib/paddlenlp/transformers/llama
copying paddlenlp/transformers/llama/configuration.py -> build/lib/paddlenlp/transformers/llama
copying paddlenlp/transformers/llama/modeling_pp.py -> build/lib/paddlenlp/transformers/llama
copying paddlenlp/transformers/llama/modeling.py -> build/lib/paddlenlp/transformers/llama
copying paddlenlp/transformers/llama/tokenizer.py -> build/lib/paddlenlp/transformers/llama
creating build/lib/paddlenlp/transformers/ernie_gen
copying paddlenlp/transformers/ernie_gen/__init__.py -> build/lib/paddlenlp/transformers/ernie_gen
copying paddlenlp/transformers/ernie_gen/modeling.py -> build/lib/paddlenlp/transformers/ernie_gen
creating build/lib/paddlenlp/transformers/bert
copying paddlenlp/transformers/bert/__init__.py -> build/lib/paddlenlp/transformers/bert
copying paddlenlp/transformers/bert/configuration.py -> build/lib/paddlenlp/transformers/bert
copying paddlenlp/transformers/bert/modeling.py -> build/lib/paddlenlp/transformers/bert
copying paddlenlp/transformers/bert/fast_tokenizer.py -> build/lib/paddlenlp/transformers/bert
copying paddlenlp/transformers/bert/tokenizer.py -> build/lib/paddlenlp/transformers/bert
creating build/lib/paddlenlp/transformers/tinybert
copying paddlenlp/transformers/tinybert/__init__.py -> build/lib/paddlenlp/transformers/tinybert
copying paddlenlp/transformers/tinybert/configuration.py -> build/lib/paddlenlp/transformers/tinybert
copying paddlenlp/transformers/tinybert/modeling.py -> build/lib/paddlenlp/transformers/tinybert
copying paddlenlp/transformers/tinybert/fast_tokenizer.py -> build/lib/paddlenlp/transformers/tinybert
copying paddlenlp/transformers/tinybert/tokenizer.py -> build/lib/paddlenlp/transformers/tinybert
creating build/lib/paddlenlp/transformers/funnel
copying paddlenlp/transformers/funnel/__init__.py -> build/lib/paddlenlp/transformers/funnel
copying paddlenlp/transformers/funnel/configuration.py -> build/lib/paddlenlp/transformers/funnel
copying paddlenlp/transformers/funnel/modeling.py -> build/lib/paddlenlp/transformers/funnel
copying paddlenlp/transformers/funnel/tokenizer.py -> build/lib/paddlenlp/transformers/funnel
creating build/lib/paddlenlp/transformers/xlnet
copying paddlenlp/transformers/xlnet/__init__.py -> build/lib/paddlenlp/transformers/xlnet
copying paddlenlp/transformers/xlnet/converter.py -> build/lib/paddlenlp/transformers/xlnet
copying paddlenlp/transformers/xlnet/configuration.py -> build/lib/paddlenlp/transformers/xlnet
copying paddlenlp/transformers/xlnet/modeling.py -> build/lib/paddlenlp/transformers/xlnet
copying paddlenlp/transformers/xlnet/tokenizer.py -> build/lib/paddlenlp/transformers/xlnet
creating build/lib/paddlenlp/transformers/minigpt4
copying paddlenlp/transformers/minigpt4/__init__.py -> build/lib/paddlenlp/transformers/minigpt4
copying paddlenlp/transformers/minigpt4/processing.py -> build/lib/paddlenlp/transformers/minigpt4
copying paddlenlp/transformers/minigpt4/configuration.py -> build/lib/paddlenlp/transformers/minigpt4
copying paddlenlp/transformers/minigpt4/image_processing.py -> build/lib/paddlenlp/transformers/minigpt4
copying paddlenlp/transformers/minigpt4/modeling.py -> build/lib/paddlenlp/transformers/minigpt4
creating build/lib/paddlenlp/transformers/blenderbot
copying paddlenlp/transformers/blenderbot/__init__.py -> build/lib/paddlenlp/transformers/blenderbot
copying paddlenlp/transformers/blenderbot/configuration.py -> build/lib/paddlenlp/transformers/blenderbot
copying paddlenlp/transformers/blenderbot/modeling.py -> build/lib/paddlenlp/transformers/blenderbot
copying paddlenlp/transformers/blenderbot/tokenizer.py -> build/lib/paddlenlp/transformers/blenderbot
creating build/lib/paddlenlp/transformers/roberta
copying paddlenlp/transformers/roberta/__init__.py -> build/lib/paddlenlp/transformers/roberta
copying paddlenlp/transformers/roberta/converter.py -> build/lib/paddlenlp/transformers/roberta
copying paddlenlp/transformers/roberta/configuration.py -> build/lib/paddlenlp/transformers/roberta
copying paddlenlp/transformers/roberta/modeling.py -> build/lib/paddlenlp/transformers/roberta
copying paddlenlp/transformers/roberta/tokenizer.py -> build/lib/paddlenlp/transformers/roberta
creating build/lib/paddlenlp/transformers/opt
copying paddlenlp/transformers/opt/__init__.py -> build/lib/paddlenlp/transformers/opt
copying paddlenlp/transformers/opt/configuration.py -> build/lib/paddlenlp/transformers/opt
copying paddlenlp/transformers/opt/convert_torch_to_paddle.py -> build/lib/paddlenlp/transformers/opt
copying paddlenlp/transformers/opt/modeling.py -> build/lib/paddlenlp/transformers/opt
creating build/lib/paddlenlp/transformers/luke
copying paddlenlp/transformers/luke/__init__.py -> build/lib/paddlenlp/transformers/luke
copying paddlenlp/transformers/luke/configuration.py -> build/lib/paddlenlp/transformers/luke
copying paddlenlp/transformers/luke/modeling.py -> build/lib/paddlenlp/transformers/luke
copying paddlenlp/transformers/luke/tokenizer.py -> build/lib/paddlenlp/transformers/luke
creating build/lib/paddlenlp/transformers/distilbert
copying paddlenlp/transformers/distilbert/__init__.py -> build/lib/paddlenlp/transformers/distilbert
copying paddlenlp/transformers/distilbert/configuration.py -> build/lib/paddlenlp/transformers/distilbert
copying paddlenlp/transformers/distilbert/modeling.py -> build/lib/paddlenlp/transformers/distilbert
copying paddlenlp/transformers/distilbert/tokenizer.py -> build/lib/paddlenlp/transformers/distilbert
creating build/lib/paddlenlp/transformers/chatglm_v2
copying paddlenlp/transformers/chatglm_v2/chatglm-legacy-checkpoints-convert.py -> build/lib/paddlenlp/transformers/chatglm_v2
copying paddlenlp/transformers/chatglm_v2/__init__.py -> build/lib/paddlenlp/transformers/chatglm_v2
copying paddlenlp/transformers/chatglm_v2/configuration.py -> build/lib/paddlenlp/transformers/chatglm_v2
copying paddlenlp/transformers/chatglm_v2/modeling.py -> build/lib/paddlenlp/transformers/chatglm_v2
copying paddlenlp/transformers/chatglm_v2/tokenizer.py -> build/lib/paddlenlp/transformers/chatglm_v2
creating build/lib/paddlenlp/transformers/mt5
copying paddlenlp/transformers/mt5/__init__.py -> build/lib/paddlenlp/transformers/mt5
copying paddlenlp/transformers/mt5/converter.py -> build/lib/paddlenlp/transformers/mt5
copying paddlenlp/transformers/mt5/configuration.py -> build/lib/paddlenlp/transformers/mt5
copying paddlenlp/transformers/mt5/modeling.py -> build/lib/paddlenlp/transformers/mt5
creating build/lib/paddlenlp/transformers/roformer
copying paddlenlp/transformers/roformer/__init__.py -> build/lib/paddlenlp/transformers/roformer
copying paddlenlp/transformers/roformer/configuration.py -> build/lib/paddlenlp/transformers/roformer
copying paddlenlp/transformers/roformer/modeling.py -> build/lib/paddlenlp/transformers/roformer
copying paddlenlp/transformers/roformer/tokenizer.py -> build/lib/paddlenlp/transformers/roformer
creating build/lib/paddlenlp/transformers/ernie
copying paddlenlp/transformers/ernie/__init__.py -> build/lib/paddlenlp/transformers/ernie
copying paddlenlp/transformers/ernie/configuration.py -> build/lib/paddlenlp/transformers/ernie
copying paddlenlp/transformers/ernie/modeling.py -> build/lib/paddlenlp/transformers/ernie
copying paddlenlp/transformers/ernie/fast_tokenizer.py -> build/lib/paddlenlp/transformers/ernie
copying paddlenlp/transformers/ernie/tokenizer.py -> build/lib/paddlenlp/transformers/ernie
creating build/lib/paddlenlp/transformers/chinesebert
copying paddlenlp/transformers/chinesebert/__init__.py -> build/lib/paddlenlp/transformers/chinesebert
copying paddlenlp/transformers/chinesebert/configuration.py -> build/lib/paddlenlp/transformers/chinesebert
copying paddlenlp/transformers/chinesebert/modeling.py -> build/lib/paddlenlp/transformers/chinesebert
copying paddlenlp/transformers/chinesebert/tokenizer.py -> build/lib/paddlenlp/transformers/chinesebert
creating build/lib/paddlenlp/transformers/ernie_ctm
copying paddlenlp/transformers/ernie_ctm/__init__.py -> build/lib/paddlenlp/transformers/ernie_ctm
copying paddlenlp/transformers/ernie_ctm/configuration.py -> build/lib/paddlenlp/transformers/ernie_ctm
copying paddlenlp/transformers/ernie_ctm/modeling.py -> build/lib/paddlenlp/transformers/ernie_ctm
copying paddlenlp/transformers/ernie_ctm/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_ctm
creating build/lib/paddlenlp/transformers/codegen
copying paddlenlp/transformers/codegen/__init__.py -> build/lib/paddlenlp/transformers/codegen
copying paddlenlp/transformers/codegen/configuration.py -> build/lib/paddlenlp/transformers/codegen
copying paddlenlp/transformers/codegen/modeling.py -> build/lib/paddlenlp/transformers/codegen
copying paddlenlp/transformers/codegen/tokenizer.py -> build/lib/paddlenlp/transformers/codegen
creating build/lib/paddlenlp/transformers/unified_transformer
copying paddlenlp/transformers/unified_transformer/__init__.py -> build/lib/paddlenlp/transformers/unified_transformer
copying paddlenlp/transformers/unified_transformer/configuration.py -> build/lib/paddlenlp/transformers/unified_transformer
copying paddlenlp/transformers/unified_transformer/convert.py -> build/lib/paddlenlp/transformers/unified_transformer
copying paddlenlp/transformers/unified_transformer/modeling.py -> build/lib/paddlenlp/transformers/unified_transformer
copying paddlenlp/transformers/unified_transformer/tokenizer.py -> build/lib/paddlenlp/transformers/unified_transformer
creating build/lib/paddlenlp/transformers/mobilebert
copying paddlenlp/transformers/mobilebert/__init__.py -> build/lib/paddlenlp/transformers/mobilebert
copying paddlenlp/transformers/mobilebert/configuration.py -> build/lib/paddlenlp/transformers/mobilebert
copying paddlenlp/transformers/mobilebert/modeling.py -> build/lib/paddlenlp/transformers/mobilebert
copying paddlenlp/transformers/mobilebert/tokenizer.py -> build/lib/paddlenlp/transformers/mobilebert
creating build/lib/paddlenlp/transformers/dpt
copying paddlenlp/transformers/dpt/__init__.py -> build/lib/paddlenlp/transformers/dpt
copying paddlenlp/transformers/dpt/configuration.py -> build/lib/paddlenlp/transformers/dpt
copying paddlenlp/transformers/dpt/image_processing.py -> build/lib/paddlenlp/transformers/dpt
copying paddlenlp/transformers/dpt/modeling.py -> build/lib/paddlenlp/transformers/dpt
creating build/lib/paddlenlp/transformers/ppminilm
copying paddlenlp/transformers/ppminilm/__init__.py -> build/lib/paddlenlp/transformers/ppminilm
copying paddlenlp/transformers/ppminilm/configuration.py -> build/lib/paddlenlp/transformers/ppminilm
copying paddlenlp/transformers/ppminilm/modeling.py -> build/lib/paddlenlp/transformers/ppminilm
copying paddlenlp/transformers/ppminilm/tokenizer.py -> build/lib/paddlenlp/transformers/ppminilm
creating build/lib/paddlenlp/transformers/t5
copying paddlenlp/transformers/t5/__init__.py -> build/lib/paddlenlp/transformers/t5
copying paddlenlp/transformers/t5/configuration.py -> build/lib/paddlenlp/transformers/t5
copying paddlenlp/transformers/t5/modeling.py -> build/lib/paddlenlp/transformers/t5
copying paddlenlp/transformers/t5/tokenizer.py -> build/lib/paddlenlp/transformers/t5
creating build/lib/paddlenlp/transformers/auto
copying paddlenlp/transformers/auto/__init__.py -> build/lib/paddlenlp/transformers/auto
copying paddlenlp/transformers/auto/processing.py -> build/lib/paddlenlp/transformers/auto
copying paddlenlp/transformers/auto/configuration.py -> build/lib/paddlenlp/transformers/auto
copying paddlenlp/transformers/auto/modeling.py -> build/lib/paddlenlp/transformers/auto
copying paddlenlp/transformers/auto/tokenizer.py -> build/lib/paddlenlp/transformers/auto
creating build/lib/paddlenlp/transformers/qwen
copying paddlenlp/transformers/qwen/__init__.py -> build/lib/paddlenlp/transformers/qwen
copying paddlenlp/transformers/qwen/configuration.py -> build/lib/paddlenlp/transformers/qwen
copying paddlenlp/transformers/qwen/modeling_pp.py -> build/lib/paddlenlp/transformers/qwen
copying paddlenlp/transformers/qwen/modeling.py -> build/lib/paddlenlp/transformers/qwen
copying paddlenlp/transformers/qwen/tokenizer.py -> build/lib/paddlenlp/transformers/qwen
creating build/lib/paddlenlp/transformers/squeezebert
copying paddlenlp/transformers/squeezebert/__init__.py -> build/lib/paddlenlp/transformers/squeezebert
copying paddlenlp/transformers/squeezebert/configuration.py -> build/lib/paddlenlp/transformers/squeezebert
copying paddlenlp/transformers/squeezebert/modeling.py -> build/lib/paddlenlp/transformers/squeezebert
copying paddlenlp/transformers/squeezebert/tokenizer.py -> build/lib/paddlenlp/transformers/squeezebert
creating build/lib/paddlenlp/transformers/clap
copying paddlenlp/transformers/clap/__init__.py -> build/lib/paddlenlp/transformers/clap
copying paddlenlp/transformers/clap/processing.py -> build/lib/paddlenlp/transformers/clap
copying paddlenlp/transformers/clap/configuration.py -> build/lib/paddlenlp/transformers/clap
copying paddlenlp/transformers/clap/feature_extraction.py -> build/lib/paddlenlp/transformers/clap
copying paddlenlp/transformers/clap/modeling.py -> build/lib/paddlenlp/transformers/clap
creating build/lib/paddlenlp/transformers/ernie_layout
copying paddlenlp/transformers/ernie_layout/__init__.py -> build/lib/paddlenlp/transformers/ernie_layout
copying paddlenlp/transformers/ernie_layout/configuration.py -> build/lib/paddlenlp/transformers/ernie_layout
copying paddlenlp/transformers/ernie_layout/visual_backbone.py -> build/lib/paddlenlp/transformers/ernie_layout
copying paddlenlp/transformers/ernie_layout/modeling.py -> build/lib/paddlenlp/transformers/ernie_layout
copying paddlenlp/transformers/ernie_layout/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_layout
creating build/lib/paddlenlp/transformers/megatronbert
copying paddlenlp/transformers/megatronbert/__init__.py -> build/lib/paddlenlp/transformers/megatronbert
copying paddlenlp/transformers/megatronbert/configuration.py -> build/lib/paddlenlp/transformers/megatronbert
copying paddlenlp/transformers/megatronbert/modeling.py -> build/lib/paddlenlp/transformers/megatronbert
copying paddlenlp/transformers/megatronbert/tokenizer.py -> build/lib/paddlenlp/transformers/megatronbert
creating build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/__init__.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/processing.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/converter.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/configuration.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/image_processing.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/feature_extraction.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/modeling.py -> build/lib/paddlenlp/transformers/chineseclip
copying paddlenlp/transformers/chineseclip/tokenizer.py -> build/lib/paddlenlp/transformers/chineseclip
creating build/lib/paddlenlp/transformers/xlm
copying paddlenlp/transformers/xlm/__init__.py -> build/lib/paddlenlp/transformers/xlm
copying paddlenlp/transformers/xlm/configuration.py -> build/lib/paddlenlp/transformers/xlm
copying paddlenlp/transformers/xlm/modeling.py -> build/lib/paddlenlp/transformers/xlm
copying paddlenlp/transformers/xlm/tokenizer.py -> build/lib/paddlenlp/transformers/xlm
creating build/lib/paddlenlp/transformers/semantic_search
copying paddlenlp/transformers/semantic_search/__init__.py -> build/lib/paddlenlp/transformers/semantic_search
copying paddlenlp/transformers/semantic_search/modeling.py -> build/lib/paddlenlp/transformers/semantic_search
creating build/lib/paddlenlp/transformers/nezha
copying paddlenlp/transformers/nezha/__init__.py -> build/lib/paddlenlp/transformers/nezha
copying paddlenlp/transformers/nezha/configuration.py -> build/lib/paddlenlp/transformers/nezha
copying paddlenlp/transformers/nezha/modeling.py -> build/lib/paddlenlp/transformers/nezha
copying paddlenlp/transformers/nezha/tokenizer.py -> build/lib/paddlenlp/transformers/nezha
creating build/lib/paddlenlp/transformers/bloom
copying paddlenlp/transformers/bloom/__init__.py -> build/lib/paddlenlp/transformers/bloom
copying paddlenlp/transformers/bloom/configuration.py -> build/lib/paddlenlp/transformers/bloom
copying paddlenlp/transformers/bloom/processor.py -> build/lib/paddlenlp/transformers/bloom
copying paddlenlp/transformers/bloom/modeling.py -> build/lib/paddlenlp/transformers/bloom
copying paddlenlp/transformers/bloom/tokenizer.py -> build/lib/paddlenlp/transformers/bloom
creating build/lib/paddlenlp/transformers/unimo
copying paddlenlp/transformers/unimo/__init__.py -> build/lib/paddlenlp/transformers/unimo
copying paddlenlp/transformers/unimo/configuration.py -> build/lib/paddlenlp/transformers/unimo
copying paddlenlp/transformers/unimo/modeling.py -> build/lib/paddlenlp/transformers/unimo
copying paddlenlp/transformers/unimo/tokenizer.py -> build/lib/paddlenlp/transformers/unimo
creating build/lib/paddlenlp/transformers/clipseg
copying paddlenlp/transformers/clipseg/__init__.py -> build/lib/paddlenlp/transformers/clipseg
copying paddlenlp/transformers/clipseg/processing.py -> build/lib/paddlenlp/transformers/clipseg
copying paddlenlp/transformers/clipseg/configuration.py -> build/lib/paddlenlp/transformers/clipseg
copying paddlenlp/transformers/clipseg/image_processing.py -> build/lib/paddlenlp/transformers/clipseg
copying paddlenlp/transformers/clipseg/modeling.py -> build/lib/paddlenlp/transformers/clipseg
creating build/lib/paddlenlp/transformers/ernie_m
copying paddlenlp/transformers/ernie_m/__init__.py -> build/lib/paddlenlp/transformers/ernie_m
copying paddlenlp/transformers/ernie_m/configuration.py -> build/lib/paddlenlp/transformers/ernie_m
copying paddlenlp/transformers/ernie_m/modeling.py -> build/lib/paddlenlp/transformers/ernie_m
copying paddlenlp/transformers/ernie_m/fast_tokenizer.py -> build/lib/paddlenlp/transformers/ernie_m
copying paddlenlp/transformers/ernie_m/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_m
creating build/lib/paddlenlp/transformers/ernie_gram
copying paddlenlp/transformers/ernie_gram/matching_param_name.py -> build/lib/paddlenlp/transformers/ernie_gram
copying paddlenlp/transformers/ernie_gram/__init__.py -> build/lib/paddlenlp/transformers/ernie_gram
copying paddlenlp/transformers/ernie_gram/configuration.py -> build/lib/paddlenlp/transformers/ernie_gram
copying paddlenlp/transformers/ernie_gram/modeling.py -> build/lib/paddlenlp/transformers/ernie_gram
copying paddlenlp/transformers/ernie_gram/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_gram
creating build/lib/paddlenlp/transformers/chatglm
copying paddlenlp/transformers/chatglm/__init__.py -> build/lib/paddlenlp/transformers/chatglm
copying paddlenlp/transformers/chatglm/configuration.py -> build/lib/paddlenlp/transformers/chatglm
copying paddlenlp/transformers/chatglm/modeling.py -> build/lib/paddlenlp/transformers/chatglm
copying paddlenlp/transformers/chatglm/tokenizer.py -> build/lib/paddlenlp/transformers/chatglm
creating build/lib/paddlenlp/transformers/ernie_code
copying paddlenlp/transformers/ernie_code/__init__.py -> build/lib/paddlenlp/transformers/ernie_code
copying paddlenlp/transformers/ernie_code/configuration.py -> build/lib/paddlenlp/transformers/ernie_code
copying paddlenlp/transformers/ernie_code/modeling.py -> build/lib/paddlenlp/transformers/ernie_code
copying paddlenlp/transformers/ernie_code/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_code
creating build/lib/paddlenlp/transformers/gpt
copying paddlenlp/transformers/gpt/__init__.py -> build/lib/paddlenlp/transformers/gpt
copying paddlenlp/transformers/gpt/configuration.py -> build/lib/paddlenlp/transformers/gpt
copying paddlenlp/transformers/gpt/modeling_pp.py -> build/lib/paddlenlp/transformers/gpt
copying paddlenlp/transformers/gpt/modeling.py -> build/lib/paddlenlp/transformers/gpt
copying paddlenlp/transformers/gpt/tokenizer.py -> build/lib/paddlenlp/transformers/gpt
creating build/lib/paddlenlp/transformers/dallebart
copying paddlenlp/transformers/dallebart/__init__.py -> build/lib/paddlenlp/transformers/dallebart
copying paddlenlp/transformers/dallebart/configuration.py -> build/lib/paddlenlp/transformers/dallebart
copying paddlenlp/transformers/dallebart/modeling.py -> build/lib/paddlenlp/transformers/dallebart
copying paddlenlp/transformers/dallebart/tokenizer.py -> build/lib/paddlenlp/transformers/dallebart
creating build/lib/paddlenlp/transformers/electra
copying paddlenlp/transformers/electra/__init__.py -> build/lib/paddlenlp/transformers/electra
copying paddlenlp/transformers/electra/converter.py -> build/lib/paddlenlp/transformers/electra
copying paddlenlp/transformers/electra/configuration.py -> build/lib/paddlenlp/transformers/electra
copying paddlenlp/transformers/electra/modeling.py -> build/lib/paddlenlp/transformers/electra
copying paddlenlp/transformers/electra/tokenizer.py -> build/lib/paddlenlp/transformers/electra
creating build/lib/paddlenlp/transformers/ernie_doc
copying paddlenlp/transformers/ernie_doc/__init__.py -> build/lib/paddlenlp/transformers/ernie_doc
copying paddlenlp/transformers/ernie_doc/configuration.py -> build/lib/paddlenlp/transformers/ernie_doc
copying paddlenlp/transformers/ernie_doc/modeling.py -> build/lib/paddlenlp/transformers/ernie_doc
copying paddlenlp/transformers/ernie_doc/tokenizer.py -> build/lib/paddlenlp/transformers/ernie_doc
creating build/lib/paddlenlp/transformers/bert_japanese
copying paddlenlp/transformers/bert_japanese/__init__.py -> build/lib/paddlenlp/transformers/bert_japanese
copying paddlenlp/transformers/bert_japanese/tokenizer.py -> build/lib/paddlenlp/transformers/bert_japanese
creating build/lib/paddlenlp/transformers/albert
copying paddlenlp/transformers/albert/__init__.py -> build/lib/paddlenlp/transformers/albert
copying paddlenlp/transformers/albert/configuration.py -> build/lib/paddlenlp/transformers/albert
copying paddlenlp/transformers/albert/modeling.py -> build/lib/paddlenlp/transformers/albert
copying paddlenlp/transformers/albert/tokenizer.py -> build/lib/paddlenlp/transformers/albert
creating build/lib/paddlenlp/transformers/pegasus
copying paddlenlp/transformers/pegasus/__init__.py -> build/lib/paddlenlp/transformers/pegasus
copying paddlenlp/transformers/pegasus/configuration.py -> build/lib/paddlenlp/transformers/pegasus
copying paddlenlp/transformers/pegasus/modeling.py -> build/lib/paddlenlp/transformers/pegasus
copying paddlenlp/transformers/pegasus/tokenizer.py -> build/lib/paddlenlp/transformers/pegasus
creating build/lib/paddlenlp/transformers/gptj
copying paddlenlp/transformers/gptj/__init__.py -> build/lib/paddlenlp/transformers/gptj
copying paddlenlp/transformers/gptj/configuration.py -> build/lib/paddlenlp/transformers/gptj
copying paddlenlp/transformers/gptj/modeling.py -> build/lib/paddlenlp/transformers/gptj
copying paddlenlp/transformers/gptj/tokenizer.py -> build/lib/paddlenlp/transformers/gptj
creating build/lib/paddlenlp/transformers/layoutlm
copying paddlenlp/transformers/layoutlm/__init__.py -> build/lib/paddlenlp/transformers/layoutlm
copying paddlenlp/transformers/layoutlm/configuration.py -> build/lib/paddlenlp/transformers/layoutlm
copying paddlenlp/transformers/layoutlm/modeling.py -> build/lib/paddlenlp/transformers/layoutlm
copying paddlenlp/transformers/layoutlm/tokenizer.py -> build/lib/paddlenlp/transformers/layoutlm
creating build/lib/paddlenlp/transformers/prophetnet
copying paddlenlp/transformers/prophetnet/__init__.py -> build/lib/paddlenlp/transformers/prophetnet
copying paddlenlp/transformers/prophetnet/configuration.py -> build/lib/paddlenlp/transformers/prophetnet
copying paddlenlp/transformers/prophetnet/modeling.py -> build/lib/paddlenlp/transformers/prophetnet
copying paddlenlp/transformers/prophetnet/tokenizer.py -> build/lib/paddlenlp/transformers/prophetnet
creating build/lib/paddlenlp/transformers/bart
copying paddlenlp/transformers/bart/__init__.py -> build/lib/paddlenlp/transformers/bart
copying paddlenlp/transformers/bart/configuration.py -> build/lib/paddlenlp/transformers/bart
copying paddlenlp/transformers/bart/modeling.py -> build/lib/paddlenlp/transformers/bart
copying paddlenlp/transformers/bart/tokenizer.py -> build/lib/paddlenlp/transformers/bart
creating build/lib/paddlenlp/transformers/rembert
copying paddlenlp/transformers/rembert/__init__.py -> build/lib/paddlenlp/transformers/rembert
copying paddlenlp/transformers/rembert/configuration.py -> build/lib/paddlenlp/transformers/rembert
copying paddlenlp/transformers/rembert/modeling.py -> build/lib/paddlenlp/transformers/rembert
copying paddlenlp/transformers/rembert/tokenizer.py -> build/lib/paddlenlp/transformers/rembert
creating build/lib/paddlenlp/transformers/mpnet
copying paddlenlp/transformers/mpnet/__init__.py -> build/lib/paddlenlp/transformers/mpnet
copying paddlenlp/transformers/mpnet/configuration.py -> build/lib/paddlenlp/transformers/mpnet
copying paddlenlp/transformers/mpnet/modeling.py -> build/lib/paddlenlp/transformers/mpnet
copying paddlenlp/transformers/mpnet/tokenizer.py -> build/lib/paddlenlp/transformers/mpnet
creating build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/__init__.py -> build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/processing.py -> build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/configuration.py -> build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/image_processing.py -> build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/feature_extraction.py -> build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/modeling.py -> build/lib/paddlenlp/transformers/clip
copying paddlenlp/transformers/clip/tokenizer.py -> build/lib/paddlenlp/transformers/clip
creating build/lib/paddlenlp/transformers/skep
copying paddlenlp/transformers/skep/__init__.py -> build/lib/paddlenlp/transformers/skep
copying paddlenlp/transformers/skep/configuration.py -> build/lib/paddlenlp/transformers/skep
copying paddlenlp/transformers/skep/modeling.py -> build/lib/paddlenlp/transformers/skep
copying paddlenlp/transformers/skep/tokenizer.py -> build/lib/paddlenlp/transformers/skep
creating build/lib/paddlenlp/transformers/bit
copying paddlenlp/transformers/bit/__init__.py -> build/lib/paddlenlp/transformers/bit
copying paddlenlp/transformers/bit/configuration.py -> build/lib/paddlenlp/transformers/bit
copying paddlenlp/transformers/bit/image_processing.py -> build/lib/paddlenlp/transformers/bit
copying paddlenlp/transformers/bit/modeling.py -> build/lib/paddlenlp/transformers/bit
creating build/lib/paddlenlp/transformers/speecht5
copying paddlenlp/transformers/speecht5/__init__.py -> build/lib/paddlenlp/transformers/speecht5
copying paddlenlp/transformers/speecht5/processing.py -> build/lib/paddlenlp/transformers/speecht5
copying paddlenlp/transformers/speecht5/configuration.py -> build/lib/paddlenlp/transformers/speecht5
copying paddlenlp/transformers/speecht5/feature_extraction.py -> build/lib/paddlenlp/transformers/speecht5
copying paddlenlp/transformers/speecht5/modeling.py -> build/lib/paddlenlp/transformers/speecht5
copying paddlenlp/transformers/speecht5/tokenizer.py -> build/lib/paddlenlp/transformers/speecht5
creating build/lib/paddlenlp/transformers/reformer
copying paddlenlp/transformers/reformer/__init__.py -> build/lib/paddlenlp/transformers/reformer
copying paddlenlp/transformers/reformer/configuration.py -> build/lib/paddlenlp/transformers/reformer
copying paddlenlp/transformers/reformer/modeling.py -> build/lib/paddlenlp/transformers/reformer
copying paddlenlp/transformers/reformer/tokenizer.py -> build/lib/paddlenlp/transformers/reformer
creating build/lib/paddlenlp/transformers/nystromformer
copying paddlenlp/transformers/nystromformer/__init__.py -> build/lib/paddlenlp/transformers/nystromformer
copying paddlenlp/transformers/nystromformer/configuration.py -> build/lib/paddlenlp/transformers/nystromformer
copying paddlenlp/transformers/nystromformer/modeling.py -> build/lib/paddlenlp/transformers/nystromformer
copying paddlenlp/transformers/nystromformer/fast_tokenizer.py -> build/lib/paddlenlp/transformers/nystromformer
copying paddlenlp/transformers/nystromformer/tokenizer.py -> build/lib/paddlenlp/transformers/nystromformer
creating build/lib/paddlenlp/transformers/glm
copying paddlenlp/transformers/glm/__init__.py -> build/lib/paddlenlp/transformers/glm
copying paddlenlp/transformers/glm/configuration.py -> build/lib/paddlenlp/transformers/glm
copying paddlenlp/transformers/glm/modeling.py -> build/lib/paddlenlp/transformers/glm
copying paddlenlp/transformers/glm/tokenizer.py -> build/lib/paddlenlp/transformers/glm
creating build/lib/paddlenlp/transformers/mbart
copying paddlenlp/transformers/mbart/__init__.py -> build/lib/paddlenlp/transformers/mbart
copying paddlenlp/transformers/mbart/configuration.py -> build/lib/paddlenlp/transformers/mbart
copying paddlenlp/transformers/mbart/modeling.py -> build/lib/paddlenlp/transformers/mbart
copying paddlenlp/transformers/mbart/tokenizer.py -> build/lib/paddlenlp/transformers/mbart
creating build/lib/paddlenlp/experimental/autonlp
copying paddlenlp/experimental/autonlp/text_classification.py -> build/lib/paddlenlp/experimental/autonlp
copying paddlenlp/experimental/autonlp/__init__.py -> build/lib/paddlenlp/experimental/autonlp
copying paddlenlp/experimental/autonlp/utils.py -> build/lib/paddlenlp/experimental/autonlp
copying paddlenlp/experimental/autonlp/auto_trainer_base.py -> build/lib/paddlenlp/experimental/autonlp
creating build/lib/paddlenlp/experimental/transformers
copying paddlenlp/experimental/transformers/__init__.py -> build/lib/paddlenlp/experimental/transformers
copying paddlenlp/experimental/transformers/fused_transformer_layers.py -> build/lib/paddlenlp/experimental/transformers
copying paddlenlp/experimental/transformers/generation_utils.py -> build/lib/paddlenlp/experimental/transformers
creating build/lib/paddlenlp/experimental/transformers/llama
copying paddlenlp/experimental/transformers/llama/__init__.py -> build/lib/paddlenlp/experimental/transformers/llama
copying paddlenlp/experimental/transformers/llama/modeling.py -> build/lib/paddlenlp/experimental/transformers/llama
creating build/lib/paddlenlp/experimental/transformers/opt
copying paddlenlp/experimental/transformers/opt/__init__.py -> build/lib/paddlenlp/experimental/transformers/opt
copying paddlenlp/experimental/transformers/opt/modeling.py -> build/lib/paddlenlp/experimental/transformers/opt
creating build/lib/paddlenlp/experimental/transformers/chatglm_v2
copying paddlenlp/experimental/transformers/chatglm_v2/__init__.py -> build/lib/paddlenlp/experimental/transformers/chatglm_v2
copying paddlenlp/experimental/transformers/chatglm_v2/modeling.py -> build/lib/paddlenlp/experimental/transformers/chatglm_v2
creating build/lib/paddlenlp/experimental/transformers/bloom
copying paddlenlp/experimental/transformers/bloom/__init__.py -> build/lib/paddlenlp/experimental/transformers/bloom
copying paddlenlp/experimental/transformers/bloom/modeling.py -> build/lib/paddlenlp/experimental/transformers/bloom
creating build/lib/paddlenlp/experimental/transformers/chatglm
copying paddlenlp/experimental/transformers/chatglm/__init__.py -> build/lib/paddlenlp/experimental/transformers/chatglm
copying paddlenlp/experimental/transformers/chatglm/modeling.py -> build/lib/paddlenlp/experimental/transformers/chatglm
creating build/lib/paddlenlp/experimental/transformers/gpt
copying paddlenlp/experimental/transformers/gpt/__init__.py -> build/lib/paddlenlp/experimental/transformers/gpt
copying paddlenlp/experimental/transformers/gpt/modeling.py -> build/lib/paddlenlp/experimental/transformers/gpt
creating build/lib/paddlenlp/taskflow/models
copying paddlenlp/taskflow/models/lexical_analysis_model.py -> build/lib/paddlenlp/taskflow/models
copying paddlenlp/taskflow/models/text_correction_model.py -> build/lib/paddlenlp/taskflow/models
copying paddlenlp/taskflow/models/__init__.py -> build/lib/paddlenlp/taskflow/models
copying paddlenlp/taskflow/models/sentiment_analysis_model.py -> build/lib/paddlenlp/taskflow/models
copying paddlenlp/taskflow/models/dependency_parsing_model.py -> build/lib/paddlenlp/taskflow/models
creating build/lib/paddlenlp/cli/utils
copying paddlenlp/cli/utils/__init__.py -> build/lib/paddlenlp/cli/utils
copying paddlenlp/cli/utils/tabulate.py -> build/lib/paddlenlp/cli/utils
creating build/lib/paddlenlp/peft/prefix
copying paddlenlp/peft/prefix/__init__.py -> build/lib/paddlenlp/peft/prefix
copying paddlenlp/peft/prefix/utils.py -> build/lib/paddlenlp/peft/prefix
copying paddlenlp/peft/prefix/prefix_config.py -> build/lib/paddlenlp/peft/prefix
copying paddlenlp/peft/prefix/prefix_model.py -> build/lib/paddlenlp/peft/prefix
creating build/lib/paddlenlp/peft/lora
copying paddlenlp/peft/lora/lora_layers.py -> build/lib/paddlenlp/peft/lora
copying paddlenlp/peft/lora/__init__.py -> build/lib/paddlenlp/peft/lora
copying paddlenlp/peft/lora/lora_model.py -> build/lib/paddlenlp/peft/lora
copying paddlenlp/peft/lora/lora_quantization_layers.py -> build/lib/paddlenlp/peft/lora
copying paddlenlp/peft/lora/lora_quant_layers.py -> build/lib/paddlenlp/peft/lora
copying paddlenlp/peft/lora/lora_config.py -> build/lib/paddlenlp/peft/lora
copying paddlenlp/ops/CMakeLists.txt -> build/lib/paddlenlp/ops
copying paddlenlp/ops/README.md -> build/lib/paddlenlp/ops
creating build/lib/paddlenlp/ops/cmake
copying paddlenlp/ops/cmake/FindNCCL.cmake -> build/lib/paddlenlp/ops/cmake
creating build/lib/paddlenlp/ops/cmake/external
copying paddlenlp/ops/cmake/external/boost.cmake -> build/lib/paddlenlp/ops/cmake/external
copying paddlenlp/ops/fast_transformer/CMakeLists.txt -> build/lib/paddlenlp/ops/fast_transformer
creating build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_t5_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_opt_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_force_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_unified_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/cublas_handle.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_mbart_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/CMakeLists.txt -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/utils.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_bart_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/parallel_utils.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_gpt_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_force_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_t5_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_bart_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/pd_traits.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_encoder_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_miro_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_encoder_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_decoder_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_miro_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_mbart_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_unified_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_pegasus_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_decoder_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_mbart_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_unified_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_miro_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_gptj_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_pegasus_decoding_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_pegasus_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_opt_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/parallel_utils.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_bart_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_gptj_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/cublas_handle.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_t5_decoding_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_force_decoding_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_decoder_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_gptj_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_gpt_op.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_gpt_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_opt_op.cu -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/utils.cc -> build/lib/paddlenlp/ops/fast_transformer/src
copying paddlenlp/ops/fast_transformer/src/fusion_encoder_op.h -> build/lib/paddlenlp/ops/fast_transformer/src
creating build/lib/paddlenlp/ops/fast_transformer/src/demo
copying paddlenlp/ops/fast_transformer/src/demo/gpt.cc -> build/lib/paddlenlp/ops/fast_transformer/src/demo
copying paddlenlp/ops/fast_transformer/src/demo/utf8.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo
copying paddlenlp/ops/fast_transformer/src/demo/transformer_e2e.cc -> build/lib/paddlenlp/ops/fast_transformer/src/demo
copying paddlenlp/ops/fast_transformer/src/demo/helper.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo
creating build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8
copying paddlenlp/ops/fast_transformer/src/demo/utf8/cpp17.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8
copying paddlenlp/ops/fast_transformer/src/demo/utf8/checked.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8
copying paddlenlp/ops/fast_transformer/src/demo/utf8/unchecked.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8
copying paddlenlp/ops/fast_transformer/src/demo/utf8/core.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8
copying paddlenlp/ops/fast_transformer/src/demo/utf8/cpp11.h -> build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8
creating build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/plato_inference.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/unimo_text_export_model_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/bart_export_model_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/encoder_decoding_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/unimo_text_inference.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/mbart_decoding_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/mbart_inference.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/t5_export_model_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/mbart_export_model_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/gpt_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/t5_inference.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/plato_export_model_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/encoder_decoder_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/bart_decoding_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/bart_inference.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/decoder_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/gpt_export_model_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
copying paddlenlp/ops/fast_transformer/sample/decoding_sample.py -> build/lib/paddlenlp/ops/fast_transformer/sample
creating build/lib/paddlenlp/ops/fast_transformer/sample/config
copying paddlenlp/ops/fast_transformer/sample/config/decoder.sample.yaml -> build/lib/paddlenlp/ops/fast_transformer/sample/config
copying paddlenlp/ops/fast_transformer/sample/config/decoding.sample.yaml -> build/lib/paddlenlp/ops/fast_transformer/sample/config
creating build/lib/paddlenlp/ops/patches
creating build/lib/paddlenlp/ops/patches/FasterTransformer
copying paddlenlp/ops/patches/FasterTransformer/CMakeLists.txt -> build/lib/paddlenlp/ops/patches/FasterTransformer
creating build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/CMakeLists.txt -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/gptj.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/open_decoder.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/gpt.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/t5_sampling.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/decoding_sampling.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/decoding_beamsearch.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/t5_beamsearch.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/bert_encoder_transformer.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/opt.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/standard_encoder.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer
creating build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/common_structure.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/allocator.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/common.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/arguments.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
creating build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/attention_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/masked_multihead_attention.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_decoder.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/decoding_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_kernels.cuh -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/online_softmax_beamsearch_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/topk_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/open_attention.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/cuda_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/cuda_kernels.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/attention_kernels.cuh -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/open_decoder.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/topk_kernels.cuh -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_decoding_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/lightseq_kernels.cu -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/masked_multihead_attention.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/open_decoder.cuh -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/masked_multihead_attention_utils.h -> build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying paddlenlp/transformers/layoutxlm/visual_backbone.yaml -> build/lib/paddlenlp/transformers/layoutxlm
creating build/bdist.linux-x86_64
creating build/bdist.linux-x86_64/egg
creating build/bdist.linux-x86_64/egg/paddlenlp
creating build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/trainer_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/compression_args.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
creating build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils
copying build/lib/paddlenlp/trainer/utils/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils
creating build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard
copying build/lib/paddlenlp/trainer/utils/reshard/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard
copying build/lib/paddlenlp/trainer/utils/reshard/sharding_v2.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard
copying build/lib/paddlenlp/trainer/utils/reshard/common.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard
copying build/lib/paddlenlp/trainer/utils/reshard/sharding_v1.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard
copying build/lib/paddlenlp/trainer/utils/sharding_io.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils
copying build/lib/paddlenlp/trainer/utils/doc.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils
copying build/lib/paddlenlp/trainer/utils/helper.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils
copying build/lib/paddlenlp/trainer/trainer_compress.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/training_args_seq2seq.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/trainer.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/training_args.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/integrations.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/trainer_seq2seq.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/argparser.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
copying build/lib/paddlenlp/trainer/trainer_callback.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer
creating build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins
copying build/lib/paddlenlp/trainer/plugins/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins
copying build/lib/paddlenlp/trainer/plugins/npu_plugin.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins
copying build/lib/paddlenlp/trainer/plugins/timer.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins
copying build/lib/paddlenlp/trainer/plugins/unified_checkpoint.py -> build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins
copying build/lib/paddlenlp/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp
creating build/bdist.linux-x86_64/egg/paddlenlp/quantization
copying build/lib/paddlenlp/quantization/qlora.py -> build/bdist.linux-x86_64/egg/paddlenlp/quantization
copying build/lib/paddlenlp/quantization/quantization_config.py -> build/bdist.linux-x86_64/egg/paddlenlp/quantization
copying build/lib/paddlenlp/quantization/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/quantization
copying build/lib/paddlenlp/quantization/quantization_linear.py -> build/bdist.linux-x86_64/egg/paddlenlp/quantization
copying build/lib/paddlenlp/quantization/quantization_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/quantization
creating build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/span.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/sighan.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/rouge.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/dureader.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/mrr.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/perplexity.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/chunk.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/glue.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/bleu.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/squad.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
copying build/lib/paddlenlp/metrics/distinct.py -> build/bdist.linux-x86_64/egg/paddlenlp/metrics
creating build/bdist.linux-x86_64/egg/paddlenlp/generation
copying build/lib/paddlenlp/generation/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/generation
copying build/lib/paddlenlp/generation/configuration_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/generation
copying build/lib/paddlenlp/generation/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/generation
copying build/lib/paddlenlp/generation/stopping_criteria.py -> build/bdist.linux-x86_64/egg/paddlenlp/generation
copying build/lib/paddlenlp/generation/streamers.py -> build/bdist.linux-x86_64/egg/paddlenlp/generation
copying build/lib/paddlenlp/generation/logits_process.py -> build/bdist.linux-x86_64/egg/paddlenlp/generation
creating build/bdist.linux-x86_64/egg/paddlenlp/losses
copying build/lib/paddlenlp/losses/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/losses
copying build/lib/paddlenlp/losses/rdrop.py -> build/bdist.linux-x86_64/egg/paddlenlp/losses
creating build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/profiler.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/initializer.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/distributed.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/tools.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/log.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/converter.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/doc_parser.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/downloader.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/import_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/env.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/image_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/batch_sampler.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/serialization.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
copying build/lib/paddlenlp/utils/ie_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/utils
creating build/bdist.linux-x86_64/egg/paddlenlp/version
copying build/lib/paddlenlp/version/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/version
copying build/lib/paddlenlp/version/git.py -> build/bdist.linux-x86_64/egg/paddlenlp/version
creating build/bdist.linux-x86_64/egg/paddlenlp/layers
copying build/lib/paddlenlp/layers/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/layers
copying build/lib/paddlenlp/layers/crf.py -> build/bdist.linux-x86_64/egg/paddlenlp/layers
copying build/lib/paddlenlp/layers/linear.py -> build/bdist.linux-x86_64/egg/paddlenlp/layers
copying build/lib/paddlenlp/layers/sequence.py -> build/bdist.linux-x86_64/egg/paddlenlp/layers
copying build/lib/paddlenlp/layers/globalpointer.py -> build/bdist.linux-x86_64/egg/paddlenlp/layers
copying build/lib/paddlenlp/layers/tcn.py -> build/bdist.linux-x86_64/egg/paddlenlp/layers
creating build/bdist.linux-x86_64/egg/paddlenlp/embeddings
copying build/lib/paddlenlp/embeddings/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/embeddings
copying build/lib/paddlenlp/embeddings/token_embedding.py -> build/bdist.linux-x86_64/egg/paddlenlp/embeddings
copying build/lib/paddlenlp/embeddings/constant.py -> build/bdist.linux-x86_64/egg/paddlenlp/embeddings
creating build/bdist.linux-x86_64/egg/paddlenlp/ops
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed
copying build/lib/paddlenlp/ops/distributed/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils
copying build/lib/paddlenlp/ops/distributed/utils/topo.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils
copying build/lib/paddlenlp/ops/distributed/utils/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils
copying build/lib/paddlenlp/ops/distributed/utils/random.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils
copying build/lib/paddlenlp/ops/distributed/parallel.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed
copying build/lib/paddlenlp/ops/CMakeLists.txt -> build/bdist.linux-x86_64/egg/paddlenlp/ops
copying build/lib/paddlenlp/ops/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops
copying build/lib/paddlenlp/ops/einsum.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/cmake
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/cmake/external
copying build/lib/paddlenlp/ops/cmake/external/boost.cmake -> build/bdist.linux-x86_64/egg/paddlenlp/ops/cmake/external
copying build/lib/paddlenlp/ops/cmake/FindNCCL.cmake -> build/bdist.linux-x86_64/egg/paddlenlp/ops/cmake
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/patches
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/CMakeLists.txt -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/CMakeLists.txt -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/gptj.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/common_structure.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/allocator.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/common.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils/arguments.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/utils
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/open_decoder.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/gpt.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/attention_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/masked_multihead_attention.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_decoder.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/decoding_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_kernels.cuh -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/online_softmax_beamsearch_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/topk_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/open_attention.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/cuda_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/cuda_kernels.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/attention_kernels.cuh -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/open_decoder.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/topk_kernels.cuh -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/transformer_decoding_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/lightseq_kernels.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/masked_multihead_attention.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/open_decoder.cuh -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda/masked_multihead_attention_utils.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer/cuda
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/t5_sampling.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/decoding_sampling.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/decoding_beamsearch.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/t5_beamsearch.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/bert_encoder_transformer.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/opt.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/patches/FasterTransformer/fastertransformer/standard_encoder.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/patches/FasterTransformer/fastertransformer
copying build/lib/paddlenlp/ops/ext_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer
copying build/lib/paddlenlp/ops/optimizer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer
copying build/lib/paddlenlp/ops/optimizer/lr.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer
copying build/lib/paddlenlp/ops/optimizer/ema.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer
copying build/lib/paddlenlp/ops/optimizer/adamwdl.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_t5_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_opt_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_force_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_unified_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/cublas_handle.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_mbart_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/CMakeLists.txt -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/utils.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_bart_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/parallel_utils.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/gpt.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/transformer_e2e.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/helper.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo/utf8
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8/cpp17.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo/utf8
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8/checked.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo/utf8
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8/unchecked.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo/utf8
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8/core.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo/utf8
copying build/lib/paddlenlp/ops/fast_transformer/src/demo/utf8/cpp11.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src/demo/utf8
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_gpt_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_force_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_t5_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_bart_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/pd_traits.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_encoder_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_miro_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_encoder_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_decoder_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_miro_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_mbart_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_unified_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_pegasus_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_decoder_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_mbart_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_unified_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_miro_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_gptj_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_pegasus_decoding_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_pegasus_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_opt_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/parallel_utils.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_bart_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_gptj_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/cublas_handle.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_t5_decoding_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_force_decoding_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_decoder_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_gptj_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_gpt_op.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_gpt_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_opt_op.cu -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/utils.cc -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/src/fusion_encoder_op.h -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/src
copying build/lib/paddlenlp/ops/fast_transformer/CMakeLists.txt -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer
copying build/lib/paddlenlp/ops/fast_transformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer
copying build/lib/paddlenlp/ops/fast_transformer/transformer/decoding.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer
copying build/lib/paddlenlp/ops/fast_transformer/transformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer
copying build/lib/paddlenlp/ops/fast_transformer/transformer/fast_transformer.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer
copying build/lib/paddlenlp/ops/fast_transformer/transformer/encoder.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer
copying build/lib/paddlenlp/ops/fast_transformer/transformer/decoder.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/plato_inference.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/unimo_text_export_model_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/bart_export_model_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/encoder_decoding_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/unimo_text_inference.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/mbart_decoding_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/mbart_inference.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/t5_export_model_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/mbart_export_model_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
creating build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/config
copying build/lib/paddlenlp/ops/fast_transformer/sample/config/decoder.sample.yaml -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/config
copying build/lib/paddlenlp/ops/fast_transformer/sample/config/decoding.sample.yaml -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/config
copying build/lib/paddlenlp/ops/fast_transformer/sample/gpt_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/t5_inference.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/plato_export_model_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/encoder_decoder_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/bart_decoding_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/bart_inference.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/decoder_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/gpt_export_model_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/fast_transformer/sample/decoding_sample.py -> build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample
copying build/lib/paddlenlp/ops/README.md -> build/bdist.linux-x86_64/egg/paddlenlp/ops
creating build/bdist.linux-x86_64/egg/paddlenlp/server
copying build/lib/paddlenlp/server/predictor.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
copying build/lib/paddlenlp/server/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
copying build/lib/paddlenlp/server/server.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
copying build/lib/paddlenlp/server/base_router.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
creating build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/token_model_handler.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/custom_model_handler.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/taskflow_handler.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/qa_model_handler.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/cls_post_handler.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/handlers/base_handler.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/handlers
copying build/lib/paddlenlp/server/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
copying build/lib/paddlenlp/server/model_manager.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
copying build/lib/paddlenlp/server/taskflow_manager.py -> build/bdist.linux-x86_64/egg/paddlenlp/server
creating build/bdist.linux-x86_64/egg/paddlenlp/server/http_router
copying build/lib/paddlenlp/server/http_router/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/http_router
copying build/lib/paddlenlp/server/http_router/router.py -> build/bdist.linux-x86_64/egg/paddlenlp/server/http_router
creating build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/verbalizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/prompt_trainer.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/prompt_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/prompt_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/prompt_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/prompt_args.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
copying build/lib/paddlenlp/prompt/template.py -> build/bdist.linux-x86_64/egg/paddlenlp/prompt
creating build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/bellegroup.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/dureader_robust.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/cblue.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/advertisegen.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/nlpcc13_evsam05_thu.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/imdb.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/nlpcc_dbqa.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/cail2018_small.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/cote.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/dataset.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/lcsts_new.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/xnli_cn.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/ptb.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
creating build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/dureader_robust.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/xfund_zh.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/imdb.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/mt_eng_vietnamese.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/cote.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/ptb_text_only.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/clue.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/cmrc2018.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/msra_ner.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/docvqa_zh.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/funsd.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/squad_v2.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/chnsenticorp.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/language_pair.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/seabsa16.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/glue.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/xnli.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/duconv.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/cnn_dailymail.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/rvl_cdip_sampled.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/hf_datasets/squad.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets
copying build/lib/paddlenlp/datasets/wmt14ende.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/wos.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/chnsenticorp_v2.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/thucnews.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/clue.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/yahoo_answer_100k.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/poetry.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/fewclue.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/nlpcc14_sc.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/lcqmc_v2.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/bq_corpus.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/nlpcc13_evsam05_hit.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/couplet.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/cmrc2018.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/iwslt15.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/paws-x.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/msra_ner.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/intokens_dataset.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/drcd.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/dureader_qg.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/dureader_yesno.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/hyp.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/drcd_cn.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/bstc.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/dureader_checklist.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/triviaqa.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/chnsenticorp.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/lcqmc.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/seabsa16.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/glue.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/peoples_daily_ner.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/c3.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/xnli.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/cail2019_scm.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/duconv.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/cnn_dailymail.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/squad.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/sighan-cn.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
copying build/lib/paddlenlp/datasets/conll2002.py -> build/bdist.linux-x86_64/egg/paddlenlp/datasets
creating build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/iterator.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/vocab.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/data_collator.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/blendable_dataset.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/collate.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/indexed_dataset.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/sampler.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/causal_dataset.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/dist_dataloader.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
copying build/lib/paddlenlp/data/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/data
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw
copying build/lib/paddlenlp/transformers/rw/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw
copying build/lib/paddlenlp/transformers/rw/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw
copying build/lib/paddlenlp/transformers/rw/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw
copying build/lib/paddlenlp/transformers/rw/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha
copying build/lib/paddlenlp/transformers/gau_alpha/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha
copying build/lib/paddlenlp/transformers/gau_alpha/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha
copying build/lib/paddlenlp/transformers/gau_alpha/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha
copying build/lib/paddlenlp/transformers/gau_alpha/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist
copying build/lib/paddlenlp/transformers/artist/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist
copying build/lib/paddlenlp/transformers/artist/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist
copying build/lib/paddlenlp/transformers/artist/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist
copying build/lib/paddlenlp/transformers/artist/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist
copying build/lib/paddlenlp/transformers/sentencepiece_model_pb2.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet
copying build/lib/paddlenlp/transformers/fnet/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet
copying build/lib/paddlenlp/transformers/fnet/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet
copying build/lib/paddlenlp/transformers/fnet/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet
copying build/lib/paddlenlp/transformers/fnet/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert
copying build/lib/paddlenlp/transformers/convbert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert
copying build/lib/paddlenlp/transformers/convbert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert
copying build/lib/paddlenlp/transformers/convbert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert
copying build/lib/paddlenlp/transformers/convbert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm
copying build/lib/paddlenlp/transformers/visualglm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm
copying build/lib/paddlenlp/transformers/visualglm/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm
copying build/lib/paddlenlp/transformers/visualglm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm
copying build/lib/paddlenlp/transformers/visualglm/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm
copying build/lib/paddlenlp/transformers/visualglm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm
copying build/lib/paddlenlp/transformers/image_transforms.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2
copying build/lib/paddlenlp/transformers/roformerv2/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2
copying build/lib/paddlenlp/transformers/roformerv2/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2
copying build/lib/paddlenlp/transformers/roformerv2/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2
copying build/lib/paddlenlp/transformers/roformerv2/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2
copying build/lib/paddlenlp/transformers/processing_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
copying build/lib/paddlenlp/transformers/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
copying build/lib/paddlenlp/transformers/blip/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
copying build/lib/paddlenlp/transformers/blip/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
copying build/lib/paddlenlp/transformers/blip/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
copying build/lib/paddlenlp/transformers/blip/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
copying build/lib/paddlenlp/transformers/blip/modeling_text.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
copying build/lib/paddlenlp/transformers/blip/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl
copying build/lib/paddlenlp/transformers/ctrl/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl
copying build/lib/paddlenlp/transformers/ctrl/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl
copying build/lib/paddlenlp/transformers/ctrl/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl
copying build/lib/paddlenlp/transformers/ctrl/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2
copying build/lib/paddlenlp/transformers/blip_2/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2
copying build/lib/paddlenlp/transformers/blip_2/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2
copying build/lib/paddlenlp/transformers/blip_2/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2
copying build/lib/paddlenlp/transformers/blip_2/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2
copying build/lib/paddlenlp/transformers/layoutlmv2/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2
copying build/lib/paddlenlp/transformers/layoutlmv2/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2
copying build/lib/paddlenlp/transformers/layoutlmv2/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2
copying build/lib/paddlenlp/transformers/layoutlmv2/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small
copying build/lib/paddlenlp/transformers/blenderbot_small/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small
copying build/lib/paddlenlp/transformers/blenderbot_small/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small
copying build/lib/paddlenlp/transformers/blenderbot_small/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small
copying build/lib/paddlenlp/transformers/blenderbot_small/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small
copying build/lib/paddlenlp/transformers/sequence_parallel_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
copying build/lib/paddlenlp/transformers/configuration_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/layoutxlm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/layoutxlm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/layoutxlm/visual_backbone.yaml -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/layoutxlm/visual_backbone.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/layoutxlm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/layoutxlm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm
copying build/lib/paddlenlp/transformers/image_processing_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/transformer
copying build/lib/paddlenlp/transformers/transformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/transformer
copying build/lib/paddlenlp/transformers/transformer/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/transformer
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
copying build/lib/paddlenlp/transformers/ernie_vil/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird
copying build/lib/paddlenlp/transformers/bigbird/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird
copying build/lib/paddlenlp/transformers/bigbird/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird
copying build/lib/paddlenlp/transformers/bigbird/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird
copying build/lib/paddlenlp/transformers/bigbird/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
copying build/lib/paddlenlp/transformers/llama/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
copying build/lib/paddlenlp/transformers/llama/modeling_auto.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
copying build/lib/paddlenlp/transformers/llama/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
copying build/lib/paddlenlp/transformers/llama/modeling_pp.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
copying build/lib/paddlenlp/transformers/llama/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
copying build/lib/paddlenlp/transformers/llama/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gen
copying build/lib/paddlenlp/transformers/ernie_gen/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gen
copying build/lib/paddlenlp/transformers/ernie_gen/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gen
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert
copying build/lib/paddlenlp/transformers/bert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert
copying build/lib/paddlenlp/transformers/bert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert
copying build/lib/paddlenlp/transformers/bert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert
copying build/lib/paddlenlp/transformers/bert/fast_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert
copying build/lib/paddlenlp/transformers/bert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert
copying build/lib/paddlenlp/transformers/tinybert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert
copying build/lib/paddlenlp/transformers/tinybert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert
copying build/lib/paddlenlp/transformers/tinybert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert
copying build/lib/paddlenlp/transformers/tinybert/fast_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert
copying build/lib/paddlenlp/transformers/tinybert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel
copying build/lib/paddlenlp/transformers/funnel/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel
copying build/lib/paddlenlp/transformers/funnel/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel
copying build/lib/paddlenlp/transformers/funnel/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel
copying build/lib/paddlenlp/transformers/funnel/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet
copying build/lib/paddlenlp/transformers/xlnet/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet
copying build/lib/paddlenlp/transformers/xlnet/converter.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet
copying build/lib/paddlenlp/transformers/xlnet/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet
copying build/lib/paddlenlp/transformers/xlnet/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet
copying build/lib/paddlenlp/transformers/xlnet/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet
copying build/lib/paddlenlp/transformers/attention_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4
copying build/lib/paddlenlp/transformers/minigpt4/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4
copying build/lib/paddlenlp/transformers/minigpt4/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4
copying build/lib/paddlenlp/transformers/minigpt4/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4
copying build/lib/paddlenlp/transformers/minigpt4/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4
copying build/lib/paddlenlp/transformers/minigpt4/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot
copying build/lib/paddlenlp/transformers/blenderbot/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot
copying build/lib/paddlenlp/transformers/blenderbot/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot
copying build/lib/paddlenlp/transformers/blenderbot/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot
copying build/lib/paddlenlp/transformers/blenderbot/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta
copying build/lib/paddlenlp/transformers/roberta/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta
copying build/lib/paddlenlp/transformers/roberta/converter.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta
copying build/lib/paddlenlp/transformers/roberta/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta
copying build/lib/paddlenlp/transformers/roberta/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta
copying build/lib/paddlenlp/transformers/roberta/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt
copying build/lib/paddlenlp/transformers/opt/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt
copying build/lib/paddlenlp/transformers/opt/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt
copying build/lib/paddlenlp/transformers/opt/convert_torch_to_paddle.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt
copying build/lib/paddlenlp/transformers/opt/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke
copying build/lib/paddlenlp/transformers/luke/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke
copying build/lib/paddlenlp/transformers/luke/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke
copying build/lib/paddlenlp/transformers/luke/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke
copying build/lib/paddlenlp/transformers/luke/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke
copying build/lib/paddlenlp/transformers/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert
copying build/lib/paddlenlp/transformers/distilbert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert
copying build/lib/paddlenlp/transformers/distilbert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert
copying build/lib/paddlenlp/transformers/distilbert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert
copying build/lib/paddlenlp/transformers/distilbert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2
copying build/lib/paddlenlp/transformers/chatglm_v2/chatglm-legacy-checkpoints-convert.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2
copying build/lib/paddlenlp/transformers/chatglm_v2/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2
copying build/lib/paddlenlp/transformers/chatglm_v2/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2
copying build/lib/paddlenlp/transformers/chatglm_v2/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2
copying build/lib/paddlenlp/transformers/chatglm_v2/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2
copying build/lib/paddlenlp/transformers/optimization.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5
copying build/lib/paddlenlp/transformers/mt5/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5
copying build/lib/paddlenlp/transformers/mt5/converter.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5
copying build/lib/paddlenlp/transformers/mt5/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5
copying build/lib/paddlenlp/transformers/mt5/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer
copying build/lib/paddlenlp/transformers/roformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer
copying build/lib/paddlenlp/transformers/roformer/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer
copying build/lib/paddlenlp/transformers/roformer/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer
copying build/lib/paddlenlp/transformers/roformer/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie
copying build/lib/paddlenlp/transformers/ernie/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie
copying build/lib/paddlenlp/transformers/ernie/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie
copying build/lib/paddlenlp/transformers/ernie/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie
copying build/lib/paddlenlp/transformers/ernie/fast_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie
copying build/lib/paddlenlp/transformers/ernie/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert
copying build/lib/paddlenlp/transformers/chinesebert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert
copying build/lib/paddlenlp/transformers/chinesebert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert
copying build/lib/paddlenlp/transformers/chinesebert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert
copying build/lib/paddlenlp/transformers/chinesebert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert
copying build/lib/paddlenlp/transformers/tokenizer_utils_fast.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm
copying build/lib/paddlenlp/transformers/ernie_ctm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm
copying build/lib/paddlenlp/transformers/ernie_ctm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm
copying build/lib/paddlenlp/transformers/ernie_ctm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm
copying build/lib/paddlenlp/transformers/ernie_ctm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm
copying build/lib/paddlenlp/transformers/convert_slow_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
copying build/lib/paddlenlp/transformers/segment_parallel_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen
copying build/lib/paddlenlp/transformers/codegen/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen
copying build/lib/paddlenlp/transformers/codegen/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen
copying build/lib/paddlenlp/transformers/codegen/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen
copying build/lib/paddlenlp/transformers/codegen/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer
copying build/lib/paddlenlp/transformers/unified_transformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer
copying build/lib/paddlenlp/transformers/unified_transformer/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer
copying build/lib/paddlenlp/transformers/unified_transformer/convert.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer
copying build/lib/paddlenlp/transformers/unified_transformer/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer
copying build/lib/paddlenlp/transformers/unified_transformer/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert
copying build/lib/paddlenlp/transformers/mobilebert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert
copying build/lib/paddlenlp/transformers/mobilebert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert
copying build/lib/paddlenlp/transformers/mobilebert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert
copying build/lib/paddlenlp/transformers/mobilebert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert
copying build/lib/paddlenlp/transformers/model_outputs.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
copying build/lib/paddlenlp/transformers/tokenizer_utils_base.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt
copying build/lib/paddlenlp/transformers/dpt/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt
copying build/lib/paddlenlp/transformers/dpt/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt
copying build/lib/paddlenlp/transformers/dpt/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt
copying build/lib/paddlenlp/transformers/dpt/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm
copying build/lib/paddlenlp/transformers/ppminilm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm
copying build/lib/paddlenlp/transformers/ppminilm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm
copying build/lib/paddlenlp/transformers/ppminilm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm
copying build/lib/paddlenlp/transformers/ppminilm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5
copying build/lib/paddlenlp/transformers/t5/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5
copying build/lib/paddlenlp/transformers/t5/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5
copying build/lib/paddlenlp/transformers/t5/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5
copying build/lib/paddlenlp/transformers/t5/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto
copying build/lib/paddlenlp/transformers/auto/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto
copying build/lib/paddlenlp/transformers/auto/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto
copying build/lib/paddlenlp/transformers/auto/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto
copying build/lib/paddlenlp/transformers/auto/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto
copying build/lib/paddlenlp/transformers/auto/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto
copying build/lib/paddlenlp/transformers/model_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen
copying build/lib/paddlenlp/transformers/qwen/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen
copying build/lib/paddlenlp/transformers/qwen/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen
copying build/lib/paddlenlp/transformers/qwen/modeling_pp.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen
copying build/lib/paddlenlp/transformers/qwen/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen
copying build/lib/paddlenlp/transformers/qwen/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen
copying build/lib/paddlenlp/transformers/conversion_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert
copying build/lib/paddlenlp/transformers/squeezebert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert
copying build/lib/paddlenlp/transformers/squeezebert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert
copying build/lib/paddlenlp/transformers/squeezebert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert
copying build/lib/paddlenlp/transformers/squeezebert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap
copying build/lib/paddlenlp/transformers/clap/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap
copying build/lib/paddlenlp/transformers/clap/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap
copying build/lib/paddlenlp/transformers/clap/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap
copying build/lib/paddlenlp/transformers/clap/feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap
copying build/lib/paddlenlp/transformers/clap/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap
copying build/lib/paddlenlp/transformers/tokenizer_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout
copying build/lib/paddlenlp/transformers/ernie_layout/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout
copying build/lib/paddlenlp/transformers/ernie_layout/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout
copying build/lib/paddlenlp/transformers/ernie_layout/visual_backbone.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout
copying build/lib/paddlenlp/transformers/ernie_layout/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout
copying build/lib/paddlenlp/transformers/ernie_layout/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert
copying build/lib/paddlenlp/transformers/megatronbert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert
copying build/lib/paddlenlp/transformers/megatronbert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert
copying build/lib/paddlenlp/transformers/megatronbert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert
copying build/lib/paddlenlp/transformers/megatronbert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert
copying build/lib/paddlenlp/transformers/activations.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/converter.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
copying build/lib/paddlenlp/transformers/chineseclip/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm
copying build/lib/paddlenlp/transformers/xlm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm
copying build/lib/paddlenlp/transformers/xlm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm
copying build/lib/paddlenlp/transformers/xlm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm
copying build/lib/paddlenlp/transformers/xlm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/semantic_search
copying build/lib/paddlenlp/transformers/semantic_search/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/semantic_search
copying build/lib/paddlenlp/transformers/semantic_search/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/semantic_search
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha
copying build/lib/paddlenlp/transformers/nezha/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha
copying build/lib/paddlenlp/transformers/nezha/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha
copying build/lib/paddlenlp/transformers/nezha/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha
copying build/lib/paddlenlp/transformers/nezha/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom
copying build/lib/paddlenlp/transformers/bloom/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom
copying build/lib/paddlenlp/transformers/bloom/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom
copying build/lib/paddlenlp/transformers/bloom/processor.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom
copying build/lib/paddlenlp/transformers/bloom/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom
copying build/lib/paddlenlp/transformers/bloom/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo
copying build/lib/paddlenlp/transformers/unimo/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo
copying build/lib/paddlenlp/transformers/unimo/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo
copying build/lib/paddlenlp/transformers/unimo/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo
copying build/lib/paddlenlp/transformers/unimo/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo
copying build/lib/paddlenlp/transformers/feature_extraction_sequence_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg
copying build/lib/paddlenlp/transformers/clipseg/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg
copying build/lib/paddlenlp/transformers/clipseg/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg
copying build/lib/paddlenlp/transformers/clipseg/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg
copying build/lib/paddlenlp/transformers/clipseg/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg
copying build/lib/paddlenlp/transformers/clipseg/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg
copying build/lib/paddlenlp/transformers/audio_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m
copying build/lib/paddlenlp/transformers/ernie_m/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m
copying build/lib/paddlenlp/transformers/ernie_m/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m
copying build/lib/paddlenlp/transformers/ernie_m/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m
copying build/lib/paddlenlp/transformers/ernie_m/fast_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m
copying build/lib/paddlenlp/transformers/ernie_m/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram
copying build/lib/paddlenlp/transformers/ernie_gram/matching_param_name.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram
copying build/lib/paddlenlp/transformers/ernie_gram/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram
copying build/lib/paddlenlp/transformers/ernie_gram/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram
copying build/lib/paddlenlp/transformers/ernie_gram/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram
copying build/lib/paddlenlp/transformers/ernie_gram/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm
copying build/lib/paddlenlp/transformers/chatglm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm
copying build/lib/paddlenlp/transformers/chatglm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm
copying build/lib/paddlenlp/transformers/chatglm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm
copying build/lib/paddlenlp/transformers/chatglm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code
copying build/lib/paddlenlp/transformers/ernie_code/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code
copying build/lib/paddlenlp/transformers/ernie_code/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code
copying build/lib/paddlenlp/transformers/ernie_code/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code
copying build/lib/paddlenlp/transformers/ernie_code/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt
copying build/lib/paddlenlp/transformers/gpt/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt
copying build/lib/paddlenlp/transformers/gpt/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt
copying build/lib/paddlenlp/transformers/gpt/modeling_pp.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt
copying build/lib/paddlenlp/transformers/gpt/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt
copying build/lib/paddlenlp/transformers/gpt/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart
copying build/lib/paddlenlp/transformers/dallebart/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart
copying build/lib/paddlenlp/transformers/dallebart/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart
copying build/lib/paddlenlp/transformers/dallebart/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart
copying build/lib/paddlenlp/transformers/dallebart/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra
copying build/lib/paddlenlp/transformers/electra/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra
copying build/lib/paddlenlp/transformers/electra/converter.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra
copying build/lib/paddlenlp/transformers/electra/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra
copying build/lib/paddlenlp/transformers/electra/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra
copying build/lib/paddlenlp/transformers/electra/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra
copying build/lib/paddlenlp/transformers/export.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc
copying build/lib/paddlenlp/transformers/ernie_doc/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc
copying build/lib/paddlenlp/transformers/ernie_doc/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc
copying build/lib/paddlenlp/transformers/ernie_doc/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc
copying build/lib/paddlenlp/transformers/ernie_doc/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc
copying build/lib/paddlenlp/transformers/feature_extraction_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert_japanese
copying build/lib/paddlenlp/transformers/bert_japanese/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert_japanese
copying build/lib/paddlenlp/transformers/bert_japanese/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert_japanese
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert
copying build/lib/paddlenlp/transformers/albert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert
copying build/lib/paddlenlp/transformers/albert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert
copying build/lib/paddlenlp/transformers/albert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert
copying build/lib/paddlenlp/transformers/albert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus
copying build/lib/paddlenlp/transformers/pegasus/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus
copying build/lib/paddlenlp/transformers/pegasus/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus
copying build/lib/paddlenlp/transformers/pegasus/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus
copying build/lib/paddlenlp/transformers/pegasus/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj
copying build/lib/paddlenlp/transformers/gptj/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj
copying build/lib/paddlenlp/transformers/gptj/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj
copying build/lib/paddlenlp/transformers/gptj/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj
copying build/lib/paddlenlp/transformers/gptj/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm
copying build/lib/paddlenlp/transformers/layoutlm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm
copying build/lib/paddlenlp/transformers/layoutlm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm
copying build/lib/paddlenlp/transformers/layoutlm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm
copying build/lib/paddlenlp/transformers/layoutlm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm
copying build/lib/paddlenlp/transformers/distill_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet
copying build/lib/paddlenlp/transformers/prophetnet/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet
copying build/lib/paddlenlp/transformers/prophetnet/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet
copying build/lib/paddlenlp/transformers/prophetnet/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet
copying build/lib/paddlenlp/transformers/prophetnet/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart
copying build/lib/paddlenlp/transformers/bart/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart
copying build/lib/paddlenlp/transformers/bart/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart
copying build/lib/paddlenlp/transformers/bart/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart
copying build/lib/paddlenlp/transformers/bart/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart
copying build/lib/paddlenlp/transformers/aistudio_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert
copying build/lib/paddlenlp/transformers/rembert/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert
copying build/lib/paddlenlp/transformers/rembert/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert
copying build/lib/paddlenlp/transformers/rembert/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert
copying build/lib/paddlenlp/transformers/rembert/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet
copying build/lib/paddlenlp/transformers/mpnet/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet
copying build/lib/paddlenlp/transformers/mpnet/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet
copying build/lib/paddlenlp/transformers/mpnet/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet
copying build/lib/paddlenlp/transformers/mpnet/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
copying build/lib/paddlenlp/transformers/clip/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep
copying build/lib/paddlenlp/transformers/skep/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep
copying build/lib/paddlenlp/transformers/skep/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep
copying build/lib/paddlenlp/transformers/skep/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep
copying build/lib/paddlenlp/transformers/skep/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit
copying build/lib/paddlenlp/transformers/bit/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit
copying build/lib/paddlenlp/transformers/bit/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit
copying build/lib/paddlenlp/transformers/bit/image_processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit
copying build/lib/paddlenlp/transformers/bit/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/speecht5/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/speecht5/processing.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/speecht5/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/speecht5/feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/speecht5/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/speecht5/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5
copying build/lib/paddlenlp/transformers/image_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer
copying build/lib/paddlenlp/transformers/reformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer
copying build/lib/paddlenlp/transformers/reformer/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer
copying build/lib/paddlenlp/transformers/reformer/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer
copying build/lib/paddlenlp/transformers/reformer/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer
copying build/lib/paddlenlp/transformers/ofa_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer
copying build/lib/paddlenlp/transformers/nystromformer/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer
copying build/lib/paddlenlp/transformers/nystromformer/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer
copying build/lib/paddlenlp/transformers/nystromformer/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer
copying build/lib/paddlenlp/transformers/nystromformer/fast_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer
copying build/lib/paddlenlp/transformers/nystromformer/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm
copying build/lib/paddlenlp/transformers/glm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm
copying build/lib/paddlenlp/transformers/glm/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm
copying build/lib/paddlenlp/transformers/glm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm
copying build/lib/paddlenlp/transformers/glm/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm
creating build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart
copying build/lib/paddlenlp/transformers/mbart/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart
copying build/lib/paddlenlp/transformers/mbart/configuration.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart
copying build/lib/paddlenlp/transformers/mbart/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart
copying build/lib/paddlenlp/transformers/mbart/tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental
copying build/lib/paddlenlp/experimental/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental
copying build/lib/paddlenlp/experimental/ernie_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp
copying build/lib/paddlenlp/experimental/autonlp/text_classification.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp
copying build/lib/paddlenlp/experimental/autonlp/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp
copying build/lib/paddlenlp/experimental/autonlp/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp
copying build/lib/paddlenlp/experimental/autonlp/auto_trainer_base.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp
copying build/lib/paddlenlp/experimental/model_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers
copying build/lib/paddlenlp/experimental/transformers/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers
copying build/lib/paddlenlp/experimental/transformers/fused_transformer_layers.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/llama
copying build/lib/paddlenlp/experimental/transformers/llama/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/llama
copying build/lib/paddlenlp/experimental/transformers/llama/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/llama
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/opt
copying build/lib/paddlenlp/experimental/transformers/opt/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/opt
copying build/lib/paddlenlp/experimental/transformers/opt/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/opt
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm_v2
copying build/lib/paddlenlp/experimental/transformers/chatglm_v2/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm_v2
copying build/lib/paddlenlp/experimental/transformers/chatglm_v2/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm_v2
copying build/lib/paddlenlp/experimental/transformers/generation_utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/bloom
copying build/lib/paddlenlp/experimental/transformers/bloom/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/bloom
copying build/lib/paddlenlp/experimental/transformers/bloom/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/bloom
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm
copying build/lib/paddlenlp/experimental/transformers/chatglm/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm
copying build/lib/paddlenlp/experimental/transformers/chatglm/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm
creating build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/gpt
copying build/lib/paddlenlp/experimental/transformers/gpt/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/gpt
copying build/lib/paddlenlp/experimental/transformers/gpt/modeling.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/gpt
copying build/lib/paddlenlp/experimental/faster_tokenizer.py -> build/bdist.linux-x86_64/egg/paddlenlp/experimental
creating build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text_summarization.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/dialogue.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text_classification.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text_generation.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/fill_mask.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text_feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/zero_shot_text_classification.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/lexical_analysis.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/taskflow.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text_similarity.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text_correction.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/task.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
creating build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models
copying build/lib/paddlenlp/taskflow/models/lexical_analysis_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models
copying build/lib/paddlenlp/taskflow/models/text_correction_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models
copying build/lib/paddlenlp/taskflow/models/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models
copying build/lib/paddlenlp/taskflow/models/sentiment_analysis_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models
copying build/lib/paddlenlp/taskflow/models/dependency_parsing_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models
copying build/lib/paddlenlp/taskflow/sentiment_analysis.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/word_segmentation.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/named_entity_recognition.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/question_answering.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/information_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/knowledge_mining.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/pos_tagging.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/question_generation.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/code_generation.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/document_intelligence.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/multimodal_feature_extraction.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/dependency_parsing.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/text2text_generation.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
copying build/lib/paddlenlp/taskflow/poetry_generation.py -> build/bdist.linux-x86_64/egg/paddlenlp/taskflow
creating build/bdist.linux-x86_64/egg/paddlenlp/dataaug
copying build/lib/paddlenlp/dataaug/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/dataaug
copying build/lib/paddlenlp/dataaug/char.py -> build/bdist.linux-x86_64/egg/paddlenlp/dataaug
copying build/lib/paddlenlp/dataaug/base_augment.py -> build/bdist.linux-x86_64/egg/paddlenlp/dataaug
copying build/lib/paddlenlp/dataaug/sentence.py -> build/bdist.linux-x86_64/egg/paddlenlp/dataaug
copying build/lib/paddlenlp/dataaug/word.py -> build/bdist.linux-x86_64/egg/paddlenlp/dataaug
creating build/bdist.linux-x86_64/egg/paddlenlp/seq2vec
copying build/lib/paddlenlp/seq2vec/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/seq2vec
copying build/lib/paddlenlp/seq2vec/encoder.py -> build/bdist.linux-x86_64/egg/paddlenlp/seq2vec
creating build/bdist.linux-x86_64/egg/paddlenlp/cli
copying build/lib/paddlenlp/cli/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli
copying build/lib/paddlenlp/cli/server.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli
copying build/lib/paddlenlp/cli/bos_community.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli
copying build/lib/paddlenlp/cli/main.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli
creating build/bdist.linux-x86_64/egg/paddlenlp/cli/utils
copying build/lib/paddlenlp/cli/utils/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli/utils
copying build/lib/paddlenlp/cli/utils/tabulate.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli/utils
copying build/lib/paddlenlp/cli/install.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli
copying build/lib/paddlenlp/cli/download.py -> build/bdist.linux-x86_64/egg/paddlenlp/cli
creating build/bdist.linux-x86_64/egg/paddlenlp/peft
copying build/lib/paddlenlp/peft/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft
creating build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix
copying build/lib/paddlenlp/peft/prefix/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix
copying build/lib/paddlenlp/peft/prefix/utils.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix
copying build/lib/paddlenlp/peft/prefix/prefix_config.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix
copying build/lib/paddlenlp/peft/prefix/prefix_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix
creating build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
copying build/lib/paddlenlp/peft/lora/lora_layers.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
copying build/lib/paddlenlp/peft/lora/__init__.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
copying build/lib/paddlenlp/peft/lora/lora_model.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
copying build/lib/paddlenlp/peft/lora/lora_quantization_layers.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
copying build/lib/paddlenlp/peft/lora/lora_quant_layers.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
copying build/lib/paddlenlp/peft/lora/lora_config.py -> build/bdist.linux-x86_64/egg/paddlenlp/peft/lora
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/trainer_utils.py to trainer_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/compression_args.py to compression_args.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard/sharding_v2.py to sharding_v2.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard/common.py to common.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/reshard/sharding_v1.py to sharding_v1.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/sharding_io.py to sharding_io.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/doc.py to doc.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/utils/helper.py to helper.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/trainer_compress.py to trainer_compress.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/training_args_seq2seq.py to training_args_seq2seq.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/trainer.py to trainer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/training_args.py to training_args.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/integrations.py to integrations.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/trainer_seq2seq.py to trainer_seq2seq.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/argparser.py to argparser.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/trainer_callback.py to trainer_callback.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins/npu_plugin.py to npu_plugin.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins/timer.py to timer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/trainer/plugins/unified_checkpoint.py to unified_checkpoint.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/quantization/qlora.py to qlora.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/quantization/quantization_config.py to quantization_config.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/quantization/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/quantization/quantization_linear.py to quantization_linear.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/quantization/quantization_utils.py to quantization_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/span.py to span.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/sighan.py to sighan.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/rouge.py to rouge.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/dureader.py to dureader.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/mrr.py to mrr.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/perplexity.py to perplexity.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/chunk.py to chunk.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/glue.py to glue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/bleu.py to bleu.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/squad.py to squad.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/metrics/distinct.py to distinct.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/generation/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/generation/configuration_utils.py to configuration_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/generation/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/generation/stopping_criteria.py to stopping_criteria.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/generation/streamers.py to streamers.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/generation/logits_process.py to logits_process.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/losses/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/losses/rdrop.py to rdrop.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/profiler.py to profiler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/initializer.py to initializer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/distributed.py to distributed.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/tools.py to tools.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/log.py to log.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/converter.py to converter.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/doc_parser.py to doc_parser.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/downloader.py to downloader.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/import_utils.py to import_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/env.py to env.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/image_utils.py to image_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/batch_sampler.py to batch_sampler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/serialization.py to serialization.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/utils/ie_utils.py to ie_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/version/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/version/git.py to git.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/layers/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/layers/crf.py to crf.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/layers/linear.py to linear.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/layers/sequence.py to sequence.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/layers/globalpointer.py to globalpointer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/layers/tcn.py to tcn.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/embeddings/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/embeddings/token_embedding.py to token_embedding.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/embeddings/constant.py to constant.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils/topo.py to topo.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/utils/random.py to random.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/distributed/parallel.py to parallel.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/einsum.py to einsum.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/ext_utils.py to ext_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer/lr.py to lr.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer/ema.py to ema.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/optimizer/adamwdl.py to adamwdl.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer/decoding.py to decoding.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer/fast_transformer.py to fast_transformer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer/encoder.py to encoder.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/transformer/decoder.py to decoder.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/plato_inference.py to plato_inference.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/unimo_text_export_model_sample.py to unimo_text_export_model_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/bart_export_model_sample.py to bart_export_model_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/encoder_decoding_sample.py to encoder_decoding_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/unimo_text_inference.py to unimo_text_inference.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/mbart_decoding_sample.py to mbart_decoding_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/mbart_inference.py to mbart_inference.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/t5_export_model_sample.py to t5_export_model_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/mbart_export_model_sample.py to mbart_export_model_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/gpt_sample.py to gpt_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/t5_inference.py to t5_inference.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/plato_export_model_sample.py to plato_export_model_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/encoder_decoder_sample.py to encoder_decoder_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/bart_decoding_sample.py to bart_decoding_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/bart_inference.py to bart_inference.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/decoder_sample.py to decoder_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/gpt_export_model_sample.py to gpt_export_model_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/ops/fast_transformer/sample/decoding_sample.py to decoding_sample.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/predictor.py to predictor.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/server.py to server.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/base_router.py to base_router.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/token_model_handler.py to token_model_handler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/custom_model_handler.py to custom_model_handler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/taskflow_handler.py to taskflow_handler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/qa_model_handler.py to qa_model_handler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/cls_post_handler.py to cls_post_handler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/handlers/base_handler.py to base_handler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/model_manager.py to model_manager.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/taskflow_manager.py to taskflow_manager.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/http_router/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/server/http_router/router.py to router.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/verbalizer.py to verbalizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/prompt_trainer.py to prompt_trainer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/prompt_tokenizer.py to prompt_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/prompt_model.py to prompt_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/prompt_utils.py to prompt_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/prompt_args.py to prompt_args.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/prompt/template.py to template.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/bellegroup.py to bellegroup.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/dureader_robust.py to dureader_robust.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/cblue.py to cblue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/advertisegen.py to advertisegen.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/nlpcc13_evsam05_thu.py to nlpcc13_evsam05_thu.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/imdb.py to imdb.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/nlpcc_dbqa.py to nlpcc_dbqa.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/cail2018_small.py to cail2018_small.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/cote.py to cote.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/dataset.py to dataset.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/lcsts_new.py to lcsts_new.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/xnli_cn.py to xnli_cn.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/ptb.py to ptb.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/dureader_robust.py to dureader_robust.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/xfund_zh.py to xfund_zh.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/imdb.py to imdb.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/mt_eng_vietnamese.py to mt_eng_vietnamese.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/cote.py to cote.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/ptb_text_only.py to ptb_text_only.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/clue.py to clue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/cmrc2018.py to cmrc2018.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/msra_ner.py to msra_ner.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/docvqa_zh.py to docvqa_zh.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/funsd.py to funsd.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/squad_v2.py to squad_v2.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/chnsenticorp.py to chnsenticorp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/language_pair.py to language_pair.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/seabsa16.py to seabsa16.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/glue.py to glue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/xnli.py to xnli.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/duconv.py to duconv.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/cnn_dailymail.py to cnn_dailymail.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/rvl_cdip_sampled.py to rvl_cdip_sampled.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hf_datasets/squad.py to squad.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/wmt14ende.py to wmt14ende.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/wos.py to wos.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/chnsenticorp_v2.py to chnsenticorp_v2.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/thucnews.py to thucnews.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/clue.py to clue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/yahoo_answer_100k.py to yahoo_answer_100k.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/poetry.py to poetry.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/fewclue.py to fewclue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/nlpcc14_sc.py to nlpcc14_sc.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/lcqmc_v2.py to lcqmc_v2.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/bq_corpus.py to bq_corpus.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/nlpcc13_evsam05_hit.py to nlpcc13_evsam05_hit.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/couplet.py to couplet.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/cmrc2018.py to cmrc2018.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/iwslt15.py to iwslt15.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/paws-x.py to paws-x.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/msra_ner.py to msra_ner.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/intokens_dataset.py to intokens_dataset.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/drcd.py to drcd.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/dureader_qg.py to dureader_qg.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/dureader_yesno.py to dureader_yesno.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/hyp.py to hyp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/drcd_cn.py to drcd_cn.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/bstc.py to bstc.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/dureader_checklist.py to dureader_checklist.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/triviaqa.py to triviaqa.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/chnsenticorp.py to chnsenticorp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/lcqmc.py to lcqmc.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/seabsa16.py to seabsa16.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/glue.py to glue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/peoples_daily_ner.py to peoples_daily_ner.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/c3.py to c3.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/xnli.py to xnli.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/cail2019_scm.py to cail2019_scm.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/duconv.py to duconv.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/cnn_dailymail.py to cnn_dailymail.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/squad.py to squad.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/sighan-cn.py to sighan-cn.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/datasets/conll2002.py to conll2002.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/iterator.py to iterator.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/vocab.py to vocab.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/data_collator.py to data_collator.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/blendable_dataset.py to blendable_dataset.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/collate.py to collate.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/indexed_dataset.py to indexed_dataset.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/sampler.py to sampler.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/causal_dataset.py to causal_dataset.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/dist_dataloader.py to dist_dataloader.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/data/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rw/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gau_alpha/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/artist/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/sentencepiece_model_pb2.py to sentencepiece_model_pb2.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/fnet/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/convbert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/visualglm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/image_transforms.py to image_transforms.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformerv2/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/processing_utils.py to processing_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip/modeling_text.py to modeling_text.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ctrl/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blip_2/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlmv2/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot_small/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/sequence_parallel_utils.py to sequence_parallel_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/configuration_utils.py to configuration_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm/visual_backbone.py to visual_backbone.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutxlm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/image_processing_utils.py to image_processing_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/transformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/transformer/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/feature_extraction.py to feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_vil/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bigbird/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama/modeling_auto.py to modeling_auto.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama/modeling_pp.py to modeling_pp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/llama/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gen/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gen/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert/fast_tokenizer.py to fast_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert/fast_tokenizer.py to fast_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tinybert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/funnel/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet/converter.py to converter.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlnet/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/attention_utils.py to attention_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/minigpt4/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/blenderbot/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta/converter.py to converter.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roberta/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt/convert_torch_to_paddle.py to convert_torch_to_paddle.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/opt/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/luke/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/distilbert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2/chatglm-legacy-checkpoints-convert.py to chatglm-legacy-checkpoints-convert.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm_v2/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/optimization.py to optimization.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5/converter.py to converter.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mt5/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/roformer/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie/fast_tokenizer.py to fast_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chinesebert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tokenizer_utils_fast.py to tokenizer_utils_fast.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_ctm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/convert_slow_tokenizer.py to convert_slow_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/segment_parallel_utils.py to segment_parallel_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/codegen/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer/convert.py to convert.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unified_transformer/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mobilebert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/model_outputs.py to model_outputs.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tokenizer_utils_base.py to tokenizer_utils_base.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dpt/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ppminilm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/t5/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/auto/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/model_utils.py to model_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen/modeling_pp.py to modeling_pp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/qwen/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/conversion_utils.py to conversion_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/squeezebert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap/feature_extraction.py to feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clap/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/tokenizer_utils.py to tokenizer_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout/visual_backbone.py to visual_backbone.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_layout/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/megatronbert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/activations.py to activations.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/converter.py to converter.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/feature_extraction.py to feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chineseclip/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/xlm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/semantic_search/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/semantic_search/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nezha/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom/processor.py to processor.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bloom/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/unimo/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/feature_extraction_sequence_utils.py to feature_extraction_sequence_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clipseg/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/audio_utils.py to audio_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m/fast_tokenizer.py to fast_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_m/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram/matching_param_name.py to matching_param_name.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_gram/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/chatglm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_code/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt/modeling_pp.py to modeling_pp.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gpt/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/dallebart/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra/converter.py to converter.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/electra/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/export.py to export.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ernie_doc/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/feature_extraction_utils.py to feature_extraction_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert_japanese/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bert_japanese/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/albert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/pegasus/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/gptj/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/layoutlm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/distill_utils.py to distill_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/prophetnet/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bart/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/aistudio_utils.py to aistudio_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/rembert/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mpnet/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/feature_extraction.py to feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/clip/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/skep/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit/image_processing.py to image_processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/bit/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5/processing.py to processing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5/feature_extraction.py to feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/speecht5/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/image_utils.py to image_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/reformer/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/ofa_utils.py to ofa_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer/fast_tokenizer.py to fast_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/nystromformer/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/glm/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart/configuration.py to configuration.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/transformers/mbart/tokenizer.py to tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/ernie_model.py to ernie_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp/text_classification.py to text_classification.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/autonlp/auto_trainer_base.py to auto_trainer_base.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/model_utils.py to model_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/fused_transformer_layers.py to fused_transformer_layers.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/llama/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/llama/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/opt/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/opt/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm_v2/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm_v2/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/generation_utils.py to generation_utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/bloom/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/bloom/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/chatglm/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/gpt/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/transformers/gpt/modeling.py to modeling.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/experimental/faster_tokenizer.py to faster_tokenizer.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text_summarization.py to text_summarization.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/dialogue.py to dialogue.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text_classification.py to text_classification.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text_generation.py to text_generation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/fill_mask.py to fill_mask.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text_feature_extraction.py to text_feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/zero_shot_text_classification.py to zero_shot_text_classification.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/lexical_analysis.py to lexical_analysis.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/taskflow.py to taskflow.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text_similarity.py to text_similarity.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text_correction.py to text_correction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/task.py to task.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models/lexical_analysis_model.py to lexical_analysis_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models/text_correction_model.py to text_correction_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models/sentiment_analysis_model.py to sentiment_analysis_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/models/dependency_parsing_model.py to dependency_parsing_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/sentiment_analysis.py to sentiment_analysis.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/word_segmentation.py to word_segmentation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/named_entity_recognition.py to named_entity_recognition.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/question_answering.py to question_answering.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/information_extraction.py to information_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/knowledge_mining.py to knowledge_mining.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/pos_tagging.py to pos_tagging.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/question_generation.py to question_generation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/code_generation.py to code_generation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/document_intelligence.py to document_intelligence.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/multimodal_feature_extraction.py to multimodal_feature_extraction.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/dependency_parsing.py to dependency_parsing.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/text2text_generation.py to text2text_generation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/taskflow/poetry_generation.py to poetry_generation.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/dataaug/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/dataaug/char.py to char.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/dataaug/base_augment.py to base_augment.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/dataaug/sentence.py to sentence.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/dataaug/word.py to word.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/seq2vec/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/seq2vec/encoder.py to encoder.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/server.py to server.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/bos_community.py to bos_community.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/main.py to main.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/utils/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/utils/tabulate.py to tabulate.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/install.py to install.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/cli/download.py to download.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix/utils.py to utils.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix/prefix_config.py to prefix_config.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/prefix/prefix_model.py to prefix_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/lora/lora_layers.py to lora_layers.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/lora/__init__.py to __init__.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/lora/lora_model.py to lora_model.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/lora/lora_quantization_layers.py to lora_quantization_layers.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/lora/lora_quant_layers.py to lora_quant_layers.cpython-39.pyc
byte-compiling build/bdist.linux-x86_64/egg/paddlenlp/peft/lora/lora_config.py to lora_config.cpython-39.pyc
creating build/bdist.linux-x86_64/egg/EGG-INFO
copying paddlenlp.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO
copying paddlenlp.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying paddlenlp.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying paddlenlp.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying paddlenlp.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
copying paddlenlp.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO
zip_safe flag not set; analyzing archive contents...
paddlenlp.datasets.__pycache__.dataset.cpython-39: module references __path__
paddlenlp.experimental.transformers.llama.__pycache__.modeling.cpython-39: module references __file__
paddlenlp.ops.__pycache__.ext_utils.cpython-39: module references __file__
paddlenlp.trainer.__pycache__.trainer.cpython-39: module MAY be using inspect.stack
paddlenlp.transformers.__pycache__.configuration_utils.cpython-39: module references __file__
paddlenlp.transformers.__pycache__.utils.cpython-39: module MAY be using inspect.stack
paddlenlp.transformers.auto.__pycache__.configuration.cpython-39: module references __file__
paddlenlp.transformers.layoutxlm.__pycache__.visual_backbone.cpython-39: module references __file__
creating dist
creating 'dist/paddlenlp-2.6.1.post0-py3.9.egg' and adding 'build/bdist.linux-x86_64/egg' to it
removing 'build/bdist.linux-x86_64/egg' (and everything under it)
Processing paddlenlp-2.6.1.post0-py3.9.egg
creating /opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg
Extracting paddlenlp-2.6.1.post0-py3.9.egg to /opt/py39/lib/python3.9/site-packages
Adding paddlenlp 2.6.1.post0 to easy-install.pth file
Installing paddlenlp script to /opt/py39/bin

Installed /opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg
Processing dependencies for paddlenlp==2.6.1.post0
Searching for protobuf==4.25.1
Best match: protobuf 4.25.1
Adding protobuf 4.25.1 to easy-install.pth file
detected new path './paddlenlp-2.6.1.post0-py3.9.egg'

Using /opt/py39/lib/python3.9/site-packages
Searching for Jinja2==3.1.2
Best match: Jinja2 3.1.2
Adding Jinja2 3.1.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for aistudio-sdk==0.1.5
Best match: aistudio-sdk 0.1.5
Adding aistudio-sdk 0.1.5 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for tool-helpers==0.1.1
Best match: tool-helpers 0.1.1
Adding tool-helpers 0.1.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for safetensors==0.4.1
Best match: safetensors 0.4.1
Adding safetensors 0.4.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for rich==13.7.0
Best match: rich 13.7.0
Adding rich 13.7.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for typer==0.9.0
Best match: typer 0.9.0
Adding typer 0.9.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for uvicorn==0.24.0.post1
Best match: uvicorn 0.24.0.post1
Adding uvicorn 0.24.0.post1 to easy-install.pth file
Installing uvicorn script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for fastapi==0.105.0
Best match: fastapi 0.105.0
Adding fastapi 0.105.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for visualdl==2.5.3
Best match: visualdl 2.5.3
Adding visualdl 2.5.3 to easy-install.pth file
Installing visualdl script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for flask-babel==4.0.0
Best match: flask-babel 4.0.0
Adding flask-babel 4.0.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for paddle2onnx==1.1.0
Best match: paddle2onnx 1.1.0
Adding paddle2onnx 1.1.0 to easy-install.pth file
Installing paddle2onnx script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for onnx==1.15.0
Best match: onnx 1.15.0
Adding onnx 1.15.0 to easy-install.pth file
Installing backend-test-tools script to /opt/py39/bin
Installing check-model script to /opt/py39/bin
Installing check-node script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for huggingface-hub==0.20.1
Best match: huggingface-hub 0.20.1
Adding huggingface-hub 0.20.1 to easy-install.pth file
Installing huggingface-cli script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for sentencepiece==0.1.99
Best match: sentencepiece 0.1.99
Adding sentencepiece 0.1.99 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for paddlefsl==1.1.0
Best match: paddlefsl 1.1.0
Adding paddlefsl 1.1.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for tqdm==4.66.1
Best match: tqdm 4.66.1
Adding tqdm 4.66.1 to easy-install.pth file
Installing tqdm script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for datasets==2.15.0
Best match: datasets 2.15.0
Adding datasets 2.15.0 to easy-install.pth file
Installing datasets-cli script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for multiprocess==0.70.12.2
Best match: multiprocess 0.70.12.2
Adding multiprocess 0.70.12.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for dill==0.3.4
Best match: dill 0.3.4
Adding dill 0.3.4 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for seqeval==1.2.2
Best match: seqeval 1.2.2
Adding seqeval 1.2.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for colorama==0.4.6
Best match: colorama 0.4.6
Adding colorama 0.4.6 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for colorlog==6.8.0
Best match: colorlog 6.8.0
Adding colorlog 6.8.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for jieba==0.42.1
Best match: jieba 0.42.1
Adding jieba 0.42.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for MarkupSafe==2.1.3
Best match: MarkupSafe 2.1.3
Adding MarkupSafe 2.1.3 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for requests==2.31.0
Best match: requests 2.31.0
Adding requests 2.31.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pybind11==2.11.1
Best match: pybind11 2.11.1
Adding pybind11 2.11.1 to easy-install.pth file
Installing pybind11-config script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for numpy==1.26.2
Best match: numpy 1.26.2
Adding numpy 1.26.2 to easy-install.pth file
Installing f2py script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for Pygments==2.16.1
Best match: Pygments 2.16.1
Adding Pygments 2.16.1 to easy-install.pth file
Installing pygmentize script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for markdown-it-py==2.2.0
Best match: markdown-it-py 2.2.0
Adding markdown-it-py 2.2.0 to easy-install.pth file
Installing markdown-it script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for typing-extensions==4.9.0
Best match: typing-extensions 4.9.0
Adding typing-extensions 4.9.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for click==8.1.6
Best match: click 8.1.6
Adding click 8.1.6 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for h11==0.14.0
Best match: h11 0.14.0
Adding h11 0.14.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for starlette==0.27.0
Best match: starlette 0.27.0
Adding starlette 0.27.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pydantic==2.1.1
Best match: pydantic 2.1.1
Adding pydantic 2.1.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for anyio==3.7.1
Best match: anyio 3.7.1
Adding anyio 3.7.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for psutil==5.9.5
Best match: psutil 5.9.5
Adding psutil 5.9.5 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for rarfile==4.0
Best match: rarfile 4.0
Adding rarfile 4.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for packaging==23.1
Best match: packaging 23.1
Adding packaging 23.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pandas==2.0.3
Best match: pandas 2.0.3
Adding pandas 2.0.3 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for matplotlib==3.7.2
Best match: matplotlib 3.7.2
Adding matplotlib 3.7.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for six==1.16.0
Best match: six 1.16.0
Adding six 1.16.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for Pillow==10.1.0
Best match: Pillow 10.1.0
Adding Pillow 10.1.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for Flask==2.3.2
Best match: Flask 2.3.2
Adding Flask 2.3.2 to easy-install.pth file
Installing flask script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for bce-python-sdk==0.8.87
Best match: bce-python-sdk 0.8.87
Adding bce-python-sdk 0.8.87 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pytz==2023.3
Best match: pytz 2023.3
Adding pytz 2023.3 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for Babel==2.12.1
Best match: Babel 2.12.1
Adding Babel 2.12.1 to easy-install.pth file
Installing pybabel script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for PyYAML==6.0.1
Best match: PyYAML 6.0.1
Adding PyYAML 6.0.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for fsspec==2023.6.0
Best match: fsspec 2023.6.0
Adding fsspec 2023.6.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for filelock==3.12.2
Best match: filelock 3.12.2
Adding filelock 3.12.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for aiohttp==3.8.5
Best match: aiohttp 3.8.5
Adding aiohttp 3.8.5 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for xxhash==3.4.1
Best match: xxhash 3.4.1
Adding xxhash 3.4.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pyarrow-hotfix==0.6
Best match: pyarrow-hotfix 0.6
Adding pyarrow-hotfix 0.6 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pyarrow==14.0.2
Best match: pyarrow 14.0.2
Adding pyarrow 14.0.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for scikit-learn==1.3.0
Best match: scikit-learn 1.3.0
Adding scikit-learn 1.3.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for certifi==2023.11.17
Best match: certifi 2023.11.17
Adding certifi 2023.11.17 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for urllib3==2.0.4
Best match: urllib3 2.0.4
Adding urllib3 2.0.4 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for idna==3.6
Best match: idna 3.6
Adding idna 3.6 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for charset-normalizer==3.2.0
Best match: charset-normalizer 3.2.0
Adding charset-normalizer 3.2.0 to easy-install.pth file
Installing normalizer script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for mdurl==0.1.2
Best match: mdurl 0.1.2
Adding mdurl 0.1.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pydantic-core==2.4.0
Best match: pydantic-core 2.4.0
Adding pydantic-core 2.4.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for annotated-types==0.5.0
Best match: annotated-types 0.5.0
Adding annotated-types 0.5.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for exceptiongroup==1.2.0
Best match: exceptiongroup 1.2.0
Adding exceptiongroup 1.2.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for sniffio==1.3.0
Best match: sniffio 1.3.0
Adding sniffio 1.3.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for tzdata==2023.3
Best match: tzdata 2023.3
Adding tzdata 2023.3 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for python-dateutil==2.8.2
Best match: python-dateutil 2.8.2
Adding python-dateutil 2.8.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for importlib-resources==6.0.1
Best match: importlib-resources 6.0.1
Adding importlib-resources 6.0.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for pyparsing==3.0.9
Best match: pyparsing 3.0.9
Adding pyparsing 3.0.9 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for kiwisolver==1.4.4
Best match: kiwisolver 1.4.4
Adding kiwisolver 1.4.4 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for fonttools==4.42.0
Best match: fonttools 4.42.0
Adding fonttools 4.42.0 to easy-install.pth file
Installing fonttools script to /opt/py39/bin
Installing pyftmerge script to /opt/py39/bin
Installing pyftsubset script to /opt/py39/bin
Installing ttx script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for cycler==0.11.0
Best match: cycler 0.11.0
Adding cycler 0.11.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for contourpy==1.1.0
Best match: contourpy 1.1.0
Adding contourpy 1.1.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for importlib-metadata==6.8.0
Best match: importlib-metadata 6.8.0
Adding importlib-metadata 6.8.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for blinker==1.6.2
Best match: blinker 1.6.2
Adding blinker 1.6.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for itsdangerous==2.1.2
Best match: itsdangerous 2.1.2
Adding itsdangerous 2.1.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for Werkzeug==2.3.6
Best match: Werkzeug 2.3.6
Adding Werkzeug 2.3.6 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for future==0.18.3
Best match: future 0.18.3
Adding future 0.18.3 to easy-install.pth file
Installing futurize script to /opt/py39/bin
Installing pasteurize script to /opt/py39/bin

Using /opt/py39/lib/python3.9/site-packages
Searching for pycryptodome==3.18.0
Best match: pycryptodome 3.18.0
Adding pycryptodome 3.18.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for aiosignal==1.3.1
Best match: aiosignal 1.3.1
Adding aiosignal 1.3.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for frozenlist==1.4.0
Best match: frozenlist 1.4.0
Adding frozenlist 1.4.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for yarl==1.9.2
Best match: yarl 1.9.2
Adding yarl 1.9.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for async-timeout==4.0.2
Best match: async-timeout 4.0.2
Adding async-timeout 4.0.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for multidict==6.0.4
Best match: multidict 6.0.4
Adding multidict 6.0.4 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for attrs==23.1.0
Best match: attrs 23.1.0
Adding attrs 23.1.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for threadpoolctl==3.2.0
Best match: threadpoolctl 3.2.0
Adding threadpoolctl 3.2.0 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for joblib==1.3.1
Best match: joblib 1.3.1
Adding joblib 1.3.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for scipy==1.11.1
Best match: scipy 1.11.1
Adding scipy 1.11.1 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Searching for zipp==3.16.2
Best match: zipp 3.16.2
Adding zipp 3.16.2 to easy-install.pth file

Using /opt/py39/lib/python3.9/site-packages
Finished processing dependencies for paddlenlp==2.6.1.post0

+ echo ''
+ [[ PaddleNLP == \P\a\d\d\l\e\V\i\d\e\o ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\N\L\P ]]
+ python -m pip install -e ./ppdiffusers
Looking in indexes: https://mirror.baidu.com/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Obtaining file:///workspace/PaddleNLP/ppdiffusers
ERROR: file:///workspace/PaddleNLP/ppdiffusers does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.
+ cd tests
+ rm --rf 'test_tipc/output/*'
rm: unrecognized option '--rf'
Try 'rm --help' for more information.
+ sed -i 's/wget /wget -nv /g' test_tipc/prepare.sh
+ cp /workspace/model_list.py ./
+ cp -r /workspace/configs/ ./
+ cp /workspace/report.py ./
+ [[ PaddleNLP == \P\a\d\d\l\e\O\C\R ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
+ echo ''
+ touch TIMEOUT

+ [[ PaddleNLP == \P\a\d\d\l\e\R\e\c ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\O\C\R ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\G\A\N ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\S\e\g ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\D\e\t\e\c\t\i\o\n ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\V\i\d\e\o ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\N\L\P ]]
+ grep_models='bigru_crf|ernie_information_extraction|bert_base_text_cls|bert_for_question_answering|ernie_text_cls|ernie_text_matching|ernie_tiny|ernie_information_extraction|ernie3_for_sequence_classification|seq2seq|xlnet'
+ touch full_chain_list_all_tmp
+ touch full_chain_list_all
+ mode=lite_train_lite_infer
+ time_out=3600
+ [[ PaddleNLP == \P\a\d\d\l\e\D\e\t\e\c\t\i\o\n ]]
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
+ file_txt='*train_infer_python.txt*'
+ python model_list.py PaddleNLP /workspace/PaddleNLP/tests/test_tipc/configs/ '*train_infer_python.txt*' full_chain_list_all_tmp
+ rm -rf 'core.*'
+ rm -rf 'kernel_meta*'
+ rm -rf 'test_tipc/output/*'
+ '[' '!' 'bigru_crf|ernie_information_extraction|bert_base_text_cls|bert_for_question_answering|ernie_text_cls|ernie_text_matching|ernie_tiny|ernie_information_extraction|ernie3_for_sequence_classification|seq2seq|xlnet' ']'
+ '[' '!' ']'
+ grep_v_models=undefined
+ [[ bigru_crf|ernie_information_extraction|bert_base_text_cls|bert_for_question_answering|ernie_text_cls|ernie_text_matching|ernie_tiny|ernie_information_extraction|ernie3_for_sequence_classification|seq2seq|xlnet =~ undefined ]]
+ [[ undefined =~ undefined ]]
+ cat full_chain_list_all_tmp
+ sort
+ uniq
+ grep -E 'bigru_crf|ernie_information_extraction|bert_base_text_cls|bert_for_question_answering|ernie_text_cls|ernie_text_matching|ernie_tiny|ernie_information_extraction|ernie3_for_sequence_classification|seq2seq|xlnet'
+ cat full_chain_list_all
test_tipc/configs/bert_base_text_cls/train_infer_python.txt
test_tipc/configs/bert_for_question_answering/train_infer_python.txt
test_tipc/configs/bigru_crf/train_infer_python.txt
test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
test_tipc/configs/ernie_information_extraction/train_infer_python.txt
test_tipc/configs/ernie_text_cls/train_infer_python.txt
test_tipc/configs/ernie_text_matching/train_infer_python.txt
test_tipc/configs/ernie_tiny/train_infer_python.txt
test_tipc/configs/seq2seq/train_infer_python.txt
test_tipc/configs/xlnet/train_infer_python.txt
+ cat full_chain_list_all
+ read config_file
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703125051
+ echo ==START==test_tipc/configs/bert_base_text_cls/train_infer_python.txt
==START==test_tipc/configs/bert_base_text_cls/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/bert_base_text_cls/train_infer_python.txt
+ dataline='===========================train_params=========================== 
model_name:bert_base_text_cls
python:python
gpu_list:0|0,1
--device:gpu|gpu
null:null
--epoch:lite_train_lite_infer=1|lite_train_whole_infer=1|whole_train_whole_infer=3
--save_dir:null
--batch_size:lite_train_lite_infer=32|lite_train_whole_infer=32|whole_train_whole_infer=32
null:null
null:model
null:null
null:null
##
trainer:norm
norm_train:./test_tipc/bert_base_text_cls/train.py --max_steps 150
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
--output_path:null
--params_path:null
norm_export:./test_tipc/bert_base_text_cls/export_model.py
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:bert_base_text_cls
++ strs=model_name:bert_base_text_cls
++ IFS=:
++ array=(${strs})
++ tmp=bert_base_text_cls
++ echo bert_base_text_cls
+ model_name=bert_base_text_cls
+ sleep 10
+ run run_model test_tipc/configs/bert_base_text_cls/train_infer_python.txt lite_train_lite_infer 3600 bert_base_text_cls
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ waitfor=7200
+ command='run_model
test_tipc/configs/bert_base_text_cls/train_infer_python.txt
lite_train_lite_infer
3600
bert_base_text_cls'
+ commandpid=200
+ run_model test_tipc/configs/bert_base_text_cls/train_infer_python.txt lite_train_lite_infer 3600 bert_base_text_cls
+ config_file=test_tipc/configs/bert_base_text_cls/train_infer_python.txt
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/bert_base_text_cls/train_infer_python.txt lite_train_lite_infer
+ watchdog=201
+ wait 200
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/bert_base_text_cls/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/bert_base_text_cls/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
  0%|          | 0/7265 [00:00<?, ?it/s]  0%|          | 19/7265 [00:00<00:40, 179.72it/s]  1%|          | 67/7265 [00:00<00:25, 287.06it/s]  2%|▏         | 115/7265 [00:00<00:20, 340.74it/s]  2%|▏         | 165/7265 [00:00<00:17, 396.27it/s]  3%|▎         | 227/7265 [00:00<00:15, 444.59it/s]  4%|▍         | 291/7265 [00:00<00:14, 491.79it/s]  5%|▍         | 355/7265 [00:00<00:13, 523.95it/s]  6%|▌         | 435/7265 [00:00<00:11, 583.57it/s]  7%|▋         | 515/7265 [00:01<00:10, 629.04it/s]  8%|▊         | 611/7265 [00:01<00:09, 700.19it/s] 10%|▉         | 707/7265 [00:01<00:08, 752.30it/s] 11%|█         | 803/7265 [00:01<00:08, 793.12it/s] 13%|█▎        | 915/7265 [00:01<00:07, 860.50it/s] 14%|█▍        | 1027/7265 [00:01<00:06, 909.72it/s] 16%|█▌        | 1155/7265 [00:01<00:06, 983.50it/s] 18%|█▊        | 1283/7265 [00:01<00:05, 1041.39it/s] 19%|█▉        | 1411/7265 [00:01<00:05, 1077.37it/s] 21%|██▏       | 1554/7265 [00:02<00:04, 1143.36it/s] 23%|██▎       | 1698/7265 [00:02<00:04, 1199.76it/s] 26%|██▌       | 1858/7265 [00:02<00:04, 1278.06it/s] 28%|██▊       | 2018/7265 [00:02<00:03, 1331.09it/s] 30%|██▉       | 2178/7265 [00:02<00:03, 1367.09it/s] 32%|███▏      | 2354/7265 [00:02<00:03, 1435.04it/s] 35%|███▌      | 2546/7265 [00:02<00:03, 1527.67it/s] 38%|███▊      | 2738/7265 [00:02<00:02, 1586.82it/s] 40%|████      | 2930/7265 [00:02<00:02, 1629.81it/s] 43%|████▎     | 3122/7265 [00:02<00:02, 1682.14it/s] 46%|████▌     | 3330/7265 [00:03<00:02, 1744.58it/s] 49%|████▊     | 3538/7265 [00:03<00:02, 1820.42it/s] 52%|█████▏    | 3746/7265 [00:03<00:01, 1863.52it/s] 55%|█████▍    | 3970/7265 [00:03<00:01, 1950.42it/s] 58%|█████▊    | 4194/7265 [00:03<00:01, 1975.43it/s] 61%|██████    | 4434/7265 [00:03<00:01, 2038.62it/s] 64%|██████▍   | 4674/7265 [00:03<00:01, 2090.28it/s] 68%|██████▊   | 4914/7265 [00:03<00:01, 2177.32it/s] 71%|███████   | 5133/7265 [00:03<00:00, 2174.31it/s] 74%|███████▎  | 5351/7265 [00:04<00:00, 2154.58it/s] 77%|███████▋  | 5618/7265 [00:04<00:00, 2238.81it/s] 81%|████████  | 5890/7265 [00:04<00:00, 2321.53it/s] 85%|████████▌ | 6178/7265 [00:04<00:00, 2410.73it/s] 89%|████████▉ | 6466/7265 [00:04<00:00, 2470.78it/s] 93%|█████████▎| 6754/7265 [00:04<00:00, 2522.95it/s] 97%|█████████▋| 7058/7265 [00:04<00:00, 2593.46it/s]100%|██████████| 7265/7265 [00:04<00:00, 1524.11it/s]
[32m[2023-12-21 10:17:50,310] [    INFO][0m - We are using <class 'paddlenlp.transformers.bert.modeling.BertForSequenceClassification'> to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:17:50,312] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/bert-base-uncased/config.json[0m
[32m[2023-12-21 10:17:50,456] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/bert-base-uncased.pdparams[0m
[32m[2023-12-21 10:17:50,457] [    INFO][0m - Downloading bert-base-uncased.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/bert-base-uncased.pdparams[0m
  0%|          | 0.00/775M [00:00<?, ?B/s]  0%|          | 35.0k/775M [00:00<43:38, 310kB/s]  0%|          | 83.0k/775M [00:00<35:46, 378kB/s]  0%|          | 147k/775M [00:00<29:25, 460kB/s]   0%|          | 227k/775M [00:00<24:16, 557kB/s]  0%|          | 307k/775M [00:00<22:07, 612kB/s]  0%|          | 403k/775M [00:00<19:31, 693kB/s]  0%|          | 515k/775M [00:00<17:17, 783kB/s]  0%|          | 643k/775M [00:00<15:17, 884kB/s]  0%|          | 771k/775M [00:01<13:58, 968kB/s]  0%|          | 915k/775M [00:01<12:46, 1.06MB/s]  0%|          | 1.02M/775M [00:01<12:02, 1.12MB/s]  0%|          | 1.16M/775M [00:01<11:08, 1.21MB/s]  0%|          | 1.30M/775M [00:01<10:31, 1.28MB/s]  0%|          | 1.47M/775M [00:01<09:50, 1.37MB/s]  0%|          | 1.66M/775M [00:01<08:55, 1.51MB/s]  0%|          | 1.82M/775M [00:01<08:41, 1.55MB/s]  0%|          | 2.02M/775M [00:01<07:55, 1.70MB/s]  0%|          | 2.19M/775M [00:01<07:49, 1.73MB/s]  0%|          | 2.42M/775M [00:02<07:16, 1.86MB/s]  0%|          | 2.66M/775M [00:02<06:44, 2.00MB/s]  0%|          | 2.86M/775M [00:02<06:51, 1.97MB/s]  0%|          | 3.11M/775M [00:02<06:34, 2.05MB/s]  0%|          | 3.38M/775M [00:02<06:13, 2.16MB/s]  0%|          | 3.66M/775M [00:02<05:55, 2.27MB/s]  1%|          | 3.96M/775M [00:02<05:37, 2.39MB/s]  1%|          | 4.25M/775M [00:02<05:25, 2.48MB/s]  1%|          | 4.56M/775M [00:03<05:12, 2.58MB/s]  1%|          | 4.89M/775M [00:03<05:00, 2.69MB/s]  1%|          | 5.19M/775M [00:03<04:48, 2.79MB/s]  1%|          | 5.46M/775M [00:03<04:48, 2.79MB/s]  1%|          | 5.78M/775M [00:03<04:41, 2.86MB/s]  1%|          | 6.13M/775M [00:03<04:23, 3.06MB/s]  1%|          | 6.42M/775M [00:03<04:23, 3.05MB/s]  1%|          | 6.78M/775M [00:03<04:07, 3.26MB/s]  1%|          | 7.10M/775M [00:03<04:08, 3.24MB/s]  1%|          | 7.49M/775M [00:03<03:52, 3.46MB/s]  1%|          | 7.83M/775M [00:04<03:52, 3.46MB/s]  1%|          | 8.24M/775M [00:04<03:38, 3.68MB/s]  1%|          | 8.59M/775M [00:04<03:46, 3.55MB/s]  1%|          | 9.02M/775M [00:04<03:41, 3.63MB/s]  1%|          | 9.47M/775M [00:04<03:33, 3.76MB/s]  1%|▏         | 9.94M/775M [00:04<03:25, 3.90MB/s]  1%|▏         | 10.4M/775M [00:04<03:20, 4.00MB/s]  1%|▏         | 10.9M/775M [00:04<03:06, 4.29MB/s]  1%|▏         | 11.3M/775M [00:04<03:11, 4.18MB/s]  2%|▏         | 11.8M/775M [00:05<03:09, 4.23MB/s]  2%|▏         | 12.3M/775M [00:05<02:57, 4.51MB/s]  2%|▏         | 12.7M/775M [00:05<03:00, 4.42MB/s]  2%|▏         | 13.2M/775M [00:05<02:52, 4.62MB/s]  2%|▏         | 13.6M/775M [00:05<03:24, 3.90MB/s]  2%|▏         | 14.0M/775M [00:05<04:21, 3.05MB/s]  2%|▏         | 14.4M/775M [00:05<05:26, 2.44MB/s]  2%|▏         | 14.6M/775M [00:06<05:19, 2.49MB/s]  2%|▏         | 15.0M/775M [00:06<04:53, 2.72MB/s]  2%|▏         | 15.4M/775M [00:06<04:13, 3.14MB/s]  2%|▏         | 15.9M/775M [00:06<03:45, 3.52MB/s]  2%|▏         | 16.5M/775M [00:06<03:08, 4.22MB/s]  2%|▏         | 16.9M/775M [00:06<03:02, 4.35MB/s]  2%|▏         | 17.5M/775M [00:06<02:43, 4.85MB/s]  2%|▏         | 18.0M/775M [00:06<02:40, 4.94MB/s]  2%|▏         | 18.6M/775M [00:06<02:29, 5.29MB/s]  2%|▏         | 19.2M/775M [00:07<02:29, 5.30MB/s]  3%|▎         | 19.8M/775M [00:07<02:21, 5.61MB/s]  3%|▎         | 20.3M/775M [00:07<02:23, 5.50MB/s]  3%|▎         | 20.9M/775M [00:07<02:18, 5.70MB/s]  3%|▎         | 21.5M/775M [00:07<02:15, 5.82MB/s]  3%|▎         | 22.1M/775M [00:07<02:11, 6.01MB/s]  3%|▎         | 22.8M/775M [00:07<02:10, 6.03MB/s]  3%|▎         | 23.4M/775M [00:07<02:06, 6.21MB/s]  3%|▎         | 24.0M/775M [00:07<02:06, 6.21MB/s]  3%|▎         | 24.6M/775M [00:07<02:04, 6.34MB/s]  3%|▎         | 25.3M/775M [00:08<02:01, 6.44MB/s]  3%|▎         | 26.0M/775M [00:08<01:59, 6.55MB/s]  3%|▎         | 26.6M/775M [00:08<01:58, 6.64MB/s]  4%|▎         | 27.3M/775M [00:08<01:57, 6.65MB/s]  4%|▎         | 28.0M/775M [00:08<01:53, 6.91MB/s]  4%|▎         | 28.6M/775M [00:08<01:55, 6.78MB/s]  4%|▍         | 29.3M/775M [00:08<01:51, 7.00MB/s]  4%|▍         | 30.0M/775M [00:08<01:52, 6.95MB/s]  4%|▍         | 30.8M/775M [00:08<01:48, 7.19MB/s]  4%|▍         | 31.5M/775M [00:08<01:49, 7.12MB/s]  4%|▍         | 32.3M/775M [00:09<01:43, 7.54MB/s]  4%|▍         | 33.0M/775M [00:09<01:42, 7.59MB/s]  4%|▍         | 33.8M/775M [00:09<01:42, 7.58MB/s]  4%|▍         | 34.7M/775M [00:09<01:34, 8.18MB/s]  5%|▍         | 35.5M/775M [00:09<01:33, 8.27MB/s]  5%|▍         | 36.5M/775M [00:09<01:28, 8.73MB/s]  5%|▍         | 37.5M/775M [00:09<01:23, 9.24MB/s]  5%|▍         | 38.5M/775M [00:09<01:21, 9.50MB/s]  5%|▌         | 39.5M/775M [00:09<01:19, 9.75MB/s]  5%|▌         | 40.7M/775M [00:10<01:15, 10.2MB/s]  5%|▌         | 41.8M/775M [00:10<01:12, 10.6MB/s]  6%|▌         | 43.0M/775M [00:10<01:09, 11.1MB/s]  6%|▌         | 44.1M/775M [00:10<01:14, 10.3MB/s]  6%|▌         | 45.1M/775M [00:10<01:17, 9.81MB/s]  6%|▌         | 46.0M/775M [00:10<01:19, 9.60MB/s]  6%|▌         | 47.0M/775M [00:10<01:18, 9.77MB/s]  6%|▌         | 47.9M/775M [00:10<01:20, 9.45MB/s]  6%|▋         | 48.9M/775M [00:10<01:20, 9.44MB/s]  6%|▋         | 49.9M/775M [00:10<01:18, 9.68MB/s]  7%|▋         | 50.8M/775M [00:11<01:18, 9.61MB/s]  7%|▋         | 51.8M/775M [00:11<01:17, 9.78MB/s]  7%|▋         | 52.8M/775M [00:11<01:16, 9.83MB/s]  7%|▋         | 53.8M/775M [00:11<01:14, 10.2MB/s]  7%|▋         | 54.8M/775M [00:11<01:14, 10.1MB/s]  7%|▋         | 55.8M/775M [00:11<01:12, 10.4MB/s]  7%|▋         | 56.9M/775M [00:11<01:12, 10.4MB/s]  7%|▋         | 57.9M/775M [00:11<01:10, 10.6MB/s]  8%|▊         | 59.0M/775M [00:11<01:10, 10.7MB/s]  8%|▊         | 60.1M/775M [00:12<01:09, 10.8MB/s]  8%|▊         | 61.2M/775M [00:12<01:07, 11.0MB/s]  8%|▊         | 62.2M/775M [00:12<01:07, 11.0MB/s]  8%|▊         | 63.3M/775M [00:12<01:07, 11.1MB/s]  8%|▊         | 64.4M/775M [00:12<01:07, 11.0MB/s]  8%|▊         | 65.5M/775M [00:12<01:06, 11.2MB/s]  9%|▊         | 66.6M/775M [00:12<01:05, 11.3MB/s]  9%|▊         | 67.8M/775M [00:12<01:05, 11.4MB/s]  9%|▉         | 68.9M/775M [00:12<01:05, 11.2MB/s]  9%|▉         | 70.0M/775M [00:12<01:11, 10.3MB/s]  9%|▉         | 70.9M/775M [00:13<01:15, 9.77MB/s]  9%|▉         | 71.9M/775M [00:13<01:18, 9.43MB/s]  9%|▉         | 72.8M/775M [00:13<01:21, 9.06MB/s] 10%|▉         | 73.7M/775M [00:13<01:23, 8.81MB/s] 10%|▉         | 74.6M/775M [00:13<01:20, 9.15MB/s] 10%|▉         | 75.5M/775M [00:13<01:22, 8.91MB/s] 10%|▉         | 76.4M/775M [00:13<01:22, 8.83MB/s] 10%|▉         | 77.3M/775M [00:13<01:20, 9.08MB/s] 10%|█         | 78.2M/775M [00:13<01:21, 8.95MB/s] 10%|█         | 79.2M/775M [00:14<01:17, 9.38MB/s] 10%|█         | 80.1M/775M [00:14<01:19, 9.11MB/s] 10%|█         | 81.0M/775M [00:14<01:19, 9.13MB/s] 11%|█         | 82.0M/775M [00:14<01:18, 9.29MB/s] 11%|█         | 82.9M/775M [00:14<01:17, 9.33MB/s] 11%|█         | 83.9M/775M [00:14<01:16, 9.43MB/s] 11%|█         | 84.8M/775M [00:14<01:15, 9.57MB/s] 11%|█         | 85.8M/775M [00:14<01:15, 9.62MB/s] 11%|█         | 86.7M/775M [00:14<01:14, 9.71MB/s] 11%|█▏        | 87.7M/775M [00:14<01:13, 9.77MB/s] 11%|█▏        | 88.7M/775M [00:15<01:12, 9.92MB/s] 12%|█▏        | 89.7M/775M [00:15<01:12, 9.92MB/s] 12%|█▏        | 90.6M/775M [00:15<01:11, 10.1MB/s] 12%|█▏        | 91.6M/775M [00:15<01:11, 9.95MB/s] 12%|█▏        | 92.6M/775M [00:15<01:10, 10.1MB/s] 12%|█▏        | 93.6M/775M [00:15<01:10, 10.2MB/s] 12%|█▏        | 94.7M/775M [00:15<01:08, 10.3MB/s] 12%|█▏        | 95.7M/775M [00:15<01:09, 10.3MB/s] 12%|█▏        | 96.7M/775M [00:15<01:08, 10.3MB/s] 13%|█▎        | 97.7M/775M [00:16<01:19, 8.92MB/s] 13%|█▎        | 98.6M/775M [00:16<01:44, 6.76MB/s] 13%|█▎        | 99.3M/775M [00:16<02:07, 5.56MB/s] 13%|█▎        | 99.9M/775M [00:16<02:27, 4.81MB/s] 13%|█▎        | 100M/775M [00:16<02:36, 4.50MB/s]  13%|█▎        | 101M/775M [00:16<02:34, 4.57MB/s] 13%|█▎        | 101M/775M [00:17<02:36, 4.52MB/s] 13%|█▎        | 102M/775M [00:17<02:35, 4.54MB/s] 13%|█▎        | 102M/775M [00:17<02:30, 4.69MB/s] 13%|█▎        | 103M/775M [00:17<02:22, 4.93MB/s] 13%|█▎        | 104M/775M [00:17<02:18, 5.09MB/s] 13%|█▎        | 104M/775M [00:17<02:08, 5.47MB/s] 14%|█▎        | 105M/775M [00:17<02:02, 5.73MB/s] 14%|█▎        | 105M/775M [00:17<01:57, 5.95MB/s] 14%|█▎        | 106M/775M [00:17<02:01, 5.75MB/s] 14%|█▍        | 107M/775M [00:18<02:07, 5.50MB/s] 14%|█▍        | 107M/775M [00:18<02:21, 4.94MB/s] 14%|█▍        | 108M/775M [00:18<02:16, 5.12MB/s] 14%|█▍        | 108M/775M [00:18<02:21, 4.95MB/s] 14%|█▍        | 109M/775M [00:18<02:10, 5.36MB/s] 14%|█▍        | 110M/775M [00:18<01:53, 6.17MB/s] 14%|█▍        | 111M/775M [00:18<01:38, 7.10MB/s] 14%|█▍        | 112M/775M [00:18<01:29, 7.74MB/s] 15%|█▍        | 113M/775M [00:18<01:27, 7.96MB/s] 15%|█▍        | 113M/775M [00:19<01:23, 8.28MB/s] 15%|█▍        | 114M/775M [00:19<01:23, 8.34MB/s] 15%|█▍        | 115M/775M [00:19<01:25, 8.10MB/s] 15%|█▌        | 116M/775M [00:19<01:16, 8.99MB/s] 15%|█▌        | 117M/775M [00:19<01:09, 9.92MB/s] 15%|█▌        | 119M/775M [00:19<01:06, 10.4MB/s] 15%|█▌        | 120M/775M [00:19<01:01, 11.2MB/s] 16%|█▌        | 121M/775M [00:19<01:03, 10.9MB/s] 16%|█▌        | 122M/775M [00:19<01:02, 10.9MB/s] 16%|█▌        | 123M/775M [00:20<01:06, 10.3MB/s] 16%|█▌        | 124M/775M [00:20<01:06, 10.2MB/s] 16%|█▌        | 125M/775M [00:20<01:10, 9.68MB/s] 16%|█▋        | 126M/775M [00:20<01:09, 9.82MB/s] 16%|█▋        | 127M/775M [00:20<01:10, 9.64MB/s] 17%|█▋        | 128M/775M [00:20<01:08, 9.96MB/s] 17%|█▋        | 129M/775M [00:20<01:09, 9.75MB/s] 17%|█▋        | 130M/775M [00:20<01:08, 9.84MB/s] 17%|█▋        | 131M/775M [00:20<01:05, 10.4MB/s] 17%|█▋        | 132M/775M [00:20<01:06, 10.1MB/s] 17%|█▋        | 133M/775M [00:21<01:03, 10.5MB/s] 17%|█▋        | 134M/775M [00:21<01:04, 10.5MB/s] 17%|█▋        | 135M/775M [00:21<01:08, 9.73MB/s] 18%|█▊        | 136M/775M [00:21<01:06, 10.1MB/s] 18%|█▊        | 137M/775M [00:21<01:13, 9.07MB/s] 18%|█▊        | 138M/775M [00:21<01:16, 8.70MB/s] 18%|█▊        | 139M/775M [00:21<01:16, 8.72MB/s] 18%|█▊        | 140M/775M [00:21<01:16, 8.67MB/s] 18%|█▊        | 141M/775M [00:21<01:19, 8.40MB/s] 18%|█▊        | 142M/775M [00:22<01:18, 8.42MB/s] 18%|█▊        | 142M/775M [00:22<01:17, 8.57MB/s] 18%|█▊        | 143M/775M [00:22<01:18, 8.44MB/s] 19%|█▊        | 144M/775M [00:22<01:15, 8.73MB/s] 19%|█▊        | 145M/775M [00:22<01:16, 8.66MB/s] 19%|█▉        | 146M/775M [00:22<01:16, 8.56MB/s] 19%|█▉        | 147M/775M [00:22<01:14, 8.85MB/s] 19%|█▉        | 148M/775M [00:22<01:15, 8.70MB/s] 19%|█▉        | 148M/775M [00:22<01:16, 8.58MB/s] 19%|█▉        | 149M/775M [00:23<01:40, 6.54MB/s] 19%|█▉        | 150M/775M [00:23<02:24, 4.54MB/s] 19%|█▉        | 151M/775M [00:23<02:42, 4.03MB/s] 19%|█▉        | 151M/775M [00:23<02:56, 3.70MB/s] 20%|█▉        | 151M/775M [00:23<03:02, 3.57MB/s] 20%|█▉        | 152M/775M [00:24<03:28, 3.14MB/s] 20%|█▉        | 152M/775M [00:24<03:34, 3.05MB/s] 20%|█▉        | 152M/775M [00:24<03:30, 3.10MB/s] 20%|█▉        | 153M/775M [00:24<03:22, 3.21MB/s] 20%|█▉        | 153M/775M [00:24<03:44, 2.91MB/s] 20%|█▉        | 153M/775M [00:24<03:41, 2.94MB/s] 20%|█▉        | 154M/775M [00:24<03:41, 2.94MB/s] 20%|█▉        | 154M/775M [00:24<03:42, 2.92MB/s] 20%|█▉        | 154M/775M [00:25<03:43, 2.91MB/s] 20%|█▉        | 155M/775M [00:25<03:27, 3.13MB/s] 20%|██        | 155M/775M [00:25<03:41, 2.93MB/s] 20%|██        | 155M/775M [00:25<03:39, 2.95MB/s] 20%|██        | 156M/775M [00:25<03:16, 3.29MB/s] 20%|██        | 156M/775M [00:25<03:22, 3.21MB/s] 20%|██        | 156M/775M [00:25<03:23, 3.19MB/s] 20%|██        | 157M/775M [00:25<03:55, 2.75MB/s] 20%|██        | 157M/775M [00:25<04:00, 2.69MB/s] 20%|██        | 157M/775M [00:26<04:11, 2.58MB/s] 20%|██        | 158M/775M [00:26<04:19, 2.49MB/s] 20%|██        | 158M/775M [00:26<04:23, 2.45MB/s] 20%|██        | 158M/775M [00:26<03:56, 2.73MB/s] 20%|██        | 159M/775M [00:26<03:19, 3.24MB/s] 21%|██        | 159M/775M [00:26<02:48, 3.82MB/s] 21%|██        | 160M/775M [00:26<02:31, 4.26MB/s] 21%|██        | 160M/775M [00:26<02:12, 4.85MB/s] 21%|██        | 161M/775M [00:27<01:55, 5.58MB/s] 21%|██        | 162M/775M [00:27<01:48, 5.91MB/s] 21%|██        | 163M/775M [00:27<01:46, 6.01MB/s] 21%|██        | 163M/775M [00:27<01:39, 6.47MB/s] 21%|██        | 164M/775M [00:27<01:39, 6.46MB/s] 21%|██▏       | 165M/775M [00:27<01:21, 7.89MB/s] 21%|██▏       | 166M/775M [00:27<01:15, 8.46MB/s] 22%|██▏       | 167M/775M [00:27<01:11, 8.96MB/s] 22%|██▏       | 168M/775M [00:27<01:05, 9.72MB/s] 22%|██▏       | 169M/775M [00:27<01:04, 9.85MB/s] 22%|██▏       | 170M/775M [00:28<01:00, 10.4MB/s] 22%|██▏       | 171M/775M [00:28<01:01, 10.3MB/s] 22%|██▏       | 172M/775M [00:28<01:00, 10.4MB/s] 22%|██▏       | 173M/775M [00:28<00:58, 10.8MB/s] 23%|██▎       | 174M/775M [00:28<00:58, 10.8MB/s] 23%|██▎       | 176M/775M [00:28<00:56, 11.1MB/s] 23%|██▎       | 177M/775M [00:28<00:57, 10.9MB/s] 23%|██▎       | 178M/775M [00:28<00:55, 11.3MB/s] 23%|██▎       | 179M/775M [00:28<00:55, 11.3MB/s] 23%|██▎       | 180M/775M [00:28<00:54, 11.4MB/s] 23%|██▎       | 181M/775M [00:29<00:53, 11.7MB/s] 24%|██▎       | 183M/775M [00:29<00:53, 11.6MB/s] 24%|██▎       | 184M/775M [00:29<00:55, 11.1MB/s] 24%|██▍       | 185M/775M [00:29<01:02, 9.86MB/s] 24%|██▍       | 186M/775M [00:29<01:04, 9.60MB/s] 24%|██▍       | 187M/775M [00:29<01:05, 9.44MB/s] 24%|██▍       | 188M/775M [00:29<01:04, 9.48MB/s] 24%|██▍       | 189M/775M [00:29<01:05, 9.34MB/s] 24%|██▍       | 189M/775M [00:30<01:05, 9.43MB/s] 25%|██▍       | 190M/775M [00:30<01:04, 9.46MB/s] 25%|██▍       | 191M/775M [00:30<01:07, 9.11MB/s] 25%|██▍       | 192M/775M [00:30<01:13, 8.31MB/s] 25%|██▍       | 193M/775M [00:30<01:18, 7.76MB/s] 25%|██▌       | 194M/775M [00:30<01:19, 7.65MB/s] 25%|██▌       | 195M/775M [00:30<01:21, 7.44MB/s] 25%|██▌       | 195M/775M [00:30<01:25, 7.13MB/s] 25%|██▌       | 196M/775M [00:30<01:35, 6.34MB/s] 25%|██▌       | 197M/775M [00:31<01:38, 6.15MB/s] 25%|██▌       | 197M/775M [00:31<01:36, 6.26MB/s] 26%|██▌       | 198M/775M [00:31<01:40, 5.99MB/s] 26%|██▌       | 199M/775M [00:31<01:29, 6.72MB/s] 26%|██▌       | 199M/775M [00:31<01:29, 6.72MB/s] 26%|██▌       | 200M/775M [00:31<01:31, 6.56MB/s] 26%|██▌       | 201M/775M [00:31<01:42, 5.86MB/s] 26%|██▌       | 201M/775M [00:31<01:47, 5.62MB/s] 26%|██▌       | 202M/775M [00:32<02:07, 4.71MB/s] 26%|██▌       | 202M/775M [00:32<02:09, 4.64MB/s] 26%|██▌       | 203M/775M [00:32<02:22, 4.22MB/s] 26%|██▌       | 203M/775M [00:32<02:06, 4.75MB/s] 26%|██▋       | 204M/775M [00:32<02:05, 4.75MB/s] 26%|██▋       | 204M/775M [00:32<02:01, 4.91MB/s] 26%|██▋       | 205M/775M [00:32<01:59, 5.02MB/s] 27%|██▋       | 205M/775M [00:32<02:05, 4.75MB/s] 27%|██▋       | 206M/775M [00:33<02:23, 4.17MB/s] 27%|██▋       | 206M/775M [00:33<02:47, 3.55MB/s] 27%|██▋       | 207M/775M [00:33<03:03, 3.25MB/s] 27%|██▋       | 207M/775M [00:33<03:20, 2.97MB/s] 27%|██▋       | 207M/775M [00:33<03:42, 2.67MB/s] 27%|██▋       | 208M/775M [00:33<03:37, 2.73MB/s] 27%|██▋       | 208M/775M [00:33<03:43, 2.66MB/s] 27%|██▋       | 208M/775M [00:33<03:49, 2.59MB/s] 27%|██▋       | 208M/775M [00:34<03:45, 2.63MB/s] 27%|██▋       | 209M/775M [00:34<03:21, 2.95MB/s] 27%|██▋       | 209M/775M [00:34<03:29, 2.83MB/s] 27%|██▋       | 209M/775M [00:34<03:21, 2.94MB/s] 27%|██▋       | 210M/775M [00:34<03:12, 3.08MB/s] 27%|██▋       | 210M/775M [00:34<03:00, 3.28MB/s] 27%|██▋       | 211M/775M [00:34<02:57, 3.34MB/s] 27%|██▋       | 211M/775M [00:34<02:51, 3.44MB/s] 27%|██▋       | 212M/775M [00:35<02:26, 4.02MB/s] 27%|██▋       | 212M/775M [00:35<02:02, 4.82MB/s] 28%|██▊       | 213M/775M [00:35<01:45, 5.60MB/s] 28%|██▊       | 214M/775M [00:35<01:37, 6.04MB/s] 28%|██▊       | 214M/775M [00:35<01:31, 6.41MB/s] 28%|██▊       | 215M/775M [00:35<01:27, 6.68MB/s] 28%|██▊       | 216M/775M [00:35<01:23, 7.05MB/s] 28%|██▊       | 217M/775M [00:35<01:21, 7.15MB/s] 28%|██▊       | 217M/775M [00:35<01:18, 7.41MB/s] 28%|██▊       | 218M/775M [00:35<01:17, 7.52MB/s] 28%|██▊       | 219M/775M [00:36<01:15, 7.68MB/s] 28%|██▊       | 220M/775M [00:36<01:15, 7.71MB/s] 28%|██▊       | 221M/775M [00:36<01:13, 7.88MB/s] 29%|██▊       | 221M/775M [00:36<01:13, 7.93MB/s] 29%|██▊       | 222M/775M [00:36<01:11, 8.12MB/s] 29%|██▉       | 223M/775M [00:36<01:10, 8.18MB/s] 29%|██▉       | 224M/775M [00:36<01:09, 8.29MB/s] 29%|██▉       | 225M/775M [00:36<01:09, 8.32MB/s] 29%|██▉       | 225M/775M [00:36<01:07, 8.54MB/s] 29%|██▉       | 226M/775M [00:36<01:07, 8.51MB/s] 29%|██▉       | 227M/775M [00:37<01:06, 8.70MB/s] 29%|██▉       | 228M/775M [00:37<01:05, 8.81MB/s] 30%|██▉       | 229M/775M [00:37<01:07, 8.44MB/s] 30%|██▉       | 230M/775M [00:37<01:06, 8.58MB/s] 30%|██▉       | 231M/775M [00:37<01:04, 8.88MB/s] 30%|██▉       | 232M/775M [00:37<01:04, 8.82MB/s] 30%|███       | 233M/775M [00:37<01:02, 9.16MB/s] 30%|███       | 233M/775M [00:37<01:02, 9.06MB/s] 30%|███       | 234M/775M [00:37<01:01, 9.28MB/s] 30%|███       | 235M/775M [00:38<01:00, 9.28MB/s] 30%|███       | 236M/775M [00:38<00:59, 9.48MB/s] 31%|███       | 237M/775M [00:38<00:59, 9.53MB/s] 31%|███       | 238M/775M [00:38<00:58, 9.57MB/s] 31%|███       | 239M/775M [00:38<00:58, 9.53MB/s] 31%|███       | 240M/775M [00:38<00:56, 9.87MB/s] 31%|███       | 241M/775M [00:38<00:57, 9.74MB/s] 31%|███▏      | 242M/775M [00:38<00:55, 10.1MB/s] 31%|███▏      | 243M/775M [00:38<00:55, 10.0MB/s] 32%|███▏      | 244M/775M [00:38<00:57, 9.69MB/s] 32%|███▏      | 245M/775M [00:39<01:07, 8.21MB/s] 32%|███▏      | 246M/775M [00:39<01:21, 6.80MB/s] 32%|███▏      | 247M/775M [00:39<01:26, 6.42MB/s] 32%|███▏      | 247M/775M [00:39<01:28, 6.23MB/s] 32%|███▏      | 248M/775M [00:39<01:32, 5.99MB/s] 32%|███▏      | 248M/775M [00:39<01:35, 5.78MB/s] 32%|███▏      | 249M/775M [00:39<01:44, 5.29MB/s] 32%|███▏      | 249M/775M [00:40<01:47, 5.12MB/s] 32%|███▏      | 250M/775M [00:40<01:49, 5.03MB/s] 32%|███▏      | 250M/775M [00:40<01:49, 5.04MB/s] 32%|███▏      | 251M/775M [00:40<01:56, 4.70MB/s] 32%|███▏      | 251M/775M [00:40<01:56, 4.71MB/s] 33%|███▎      | 252M/775M [00:40<02:06, 4.35MB/s] 33%|███▎      | 252M/775M [00:40<02:07, 4.29MB/s] 33%|███▎      | 253M/775M [00:40<02:14, 4.08MB/s] 33%|███▎      | 253M/775M [00:40<02:25, 3.76MB/s] 33%|███▎      | 253M/775M [00:41<02:25, 3.75MB/s] 33%|███▎      | 254M/775M [00:41<02:13, 4.09MB/s] 33%|███▎      | 254M/775M [00:41<02:22, 3.83MB/s] 33%|███▎      | 255M/775M [00:41<02:15, 4.02MB/s] 33%|███▎      | 255M/775M [00:41<02:18, 3.94MB/s] 33%|███▎      | 256M/775M [00:41<02:32, 3.58MB/s] 33%|███▎      | 256M/775M [00:41<02:44, 3.32MB/s] 33%|███▎      | 256M/775M [00:41<02:47, 3.25MB/s] 33%|███▎      | 257M/775M [00:42<02:48, 3.22MB/s] 33%|███▎      | 257M/775M [00:42<02:45, 3.27MB/s] 33%|███▎      | 257M/775M [00:42<02:41, 3.35MB/s] 33%|███▎      | 258M/775M [00:42<02:24, 3.76MB/s] 33%|███▎      | 258M/775M [00:42<02:24, 3.75MB/s] 33%|███▎      | 259M/775M [00:42<02:25, 3.72MB/s] 33%|███▎      | 259M/775M [00:42<02:25, 3.71MB/s] 33%|███▎      | 259M/775M [00:42<02:39, 3.38MB/s] 34%|███▎      | 260M/775M [00:42<03:11, 2.82MB/s] 34%|███▎      | 260M/775M [00:43<03:18, 2.73MB/s] 34%|███▎      | 260M/775M [00:43<04:06, 2.19MB/s] 34%|███▎      | 260M/775M [00:43<04:06, 2.18MB/s] 34%|███▎      | 261M/775M [00:43<03:53, 2.31MB/s] 34%|███▎      | 261M/775M [00:43<03:47, 2.37MB/s] 34%|███▎      | 261M/775M [00:43<04:02, 2.22MB/s] 34%|███▍      | 262M/775M [00:43<03:26, 2.60MB/s] 34%|███▍      | 262M/775M [00:43<03:35, 2.50MB/s] 34%|███▍      | 262M/775M [00:44<03:16, 2.73MB/s] 34%|███▍      | 263M/775M [00:44<02:55, 3.05MB/s] 34%|███▍      | 263M/775M [00:44<02:46, 3.23MB/s] 34%|███▍      | 263M/775M [00:44<02:45, 3.24MB/s] 34%|███▍      | 264M/775M [00:44<02:37, 3.39MB/s] 34%|███▍      | 264M/775M [00:44<02:51, 3.12MB/s] 34%|███▍      | 264M/775M [00:44<03:02, 2.93MB/s] 34%|███▍      | 265M/775M [00:44<02:53, 3.09MB/s] 34%|███▍      | 265M/775M [00:45<02:54, 3.06MB/s] 34%|███▍      | 266M/775M [00:45<02:57, 3.01MB/s] 34%|███▍      | 266M/775M [00:45<03:04, 2.90MB/s] 34%|███▍      | 266M/775M [00:45<02:54, 3.05MB/s] 34%|███▍      | 267M/775M [00:45<03:12, 2.77MB/s] 34%|███▍      | 267M/775M [00:45<03:09, 2.80MB/s] 35%|███▍      | 267M/775M [00:45<02:57, 3.00MB/s] 35%|███▍      | 268M/775M [00:45<03:01, 2.92MB/s] 35%|███▍      | 268M/775M [00:46<03:10, 2.79MB/s] 35%|███▍      | 268M/775M [00:46<03:06, 2.84MB/s] 35%|███▍      | 269M/775M [00:46<03:06, 2.85MB/s] 35%|███▍      | 269M/775M [00:46<03:13, 2.74MB/s] 35%|███▍      | 270M/775M [00:46<03:12, 2.75MB/s] 35%|███▍      | 270M/775M [00:46<03:26, 2.57MB/s] 35%|███▍      | 270M/775M [00:46<03:25, 2.58MB/s] 35%|███▍      | 270M/775M [00:47<03:46, 2.34MB/s] 35%|███▍      | 271M/775M [00:47<03:15, 2.71MB/s] 35%|███▌      | 271M/775M [00:47<02:44, 3.20MB/s] 35%|███▌      | 272M/775M [00:47<02:20, 3.76MB/s] 35%|███▌      | 272M/775M [00:47<01:55, 4.57MB/s] 35%|███▌      | 273M/775M [00:47<01:33, 5.60MB/s] 35%|███▌      | 274M/775M [00:47<01:25, 6.14MB/s] 35%|███▌      | 275M/775M [00:47<01:16, 6.85MB/s] 36%|███▌      | 276M/775M [00:47<01:13, 7.13MB/s] 36%|███▌      | 276M/775M [00:48<01:08, 7.61MB/s] 36%|███▌      | 277M/775M [00:48<01:07, 7.69MB/s] 36%|███▌      | 278M/775M [00:48<01:06, 7.83MB/s] 36%|███▌      | 279M/775M [00:48<01:05, 7.89MB/s] 36%|███▌      | 280M/775M [00:48<01:05, 7.97MB/s] 36%|███▌      | 281M/775M [00:48<01:01, 8.38MB/s] 36%|███▋      | 281M/775M [00:48<01:03, 8.16MB/s] 36%|███▋      | 282M/775M [00:48<01:04, 8.03MB/s] 37%|███▋      | 283M/775M [00:48<01:08, 7.51MB/s] 37%|███▋      | 284M/775M [00:49<01:12, 7.11MB/s] 37%|███▋      | 284M/775M [00:49<01:10, 7.27MB/s] 37%|███▋      | 285M/775M [00:49<01:09, 7.38MB/s] 37%|███▋      | 286M/775M [00:49<01:07, 7.55MB/s] 37%|███▋      | 287M/775M [00:49<01:04, 7.94MB/s] 37%|███▋      | 288M/775M [00:49<01:02, 8.19MB/s] 37%|███▋      | 288M/775M [00:49<01:07, 7.52MB/s] 37%|███▋      | 289M/775M [00:49<01:17, 6.57MB/s] 37%|███▋      | 290M/775M [00:49<01:20, 6.32MB/s] 37%|███▋      | 290M/775M [00:50<01:20, 6.34MB/s] 38%|███▊      | 291M/775M [00:50<01:19, 6.40MB/s] 38%|███▊      | 292M/775M [00:50<01:08, 7.43MB/s] 38%|███▊      | 293M/775M [00:50<01:01, 8.21MB/s] 38%|███▊      | 294M/775M [00:50<00:57, 8.76MB/s] 38%|███▊      | 295M/775M [00:50<00:53, 9.40MB/s] 38%|███▊      | 296M/775M [00:50<00:56, 8.89MB/s] 38%|███▊      | 297M/775M [00:50<00:56, 8.84MB/s] 38%|███▊      | 298M/775M [00:51<01:07, 7.37MB/s] 39%|███▊      | 299M/775M [00:51<01:16, 6.56MB/s] 39%|███▊      | 299M/775M [00:51<01:30, 5.51MB/s] 39%|███▊      | 300M/775M [00:51<01:38, 5.06MB/s] 39%|███▉      | 301M/775M [00:51<01:53, 4.39MB/s] 39%|███▉      | 301M/775M [00:51<02:09, 3.85MB/s] 39%|███▉      | 301M/775M [00:52<02:23, 3.45MB/s] 39%|███▉      | 302M/775M [00:52<02:17, 3.60MB/s] 39%|███▉      | 302M/775M [00:52<02:14, 3.67MB/s] 39%|███▉      | 303M/775M [00:52<02:00, 4.11MB/s] 39%|███▉      | 303M/775M [00:52<01:58, 4.17MB/s] 39%|███▉      | 304M/775M [00:52<01:55, 4.28MB/s] 39%|███▉      | 304M/775M [00:52<01:52, 4.40MB/s] 39%|███▉      | 305M/775M [00:52<01:47, 4.58MB/s] 39%|███▉      | 305M/775M [00:52<01:43, 4.77MB/s] 39%|███▉      | 306M/775M [00:53<01:46, 4.63MB/s] 40%|███▉      | 307M/775M [00:53<01:48, 4.53MB/s] 40%|███▉      | 307M/775M [00:53<01:46, 4.58MB/s] 40%|███▉      | 308M/775M [00:53<01:40, 4.88MB/s] 40%|███▉      | 308M/775M [00:53<01:35, 5.10MB/s] 40%|███▉      | 309M/775M [00:53<01:28, 5.51MB/s] 40%|███▉      | 310M/775M [00:53<01:23, 5.81MB/s] 40%|████      | 310M/775M [00:53<01:19, 6.11MB/s] 40%|████      | 311M/775M [00:53<01:09, 6.99MB/s] 40%|████      | 312M/775M [00:54<01:01, 7.87MB/s] 40%|████      | 314M/775M [00:54<00:55, 8.64MB/s] 41%|████      | 314M/775M [00:54<01:02, 7.75MB/s] 41%|████      | 315M/775M [00:54<01:03, 7.59MB/s] 41%|████      | 316M/775M [00:54<01:01, 7.79MB/s] 41%|████      | 317M/775M [00:54<01:04, 7.49MB/s] 41%|████      | 317M/775M [00:54<01:04, 7.48MB/s] 41%|████      | 318M/775M [00:54<01:01, 7.80MB/s] 41%|████      | 319M/775M [00:54<01:03, 7.52MB/s] 41%|████▏     | 320M/775M [00:55<01:02, 7.61MB/s] 41%|████▏     | 321M/775M [00:55<01:00, 7.81MB/s] 41%|████▏     | 321M/775M [00:55<01:01, 7.75MB/s] 42%|████▏     | 322M/775M [00:55<01:01, 7.69MB/s] 42%|████▏     | 323M/775M [00:55<01:00, 7.80MB/s] 42%|████▏     | 324M/775M [00:55<00:59, 7.97MB/s] 42%|████▏     | 325M/775M [00:55<00:58, 8.07MB/s] 42%|████▏     | 325M/775M [00:55<00:58, 8.10MB/s] 42%|████▏     | 326M/775M [00:55<00:57, 8.16MB/s] 42%|████▏     | 327M/775M [00:56<00:56, 8.30MB/s] 42%|████▏     | 328M/775M [00:56<00:56, 8.31MB/s] 42%|████▏     | 329M/775M [00:56<00:55, 8.44MB/s] 43%|████▎     | 330M/775M [00:56<00:54, 8.52MB/s] 43%|████▎     | 330M/775M [00:56<00:53, 8.75MB/s] 43%|████▎     | 331M/775M [00:56<00:53, 8.74MB/s] 43%|████▎     | 332M/775M [00:56<00:52, 8.79MB/s] 43%|████▎     | 333M/775M [00:56<00:52, 8.88MB/s] 43%|████▎     | 334M/775M [00:56<00:51, 8.93MB/s] 43%|████▎     | 335M/775M [00:56<00:50, 9.09MB/s] 43%|████▎     | 336M/775M [00:57<00:50, 9.10MB/s] 43%|████▎     | 337M/775M [00:57<00:48, 9.40MB/s] 44%|████▎     | 338M/775M [00:57<00:48, 9.37MB/s] 44%|████▎     | 339M/775M [00:57<00:48, 9.34MB/s] 44%|████▍     | 339M/775M [00:57<00:48, 9.33MB/s] 44%|████▍     | 340M/775M [00:57<00:48, 9.37MB/s] 44%|████▍     | 341M/775M [00:57<00:46, 9.69MB/s] 44%|████▍     | 342M/775M [00:57<00:46, 9.67MB/s] 44%|████▍     | 343M/775M [00:57<00:45, 9.87MB/s] 44%|████▍     | 344M/775M [00:57<00:45, 9.85MB/s] 45%|████▍     | 345M/775M [00:58<00:44, 10.1MB/s] 45%|████▍     | 346M/775M [00:58<00:44, 10.1MB/s] 45%|████▍     | 347M/775M [00:58<00:43, 10.3MB/s] 45%|████▍     | 348M/775M [00:58<00:43, 10.2MB/s] 45%|████▌     | 349M/775M [00:58<00:44, 9.96MB/s] 45%|████▌     | 350M/775M [00:58<00:46, 9.54MB/s] 45%|████▌     | 351M/775M [00:58<00:54, 8.16MB/s] 45%|████▌     | 352M/775M [00:58<01:02, 7.13MB/s] 46%|████▌     | 353M/775M [00:59<01:04, 6.84MB/s] 46%|████▌     | 353M/775M [00:59<01:06, 6.60MB/s] 46%|████▌     | 354M/775M [00:59<01:10, 6.23MB/s] 46%|████▌     | 355M/775M [00:59<01:13, 6.01MB/s] 46%|████▌     | 355M/775M [00:59<01:13, 6.00MB/s] 46%|████▌     | 356M/775M [00:59<01:12, 6.04MB/s] 46%|████▌     | 356M/775M [00:59<01:12, 6.03MB/s] 46%|████▌     | 357M/775M [00:59<01:12, 6.08MB/s] 46%|████▌     | 358M/775M [00:59<01:10, 6.17MB/s] 46%|████▌     | 358M/775M [01:00<01:11, 6.12MB/s] 46%|████▋     | 359M/775M [01:00<01:08, 6.39MB/s] 46%|████▋     | 360M/775M [01:00<01:09, 6.28MB/s] 47%|████▋     | 360M/775M [01:00<01:05, 6.61MB/s] 47%|████▋     | 361M/775M [01:00<01:06, 6.48MB/s] 47%|████▋     | 362M/775M [01:00<01:06, 6.50MB/s] 47%|████▋     | 362M/775M [01:00<01:02, 6.97MB/s] 47%|████▋     | 363M/775M [01:00<01:03, 6.78MB/s] 47%|████▋     | 364M/775M [01:00<01:03, 6.77MB/s] 47%|████▋     | 365M/775M [01:01<01:02, 6.93MB/s] 47%|████▋     | 365M/775M [01:01<00:58, 7.32MB/s] 47%|████▋     | 366M/775M [01:01<01:00, 7.09MB/s] 47%|████▋     | 367M/775M [01:01<01:00, 7.12MB/s] 47%|████▋     | 368M/775M [01:01<00:58, 7.24MB/s] 48%|████▊     | 368M/775M [01:01<00:57, 7.37MB/s] 48%|████▊     | 369M/775M [01:01<00:56, 7.54MB/s] 48%|████▊     | 370M/775M [01:01<00:55, 7.59MB/s] 48%|████▊     | 371M/775M [01:01<00:54, 7.70MB/s] 48%|████▊     | 371M/775M [01:01<00:54, 7.74MB/s] 48%|████▊     | 372M/775M [01:02<00:53, 7.93MB/s] 48%|████▊     | 373M/775M [01:02<00:52, 7.98MB/s] 48%|████▊     | 374M/775M [01:02<00:52, 7.94MB/s] 48%|████▊     | 375M/775M [01:02<00:51, 8.10MB/s] 48%|████▊     | 375M/775M [01:02<00:51, 8.17MB/s] 49%|████▊     | 376M/775M [01:02<00:50, 8.27MB/s] 49%|████▊     | 377M/775M [01:02<00:49, 8.35MB/s] 49%|████▉     | 378M/775M [01:02<00:48, 8.55MB/s] 49%|████▉     | 379M/775M [01:02<00:48, 8.62MB/s] 49%|████▉     | 380M/775M [01:02<00:47, 8.74MB/s] 49%|████▉     | 381M/775M [01:03<00:46, 8.84MB/s] 49%|████▉     | 381M/775M [01:03<00:46, 8.80MB/s] 49%|████▉     | 382M/775M [01:03<00:51, 7.94MB/s] 49%|████▉     | 383M/775M [01:03<00:50, 8.13MB/s] 50%|████▉     | 384M/775M [01:03<00:54, 7.48MB/s] 50%|████▉     | 385M/775M [01:03<00:59, 6.93MB/s] 50%|████▉     | 385M/775M [01:03<01:20, 5.05MB/s] 50%|████▉     | 386M/775M [01:04<01:26, 4.71MB/s] 50%|████▉     | 386M/775M [01:04<01:39, 4.09MB/s] 50%|████▉     | 387M/775M [01:04<01:44, 3.88MB/s] 50%|████▉     | 387M/775M [01:04<01:52, 3.61MB/s] 50%|█████     | 388M/775M [01:04<02:11, 3.09MB/s] 50%|█████     | 388M/775M [01:04<02:13, 3.05MB/s] 50%|█████     | 388M/775M [01:04<02:15, 2.99MB/s] 50%|█████     | 388M/775M [01:05<02:35, 2.60MB/s] 50%|█████     | 389M/775M [01:05<02:40, 2.52MB/s] 50%|█████     | 389M/775M [01:05<02:45, 2.44MB/s] 50%|█████     | 389M/775M [01:05<02:53, 2.33MB/s] 50%|█████     | 389M/775M [01:05<02:54, 2.32MB/s] 50%|█████     | 390M/775M [01:05<03:02, 2.21MB/s] 50%|█████     | 390M/775M [01:05<03:02, 2.21MB/s] 50%|█████     | 390M/775M [01:05<03:01, 2.22MB/s] 50%|█████     | 390M/775M [01:06<03:01, 2.22MB/s] 50%|█████     | 391M/775M [01:06<03:03, 2.19MB/s] 50%|█████     | 391M/775M [01:06<03:06, 2.16MB/s] 50%|█████     | 391M/775M [01:06<03:01, 2.21MB/s] 51%|█████     | 391M/775M [01:06<03:04, 2.18MB/s] 51%|█████     | 392M/775M [01:06<03:04, 2.18MB/s] 51%|█████     | 392M/775M [01:06<03:04, 2.18MB/s] 51%|█████     | 392M/775M [01:06<03:05, 2.16MB/s] 51%|█████     | 393M/775M [01:07<02:47, 2.39MB/s] 51%|█████     | 393M/775M [01:07<02:21, 2.82MB/s] 51%|█████     | 394M/775M [01:07<01:46, 3.76MB/s] 51%|█████     | 394M/775M [01:07<01:29, 4.47MB/s] 51%|█████     | 395M/775M [01:07<01:14, 5.37MB/s] 51%|█████     | 396M/775M [01:07<00:58, 6.76MB/s] 51%|█████     | 397M/775M [01:07<00:54, 7.21MB/s] 51%|█████▏    | 398M/775M [01:07<00:48, 8.08MB/s] 51%|█████▏    | 399M/775M [01:07<00:47, 8.23MB/s] 52%|█████▏    | 400M/775M [01:07<00:45, 8.73MB/s] 52%|█████▏    | 400M/775M [01:08<00:45, 8.71MB/s] 52%|█████▏    | 401M/775M [01:08<00:44, 8.74MB/s] 52%|█████▏    | 402M/775M [01:08<00:44, 8.80MB/s] 52%|█████▏    | 403M/775M [01:08<00:43, 8.92MB/s] 52%|█████▏    | 404M/775M [01:08<00:41, 9.39MB/s] 52%|█████▏    | 405M/775M [01:08<00:42, 9.07MB/s] 52%|█████▏    | 406M/775M [01:08<00:42, 9.09MB/s] 53%|█████▎    | 407M/775M [01:08<00:42, 9.13MB/s] 53%|█████▎    | 408M/775M [01:08<00:41, 9.23MB/s] 53%|█████▎    | 409M/775M [01:09<00:41, 9.25MB/s] 53%|█████▎    | 410M/775M [01:09<00:39, 9.60MB/s] 53%|█████▎    | 411M/775M [01:09<00:41, 9.11MB/s] 53%|█████▎    | 412M/775M [01:09<00:41, 9.11MB/s] 53%|█████▎    | 413M/775M [01:09<00:40, 9.26MB/s] 53%|█████▎    | 414M/775M [01:09<00:40, 9.24MB/s] 54%|█████▎    | 415M/775M [01:09<00:40, 9.39MB/s] 54%|█████▎    | 416M/775M [01:09<00:39, 9.51MB/s] 54%|█████▍    | 417M/775M [01:09<00:38, 9.64MB/s] 54%|█████▍    | 418M/775M [01:10<00:38, 9.62MB/s] 54%|█████▍    | 419M/775M [01:10<00:38, 9.76MB/s] 54%|█████▍    | 420M/775M [01:10<00:37, 9.82MB/s] 54%|█████▍    | 421M/775M [01:10<00:38, 9.74MB/s] 54%|█████▍    | 422M/775M [01:10<00:37, 9.90MB/s] 55%|█████▍    | 423M/775M [01:10<00:36, 10.2MB/s] 55%|█████▍    | 424M/775M [01:10<00:36, 10.1MB/s] 55%|█████▍    | 425M/775M [01:10<00:35, 10.5MB/s] 55%|█████▍    | 426M/775M [01:10<00:35, 10.3MB/s] 55%|█████▌    | 427M/775M [01:10<00:35, 10.3MB/s] 55%|█████▌    | 428M/775M [01:11<00:34, 10.4MB/s] 55%|█████▌    | 429M/775M [01:11<00:34, 10.4MB/s] 56%|█████▌    | 430M/775M [01:11<00:33, 10.7MB/s] 56%|█████▌    | 431M/775M [01:11<00:34, 10.5MB/s] 56%|█████▌    | 432M/775M [01:11<00:34, 10.5MB/s] 56%|█████▌    | 433M/775M [01:11<00:32, 10.9MB/s] 56%|█████▌    | 434M/775M [01:11<00:33, 10.8MB/s] 56%|█████▌    | 435M/775M [01:11<00:32, 11.1MB/s] 56%|█████▋    | 437M/775M [01:11<00:31, 11.1MB/s] 57%|█████▋    | 438M/775M [01:11<00:31, 11.2MB/s] 57%|█████▋    | 439M/775M [01:12<00:31, 11.2MB/s] 57%|█████▋    | 440M/775M [01:12<00:30, 11.5MB/s] 57%|█████▋    | 441M/775M [01:12<00:30, 11.5MB/s] 57%|█████▋    | 442M/775M [01:12<00:30, 11.4MB/s] 57%|█████▋    | 443M/775M [01:12<00:29, 11.8MB/s] 57%|█████▋    | 445M/775M [01:12<00:29, 11.8MB/s] 58%|█████▊    | 446M/775M [01:12<00:28, 11.9MB/s] 58%|█████▊    | 447M/775M [01:12<00:28, 11.9MB/s] 58%|█████▊    | 448M/775M [01:12<00:28, 12.1MB/s] 58%|█████▊    | 449M/775M [01:13<00:28, 12.1MB/s] 58%|█████▊    | 451M/775M [01:13<00:27, 12.3MB/s] 58%|█████▊    | 452M/775M [01:13<00:27, 12.3MB/s] 58%|█████▊    | 453M/775M [01:13<00:29, 11.3MB/s] 59%|█████▊    | 454M/775M [01:13<00:31, 10.6MB/s] 59%|█████▉    | 455M/775M [01:13<00:32, 10.2MB/s] 59%|█████▉    | 456M/775M [01:13<00:34, 9.75MB/s] 59%|█████▉    | 457M/775M [01:13<00:36, 9.17MB/s] 59%|█████▉    | 458M/775M [01:13<00:40, 8.23MB/s] 59%|█████▉    | 459M/775M [01:14<00:41, 7.90MB/s] 59%|█████▉    | 460M/775M [01:14<00:43, 7.64MB/s] 59%|█████▉    | 460M/775M [01:14<00:43, 7.53MB/s] 60%|█████▉    | 461M/775M [01:14<00:44, 7.38MB/s] 60%|█████▉    | 462M/775M [01:14<00:44, 7.42MB/s] 60%|█████▉    | 462M/775M [01:14<00:44, 7.39MB/s] 60%|█████▉    | 463M/775M [01:14<00:43, 7.55MB/s] 60%|█████▉    | 464M/775M [01:14<00:43, 7.45MB/s] 60%|█████▉    | 465M/775M [01:14<00:43, 7.55MB/s] 60%|██████    | 465M/775M [01:15<00:42, 7.60MB/s] 60%|██████    | 466M/775M [01:15<00:42, 7.59MB/s] 60%|██████    | 467M/775M [01:15<00:42, 7.56MB/s] 60%|██████    | 468M/775M [01:15<00:41, 7.84MB/s] 60%|██████    | 469M/775M [01:15<00:41, 7.79MB/s] 61%|██████    | 469M/775M [01:15<00:39, 8.09MB/s] 61%|██████    | 470M/775M [01:15<00:39, 8.00MB/s] 61%|██████    | 471M/775M [01:15<00:38, 8.27MB/s] 61%|██████    | 472M/775M [01:15<00:39, 8.11MB/s] 61%|██████    | 473M/775M [01:15<00:38, 8.15MB/s] 61%|██████    | 474M/775M [01:16<00:37, 8.50MB/s] 61%|██████    | 474M/775M [01:16<00:37, 8.40MB/s] 61%|██████▏   | 475M/775M [01:16<00:37, 8.38MB/s] 61%|██████▏   | 476M/775M [01:16<00:36, 8.47MB/s] 62%|██████▏   | 477M/775M [01:16<00:35, 8.72MB/s] 62%|██████▏   | 478M/775M [01:16<00:35, 8.72MB/s] 62%|██████▏   | 479M/775M [01:16<00:34, 8.89MB/s] 62%|██████▏   | 480M/775M [01:16<00:34, 8.95MB/s] 62%|██████▏   | 481M/775M [01:16<00:33, 9.07MB/s] 62%|██████▏   | 482M/775M [01:17<00:33, 9.19MB/s] 62%|██████▏   | 482M/775M [01:17<00:33, 9.28MB/s] 62%|██████▏   | 483M/775M [01:17<00:32, 9.42MB/s] 63%|██████▎   | 484M/775M [01:17<00:33, 9.13MB/s] 63%|██████▎   | 485M/775M [01:17<00:32, 9.21MB/s] 63%|██████▎   | 486M/775M [01:17<00:30, 9.76MB/s] 63%|██████▎   | 487M/775M [01:17<00:31, 9.53MB/s] 63%|██████▎   | 488M/775M [01:17<00:30, 9.69MB/s] 63%|██████▎   | 489M/775M [01:17<00:29, 10.1MB/s] 63%|██████▎   | 490M/775M [01:17<00:30, 9.90MB/s] 63%|██████▎   | 491M/775M [01:18<00:29, 10.0MB/s] 64%|██████▎   | 492M/775M [01:18<00:29, 10.1MB/s] 64%|██████▎   | 493M/775M [01:18<00:30, 9.83MB/s] 64%|██████▍   | 494M/775M [01:18<00:32, 9.17MB/s] 64%|██████▍   | 495M/775M [01:18<00:33, 8.75MB/s] 64%|██████▍   | 496M/775M [01:18<00:34, 8.36MB/s] 64%|██████▍   | 497M/775M [01:18<00:36, 8.03MB/s] 64%|██████▍   | 498M/775M [01:18<00:35, 8.29MB/s] 64%|██████▍   | 498M/775M [01:18<00:35, 8.10MB/s] 64%|██████▍   | 499M/775M [01:19<00:35, 8.21MB/s] 65%|██████▍   | 500M/775M [01:19<00:33, 8.47MB/s] 65%|██████▍   | 501M/775M [01:19<00:35, 8.10MB/s] 65%|██████▍   | 502M/775M [01:19<00:34, 8.24MB/s] 65%|██████▍   | 503M/775M [01:19<00:37, 7.61MB/s] 65%|██████▍   | 503M/775M [01:19<00:39, 7.24MB/s] 65%|██████▌   | 504M/775M [01:19<00:40, 6.92MB/s] 65%|██████▌   | 505M/775M [01:19<00:41, 6.84MB/s] 65%|██████▌   | 505M/775M [01:19<00:41, 6.82MB/s] 65%|██████▌   | 506M/775M [01:20<00:41, 6.82MB/s] 65%|██████▌   | 507M/775M [01:20<00:41, 6.77MB/s] 65%|██████▌   | 507M/775M [01:20<00:44, 6.34MB/s] 66%|██████▌   | 508M/775M [01:20<00:45, 6.17MB/s] 66%|██████▌   | 509M/775M [01:20<00:47, 5.87MB/s] 66%|██████▌   | 509M/775M [01:20<00:49, 5.61MB/s] 66%|██████▌   | 510M/775M [01:20<00:50, 5.46MB/s] 66%|██████▌   | 510M/775M [01:20<00:51, 5.39MB/s] 66%|██████▌   | 511M/775M [01:20<00:51, 5.40MB/s] 66%|██████▌   | 512M/775M [01:21<00:50, 5.42MB/s] 66%|██████▌   | 512M/775M [01:21<00:50, 5.46MB/s] 66%|██████▌   | 513M/775M [01:21<00:50, 5.49MB/s] 66%|██████▋   | 513M/775M [01:21<00:49, 5.50MB/s] 66%|██████▋   | 514M/775M [01:21<00:47, 5.72MB/s] 66%|██████▋   | 514M/775M [01:21<00:47, 5.70MB/s] 66%|██████▋   | 515M/775M [01:21<00:47, 5.73MB/s] 67%|██████▋   | 516M/775M [01:21<00:45, 5.93MB/s] 67%|██████▋   | 516M/775M [01:21<00:45, 5.98MB/s] 67%|██████▋   | 517M/775M [01:22<00:44, 6.12MB/s] 67%|██████▋   | 518M/775M [01:22<00:43, 6.16MB/s] 67%|██████▋   | 518M/775M [01:22<00:42, 6.28MB/s] 67%|██████▋   | 519M/775M [01:22<00:42, 6.36MB/s] 67%|██████▋   | 519M/775M [01:22<00:42, 6.33MB/s] 67%|██████▋   | 520M/775M [01:22<00:40, 6.62MB/s] 67%|██████▋   | 521M/775M [01:22<00:40, 6.52MB/s] 67%|██████▋   | 521M/775M [01:22<00:40, 6.52MB/s] 67%|██████▋   | 522M/775M [01:22<00:38, 6.86MB/s] 67%|██████▋   | 523M/775M [01:23<00:38, 6.77MB/s] 68%|██████▊   | 524M/775M [01:23<00:37, 7.00MB/s] 68%|██████▊   | 524M/775M [01:23<00:37, 6.97MB/s] 68%|██████▊   | 525M/775M [01:23<00:39, 6.65MB/s] 68%|██████▊   | 526M/775M [01:23<00:41, 6.31MB/s] 68%|██████▊   | 526M/775M [01:23<00:43, 6.02MB/s] 68%|██████▊   | 527M/775M [01:23<00:44, 5.84MB/s] 68%|██████▊   | 527M/775M [01:23<00:45, 5.74MB/s] 68%|██████▊   | 528M/775M [01:23<00:44, 5.83MB/s] 68%|██████▊   | 529M/775M [01:23<00:44, 5.78MB/s] 68%|██████▊   | 529M/775M [01:24<00:43, 5.93MB/s] 68%|██████▊   | 530M/775M [01:24<00:43, 5.93MB/s] 68%|██████▊   | 530M/775M [01:24<00:43, 5.95MB/s] 69%|██████▊   | 531M/775M [01:24<00:42, 5.97MB/s] 69%|██████▊   | 532M/775M [01:24<00:41, 6.17MB/s] 69%|██████▊   | 532M/775M [01:24<00:41, 6.17MB/s] 69%|██████▉   | 533M/775M [01:24<00:39, 6.44MB/s] 69%|██████▉   | 533M/775M [01:24<00:39, 6.42MB/s] 69%|██████▉   | 534M/775M [01:24<00:38, 6.56MB/s] 69%|██████▉   | 535M/775M [01:25<00:38, 6.53MB/s] 69%|██████▉   | 535M/775M [01:25<00:37, 6.69MB/s] 69%|██████▉   | 536M/775M [01:25<00:37, 6.66MB/s] 69%|██████▉   | 537M/775M [01:25<00:41, 6.00MB/s] 69%|██████▉   | 537M/775M [01:25<00:43, 5.76MB/s] 69%|██████▉   | 538M/775M [01:25<00:44, 5.52MB/s] 70%|██████▉   | 538M/775M [01:25<00:46, 5.37MB/s] 70%|██████▉   | 539M/775M [01:25<00:46, 5.36MB/s] 70%|██████▉   | 539M/775M [01:25<00:46, 5.28MB/s] 70%|██████▉   | 540M/775M [01:26<00:47, 5.20MB/s] 70%|██████▉   | 541M/775M [01:26<00:45, 5.39MB/s] 70%|██████▉   | 541M/775M [01:26<00:46, 5.32MB/s] 70%|██████▉   | 542M/775M [01:26<00:43, 5.56MB/s] 70%|██████▉   | 542M/775M [01:26<00:44, 5.47MB/s] 70%|███████   | 543M/775M [01:26<00:42, 5.72MB/s] 70%|███████   | 543M/775M [01:26<00:43, 5.63MB/s] 70%|███████   | 544M/775M [01:26<00:41, 5.89MB/s] 70%|███████   | 545M/775M [01:26<00:41, 5.80MB/s] 70%|███████   | 545M/775M [01:26<00:40, 5.90MB/s] 70%|███████   | 546M/775M [01:27<00:39, 6.07MB/s] 71%|███████   | 546M/775M [01:27<00:39, 6.10MB/s] 71%|███████   | 547M/775M [01:27<00:38, 6.19MB/s] 71%|███████   | 548M/775M [01:27<00:38, 6.22MB/s] 71%|███████   | 548M/775M [01:27<00:37, 6.38MB/s] 71%|███████   | 549M/775M [01:27<00:36, 6.50MB/s] 71%|███████   | 550M/775M [01:27<00:36, 6.55MB/s] 71%|███████   | 550M/775M [01:27<00:35, 6.60MB/s] 71%|███████   | 551M/775M [01:27<00:34, 6.74MB/s] 71%|███████   | 552M/775M [01:27<00:34, 6.79MB/s] 71%|███████▏  | 552M/775M [01:28<00:33, 6.89MB/s] 71%|███████▏  | 553M/775M [01:28<00:33, 6.96MB/s] 71%|███████▏  | 554M/775M [01:28<00:33, 6.92MB/s] 72%|███████▏  | 554M/775M [01:28<00:33, 6.94MB/s] 72%|███████▏  | 555M/775M [01:28<00:32, 7.14MB/s] 72%|███████▏  | 556M/775M [01:28<00:31, 7.20MB/s] 72%|███████▏  | 557M/775M [01:28<00:31, 7.27MB/s] 72%|███████▏  | 557M/775M [01:28<00:30, 7.40MB/s] 72%|███████▏  | 558M/775M [01:28<00:30, 7.52MB/s] 72%|███████▏  | 559M/775M [01:29<00:29, 7.69MB/s] 72%|███████▏  | 560M/775M [01:29<00:29, 7.74MB/s] 72%|███████▏  | 560M/775M [01:29<00:28, 7.82MB/s] 72%|███████▏  | 561M/775M [01:29<00:28, 7.77MB/s] 73%|███████▎  | 562M/775M [01:29<00:28, 7.88MB/s] 73%|███████▎  | 563M/775M [01:29<00:28, 7.93MB/s] 73%|███████▎  | 564M/775M [01:29<00:27, 8.09MB/s] 73%|███████▎  | 564M/775M [01:29<00:27, 8.12MB/s] 73%|███████▎  | 565M/775M [01:29<00:26, 8.26MB/s] 73%|███████▎  | 566M/775M [01:29<00:26, 8.28MB/s] 73%|███████▎  | 567M/775M [01:30<00:25, 8.48MB/s] 73%|███████▎  | 568M/775M [01:30<00:25, 8.52MB/s] 73%|███████▎  | 569M/775M [01:30<00:24, 8.71MB/s] 74%|███████▎  | 569M/775M [01:30<00:24, 8.70MB/s] 74%|███████▎  | 570M/775M [01:30<00:24, 8.90MB/s] 74%|███████▎  | 571M/775M [01:30<00:24, 8.84MB/s] 74%|███████▍  | 572M/775M [01:30<00:23, 9.00MB/s] 74%|███████▍  | 573M/775M [01:30<00:23, 9.05MB/s] 74%|███████▍  | 574M/775M [01:30<00:22, 9.21MB/s] 74%|███████▍  | 575M/775M [01:30<00:22, 9.34MB/s] 74%|███████▍  | 576M/775M [01:31<00:22, 9.36MB/s] 74%|███████▍  | 577M/775M [01:31<00:21, 9.48MB/s] 75%|███████▍  | 578M/775M [01:31<00:22, 9.38MB/s] 75%|███████▍  | 579M/775M [01:31<00:21, 9.56MB/s] 75%|███████▍  | 580M/775M [01:31<00:21, 9.56MB/s] 75%|███████▍  | 581M/775M [01:31<00:20, 9.81MB/s] 75%|███████▌  | 581M/775M [01:31<00:20, 9.76MB/s] 75%|███████▌  | 583M/775M [01:31<00:20, 10.0MB/s] 75%|███████▌  | 583M/775M [01:31<00:20, 9.98MB/s] 75%|███████▌  | 585M/775M [01:31<00:19, 10.3MB/s] 76%|███████▌  | 586M/775M [01:32<00:19, 10.2MB/s] 76%|███████▌  | 587M/775M [01:32<00:19, 10.3MB/s] 76%|███████▌  | 588M/775M [01:32<00:19, 10.3MB/s] 76%|███████▌  | 589M/775M [01:32<00:18, 10.3MB/s] 76%|███████▌  | 590M/775M [01:32<00:18, 10.5MB/s] 76%|███████▌  | 591M/775M [01:32<00:18, 10.6MB/s] 76%|███████▋  | 592M/775M [01:32<00:18, 10.7MB/s] 77%|███████▋  | 593M/775M [01:32<00:17, 10.8MB/s] 77%|███████▋  | 594M/775M [01:32<00:17, 10.7MB/s] 77%|███████▋  | 595M/775M [01:32<00:16, 11.1MB/s] 77%|███████▋  | 596M/775M [01:33<00:17, 11.0MB/s] 77%|███████▋  | 597M/775M [01:33<00:16, 11.1MB/s] 77%|███████▋  | 598M/775M [01:33<00:18, 9.82MB/s] 77%|███████▋  | 599M/775M [01:33<00:19, 9.27MB/s] 77%|███████▋  | 600M/775M [01:33<00:23, 7.79MB/s] 78%|███████▊  | 601M/775M [01:33<00:25, 7.19MB/s] 78%|███████▊  | 601M/775M [01:33<00:26, 6.88MB/s] 78%|███████▊  | 602M/775M [01:34<00:27, 6.68MB/s] 78%|███████▊  | 603M/775M [01:34<00:27, 6.54MB/s] 78%|███████▊  | 603M/775M [01:34<00:26, 6.66MB/s] 78%|███████▊  | 604M/775M [01:34<00:29, 6.11MB/s] 78%|███████▊  | 605M/775M [01:34<00:27, 6.37MB/s] 78%|███████▊  | 605M/775M [01:34<00:28, 6.32MB/s] 78%|███████▊  | 606M/775M [01:34<00:27, 6.52MB/s] 78%|███████▊  | 607M/775M [01:34<00:27, 6.44MB/s] 78%|███████▊  | 607M/775M [01:34<00:27, 6.48MB/s] 79%|███████▊  | 608M/775M [01:34<00:25, 6.81MB/s] 79%|███████▊  | 609M/775M [01:35<00:25, 6.71MB/s] 79%|███████▊  | 610M/775M [01:35<00:24, 7.02MB/s] 79%|███████▉  | 610M/775M [01:35<00:24, 6.90MB/s] 79%|███████▉  | 611M/775M [01:35<00:24, 7.01MB/s] 79%|███████▉  | 612M/775M [01:35<00:24, 7.10MB/s] 79%|███████▉  | 612M/775M [01:35<00:23, 7.20MB/s] 79%|███████▉  | 613M/775M [01:35<00:23, 7.20MB/s] 79%|███████▉  | 614M/775M [01:35<00:22, 7.44MB/s] 79%|███████▉  | 615M/775M [01:35<00:22, 7.42MB/s] 79%|███████▉  | 615M/775M [01:36<00:21, 7.61MB/s] 80%|███████▉  | 616M/775M [01:36<00:21, 7.61MB/s] 80%|███████▉  | 617M/775M [01:36<00:21, 7.78MB/s] 80%|███████▉  | 618M/775M [01:36<00:20, 7.84MB/s] 80%|███████▉  | 619M/775M [01:36<00:20, 7.89MB/s] 80%|███████▉  | 619M/775M [01:36<00:20, 8.00MB/s] 80%|████████  | 620M/775M [01:36<00:20, 8.09MB/s] 80%|████████  | 621M/775M [01:36<00:19, 8.25MB/s] 80%|████████  | 622M/775M [01:36<00:19, 8.30MB/s] 80%|████████  | 623M/775M [01:36<00:19, 8.28MB/s] 80%|████████  | 623M/775M [01:37<00:19, 8.32MB/s] 81%|████████  | 624M/775M [01:37<00:18, 8.58MB/s] 81%|████████  | 625M/775M [01:37<00:18, 8.57MB/s] 81%|████████  | 626M/775M [01:37<00:19, 8.07MB/s] 81%|████████  | 627M/775M [01:37<00:20, 7.56MB/s] 81%|████████  | 627M/775M [01:37<00:21, 7.26MB/s] 81%|████████  | 628M/775M [01:37<00:22, 6.97MB/s] 81%|████████  | 629M/775M [01:37<00:21, 6.97MB/s] 81%|████████▏ | 629M/775M [01:37<00:23, 6.52MB/s] 81%|████████▏ | 630M/775M [01:38<00:25, 6.02MB/s] 81%|████████▏ | 631M/775M [01:38<00:26, 5.79MB/s] 81%|████████▏ | 631M/775M [01:38<00:27, 5.54MB/s] 82%|████████▏ | 632M/775M [01:38<00:27, 5.42MB/s] 82%|████████▏ | 632M/775M [01:38<00:28, 5.29MB/s] 82%|████████▏ | 633M/775M [01:38<00:27, 5.46MB/s] 82%|████████▏ | 633M/775M [01:38<00:27, 5.33MB/s] 82%|████████▏ | 634M/775M [01:38<00:33, 4.42MB/s] 82%|████████▏ | 634M/775M [01:39<00:36, 4.00MB/s] 82%|████████▏ | 635M/775M [01:39<00:39, 3.73MB/s] 82%|████████▏ | 635M/775M [01:39<00:39, 3.68MB/s] 82%|████████▏ | 636M/775M [01:39<00:41, 3.56MB/s] 82%|████████▏ | 636M/775M [01:39<00:43, 3.32MB/s] 82%|████████▏ | 636M/775M [01:39<00:46, 3.11MB/s] 82%|████████▏ | 636M/775M [01:39<00:48, 3.01MB/s] 82%|████████▏ | 637M/775M [01:39<00:48, 3.01MB/s] 82%|████████▏ | 637M/775M [01:39<00:43, 3.34MB/s] 82%|████████▏ | 638M/775M [01:40<00:39, 3.68MB/s] 82%|████████▏ | 638M/775M [01:40<00:38, 3.74MB/s] 82%|████████▏ | 638M/775M [01:40<00:38, 3.72MB/s] 82%|████████▏ | 639M/775M [01:40<00:39, 3.63MB/s] 83%|████████▎ | 639M/775M [01:40<00:47, 3.02MB/s] 83%|████████▎ | 639M/775M [01:40<00:49, 2.85MB/s] 83%|████████▎ | 640M/775M [01:40<00:53, 2.67MB/s] 83%|████████▎ | 640M/775M [01:40<00:53, 2.65MB/s] 83%|████████▎ | 640M/775M [01:41<00:53, 2.64MB/s] 83%|████████▎ | 641M/775M [01:41<00:53, 2.65MB/s] 83%|████████▎ | 641M/775M [01:41<00:52, 2.68MB/s] 83%|████████▎ | 641M/775M [01:41<00:52, 2.68MB/s] 83%|████████▎ | 641M/775M [01:41<00:50, 2.79MB/s] 83%|████████▎ | 642M/775M [01:41<00:41, 3.34MB/s] 83%|████████▎ | 642M/775M [01:41<00:41, 3.31MB/s] 83%|████████▎ | 643M/775M [01:41<00:39, 3.55MB/s] 83%|████████▎ | 643M/775M [01:41<00:37, 3.66MB/s] 83%|████████▎ | 644M/775M [01:42<00:34, 4.02MB/s] 83%|████████▎ | 644M/775M [01:42<00:29, 4.66MB/s] 83%|████████▎ | 645M/775M [01:42<00:31, 4.35MB/s] 83%|████████▎ | 645M/775M [01:42<00:31, 4.37MB/s] 83%|████████▎ | 645M/775M [01:42<00:36, 3.75MB/s] 83%|████████▎ | 646M/775M [01:42<00:38, 3.52MB/s] 83%|████████▎ | 646M/775M [01:42<00:40, 3.34MB/s] 83%|████████▎ | 647M/775M [01:42<00:43, 3.10MB/s] 84%|████████▎ | 647M/775M [01:43<00:46, 2.91MB/s] 84%|████████▎ | 647M/775M [01:43<00:45, 2.92MB/s] 84%|████████▎ | 647M/775M [01:43<01:05, 2.03MB/s] 84%|████████▎ | 648M/775M [01:43<01:10, 1.90MB/s] 84%|████████▎ | 648M/775M [01:43<01:23, 1.60MB/s] 84%|████████▎ | 648M/775M [01:43<01:16, 1.73MB/s] 84%|████████▎ | 648M/775M [01:44<01:20, 1.65MB/s] 84%|████████▎ | 649M/775M [01:44<01:14, 1.77MB/s] 84%|████████▍ | 649M/775M [01:44<01:10, 1.88MB/s] 84%|████████▍ | 649M/775M [01:44<01:03, 2.08MB/s] 84%|████████▍ | 649M/775M [01:44<01:13, 1.79MB/s] 84%|████████▍ | 650M/775M [01:44<01:12, 1.81MB/s] 84%|████████▍ | 650M/775M [01:44<01:11, 1.84MB/s] 84%|████████▍ | 650M/775M [01:44<01:08, 1.91MB/s] 84%|████████▍ | 650M/775M [01:45<01:06, 1.95MB/s] 84%|████████▍ | 650M/775M [01:45<01:07, 1.93MB/s] 84%|████████▍ | 651M/775M [01:45<01:07, 1.93MB/s] 84%|████████▍ | 651M/775M [01:45<01:13, 1.77MB/s] 84%|████████▍ | 651M/775M [01:45<01:14, 1.75MB/s] 84%|████████▍ | 651M/775M [01:45<01:15, 1.72MB/s] 84%|████████▍ | 651M/775M [01:45<01:17, 1.66MB/s] 84%|████████▍ | 652M/775M [01:45<01:19, 1.63MB/s] 84%|████████▍ | 652M/775M [01:46<01:17, 1.67MB/s] 84%|████████▍ | 652M/775M [01:46<01:03, 2.03MB/s] 84%|████████▍ | 652M/775M [01:46<00:54, 2.36MB/s] 84%|████████▍ | 653M/775M [01:46<00:44, 2.90MB/s] 84%|████████▍ | 653M/775M [01:46<00:35, 3.54MB/s] 84%|████████▍ | 654M/775M [01:46<00:30, 4.13MB/s] 84%|████████▍ | 654M/775M [01:46<00:28, 4.49MB/s] 85%|████████▍ | 655M/775M [01:46<00:25, 4.89MB/s] 85%|████████▍ | 656M/775M [01:46<00:24, 5.08MB/s] 85%|████████▍ | 656M/775M [01:46<00:23, 5.37MB/s] 85%|████████▍ | 657M/775M [01:47<00:22, 5.44MB/s] 85%|████████▍ | 657M/775M [01:47<00:21, 5.74MB/s] 85%|████████▍ | 658M/775M [01:47<00:21, 5.71MB/s] 85%|████████▍ | 658M/775M [01:47<00:21, 5.80MB/s] 85%|████████▌ | 659M/775M [01:47<00:20, 5.95MB/s] 85%|████████▌ | 660M/775M [01:47<00:19, 6.07MB/s] 85%|████████▌ | 660M/775M [01:47<00:19, 6.09MB/s] 85%|████████▌ | 661M/775M [01:47<00:18, 6.30MB/s] 85%|████████▌ | 662M/775M [01:47<00:18, 6.44MB/s] 85%|████████▌ | 662M/775M [01:48<00:18, 6.42MB/s] 86%|████████▌ | 663M/775M [01:48<00:17, 6.58MB/s] 86%|████████▌ | 664M/775M [01:48<00:17, 6.53MB/s] 86%|████████▌ | 664M/775M [01:48<00:17, 6.54MB/s] 86%|████████▌ | 665M/775M [01:48<00:17, 6.65MB/s] 86%|████████▌ | 666M/775M [01:48<00:16, 6.86MB/s] 86%|████████▌ | 666M/775M [01:48<00:16, 6.95MB/s] 86%|████████▌ | 667M/775M [01:48<00:16, 6.98MB/s] 86%|████████▌ | 668M/775M [01:48<00:15, 7.12MB/s] 86%|████████▋ | 668M/775M [01:48<00:15, 7.23MB/s] 86%|████████▋ | 669M/775M [01:49<00:15, 7.33MB/s] 86%|████████▋ | 670M/775M [01:49<00:14, 7.39MB/s] 87%|████████▋ | 671M/775M [01:49<00:14, 7.49MB/s] 87%|████████▋ | 671M/775M [01:49<00:14, 7.38MB/s] 87%|████████▋ | 672M/775M [01:49<00:14, 7.66MB/s] 87%|████████▋ | 673M/775M [01:49<00:14, 7.55MB/s] 87%|████████▋ | 674M/775M [01:49<00:13, 7.91MB/s] 87%|████████▋ | 674M/775M [01:49<00:13, 7.73MB/s] 87%|████████▋ | 675M/775M [01:49<00:13, 7.92MB/s] 87%|████████▋ | 676M/775M [01:49<00:12, 8.23MB/s] 87%|████████▋ | 677M/775M [01:50<00:12, 8.03MB/s] 88%|████████▊ | 678M/775M [01:50<00:12, 8.39MB/s] 88%|████████▊ | 679M/775M [01:50<00:12, 8.25MB/s] 88%|████████▊ | 680M/775M [01:50<00:11, 8.38MB/s] 88%|████████▊ | 680M/775M [01:50<00:11, 8.37MB/s] 88%|████████▊ | 681M/775M [01:50<00:11, 8.68MB/s] 88%|████████▊ | 682M/775M [01:50<00:11, 8.58MB/s] 88%|████████▊ | 683M/775M [01:50<00:10, 8.82MB/s] 88%|████████▊ | 684M/775M [01:50<00:10, 8.80MB/s] 88%|████████▊ | 685M/775M [01:51<00:10, 9.08MB/s] 89%|████████▊ | 686M/775M [01:51<00:10, 8.94MB/s] 89%|████████▊ | 687M/775M [01:51<00:09, 9.26MB/s] 89%|████████▉ | 688M/775M [01:51<00:09, 9.16MB/s] 89%|████████▉ | 688M/775M [01:51<00:10, 8.42MB/s] 89%|████████▉ | 689M/775M [01:51<00:11, 7.52MB/s] 89%|████████▉ | 690M/775M [01:51<00:13, 6.45MB/s] 89%|████████▉ | 691M/775M [01:52<00:19, 4.57MB/s] 89%|████████▉ | 691M/775M [01:52<00:24, 3.63MB/s] 89%|████████▉ | 692M/775M [01:52<00:25, 3.38MB/s] 89%|████████▉ | 692M/775M [01:52<00:27, 3.19MB/s] 89%|████████▉ | 692M/775M [01:52<00:29, 2.94MB/s] 89%|████████▉ | 693M/775M [01:52<00:32, 2.65MB/s] 89%|████████▉ | 693M/775M [01:53<00:34, 2.47MB/s] 89%|████████▉ | 693M/775M [01:53<00:37, 2.28MB/s] 90%|████████▉ | 693M/775M [01:53<00:38, 2.19MB/s] 90%|████████▉ | 694M/775M [01:53<00:41, 2.07MB/s] 90%|████████▉ | 694M/775M [01:53<00:36, 2.29MB/s] 90%|████████▉ | 694M/775M [01:53<00:34, 2.44MB/s] 90%|████████▉ | 694M/775M [01:53<00:34, 2.41MB/s] 90%|████████▉ | 695M/775M [01:53<00:33, 2.47MB/s] 90%|████████▉ | 695M/775M [01:54<00:32, 2.55MB/s] 90%|████████▉ | 695M/775M [01:54<00:33, 2.48MB/s] 90%|████████▉ | 696M/775M [01:54<00:31, 2.63MB/s] 90%|████████▉ | 696M/775M [01:54<00:31, 2.60MB/s] 90%|████████▉ | 696M/775M [01:54<00:32, 2.57MB/s] 90%|████████▉ | 696M/775M [01:54<00:28, 2.84MB/s] 90%|████████▉ | 697M/775M [01:54<00:25, 3.19MB/s] 90%|█████████ | 697M/775M [01:54<00:22, 3.59MB/s] 90%|█████████ | 698M/775M [01:54<00:20, 3.95MB/s] 90%|█████████ | 698M/775M [01:54<00:20, 3.90MB/s] 90%|█████████ | 699M/775M [01:55<00:19, 4.19MB/s] 90%|█████████ | 699M/775M [01:55<00:18, 4.34MB/s] 90%|█████████ | 700M/775M [01:55<00:18, 4.24MB/s] 90%|█████████ | 700M/775M [01:55<00:17, 4.41MB/s] 90%|█████████ | 700M/775M [01:55<00:18, 4.32MB/s] 90%|█████████ | 701M/775M [01:55<00:19, 3.88MB/s] 91%|█████████ | 701M/775M [01:55<00:18, 4.14MB/s] 91%|█████████ | 702M/775M [01:55<00:18, 4.11MB/s] 91%|█████████ | 702M/775M [01:55<00:17, 4.45MB/s] 91%|█████████ | 703M/775M [01:56<00:15, 4.88MB/s] 91%|█████████ | 703M/775M [01:56<00:14, 5.19MB/s] 91%|█████████ | 704M/775M [01:56<00:14, 5.18MB/s] 91%|█████████ | 705M/775M [01:56<00:14, 5.12MB/s] 91%|█████████ | 705M/775M [01:56<00:15, 4.82MB/s] 91%|█████████ | 705M/775M [01:56<00:15, 4.81MB/s] 91%|█████████ | 706M/775M [01:56<00:14, 5.00MB/s] 91%|█████████ | 707M/775M [01:56<00:13, 5.34MB/s] 91%|█████████▏| 707M/775M [01:56<00:11, 5.95MB/s] 91%|█████████▏| 708M/775M [01:57<00:11, 6.32MB/s] 91%|█████████▏| 709M/775M [01:57<00:10, 6.40MB/s] 92%|█████████▏| 709M/775M [01:57<00:10, 6.55MB/s] 92%|█████████▏| 710M/775M [01:57<00:09, 6.87MB/s] 92%|█████████▏| 711M/775M [01:57<00:09, 7.18MB/s] 92%|█████████▏| 712M/775M [01:57<00:08, 7.39MB/s] 92%|█████████▏| 713M/775M [01:57<00:08, 7.59MB/s] 92%|█████████▏| 714M/775M [01:57<00:08, 7.76MB/s] 92%|█████████▏| 714M/775M [01:57<00:07, 7.92MB/s] 92%|█████████▏| 715M/775M [01:58<00:07, 8.03MB/s] 92%|█████████▏| 716M/775M [01:58<00:08, 7.56MB/s] 93%|█████████▎| 717M/775M [01:58<00:08, 7.21MB/s] 93%|█████████▎| 717M/775M [01:58<00:08, 6.72MB/s] 93%|█████████▎| 718M/775M [01:58<00:09, 6.36MB/s] 93%|█████████▎| 719M/775M [01:58<00:08, 6.53MB/s] 93%|█████████▎| 719M/775M [01:58<00:09, 6.25MB/s] 93%|█████████▎| 720M/775M [01:58<00:09, 6.36MB/s] 93%|█████████▎| 721M/775M [01:58<00:08, 6.58MB/s] 93%|█████████▎| 721M/775M [01:59<00:08, 6.52MB/s] 93%|█████████▎| 722M/775M [01:59<00:08, 6.60MB/s] 93%|█████████▎| 723M/775M [01:59<00:07, 6.89MB/s] 93%|█████████▎| 723M/775M [01:59<00:08, 6.55MB/s] 93%|█████████▎| 724M/775M [01:59<00:07, 6.89MB/s] 94%|█████████▎| 725M/775M [01:59<00:07, 6.85MB/s] 94%|█████████▎| 726M/775M [01:59<00:07, 7.07MB/s] 94%|█████████▍| 726M/775M [01:59<00:07, 6.35MB/s] 94%|█████████▍| 727M/775M [01:59<00:08, 5.78MB/s] 94%|█████████▍| 728M/775M [02:00<00:09, 5.19MB/s] 94%|█████████▍| 728M/775M [02:00<00:09, 5.21MB/s] 94%|█████████▍| 729M/775M [02:00<00:09, 5.07MB/s] 94%|█████████▍| 729M/775M [02:00<00:09, 5.10MB/s] 94%|█████████▍| 730M/775M [02:00<00:08, 5.74MB/s] 94%|█████████▍| 731M/775M [02:00<00:07, 6.49MB/s] 94%|█████████▍| 731M/775M [02:00<00:06, 6.90MB/s] 95%|█████████▍| 732M/775M [02:00<00:06, 7.39MB/s] 95%|█████████▍| 733M/775M [02:00<00:05, 7.58MB/s] 95%|█████████▍| 734M/775M [02:01<00:05, 7.85MB/s] 95%|█████████▍| 735M/775M [02:01<00:05, 7.94MB/s] 95%|█████████▍| 735M/775M [02:01<00:05, 7.92MB/s] 95%|█████████▌| 736M/775M [02:01<00:05, 7.91MB/s] 95%|█████████▌| 737M/775M [02:01<00:05, 7.89MB/s] 95%|█████████▌| 738M/775M [02:01<00:05, 6.96MB/s] 95%|█████████▌| 738M/775M [02:01<00:05, 6.49MB/s] 95%|█████████▌| 739M/775M [02:01<00:05, 6.52MB/s] 96%|█████████▌| 740M/775M [02:01<00:05, 6.99MB/s] 96%|█████████▌| 741M/775M [02:02<00:05, 6.98MB/s] 96%|█████████▌| 741M/775M [02:02<00:05, 6.37MB/s] 96%|█████████▌| 742M/775M [02:02<00:05, 6.17MB/s] 96%|█████████▌| 742M/775M [02:02<00:05, 5.63MB/s] 96%|█████████▌| 743M/775M [02:02<00:06, 5.42MB/s] 96%|█████████▌| 744M/775M [02:02<00:06, 4.97MB/s] 96%|█████████▌| 744M/775M [02:02<00:06, 4.81MB/s] 96%|█████████▌| 744M/775M [02:02<00:06, 4.66MB/s] 96%|█████████▌| 745M/775M [02:03<00:07, 4.14MB/s] 96%|█████████▌| 745M/775M [02:03<00:07, 4.01MB/s] 96%|█████████▋| 746M/775M [02:03<00:08, 3.70MB/s] 96%|█████████▋| 746M/775M [02:03<00:08, 3.65MB/s] 96%|█████████▋| 746M/775M [02:03<00:07, 3.74MB/s] 96%|█████████▋| 747M/775M [02:03<00:07, 4.09MB/s] 96%|█████████▋| 747M/775M [02:03<00:07, 3.98MB/s] 97%|█████████▋| 748M/775M [02:03<00:07, 3.97MB/s] 97%|█████████▋| 748M/775M [02:03<00:07, 3.96MB/s] 97%|█████████▋| 749M/775M [02:04<00:06, 3.91MB/s] 97%|█████████▋| 749M/775M [02:04<00:07, 3.57MB/s] 97%|█████████▋| 749M/775M [02:04<00:07, 3.41MB/s] 97%|█████████▋| 750M/775M [02:04<00:07, 3.54MB/s] 97%|█████████▋| 750M/775M [02:04<00:08, 3.13MB/s] 97%|█████████▋| 750M/775M [02:04<00:08, 3.13MB/s] 97%|█████████▋| 751M/775M [02:04<00:08, 3.14MB/s] 97%|█████████▋| 751M/775M [02:04<00:07, 3.15MB/s] 97%|█████████▋| 751M/775M [02:04<00:07, 3.29MB/s] 97%|█████████▋| 752M/775M [02:05<00:07, 3.28MB/s] 97%|█████████▋| 752M/775M [02:05<00:06, 3.54MB/s] 97%|█████████▋| 752M/775M [02:05<00:06, 3.71MB/s] 97%|█████████▋| 753M/775M [02:05<00:05, 3.81MB/s] 97%|█████████▋| 753M/775M [02:05<00:05, 3.76MB/s] 97%|█████████▋| 754M/775M [02:05<00:06, 3.59MB/s] 97%|█████████▋| 754M/775M [02:05<00:06, 3.36MB/s] 97%|█████████▋| 754M/775M [02:05<00:06, 3.32MB/s] 97%|█████████▋| 755M/775M [02:05<00:06, 3.10MB/s] 97%|█████████▋| 755M/775M [02:06<00:07, 2.84MB/s] 97%|█████████▋| 755M/775M [02:06<00:06, 3.07MB/s] 98%|█████████▊| 756M/775M [02:06<00:05, 3.44MB/s] 98%|█████████▊| 756M/775M [02:06<00:04, 4.43MB/s] 98%|█████████▊| 757M/775M [02:06<00:03, 5.33MB/s] 98%|█████████▊| 758M/775M [02:06<00:02, 6.76MB/s] 98%|█████████▊| 759M/775M [02:06<00:02, 7.79MB/s] 98%|█████████▊| 760M/775M [02:06<00:01, 8.57MB/s] 98%|█████████▊| 761M/775M [02:06<00:01, 9.07MB/s] 98%|█████████▊| 762M/775M [02:07<00:01, 9.63MB/s] 99%|█████████▊| 763M/775M [02:07<00:01, 9.84MB/s] 99%|█████████▊| 764M/775M [02:07<00:01, 9.80MB/s] 99%|█████████▉| 766M/775M [02:07<00:00, 10.4MB/s] 99%|█████████▉| 767M/775M [02:07<00:00, 10.2MB/s] 99%|█████████▉| 768M/775M [02:07<00:00, 10.8MB/s] 99%|█████████▉| 769M/775M [02:07<00:00, 10.7MB/s] 99%|█████████▉| 770M/775M [02:07<00:00, 11.0MB/s]100%|█████████▉| 771M/775M [02:07<00:00, 10.8MB/s]100%|█████████▉| 772M/775M [02:07<00:00, 11.3MB/s]100%|█████████▉| 773M/775M [02:08<00:00, 10.9MB/s]100%|█████████▉| 774M/775M [02:08<00:00, 11.1MB/s]100%|██████████| 775M/775M [02:08<00:00, 6.34MB/s]
[32m[2023-12-21 10:20:00,179] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2023-12-21 10:20:04,317] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 10:20:04.323783   312 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 10:20:04,786] [ WARNING][0m - Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder_bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 10:20:04,786] [ WARNING][0m - Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 10:20:04,790] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:20:04,790] [    INFO][0m - Downloading https://bj.bcebos.com/paddle-hapi/models/bert/bert-base-uncased-vocab.txt and saved to /root/.paddlenlp/models/bert-base-uncased[0m
[32m[2023-12-21 10:20:05,109] [    INFO][0m - Downloading bert-base-uncased-vocab.txt from https://bj.bcebos.com/paddle-hapi/models/bert/bert-base-uncased-vocab.txt[0m
  0%|          | 0.00/226k [00:00<?, ?B/s]  8%|▊         | 19.0k/226k [00:00<00:01, 187kB/s] 30%|██▉       | 67.0k/226k [00:00<00:00, 303kB/s] 51%|█████     | 115k/226k [00:00<00:00, 361kB/s]  79%|███████▉  | 179k/226k [00:00<00:00, 416kB/s]100%|██████████| 226k/226k [00:00<00:00, 438kB/s]
[32m[2023-12-21 10:20:05,851] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:20:05,851] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
[32m[2023-12-21 10:20:30,766] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 10:20:31,480] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 10:20:31,481] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 10:20:31,481] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 10, epoch: 1, batch: 10, loss: 0.59083, accu: 0.62500, speed: 4.37 step/s
global step 20, epoch: 1, batch: 20, loss: 0.47984, accu: 0.68437, speed: 4.25 step/s
global step 30, epoch: 1, batch: 30, loss: 0.76057, accu: 0.71771, speed: 4.24 step/s
global step 40, epoch: 1, batch: 40, loss: 0.45982, accu: 0.73594, speed: 4.33 step/s
global step 50, epoch: 1, batch: 50, loss: 0.40959, accu: 0.74625, speed: 4.19 step/s
global step 60, epoch: 1, batch: 60, loss: 0.23232, accu: 0.76823, speed: 4.38 step/s
global step 70, epoch: 1, batch: 70, loss: 0.24753, accu: 0.78438, speed: 4.22 step/s
global step 80, epoch: 1, batch: 80, loss: 0.46236, accu: 0.79453, speed: 4.27 step/s
global step 90, epoch: 1, batch: 90, loss: 0.35313, accu: 0.79896, speed: 4.35 step/s
global step 100, epoch: 1, batch: 100, loss: 0.33824, accu: 0.80563, speed: 4.35 step/s
eval loss: 0.33295, accu: 0.86927
global step 110, epoch: 1, batch: 110, loss: 0.19627, accu: 0.87187, speed: 2.23 step/s
global step 120, epoch: 1, batch: 120, loss: 0.36324, accu: 0.86562, speed: 4.30 step/s
global step 130, epoch: 1, batch: 130, loss: 0.36525, accu: 0.87500, speed: 4.32 step/s
global step 140, epoch: 1, batch: 140, loss: 0.14420, accu: 0.88047, speed: 4.47 step/s
global step 150, epoch: 1, batch: 150, loss: 0.29199, accu: 0.88625, speed: 4.26 step/s
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/train.py --max_steps 150 --device=xpu  --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 --epoch=1     --batch_size=32     >/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 10:20:47,282] [    INFO][0m - We are using <class 'paddlenlp.transformers.bert.modeling.BertForSequenceClassification'> to load '/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model'.[0m
[32m[2023-12-21 10:20:47,282] [    INFO][0m - Loading configuration file /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 10:20:47,283] [    INFO][0m - Loading weights file /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 10:20:47,635] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 10:20:47.644352   386 xpu_context.cc:151] Please NOTE: xpu device: 0
[32m[2023-12-21 10:20:48,132] [    INFO][0m - All model checkpoint weights were used when initializing BertForSequenceClassification.
[0m
[32m[2023-12-21 10:20:48,133] [    INFO][0m - All the weights of BertForSequenceClassification were initialized from the model checkpoint at /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 10:20:50.005010   386 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:20:55.168072   454 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:20:55.173205   454 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:20:55.427377   454 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 10:20:55.431339   454 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:20:55.484217   454 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:20:55.526893   454 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 10:20:55.555960   454 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 10:20:55.574826   454 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 10:20:55.587620   454 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:20:55.613619   454 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[37m---    enabled FC MKL-DNN for 74 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 10:20:55.617206   454 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fc with gelu activation[0m
I1221 10:20:55.622464   454 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 10:20:55.640017   454 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:20:55.675094   454 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:20:55.676684   454 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 10:20:55.676700   454 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 10:20:55.678206   454 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 10:20:55,678] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:20:55,678] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:20:55,697] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:20:55,698] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 10:20:55.722258   454 onednn_context.cc:81] oneDNN v3.2.1
Data: uneasy mishmash of styles and genres . 	 Label: negative
Data: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 	 Label: negative
Data: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 	 Label: positive
Data: director rob marshall went out gunning to make a great one . 	 Label: positive
Data: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 	 Label: positive
Data: a well-made and often lovely depiction of the mysteries of friendship . 	 Label: positive
Data: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 	 Label: positive
Data: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 	 Label: positive
Data: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 	 Label: positive
Data: this is junk food cinema at its greasiest . 	 Label: negative
Data: it 's also heavy-handed and devotes too much time to bigoted views . 	 Label: negative
Data: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid . 	 Label: positive
Data: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it . 	 Label: positive
Data: moore 's performance impresses almost as much as her work with haynes in 1995 's safe . 	 Label: positive
Data: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich . 	 Label: positive
Data: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism . 	 Label: positive
Data: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance . 	 Label: positive
Data: a remarkable 179-minute meditation on the nature of revolution . 	 Label: positive
Data: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . 	 Label: positive
Data: it 's just incredibly dull . 	 Label: negative
Data: some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future . 	 Label: negative
Data: a feel-good picture in the best sense of the term . 	 Label: positive
Data: mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor . 	 Label: negative
Data: good movie . 	 Label: positive
Data: with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry . 	 Label: positive
Data: starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . 	 Label: negative
Data: the best way to hope for any chance of enjoying this film is by lowering your expectations . 	 Label: negative
Data: while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun . 	 Label: negative
Data: this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border . 	 Label: positive
Data: here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you . 	 Label: positive
Data: most new movies have a bright sheen . 	 Label: positive
Data: so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler . 	 Label: negative
Data: nothing more than a mediocre trifle . 	 Label: negative
Data: a model of what films like this should be like . 	 Label: positive
Data: gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up . 	 Label: negative
Data: doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere . 	 Label: positive
Data: another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through . 	 Label: negative
Data: it becomes gimmicky instead of compelling . 	 Label: negative
Data: falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny . 	 Label: negative
Data: verbinski substitutes atmosphere for action , tedium for thrills . 	 Label: negative
Data: it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . 	 Label: positive
Data: a bigger holiday downer than your end-of-year 401 ( k ) statement . 	 Label: negative
Data: as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining . 	 Label: positive
Data: sensitive , insightful and beautifully rendered film . 	 Label: positive
Data: no number of fantastic sets , extras , costumes and spectacular locales can disguise the emptiness at the center of the story . 	 Label: negative
Data: not too far below the gloss you can still feel director denis villeneuve 's beating heart and the fondness he has for his characters . 	 Label: positive
Data: for benigni it was n't shakespeare whom he wanted to define his career with but pinocchio . 	 Label: negative
Data: how did it ever get made ? 	 Label: negative
Data: it would n't be my preferred way of spending 100 minutes or $ 7.00 . 	 Label: negative
Data: the furious coherence that ( deniro ) brings to this part only underscores the fuzzy sentimentality of the movie itself , which feels , as it plods toward the end , less like a movie than like the filmed reading of a script in need of polishing . 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:21:07.721864   521 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:21:07.726783   521 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:21:08.010872   521 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 10:21:08.014698   521 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:21:08.068193   521 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:21:08.110822   521 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 10:21:08.140058   521 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 10:21:08.159070   521 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 10:21:08.171859   521 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:21:08.198104   521 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[37m---    enabled FC MKL-DNN for 74 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 10:21:08.201764   521 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fc with gelu activation[0m
I1221 10:21:08.207078   521 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 10:21:08.224787   521 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:21:08.259991   521 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:21:08.261617   521 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 10:21:08.261637   521 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 10:21:08.263052   521 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 10:21:08,263] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:21:08,263] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:21:08,282] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:21:08,282] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 10:21:08.306988   521 onednn_context.cc:81] oneDNN v3.2.1
Data: uneasy mishmash of styles and genres . 	 Label: negative
Data: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 	 Label: negative
Data: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 	 Label: positive
Data: director rob marshall went out gunning to make a great one . 	 Label: positive
Data: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 	 Label: positive
Data: a well-made and often lovely depiction of the mysteries of friendship . 	 Label: positive
Data: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 	 Label: positive
Data: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 	 Label: positive
Data: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 	 Label: positive
Data: this is junk food cinema at its greasiest . 	 Label: negative
Data: it 's also heavy-handed and devotes too much time to bigoted views . 	 Label: negative
Data: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid . 	 Label: positive
Data: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it . 	 Label: positive
Data: moore 's performance impresses almost as much as her work with haynes in 1995 's safe . 	 Label: positive
Data: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich . 	 Label: positive
Data: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism . 	 Label: positive
Data: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance . 	 Label: positive
Data: a remarkable 179-minute meditation on the nature of revolution . 	 Label: positive
Data: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . 	 Label: positive
Data: it 's just incredibly dull . 	 Label: negative
Data: some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future . 	 Label: negative
Data: a feel-good picture in the best sense of the term . 	 Label: positive
Data: mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor . 	 Label: negative
Data: good movie . 	 Label: positive
Data: with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry . 	 Label: positive
Data: starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . 	 Label: negative
Data: the best way to hope for any chance of enjoying this film is by lowering your expectations . 	 Label: negative
Data: while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun . 	 Label: negative
Data: this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border . 	 Label: positive
Data: here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you . 	 Label: positive
Data: most new movies have a bright sheen . 	 Label: positive
Data: so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler . 	 Label: negative
Data: nothing more than a mediocre trifle . 	 Label: negative
Data: a model of what films like this should be like . 	 Label: positive
Data: gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up . 	 Label: negative
Data: doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere . 	 Label: positive
Data: another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through . 	 Label: negative
Data: it becomes gimmicky instead of compelling . 	 Label: negative
Data: falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny . 	 Label: negative
Data: verbinski substitutes atmosphere for action , tedium for thrills . 	 Label: negative
Data: it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . 	 Label: positive
Data: a bigger holiday downer than your end-of-year 401 ( k ) statement . 	 Label: negative
Data: as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining . 	 Label: positive
Data: sensitive , insightful and beautifully rendered film . 	 Label: positive
Data: no number of fantastic sets , extras , costumes and spectacular locales can disguise the emptiness at the center of the story . 	 Label: negative
Data: not too far below the gloss you can still feel director denis villeneuve 's beating heart and the fondness he has for his characters . 	 Label: positive
Data: for benigni it was n't shakespeare whom he wanted to define his career with but pinocchio . 	 Label: negative
Data: how did it ever get made ? 	 Label: negative
Data: it would n't be my preferred way of spending 100 minutes or $ 7.00 . 	 Label: negative
Data: the furious coherence that ( deniro ) brings to this part only underscores the fuzzy sentimentality of the movie itself , which feels , as it plods toward the end , less like a movie than like the filmed reading of a script in need of polishing . 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:21:14.221863   592 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:21:14.226994   592 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:21:14.465703   592 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 10:21:14.469434   592 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:21:14.523146   592 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:21:14.566260   592 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 10:21:14.595170   592 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 10:21:14.614238   592 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 10:21:14.627139   592 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:21:14.653319   592 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[37m---    enabled FC MKL-DNN for 74 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 10:21:14.656975   592 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fc with gelu activation[0m
I1221 10:21:14.662361   592 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 10:21:14.679919   592 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:21:14.714821   592 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:21:14.716456   592 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 10:21:14.716470   592 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 10:21:14.717831   592 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 10:21:14,718] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:21:14,718] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:21:14,737] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:21:14,737] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 10:21:14.761745   592 onednn_context.cc:81] oneDNN v3.2.1
Data: uneasy mishmash of styles and genres . 	 Label: negative
Data: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 	 Label: negative
Data: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 	 Label: positive
Data: director rob marshall went out gunning to make a great one . 	 Label: positive
Data: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 	 Label: positive
Data: a well-made and often lovely depiction of the mysteries of friendship . 	 Label: positive
Data: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 	 Label: positive
Data: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 	 Label: positive
Data: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 	 Label: positive
Data: this is junk food cinema at its greasiest . 	 Label: negative
Data: it 's also heavy-handed and devotes too much time to bigoted views . 	 Label: negative
Data: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid . 	 Label: positive
Data: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it . 	 Label: positive
Data: moore 's performance impresses almost as much as her work with haynes in 1995 's safe . 	 Label: positive
Data: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich . 	 Label: positive
Data: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism . 	 Label: positive
Data: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance . 	 Label: positive
Data: a remarkable 179-minute meditation on the nature of revolution . 	 Label: positive
Data: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . 	 Label: positive
Data: it 's just incredibly dull . 	 Label: negative
Data: some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future . 	 Label: negative
Data: a feel-good picture in the best sense of the term . 	 Label: positive
Data: mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor . 	 Label: negative
Data: good movie . 	 Label: positive
Data: with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry . 	 Label: positive
Data: starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . 	 Label: negative
Data: the best way to hope for any chance of enjoying this film is by lowering your expectations . 	 Label: negative
Data: while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun . 	 Label: negative
Data: this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border . 	 Label: positive
Data: here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you . 	 Label: positive
Data: most new movies have a bright sheen . 	 Label: positive
Data: so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler . 	 Label: negative
Data: nothing more than a mediocre trifle . 	 Label: negative
Data: a model of what films like this should be like . 	 Label: positive
Data: gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up . 	 Label: negative
Data: doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere . 	 Label: positive
Data: another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through . 	 Label: negative
Data: it becomes gimmicky instead of compelling . 	 Label: negative
Data: falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny . 	 Label: negative
Data: verbinski substitutes atmosphere for action , tedium for thrills . 	 Label: negative
Data: it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . 	 Label: positive
Data: a bigger holiday downer than your end-of-year 401 ( k ) statement . 	 Label: negative
Data: as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining . 	 Label: positive
Data: sensitive , insightful and beautifully rendered film . 	 Label: positive
Data: no number of fantastic sets , extras , costumes and spectacular locales can disguise the emptiness at the center of the story . 	 Label: negative
Data: not too far below the gloss you can still feel director denis villeneuve 's beating heart and the fondness he has for his characters . 	 Label: positive
Data: for benigni it was n't shakespeare whom he wanted to define his career with but pinocchio . 	 Label: negative
Data: how did it ever get made ? 	 Label: negative
Data: it would n't be my preferred way of spending 100 minutes or $ 7.00 . 	 Label: negative
Data: the furious coherence that ( deniro ) brings to this part only underscores the fuzzy sentimentality of the movie itself , which feels , as it plods toward the end , less like a movie than like the filmed reading of a script in need of polishing . 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/predict.py --max_steps 50 --device=npu --use_tensorrt=False --precision=fp32 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32   --benchmark=False     > /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0_usetrt_False_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0_usetrt_False_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 10:21:24,363 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 10:21:24,363 auto_parallel_config: None
LAUNCH INFO 2023-12-21 10:21:24,363 auto_tuner_json: None
LAUNCH INFO 2023-12-21 10:21:24,363 devices: 0,1
LAUNCH INFO 2023-12-21 10:21:24,363 elastic_level: -1
LAUNCH INFO 2023-12-21 10:21:24,363 elastic_timeout: 30
LAUNCH INFO 2023-12-21 10:21:24,363 enable_gpu_log: True
LAUNCH INFO 2023-12-21 10:21:24,363 gloo_port: 6767
LAUNCH INFO 2023-12-21 10:21:24,363 host: None
LAUNCH INFO 2023-12-21 10:21:24,363 ips: None
LAUNCH INFO 2023-12-21 10:21:24,363 job_id: default
LAUNCH INFO 2023-12-21 10:21:24,363 legacy: False
LAUNCH INFO 2023-12-21 10:21:24,363 log_dir: log
LAUNCH INFO 2023-12-21 10:21:24,363 log_level: INFO
LAUNCH INFO 2023-12-21 10:21:24,363 log_overwrite: False
LAUNCH INFO 2023-12-21 10:21:24,364 master: None
LAUNCH INFO 2023-12-21 10:21:24,364 max_restart: 3
LAUNCH INFO 2023-12-21 10:21:24,364 nnodes: 1
LAUNCH INFO 2023-12-21 10:21:24,364 nproc_per_node: None
LAUNCH INFO 2023-12-21 10:21:24,364 rank: -1
LAUNCH INFO 2023-12-21 10:21:24,364 run_mode: collective
LAUNCH INFO 2023-12-21 10:21:24,364 server_num: None
LAUNCH INFO 2023-12-21 10:21:24,364 servers: 
LAUNCH INFO 2023-12-21 10:21:24,364 sort_ip: False
LAUNCH INFO 2023-12-21 10:21:24,364 start_port: 6070
LAUNCH INFO 2023-12-21 10:21:24,364 trainer_num: None
LAUNCH INFO 2023-12-21 10:21:24,364 trainers: 
LAUNCH INFO 2023-12-21 10:21:24,364 training_script: ./test_tipc/bert_base_text_cls/train.py
LAUNCH INFO 2023-12-21 10:21:24,364 training_script_args: ['--max_steps', '150', '--device=xpu', '--save_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1', '--epoch=1', '--batch_size=32']
LAUNCH INFO 2023-12-21 10:21:24,364 with_gloo: 1
LAUNCH INFO 2023-12-21 10:21:24,364 --------------------------------------------------
LAUNCH INFO 2023-12-21 10:21:24,365 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 10:21:24,366 Run Pod: ylituh, replicas 2, status ready
LAUNCH INFO 2023-12-21 10:21:24,381 Watching Pod: ylituh, replicas 2, status running
LAUNCH INFO 2023-12-21 10:22:52,483 Pod completed
LAUNCH INFO 2023-12-21 10:22:52,483 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 10:21:27.622746   710 tcp_utils.cc:181] The server starts to listen on IP_ANY:61105
I1221 10:21:27.623013   710 tcp_utils.cc:130] Successfully connected to 127.0.0.1:61105
[32m[2023-12-21 10:21:31,052] [    INFO][0m - We are using <class 'paddlenlp.transformers.bert.modeling.BertForSequenceClassification'> to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:21:31,053] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2023-12-21 10:21:31,053] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/bert-base-uncased/model_state.pdparams[0m
[32m[2023-12-21 10:21:35,217] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 10:21:35.223018   710 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 10:21:35,698] [ WARNING][0m - Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder_bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 10:21:35,698] [ WARNING][0m - Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 10:21:35,703] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:21:35,703] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:21:35,723] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:21:35,723] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
global step 10, epoch: 1, batch: 10, loss: 0.57476, accu: 0.65938, speed: 2.28 step/s
global step 20, epoch: 1, batch: 20, loss: 0.50990, accu: 0.75781, speed: 2.12 step/s
global step 30, epoch: 1, batch: 30, loss: 0.41407, accu: 0.78438, speed: 2.12 step/s
global step 40, epoch: 1, batch: 40, loss: 0.43011, accu: 0.79531, speed: 2.14 step/s
global step 50, epoch: 1, batch: 50, loss: 0.14996, accu: 0.81188, speed: 2.11 step/s
global step 60, epoch: 1, batch: 60, loss: 0.22726, accu: 0.82656, speed: 2.12 step/s
global step 70, epoch: 1, batch: 70, loss: 0.11940, accu: 0.83795, speed: 2.10 step/s
global step 80, epoch: 1, batch: 80, loss: 0.20189, accu: 0.84297, speed: 2.14 step/s
global step 90, epoch: 1, batch: 90, loss: 0.25192, accu: 0.85000, speed: 2.17 step/s
global step 100, epoch: 1, batch: 100, loss: 0.31070, accu: 0.85344, speed: 2.16 step/s
eval loss: 0.25177, accu: 0.89679
[32m[2023-12-21 10:22:25,059] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 10:22:25,855] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 10:22:25,856] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 10:22:25,856] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 1, batch: 110, loss: 0.26791, accu: 0.91875, speed: 1.50 step/s
global step 120, epoch: 1, batch: 120, loss: 0.17735, accu: 0.91250, speed: 2.16 step/s
global step 130, epoch: 1, batch: 130, loss: 0.18276, accu: 0.91458, speed: 1.96 step/s
global step 140, epoch: 1, batch: 140, loss: 0.13171, accu: 0.91484, speed: 1.87 step/s
global step 150, epoch: 1, batch: 150, loss: 0.14340, accu: 0.91375, speed: 2.08 step/s
I1221 10:22:51.194732   744 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python -m paddle.distributed.launch --gpus=0,1 ./test_tipc/bert_base_text_cls/train.py --max_steps 150 --device=xpu --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 --epoch=1     --batch_size=32     - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 10:22:56,032] [    INFO][0m - We are using <class 'paddlenlp.transformers.bert.modeling.BertForSequenceClassification'> to load '/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model'.[0m
[32m[2023-12-21 10:22:56,032] [    INFO][0m - Loading configuration file /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 10:22:56,033] [    INFO][0m - Loading weights file /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 10:22:56,366] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 10:22:56.374655   774 xpu_context.cc:151] Please NOTE: xpu device: 0
[32m[2023-12-21 10:22:56,866] [    INFO][0m - All model checkpoint weights were used when initializing BertForSequenceClassification.
[0m
[32m[2023-12-21 10:22:56,866] [    INFO][0m - All the weights of BertForSequenceClassification were initialized from the model checkpoint at /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 10:22:58.799463   774 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:23:03.960016   842 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:23:03.965049   842 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:23:04.201894   842 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 10:23:04.205868   842 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:23:04.260870   842 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:23:04.305436   842 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 10:23:04.335548   842 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 10:23:04.355180   842 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 10:23:04.368520   842 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:23:04.396065   842 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[37m---    enabled FC MKL-DNN for 74 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 10:23:04.399950   842 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fc with gelu activation[0m
I1221 10:23:04.405489   842 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 10:23:04.423810   842 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:23:04.460032   842 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:23:04.461683   842 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 10:23:04.461699   842 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 10:23:04.463125   842 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 10:23:04,463] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:23:04,463] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:23:04,483] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:23:04,483] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 10:23:04.508776   842 onednn_context.cc:81] oneDNN v3.2.1
Data: uneasy mishmash of styles and genres . 	 Label: negative
Data: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 	 Label: negative
Data: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 	 Label: positive
Data: director rob marshall went out gunning to make a great one . 	 Label: negative
Data: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 	 Label: positive
Data: a well-made and often lovely depiction of the mysteries of friendship . 	 Label: positive
Data: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 	 Label: negative
Data: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 	 Label: positive
Data: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 	 Label: positive
Data: this is junk food cinema at its greasiest . 	 Label: negative
Data: it 's also heavy-handed and devotes too much time to bigoted views . 	 Label: negative
Data: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid . 	 Label: positive
Data: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it . 	 Label: positive
Data: moore 's performance impresses almost as much as her work with haynes in 1995 's safe . 	 Label: positive
Data: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich . 	 Label: positive
Data: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism . 	 Label: positive
Data: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance . 	 Label: positive
Data: a remarkable 179-minute meditation on the nature of revolution . 	 Label: positive
Data: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . 	 Label: positive
Data: it 's just incredibly dull . 	 Label: negative
Data: some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future . 	 Label: negative
Data: a feel-good picture in the best sense of the term . 	 Label: positive
Data: mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor . 	 Label: negative
Data: good movie . 	 Label: positive
Data: with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry . 	 Label: positive
Data: starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . 	 Label: negative
Data: the best way to hope for any chance of enjoying this film is by lowering your expectations . 	 Label: negative
Data: while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun . 	 Label: negative
Data: this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border . 	 Label: positive
Data: here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you . 	 Label: positive
Data: most new movies have a bright sheen . 	 Label: positive
Data: so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler . 	 Label: negative
Data: nothing more than a mediocre trifle . 	 Label: negative
Data: a model of what films like this should be like . 	 Label: positive
Data: gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up . 	 Label: negative
Data: doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere . 	 Label: positive
Data: another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through . 	 Label: negative
Data: it becomes gimmicky instead of compelling . 	 Label: negative
Data: falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny . 	 Label: negative
Data: verbinski substitutes atmosphere for action , tedium for thrills . 	 Label: negative
Data: it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . 	 Label: positive
Data: a bigger holiday downer than your end-of-year 401 ( k ) statement . 	 Label: negative
Data: as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining . 	 Label: positive
Data: sensitive , insightful and beautifully rendered film . 	 Label: positive
Data: no number of fantastic sets , extras , costumes and spectacular locales can disguise the emptiness at the center of the story . 	 Label: negative
Data: not too far below the gloss you can still feel director denis villeneuve 's beating heart and the fondness he has for his characters . 	 Label: positive
Data: for benigni it was n't shakespeare whom he wanted to define his career with but pinocchio . 	 Label: negative
Data: how did it ever get made ? 	 Label: negative
Data: it would n't be my preferred way of spending 100 minutes or $ 7.00 . 	 Label: negative
Data: the furious coherence that ( deniro ) brings to this part only underscores the fuzzy sentimentality of the movie itself , which feels , as it plods toward the end , less like a movie than like the filmed reading of a script in need of polishing . 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:23:16.364082   909 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:23:16.369066   909 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:23:16.614619   909 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 10:23:16.618371   909 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:23:16.671730   909 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:23:16.714536   909 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 10:23:16.743618   909 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 10:23:16.762517   909 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 10:23:16.775372   909 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:23:16.801605   909 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[37m---    enabled FC MKL-DNN for 74 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 10:23:16.805238   909 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fc with gelu activation[0m
I1221 10:23:16.810523   909 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 10:23:16.828241   909 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:23:16.863175   909 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:23:16.864765   909 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 10:23:16.864784   909 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 10:23:16.866129   909 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 10:23:16,866] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:23:16,866] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:23:16,885] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:23:16,885] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 10:23:16.910431   909 onednn_context.cc:81] oneDNN v3.2.1
Data: uneasy mishmash of styles and genres . 	 Label: negative
Data: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 	 Label: negative
Data: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 	 Label: positive
Data: director rob marshall went out gunning to make a great one . 	 Label: negative
Data: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 	 Label: positive
Data: a well-made and often lovely depiction of the mysteries of friendship . 	 Label: positive
Data: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 	 Label: negative
Data: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 	 Label: positive
Data: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 	 Label: positive
Data: this is junk food cinema at its greasiest . 	 Label: negative
Data: it 's also heavy-handed and devotes too much time to bigoted views . 	 Label: negative
Data: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid . 	 Label: positive
Data: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it . 	 Label: positive
Data: moore 's performance impresses almost as much as her work with haynes in 1995 's safe . 	 Label: positive
Data: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich . 	 Label: positive
Data: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism . 	 Label: positive
Data: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance . 	 Label: positive
Data: a remarkable 179-minute meditation on the nature of revolution . 	 Label: positive
Data: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . 	 Label: positive
Data: it 's just incredibly dull . 	 Label: negative
Data: some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future . 	 Label: negative
Data: a feel-good picture in the best sense of the term . 	 Label: positive
Data: mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor . 	 Label: negative
Data: good movie . 	 Label: positive
Data: with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry . 	 Label: positive
Data: starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . 	 Label: negative
Data: the best way to hope for any chance of enjoying this film is by lowering your expectations . 	 Label: negative
Data: while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun . 	 Label: negative
Data: this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border . 	 Label: positive
Data: here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you . 	 Label: positive
Data: most new movies have a bright sheen . 	 Label: positive
Data: so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler . 	 Label: negative
Data: nothing more than a mediocre trifle . 	 Label: negative
Data: a model of what films like this should be like . 	 Label: positive
Data: gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up . 	 Label: negative
Data: doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere . 	 Label: positive
Data: another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through . 	 Label: negative
Data: it becomes gimmicky instead of compelling . 	 Label: negative
Data: falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny . 	 Label: negative
Data: verbinski substitutes atmosphere for action , tedium for thrills . 	 Label: negative
Data: it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . 	 Label: positive
Data: a bigger holiday downer than your end-of-year 401 ( k ) statement . 	 Label: negative
Data: as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining . 	 Label: positive
Data: sensitive , insightful and beautifully rendered film . 	 Label: positive
Data: no number of fantastic sets , extras , costumes and spectacular locales can disguise the emptiness at the center of the story . 	 Label: negative
Data: not too far below the gloss you can still feel director denis villeneuve 's beating heart and the fondness he has for his characters . 	 Label: positive
Data: for benigni it was n't shakespeare whom he wanted to define his career with but pinocchio . 	 Label: negative
Data: how did it ever get made ? 	 Label: negative
Data: it would n't be my preferred way of spending 100 minutes or $ 7.00 . 	 Label: negative
Data: the furious coherence that ( deniro ) brings to this part only underscores the fuzzy sentimentality of the movie itself , which feels , as it plods toward the end , less like a movie than like the filmed reading of a script in need of polishing . 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:23:23.099931   980 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:23:23.105021   980 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:23:23.345918   980 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 10:23:23.349839   980 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:23:23.405145   980 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:23:23.449179   980 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 10:23:23.479171   980 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 10:23:23.498522   980 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 10:23:23.511749   980 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:23:23.538830   980 fuse_pass_base.cc:59] ---  detected 74 subgraphs
[37m---    enabled FC MKL-DNN for 74 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 10:23:23.542531   980 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fc with gelu activation[0m
I1221 10:23:23.547951   980 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 10:23:23.566161   980 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:23:23.602562   980 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:23:23.604166   980 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 10:23:23.604182   980 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 10:23:23.605592   980 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 10:23:23,605] [    INFO][0m - We are using (<class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'>, False) to load 'bert-base-uncased'.[0m
[32m[2023-12-21 10:23:23,606] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
[32m[2023-12-21 10:23:23,625] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/bert-base-uncased/tokenizer_config.json[0m
[32m[2023-12-21 10:23:23,626] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/bert-base-uncased/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 10:23:23.651326   980 onednn_context.cc:81] oneDNN v3.2.1
Data: uneasy mishmash of styles and genres . 	 Label: negative
Data: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 	 Label: negative
Data: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 	 Label: positive
Data: director rob marshall went out gunning to make a great one . 	 Label: negative
Data: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 	 Label: positive
Data: a well-made and often lovely depiction of the mysteries of friendship . 	 Label: positive
Data: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 	 Label: negative
Data: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 	 Label: positive
Data: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 	 Label: positive
Data: this is junk food cinema at its greasiest . 	 Label: negative
Data: it 's also heavy-handed and devotes too much time to bigoted views . 	 Label: negative
Data: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid . 	 Label: positive
Data: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it . 	 Label: positive
Data: moore 's performance impresses almost as much as her work with haynes in 1995 's safe . 	 Label: positive
Data: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich . 	 Label: positive
Data: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism . 	 Label: positive
Data: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance . 	 Label: positive
Data: a remarkable 179-minute meditation on the nature of revolution . 	 Label: positive
Data: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality . 	 Label: positive
Data: it 's just incredibly dull . 	 Label: negative
Data: some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future . 	 Label: negative
Data: a feel-good picture in the best sense of the term . 	 Label: positive
Data: mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor . 	 Label: negative
Data: good movie . 	 Label: positive
Data: with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry . 	 Label: positive
Data: starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . 	 Label: negative
Data: the best way to hope for any chance of enjoying this film is by lowering your expectations . 	 Label: negative
Data: while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun . 	 Label: negative
Data: this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border . 	 Label: positive
Data: here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you . 	 Label: positive
Data: most new movies have a bright sheen . 	 Label: positive
Data: so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler . 	 Label: negative
Data: nothing more than a mediocre trifle . 	 Label: negative
Data: a model of what films like this should be like . 	 Label: positive
Data: gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up . 	 Label: negative
Data: doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere . 	 Label: positive
Data: another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through . 	 Label: negative
Data: it becomes gimmicky instead of compelling . 	 Label: negative
Data: falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny . 	 Label: negative
Data: verbinski substitutes atmosphere for action , tedium for thrills . 	 Label: negative
Data: it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . 	 Label: positive
Data: a bigger holiday downer than your end-of-year 401 ( k ) statement . 	 Label: negative
Data: as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining . 	 Label: positive
Data: sensitive , insightful and beautifully rendered film . 	 Label: positive
Data: no number of fantastic sets , extras , costumes and spectacular locales can disguise the emptiness at the center of the story . 	 Label: negative
Data: not too far below the gloss you can still feel director denis villeneuve 's beating heart and the fondness he has for his characters . 	 Label: positive
Data: for benigni it was n't shakespeare whom he wanted to define his career with but pinocchio . 	 Label: negative
Data: how did it ever get made ? 	 Label: negative
Data: it would n't be my preferred way of spending 100 minutes or $ 7.00 . 	 Label: negative
Data: the furious coherence that ( deniro ) brings to this part only underscores the fuzzy sentimentality of the movie itself , which feels , as it plods toward the end , less like a movie than like the filmed reading of a script in need of polishing . 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - bert_base_text_cls - python ./test_tipc/bert_base_text_cls/predict.py --max_steps 50 --device=npu --use_tensorrt=False --precision=fp32 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32   --benchmark=False     > /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0,1_usetrt_False_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0,1_usetrt_False_precision_fp32_batchsize_32.log [0m
+ watchcat=
+ kill -9 201
+ sleep 10
run.sh: line 334:   201 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/bert_base_text_cls/train_infer_python.txt
==END==test_tipc/configs/bert_base_text_cls/train_infer_python.txt
++ date +%s
+ end=1703125422
++ echo 1703125051 1703125422
++ awk '{print $2-$1-2}'
+ time=369
+ echo 'test_tipc/configs/bert_base_text_cls/train_infer_python.txt spend time seconds 369'
+ read config_file
test_tipc/configs/bert_base_text_cls/train_infer_python.txt spend time seconds 369
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703125422
+ echo ==START==test_tipc/configs/bert_for_question_answering/train_infer_python.txt
==START==test_tipc/configs/bert_for_question_answering/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/bert_for_question_answering/train_infer_python.txt
+ dataline='===========================train_params===========================
model_name:bert_for_question_answering
python:python3.7
gpu_list:0|0,1
--device:gpu|gpu
--use_amp:null
--max_steps:null
null:null
--batch_size:null
null:null
null:null
null:null
null:null
##
trainer:norm_train
norm_train:test_tipc/train.py --model bert_for_question_answering --optimizer sgd --max_seq_len 512 --generated_inputs --learning_rate 0.01 --max_steps 150
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
null:null
null:null
norm_export:null
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:bert_for_question_answering
++ strs=model_name:bert_for_question_answering
++ IFS=:
++ array=(${strs})
++ tmp=bert_for_question_answering
++ echo bert_for_question_answering
+ model_name=bert_for_question_answering
+ sleep 10
+ run run_model test_tipc/configs/bert_for_question_answering/train_infer_python.txt lite_train_lite_infer 3600 bert_for_question_answering
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ waitfor=7200
+ command='run_model
test_tipc/configs/bert_for_question_answering/train_infer_python.txt
lite_train_lite_infer
3600
bert_for_question_answering'
+ commandpid=1059
+ run_model test_tipc/configs/bert_for_question_answering/train_infer_python.txt lite_train_lite_infer 3600 bert_for_question_answering
+ config_file=test_tipc/configs/bert_for_question_answering/train_infer_python.txt
+ watchdog=1060
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/bert_for_question_answering/train_infer_python.txt lite_train_lite_infer
+ wait 1059
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/bert_for_question_answering/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/bert_for_question_answering/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 10:23:55,676] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/bert-base-cased/config.json[0m
[32m[2023-12-21 10:23:56,032] [    INFO][0m - Downloading http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-base-cased.pdparams[0m
[32m[2023-12-21 10:23:56,032] [    INFO][0m - Downloading bert-base-cased.pdparams from http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-base-cased.pdparams[0m
Namespace(device='xpu', model='bert_for_question_answering', logging_steps=10, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=True, num_workers=4, profiler_options=None, save_model=None, batch_size=1, max_seq_len=512, data_dir=None, pad_to_max_seq_len=False, optimizer='sgd', learning_rate=0.01, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=None, model_name_or_path='bert-base-cased')
  0%|          | 0.00/501M [00:00<?, ?B/s]  0%|          | 15.0k/501M [00:00<1:09:12, 126kB/s]  0%|          | 43.0k/501M [00:00<50:34, 173kB/s]    0%|          | 97.0k/501M [00:00<30:42, 285kB/s]  0%|          | 154k/501M [00:00<26:15, 333kB/s]   0%|          | 213k/501M [00:00<24:15, 360kB/s]  0%|          | 273k/501M [00:00<21:49, 401kB/s]  0%|          | 332k/501M [00:00<20:25, 428kB/s]  0%|          | 395k/501M [00:01<19:28, 449kB/s]  0%|          | 463k/501M [00:01<18:27, 473kB/s]  0%|          | 534k/501M [00:01<17:33, 498kB/s]  0%|          | 607k/501M [00:01<16:57, 515kB/s]  0%|          | 684k/501M [00:01<16:05, 543kB/s]  0%|          | 766k/501M [00:01<14:55, 585kB/s]  0%|          | 851k/501M [00:01<13:34, 643kB/s]  0%|          | 1.01M/501M [00:01<09:07, 957kB/s]  0%|          | 1.11M/501M [00:02<09:04, 962kB/s]  0%|          | 1.21M/501M [00:02<10:44, 812kB/s]  0%|          | 1.43M/501M [00:02<08:34, 1.02MB/s]  0%|          | 1.67M/501M [00:02<07:12, 1.21MB/s]  0%|          | 1.92M/501M [00:02<05:37, 1.55MB/s]  0%|          | 2.08M/501M [00:02<06:10, 1.41MB/s]  0%|          | 2.38M/501M [00:02<05:11, 1.67MB/s]  1%|          | 2.67M/501M [00:03<04:21, 2.00MB/s]  1%|          | 2.89M/501M [00:03<04:43, 1.84MB/s]  1%|          | 3.23M/501M [00:03<03:55, 2.22MB/s]  1%|          | 3.49M/501M [00:03<04:11, 2.07MB/s]  1%|          | 3.74M/501M [00:03<03:58, 2.19MB/s]  1%|          | 4.12M/501M [00:03<03:24, 2.55MB/s]  1%|          | 4.43M/501M [00:03<03:41, 2.35MB/s]  1%|          | 4.70M/501M [00:03<03:31, 2.45MB/s]  1%|          | 4.97M/501M [00:04<03:24, 2.54MB/s]  1%|          | 5.27M/501M [00:04<03:20, 2.60MB/s]  1%|          | 5.58M/501M [00:04<03:19, 2.60MB/s]  1%|          | 5.90M/501M [00:04<03:10, 2.72MB/s]  1%|          | 6.25M/501M [00:04<03:01, 2.85MB/s]  1%|▏         | 6.66M/501M [00:04<02:38, 3.26MB/s]  1%|▏         | 7.04M/501M [00:04<02:30, 3.44MB/s]  1%|▏         | 7.44M/501M [00:04<02:21, 3.66MB/s]  2%|▏         | 7.85M/501M [00:04<02:16, 3.78MB/s]  2%|▏         | 8.29M/501M [00:04<02:12, 3.91MB/s]  2%|▏         | 8.69M/501M [00:05<02:08, 4.00MB/s]  2%|▏         | 9.15M/501M [00:05<02:02, 4.20MB/s]  2%|▏         | 9.63M/501M [00:05<02:02, 4.20MB/s]  2%|▏         | 10.2M/501M [00:05<01:56, 4.42MB/s]  2%|▏         | 10.7M/501M [00:05<01:48, 4.74MB/s]  2%|▏         | 11.3M/501M [00:05<01:42, 5.03MB/s]  2%|▏         | 11.9M/501M [00:05<01:39, 5.13MB/s]  3%|▎         | 12.5M/501M [00:05<01:32, 5.53MB/s]  3%|▎         | 13.2M/501M [00:05<01:26, 5.91MB/s]  3%|▎         | 14.1M/501M [00:06<01:14, 6.82MB/s]  3%|▎         | 15.5M/501M [00:06<00:56, 8.95MB/s]  4%|▎         | 17.7M/501M [00:06<00:38, 13.3MB/s]  4%|▍         | 20.4M/501M [00:06<00:28, 17.5MB/s]  5%|▍         | 22.9M/501M [00:06<00:24, 20.3MB/s]  5%|▌         | 25.1M/501M [00:06<00:23, 21.1MB/s]  5%|▌         | 27.4M/501M [00:06<00:22, 21.8MB/s]  6%|▌         | 29.5M/501M [00:06<00:22, 21.6MB/s]  6%|▋         | 31.5M/501M [00:06<00:28, 17.3MB/s]  7%|▋         | 33.3M/501M [00:07<00:31, 15.4MB/s]  7%|▋         | 34.9M/501M [00:07<00:33, 14.4MB/s]  7%|▋         | 36.4M/501M [00:07<00:35, 13.7MB/s]  8%|▊         | 37.7M/501M [00:07<00:37, 13.1MB/s]  8%|▊         | 39.0M/501M [00:07<00:37, 12.7MB/s]  8%|▊         | 40.3M/501M [00:07<00:43, 11.2MB/s]  8%|▊         | 41.9M/501M [00:07<00:38, 12.5MB/s]  9%|▊         | 43.1M/501M [00:07<00:39, 12.1MB/s]  9%|▉         | 44.3M/501M [00:08<00:40, 11.9MB/s]  9%|▉         | 45.5M/501M [00:08<00:40, 11.9MB/s]  9%|▉         | 46.6M/501M [00:08<00:40, 11.7MB/s] 10%|▉         | 47.7M/501M [00:08<00:41, 11.6MB/s] 10%|▉         | 48.9M/501M [00:08<00:41, 11.5MB/s] 10%|▉         | 50.0M/501M [00:08<00:40, 11.7MB/s] 10%|█         | 51.1M/501M [00:08<00:40, 11.8MB/s] 10%|█         | 52.3M/501M [00:08<00:40, 11.7MB/s] 11%|█         | 53.4M/501M [00:08<00:39, 11.9MB/s] 11%|█         | 54.6M/501M [00:09<00:38, 12.0MB/s] 11%|█         | 55.8M/501M [00:09<00:38, 12.1MB/s] 11%|█▏        | 57.0M/501M [00:09<00:38, 12.1MB/s] 12%|█▏        | 58.1M/501M [00:09<00:38, 12.1MB/s] 12%|█▏        | 59.3M/501M [00:09<00:38, 12.0MB/s] 12%|█▏        | 60.5M/501M [00:09<00:38, 12.0MB/s] 12%|█▏        | 61.6M/501M [00:09<00:38, 11.9MB/s] 13%|█▎        | 62.7M/501M [00:09<00:43, 10.7MB/s] 13%|█▎        | 64.1M/501M [00:09<00:39, 11.7MB/s] 13%|█▎        | 65.3M/501M [00:09<00:40, 11.3MB/s] 13%|█▎        | 66.4M/501M [00:10<00:41, 11.0MB/s] 13%|█▎        | 67.4M/501M [00:10<00:41, 10.9MB/s] 14%|█▎        | 68.5M/501M [00:10<00:41, 10.9MB/s] 14%|█▍        | 69.5M/501M [00:10<00:41, 10.9MB/s] 14%|█▍        | 70.6M/501M [00:10<00:41, 10.9MB/s] 14%|█▍        | 71.6M/501M [00:10<00:41, 10.9MB/s] 15%|█▍        | 72.7M/501M [00:10<00:40, 11.0MB/s] 15%|█▍        | 73.7M/501M [00:10<00:41, 10.9MB/s] 15%|█▍        | 74.8M/501M [00:10<00:40, 10.9MB/s] 15%|█▌        | 75.9M/501M [00:11<00:40, 11.1MB/s] 15%|█▌        | 77.0M/501M [00:11<00:39, 11.3MB/s] 16%|█▌        | 78.1M/501M [00:11<00:39, 11.3MB/s] 16%|█▌        | 79.2M/501M [00:11<00:38, 11.4MB/s] 16%|█▌        | 80.3M/501M [00:11<00:38, 11.5MB/s] 16%|█▋        | 81.5M/501M [00:11<00:38, 11.5MB/s] 17%|█▋        | 82.6M/501M [00:11<00:37, 11.6MB/s] 17%|█▋        | 83.7M/501M [00:11<00:42, 10.2MB/s] 17%|█▋        | 85.0M/501M [00:11<00:39, 11.1MB/s] 17%|█▋        | 86.1M/501M [00:11<00:42, 10.3MB/s] 17%|█▋        | 87.1M/501M [00:12<00:45, 9.47MB/s] 18%|█▊        | 88.0M/501M [00:12<00:45, 9.41MB/s] 18%|█▊        | 88.9M/501M [00:12<00:46, 9.35MB/s] 18%|█▊        | 89.8M/501M [00:12<00:46, 9.27MB/s] 18%|█▊        | 90.7M/501M [00:12<00:46, 9.20MB/s] 18%|█▊        | 91.6M/501M [00:12<00:46, 9.15MB/s] 18%|█▊        | 92.5M/501M [00:12<00:48, 8.75MB/s] 19%|█▊        | 93.3M/501M [00:12<00:48, 8.76MB/s] 19%|█▉        | 94.2M/501M [00:12<00:47, 8.95MB/s] 19%|█▉        | 95.2M/501M [00:13<00:46, 9.13MB/s] 19%|█▉        | 96.1M/501M [00:13<00:45, 9.23MB/s] 19%|█▉        | 97.0M/501M [00:13<00:45, 9.36MB/s] 20%|█▉        | 98.0M/501M [00:13<00:44, 9.45MB/s] 20%|█▉        | 98.9M/501M [00:13<00:44, 9.49MB/s] 20%|█▉        | 99.9M/501M [00:13<00:44, 9.52MB/s] 20%|██        | 101M/501M [00:13<00:45, 9.31MB/s]  20%|██        | 102M/501M [00:13<00:44, 9.42MB/s] 21%|██        | 103M/501M [00:13<00:43, 9.49MB/s] 21%|██        | 104M/501M [00:14<00:43, 9.62MB/s] 21%|██        | 105M/501M [00:14<00:42, 9.73MB/s] 21%|██        | 106M/501M [00:14<00:42, 9.82MB/s] 21%|██▏       | 107M/501M [00:14<00:41, 9.87MB/s] 21%|██▏       | 108M/501M [00:14<00:41, 9.95MB/s] 22%|██▏       | 109M/501M [00:14<00:41, 9.98MB/s] 22%|██▏       | 110M/501M [00:14<00:40, 10.0MB/s] 22%|██▏       | 111M/501M [00:14<00:41, 9.97MB/s] 22%|██▏       | 112M/501M [00:14<00:40, 9.97MB/s] 22%|██▏       | 113M/501M [00:14<00:40, 10.0MB/s] 23%|██▎       | 114M/501M [00:15<00:39, 10.1MB/s] 23%|██▎       | 115M/501M [00:15<00:39, 10.2MB/s] 23%|██▎       | 116M/501M [00:15<00:39, 10.3MB/s] 23%|██▎       | 117M/501M [00:15<00:38, 10.3MB/s] 23%|██▎       | 118M/501M [00:15<00:38, 10.4MB/s] 24%|██▎       | 119M/501M [00:15<00:37, 10.5MB/s] 24%|██▍       | 120M/501M [00:15<00:37, 10.6MB/s] 24%|██▍       | 121M/501M [00:15<00:43, 9.26MB/s] 24%|██▍       | 122M/501M [00:15<00:42, 9.45MB/s] 24%|██▍       | 123M/501M [00:16<00:45, 8.66MB/s] 25%|██▍       | 123M/501M [00:16<00:46, 8.59MB/s] 25%|██▍       | 124M/501M [00:16<00:46, 8.55MB/s] 25%|██▍       | 125M/501M [00:16<00:46, 8.50MB/s] 25%|██▌       | 126M/501M [00:16<00:46, 8.46MB/s] 25%|██▌       | 127M/501M [00:16<00:46, 8.42MB/s] 25%|██▌       | 127M/501M [00:16<00:46, 8.44MB/s] 26%|██▌       | 128M/501M [00:16<00:45, 8.54MB/s] 26%|██▌       | 129M/501M [00:16<00:45, 8.49MB/s] 26%|██▌       | 130M/501M [00:16<00:45, 8.63MB/s] 26%|██▌       | 131M/501M [00:17<00:44, 8.78MB/s] 26%|██▋       | 132M/501M [00:17<00:43, 8.96MB/s] 27%|██▋       | 133M/501M [00:17<00:42, 9.13MB/s] 27%|██▋       | 134M/501M [00:17<00:41, 9.27MB/s] 27%|██▋       | 135M/501M [00:17<00:40, 9.41MB/s] 27%|██▋       | 136M/501M [00:17<00:40, 9.48MB/s] 27%|██▋       | 136M/501M [00:17<00:40, 9.53MB/s] 27%|██▋       | 137M/501M [00:17<00:44, 8.63MB/s] 28%|██▊       | 138M/501M [00:17<00:43, 8.67MB/s] 28%|██▊       | 139M/501M [00:18<00:47, 7.92MB/s] 28%|██▊       | 140M/501M [00:18<00:48, 7.83MB/s] 28%|██▊       | 141M/501M [00:18<00:49, 7.59MB/s] 28%|██▊       | 141M/501M [00:18<00:49, 7.55MB/s] 28%|██▊       | 142M/501M [00:18<00:49, 7.56MB/s] 29%|██▊       | 143M/501M [00:18<00:49, 7.58MB/s] 29%|██▊       | 144M/501M [00:18<00:49, 7.63MB/s] 29%|██▉       | 144M/501M [00:18<00:50, 7.46MB/s] 29%|██▉       | 145M/501M [00:18<00:50, 7.42MB/s] 29%|██▉       | 146M/501M [00:18<00:50, 7.41MB/s] 29%|██▉       | 146M/501M [00:19<00:48, 7.58MB/s] 29%|██▉       | 147M/501M [00:19<00:48, 7.65MB/s] 30%|██▉       | 148M/501M [00:19<00:47, 7.79MB/s] 30%|██▉       | 149M/501M [00:19<00:47, 7.83MB/s] 30%|██▉       | 150M/501M [00:19<00:46, 7.93MB/s] 30%|███       | 150M/501M [00:19<00:45, 8.00MB/s] 30%|███       | 151M/501M [00:19<00:45, 8.06MB/s] 30%|███       | 152M/501M [00:19<00:45, 8.01MB/s] 31%|███       | 153M/501M [00:19<00:45, 7.99MB/s] 31%|███       | 154M/501M [00:19<00:45, 8.06MB/s] 31%|███       | 154M/501M [00:20<00:44, 8.07MB/s] 31%|███       | 155M/501M [00:20<00:44, 8.16MB/s] 31%|███       | 156M/501M [00:20<00:43, 8.29MB/s] 31%|███▏      | 157M/501M [00:20<00:43, 8.37MB/s] 31%|███▏      | 158M/501M [00:20<00:42, 8.46MB/s] 32%|███▏      | 159M/501M [00:20<00:42, 8.53MB/s] 32%|███▏      | 159M/501M [00:20<00:42, 8.37MB/s] 32%|███▏      | 160M/501M [00:20<00:44, 8.09MB/s] 32%|███▏      | 161M/501M [00:20<00:48, 7.41MB/s] 32%|███▏      | 162M/501M [00:21<00:49, 7.22MB/s] 32%|███▏      | 162M/501M [00:21<00:52, 6.80MB/s] 33%|███▎      | 163M/501M [00:21<00:52, 6.79MB/s] 33%|███▎      | 164M/501M [00:21<00:51, 6.82MB/s] 33%|███▎      | 164M/501M [00:21<00:51, 6.87MB/s] 33%|███▎      | 165M/501M [00:21<00:50, 6.97MB/s] 33%|███▎      | 166M/501M [00:21<00:51, 6.85MB/s] 33%|███▎      | 166M/501M [00:21<00:57, 6.13MB/s] 33%|███▎      | 167M/501M [00:21<00:57, 6.07MB/s] 33%|███▎      | 168M/501M [00:22<01:02, 5.63MB/s] 34%|███▎      | 168M/501M [00:22<01:02, 5.59MB/s] 34%|███▎      | 169M/501M [00:22<01:02, 5.57MB/s] 34%|███▍      | 169M/501M [00:22<01:02, 5.56MB/s] 34%|███▍      | 170M/501M [00:22<01:04, 5.40MB/s] 34%|███▍      | 170M/501M [00:22<01:08, 5.03MB/s] 34%|███▍      | 171M/501M [00:22<01:46, 3.25MB/s] 34%|███▍      | 171M/501M [00:23<02:02, 2.82MB/s] 34%|███▍      | 171M/501M [00:23<02:57, 1.94MB/s] 34%|███▍      | 172M/501M [00:23<03:08, 1.83MB/s] 34%|███▍      | 172M/501M [00:23<03:38, 1.57MB/s] 34%|███▍      | 172M/501M [00:24<03:48, 1.51MB/s] 34%|███▍      | 172M/501M [00:24<04:04, 1.41MB/s] 34%|███▍      | 172M/501M [00:24<04:21, 1.32MB/s] 34%|███▍      | 173M/501M [00:24<04:37, 1.24MB/s] 34%|███▍      | 173M/501M [00:24<04:51, 1.18MB/s] 35%|███▍      | 173M/501M [00:24<05:00, 1.14MB/s] 35%|███▍      | 173M/501M [00:24<04:22, 1.31MB/s] 35%|███▍      | 173M/501M [00:25<03:39, 1.56MB/s] 35%|███▍      | 173M/501M [00:25<03:38, 1.57MB/s] 35%|███▍      | 174M/501M [00:25<03:45, 1.52MB/s] 35%|███▍      | 174M/501M [00:25<03:41, 1.55MB/s] 35%|███▍      | 174M/501M [00:25<03:43, 1.53MB/s] 35%|███▍      | 174M/501M [00:25<03:55, 1.45MB/s] 35%|███▍      | 174M/501M [00:25<03:59, 1.43MB/s] 35%|███▍      | 174M/501M [00:25<03:57, 1.44MB/s] 35%|███▍      | 175M/501M [00:25<03:51, 1.48MB/s] 35%|███▍      | 175M/501M [00:26<03:53, 1.46MB/s] 35%|███▍      | 175M/501M [00:26<03:48, 1.49MB/s] 35%|███▍      | 175M/501M [00:26<03:47, 1.50MB/s] 35%|███▌      | 175M/501M [00:26<03:47, 1.50MB/s] 35%|███▌      | 175M/501M [00:26<03:41, 1.54MB/s] 35%|███▌      | 176M/501M [00:26<03:53, 1.46MB/s] 35%|███▌      | 176M/501M [00:26<03:59, 1.42MB/s] 35%|███▌      | 176M/501M [00:26<04:10, 1.36MB/s] 35%|███▌      | 176M/501M [00:27<04:12, 1.35MB/s] 35%|███▌      | 176M/501M [00:27<04:21, 1.30MB/s] 35%|███▌      | 176M/501M [00:27<04:29, 1.26MB/s] 35%|███▌      | 177M/501M [00:27<04:33, 1.24MB/s] 35%|███▌      | 177M/501M [00:27<04:40, 1.21MB/s] 35%|███▌      | 177M/501M [00:27<04:48, 1.17MB/s] 35%|███▌      | 177M/501M [00:27<04:20, 1.30MB/s] 35%|███▌      | 177M/501M [00:27<03:15, 1.74MB/s] 35%|███▌      | 177M/501M [00:28<03:29, 1.61MB/s] 36%|███▌      | 178M/501M [00:28<02:55, 1.93MB/s] 36%|███▌      | 178M/501M [00:28<02:33, 2.21MB/s] 36%|███▌      | 178M/501M [00:28<02:12, 2.54MB/s] 36%|███▌      | 179M/501M [00:28<01:58, 2.84MB/s] 36%|███▌      | 179M/501M [00:28<01:36, 3.48MB/s] 36%|███▌      | 180M/501M [00:28<01:24, 3.98MB/s] 36%|███▌      | 180M/501M [00:28<01:19, 4.20MB/s] 36%|███▌      | 181M/501M [00:28<01:14, 4.53MB/s] 36%|███▋      | 181M/501M [00:29<01:09, 4.81MB/s] 36%|███▋      | 182M/501M [00:29<01:07, 4.97MB/s] 36%|███▋      | 183M/501M [00:29<01:04, 5.14MB/s] 37%|███▋      | 183M/501M [00:29<01:03, 5.25MB/s] 37%|███▋      | 184M/501M [00:29<01:01, 5.39MB/s] 37%|███▋      | 184M/501M [00:29<01:00, 5.51MB/s] 37%|███▋      | 185M/501M [00:29<00:59, 5.53MB/s] 37%|███▋      | 185M/501M [00:29<00:58, 5.65MB/s] 37%|███▋      | 186M/501M [00:29<00:57, 5.75MB/s] 37%|███▋      | 187M/501M [00:30<00:56, 5.86MB/s] 37%|███▋      | 187M/501M [00:30<00:55, 5.92MB/s] 37%|███▋      | 188M/501M [00:30<00:54, 5.99MB/s] 38%|███▊      | 188M/501M [00:30<00:54, 6.01MB/s] 38%|███▊      | 189M/501M [00:30<00:53, 6.07MB/s] 38%|███▊      | 190M/501M [00:30<00:53, 6.14MB/s] 38%|███▊      | 190M/501M [00:30<00:52, 6.23MB/s] 38%|███▊      | 191M/501M [00:30<00:51, 6.30MB/s] 38%|███▊      | 191M/501M [00:30<00:50, 6.38MB/s] 38%|███▊      | 192M/501M [00:30<00:50, 6.43MB/s] 38%|███▊      | 193M/501M [00:31<00:50, 6.45MB/s] 39%|███▊      | 193M/501M [00:31<00:49, 6.49MB/s] 39%|███▉      | 194M/501M [00:31<00:48, 6.61MB/s] 39%|███▉      | 195M/501M [00:31<00:47, 6.70MB/s] 39%|███▉      | 195M/501M [00:31<00:47, 6.79MB/s] 39%|███▉      | 196M/501M [00:31<00:47, 6.74MB/s] 39%|███▉      | 197M/501M [00:31<00:47, 6.72MB/s] 39%|███▉      | 197M/501M [00:31<00:47, 6.68MB/s] 40%|███▉      | 198M/501M [00:31<00:47, 6.72MB/s] 40%|███▉      | 199M/501M [00:31<00:45, 6.90MB/s] 40%|███▉      | 199M/501M [00:32<00:44, 7.05MB/s] 40%|███▉      | 200M/501M [00:32<00:44, 7.14MB/s] 40%|████      | 201M/501M [00:32<00:43, 7.29MB/s] 40%|████      | 202M/501M [00:32<00:42, 7.42MB/s] 40%|████      | 202M/501M [00:32<00:41, 7.62MB/s] 41%|████      | 203M/501M [00:32<00:40, 7.73MB/s] 41%|████      | 204M/501M [00:32<00:39, 7.85MB/s] 41%|████      | 205M/501M [00:32<00:42, 7.26MB/s] 41%|████      | 205M/501M [00:32<00:41, 7.50MB/s] 41%|████      | 206M/501M [00:33<00:44, 6.90MB/s] 41%|████▏     | 207M/501M [00:33<00:45, 6.82MB/s] 41%|████▏     | 208M/501M [00:33<00:45, 6.75MB/s] 42%|████▏     | 208M/501M [00:33<00:45, 6.72MB/s] 42%|████▏     | 209M/501M [00:33<00:45, 6.68MB/s] 42%|████▏     | 209M/501M [00:33<00:45, 6.74MB/s] 42%|████▏     | 210M/501M [00:33<00:45, 6.74MB/s] 42%|████▏     | 211M/501M [00:33<00:45, 6.66MB/s] 42%|████▏     | 211M/501M [00:33<00:44, 6.86MB/s] 42%|████▏     | 212M/501M [00:33<00:43, 6.99MB/s] 43%|████▎     | 213M/501M [00:34<00:42, 7.11MB/s] 43%|████▎     | 214M/501M [00:34<00:41, 7.19MB/s] 43%|████▎     | 214M/501M [00:34<00:48, 6.20MB/s] 43%|████▎     | 215M/501M [00:34<00:55, 5.44MB/s] 43%|████▎     | 215M/501M [00:34<01:01, 4.90MB/s] 43%|████▎     | 216M/501M [00:34<01:07, 4.41MB/s] 43%|████▎     | 216M/501M [00:34<01:07, 4.40MB/s] 43%|████▎     | 217M/501M [00:34<01:09, 4.29MB/s] 43%|████▎     | 217M/501M [00:35<01:11, 4.18MB/s] 44%|████▎     | 218M/501M [00:35<01:06, 4.46MB/s] 44%|████▎     | 218M/501M [00:35<01:02, 4.77MB/s] 44%|████▎     | 219M/501M [00:35<00:59, 4.99MB/s] 44%|████▍     | 219M/501M [00:35<01:01, 4.78MB/s] 44%|████▍     | 220M/501M [00:35<01:00, 4.84MB/s] 44%|████▍     | 220M/501M [00:35<01:11, 4.12MB/s] 44%|████▍     | 221M/501M [00:35<01:00, 4.81MB/s] 44%|████▍     | 222M/501M [00:36<01:02, 4.64MB/s] 44%|████▍     | 222M/501M [00:36<01:02, 4.67MB/s] 45%|████▍     | 223M/501M [00:36<00:59, 4.89MB/s] 45%|████▍     | 223M/501M [00:36<00:56, 5.16MB/s] 45%|████▍     | 224M/501M [00:36<00:52, 5.50MB/s] 45%|████▍     | 225M/501M [00:36<00:50, 5.76MB/s] 45%|████▌     | 225M/501M [00:36<00:49, 5.79MB/s] 45%|████▌     | 226M/501M [00:36<00:47, 6.02MB/s] 45%|████▌     | 227M/501M [00:36<00:46, 6.17MB/s] 45%|████▌     | 227M/501M [00:37<00:47, 6.09MB/s] 45%|████▌     | 228M/501M [00:37<00:50, 5.70MB/s] 46%|████▌     | 228M/501M [00:37<00:50, 5.67MB/s] 46%|████▌     | 229M/501M [00:37<00:55, 5.17MB/s] 46%|████▌     | 229M/501M [00:37<00:51, 5.48MB/s] 46%|████▌     | 230M/501M [00:37<00:49, 5.71MB/s] 46%|████▌     | 231M/501M [00:37<00:51, 5.53MB/s] 46%|████▌     | 231M/501M [00:37<00:49, 5.75MB/s] 46%|████▋     | 232M/501M [00:37<00:49, 5.75MB/s] 46%|████▋     | 232M/501M [00:38<00:53, 5.27MB/s] 47%|████▋     | 233M/501M [00:38<00:57, 4.88MB/s] 47%|████▋     | 233M/501M [00:38<01:04, 4.31MB/s] 47%|████▋     | 234M/501M [00:38<01:15, 3.68MB/s] 47%|████▋     | 234M/501M [00:38<01:28, 3.15MB/s] 47%|████▋     | 235M/501M [00:38<01:21, 3.42MB/s] 47%|████▋     | 235M/501M [00:38<01:15, 3.68MB/s] 47%|████▋     | 236M/501M [00:39<01:10, 3.95MB/s] 47%|████▋     | 236M/501M [00:39<01:01, 4.49MB/s] 47%|████▋     | 237M/501M [00:39<01:00, 4.57MB/s] 47%|████▋     | 237M/501M [00:39<01:02, 4.44MB/s] 48%|████▊     | 238M/501M [00:39<01:04, 4.25MB/s] 48%|████▊     | 238M/501M [00:39<01:04, 4.24MB/s] 48%|████▊     | 239M/501M [00:39<01:04, 4.28MB/s] 48%|████▊     | 239M/501M [00:39<01:02, 4.38MB/s] 48%|████▊     | 240M/501M [00:40<01:02, 4.36MB/s] 48%|████▊     | 240M/501M [00:40<01:03, 4.29MB/s] 48%|████▊     | 241M/501M [00:40<01:02, 4.36MB/s] 48%|████▊     | 241M/501M [00:40<01:01, 4.40MB/s] 48%|████▊     | 242M/501M [00:40<01:01, 4.39MB/s] 48%|████▊     | 242M/501M [00:40<00:59, 4.53MB/s] 49%|████▊     | 243M/501M [00:40<00:55, 4.85MB/s] 49%|████▊     | 244M/501M [00:40<00:47, 5.67MB/s] 49%|████▉     | 244M/501M [00:40<00:42, 6.36MB/s] 49%|████▉     | 245M/501M [00:41<00:39, 6.84MB/s] 49%|████▉     | 246M/501M [00:41<00:36, 7.23MB/s] 49%|████▉     | 247M/501M [00:41<00:35, 7.53MB/s] 49%|████▉     | 248M/501M [00:41<00:34, 7.73MB/s] 50%|████▉     | 248M/501M [00:41<00:33, 7.88MB/s] 50%|████▉     | 249M/501M [00:41<00:32, 8.01MB/s] 50%|████▉     | 250M/501M [00:41<00:32, 8.16MB/s] 50%|█████     | 251M/501M [00:41<00:38, 6.88MB/s] 50%|█████     | 252M/501M [00:41<00:35, 7.38MB/s] 50%|█████     | 252M/501M [00:42<00:37, 6.87MB/s] 51%|█████     | 253M/501M [00:42<00:41, 6.19MB/s] 51%|█████     | 254M/501M [00:42<00:43, 5.99MB/s] 51%|█████     | 254M/501M [00:42<00:43, 5.92MB/s] 51%|█████     | 255M/501M [00:42<00:47, 5.41MB/s] 51%|█████     | 256M/501M [00:42<00:47, 5.42MB/s] 51%|█████     | 256M/501M [00:42<00:45, 5.59MB/s] 51%|█████▏    | 257M/501M [00:42<00:42, 6.02MB/s] 51%|█████▏    | 258M/501M [00:42<00:39, 6.40MB/s] 52%|█████▏    | 258M/501M [00:43<00:38, 6.67MB/s] 52%|█████▏    | 259M/501M [00:43<00:36, 6.93MB/s] 52%|█████▏    | 260M/501M [00:43<00:35, 7.13MB/s] 52%|█████▏    | 261M/501M [00:43<00:34, 7.29MB/s] 52%|█████▏    | 261M/501M [00:43<00:34, 7.29MB/s] 52%|█████▏    | 262M/501M [00:43<00:38, 6.42MB/s] 52%|█████▏    | 263M/501M [00:43<00:40, 6.20MB/s] 53%|█████▎    | 263M/501M [00:43<00:43, 5.78MB/s] 53%|█████▎    | 264M/501M [00:44<00:44, 5.61MB/s] 53%|█████▎    | 265M/501M [00:44<00:39, 6.20MB/s] 53%|█████▎    | 265M/501M [00:44<00:36, 6.73MB/s] 53%|█████▎    | 266M/501M [00:44<00:39, 6.25MB/s] 53%|█████▎    | 267M/501M [00:44<00:43, 5.58MB/s] 53%|█████▎    | 267M/501M [00:44<00:47, 5.12MB/s] 53%|█████▎    | 268M/501M [00:44<00:50, 4.85MB/s] 54%|█████▎    | 268M/501M [00:44<00:50, 4.86MB/s] 54%|█████▎    | 269M/501M [00:44<00:49, 4.92MB/s] 54%|█████▍    | 269M/501M [00:45<00:49, 4.92MB/s] 54%|█████▍    | 270M/501M [00:45<00:49, 4.92MB/s] 54%|█████▍    | 270M/501M [00:45<00:54, 4.40MB/s] 54%|█████▍    | 271M/501M [00:45<01:00, 3.99MB/s] 54%|█████▍    | 271M/501M [00:45<01:04, 3.76MB/s] 54%|█████▍    | 272M/501M [00:45<01:05, 3.66MB/s] 54%|█████▍    | 272M/501M [00:45<01:06, 3.59MB/s] 54%|█████▍    | 272M/501M [00:45<01:07, 3.56MB/s] 54%|█████▍    | 273M/501M [00:46<01:08, 3.49MB/s] 55%|█████▍    | 273M/501M [00:46<01:09, 3.44MB/s] 55%|█████▍    | 273M/501M [00:46<01:14, 3.18MB/s] 55%|█████▍    | 274M/501M [00:46<01:14, 3.18MB/s] 55%|█████▍    | 274M/501M [00:46<01:15, 3.13MB/s] 55%|█████▍    | 274M/501M [00:46<01:17, 3.07MB/s] 55%|█████▍    | 274M/501M [00:46<01:12, 3.25MB/s] 55%|█████▍    | 275M/501M [00:46<01:11, 3.30MB/s] 55%|█████▍    | 275M/501M [00:46<01:18, 3.01MB/s] 55%|█████▌    | 275M/501M [00:47<01:18, 3.01MB/s] 55%|█████▌    | 276M/501M [00:47<01:20, 2.94MB/s] 55%|█████▌    | 276M/501M [00:47<01:19, 2.94MB/s] 55%|█████▌    | 276M/501M [00:47<01:21, 2.90MB/s] 55%|█████▌    | 277M/501M [00:47<01:52, 2.08MB/s] 55%|█████▌    | 277M/501M [00:47<01:26, 2.70MB/s] 55%|█████▌    | 277M/501M [00:47<01:40, 2.33MB/s] 55%|█████▌    | 278M/501M [00:48<01:44, 2.23MB/s] 56%|█████▌    | 278M/501M [00:48<01:48, 2.14MB/s] 56%|█████▌    | 278M/501M [00:48<01:55, 2.02MB/s] 56%|█████▌    | 278M/501M [00:48<02:01, 1.92MB/s] 56%|█████▌    | 278M/501M [00:48<02:07, 1.83MB/s] 56%|█████▌    | 279M/501M [00:48<02:09, 1.79MB/s] 56%|█████▌    | 279M/501M [00:48<02:05, 1.85MB/s] 56%|█████▌    | 279M/501M [00:48<01:53, 2.04MB/s] 56%|█████▌    | 280M/501M [00:49<01:36, 2.39MB/s] 56%|█████▌    | 280M/501M [00:49<01:25, 2.69MB/s] 56%|█████▌    | 280M/501M [00:49<01:15, 3.07MB/s] 56%|█████▌    | 281M/501M [00:49<01:12, 3.19MB/s] 56%|█████▌    | 281M/501M [00:49<01:05, 3.51MB/s] 56%|█████▋    | 282M/501M [00:49<00:58, 3.91MB/s] 56%|█████▋    | 282M/501M [00:49<00:51, 4.46MB/s] 57%|█████▋    | 283M/501M [00:49<00:43, 5.20MB/s] 57%|█████▋    | 284M/501M [00:49<00:37, 6.04MB/s] 57%|█████▋    | 284M/501M [00:49<00:33, 6.68MB/s] 57%|█████▋    | 285M/501M [00:50<00:31, 7.13MB/s] 57%|█████▋    | 286M/501M [00:50<00:30, 7.43MB/s] 57%|█████▋    | 287M/501M [00:50<00:29, 7.64MB/s] 57%|█████▋    | 288M/501M [00:50<00:28, 7.79MB/s] 58%|█████▊    | 288M/501M [00:50<00:28, 7.89MB/s] 58%|█████▊    | 289M/501M [00:50<00:27, 7.96MB/s] 58%|█████▊    | 290M/501M [00:50<00:27, 7.97MB/s] 58%|█████▊    | 291M/501M [00:50<00:27, 8.06MB/s] 58%|█████▊    | 292M/501M [00:50<00:26, 8.15MB/s] 58%|█████▊    | 292M/501M [00:51<00:26, 8.26MB/s] 59%|█████▊    | 293M/501M [00:51<00:26, 8.34MB/s] 59%|█████▉    | 294M/501M [00:51<00:25, 8.43MB/s] 59%|█████▉    | 295M/501M [00:51<00:25, 8.52MB/s] 59%|█████▉    | 296M/501M [00:51<00:25, 8.50MB/s] 59%|█████▉    | 297M/501M [00:51<00:25, 8.48MB/s] 59%|█████▉    | 297M/501M [00:51<00:32, 6.60MB/s] 60%|█████▉    | 298M/501M [00:51<00:41, 5.13MB/s] 60%|█████▉    | 299M/501M [00:52<00:42, 4.99MB/s] 60%|█████▉    | 299M/501M [00:52<00:43, 4.86MB/s] 60%|█████▉    | 300M/501M [00:52<00:43, 4.80MB/s] 60%|██████    | 300M/501M [00:52<00:44, 4.74MB/s] 60%|██████    | 301M/501M [00:52<00:43, 4.81MB/s] 60%|██████    | 301M/501M [00:52<00:45, 4.60MB/s] 60%|██████    | 302M/501M [00:52<00:50, 4.09MB/s] 60%|██████    | 302M/501M [00:52<00:50, 4.11MB/s] 61%|██████    | 303M/501M [00:53<00:47, 4.39MB/s] 61%|██████    | 303M/501M [00:53<00:46, 4.45MB/s] 61%|██████    | 304M/501M [00:53<00:46, 4.40MB/s] 61%|██████    | 304M/501M [00:53<00:42, 4.87MB/s] 61%|██████    | 305M/501M [00:53<00:38, 5.30MB/s] 61%|██████    | 306M/501M [00:53<00:35, 5.75MB/s] 61%|██████    | 306M/501M [00:53<00:33, 6.05MB/s] 61%|██████▏   | 307M/501M [00:53<00:32, 6.23MB/s] 61%|██████▏   | 308M/501M [00:53<00:31, 6.42MB/s] 62%|██████▏   | 308M/501M [00:54<00:30, 6.69MB/s] 62%|██████▏   | 309M/501M [00:54<00:29, 6.87MB/s] 62%|██████▏   | 310M/501M [00:54<00:28, 6.99MB/s] 62%|██████▏   | 310M/501M [00:54<00:27, 7.16MB/s] 62%|██████▏   | 311M/501M [00:54<00:27, 7.30MB/s] 62%|██████▏   | 312M/501M [00:54<00:26, 7.40MB/s] 62%|██████▏   | 313M/501M [00:54<00:26, 7.49MB/s] 63%|██████▎   | 313M/501M [00:54<00:26, 7.48MB/s] 63%|██████▎   | 314M/501M [00:54<00:25, 7.58MB/s] 63%|██████▎   | 315M/501M [00:54<00:25, 7.65MB/s] 63%|██████▎   | 316M/501M [00:55<00:25, 7.71MB/s] 63%|██████▎   | 316M/501M [00:55<00:24, 7.81MB/s] 63%|██████▎   | 317M/501M [00:55<00:24, 7.94MB/s] 64%|██████▎   | 318M/501M [00:55<00:23, 7.98MB/s] 64%|██████▎   | 319M/501M [00:55<00:23, 8.07MB/s] 64%|██████▍   | 320M/501M [00:55<00:23, 8.10MB/s] 64%|██████▍   | 320M/501M [00:55<00:23, 8.10MB/s] 64%|██████▍   | 321M/501M [00:55<00:23, 7.94MB/s] 64%|██████▍   | 322M/501M [00:55<00:23, 8.07MB/s] 65%|██████▍   | 323M/501M [00:55<00:23, 8.08MB/s] 65%|██████▍   | 324M/501M [00:56<00:22, 8.14MB/s] 65%|██████▍   | 325M/501M [00:56<00:22, 8.25MB/s] 65%|██████▌   | 325M/501M [00:56<00:22, 8.32MB/s] 65%|██████▌   | 326M/501M [00:56<00:21, 8.40MB/s] 65%|██████▌   | 327M/501M [00:56<00:21, 8.47MB/s] 66%|██████▌   | 328M/501M [00:56<00:21, 8.54MB/s] 66%|██████▌   | 329M/501M [00:56<00:21, 8.58MB/s] 66%|██████▌   | 330M/501M [00:56<00:21, 8.50MB/s] 66%|██████▌   | 330M/501M [00:56<00:21, 8.43MB/s] 66%|██████▌   | 331M/501M [00:56<00:21, 8.39MB/s] 66%|██████▋   | 332M/501M [00:57<00:20, 8.45MB/s] 66%|██████▋   | 333M/501M [00:57<00:20, 8.49MB/s] 67%|██████▋   | 334M/501M [00:57<00:20, 8.52MB/s] 67%|██████▋   | 334M/501M [00:57<00:20, 8.51MB/s] 67%|██████▋   | 335M/501M [00:57<00:20, 8.48MB/s] 67%|██████▋   | 336M/501M [00:57<00:20, 8.47MB/s] 67%|██████▋   | 337M/501M [00:57<00:20, 8.53MB/s] 67%|██████▋   | 338M/501M [00:57<00:20, 8.30MB/s] 68%|██████▊   | 339M/501M [00:57<00:20, 8.43MB/s] 68%|██████▊   | 339M/501M [00:57<00:19, 8.64MB/s] 68%|██████▊   | 340M/501M [00:58<00:19, 8.81MB/s] 68%|██████▊   | 341M/501M [00:58<00:18, 8.91MB/s] 68%|██████▊   | 342M/501M [00:58<00:18, 8.97MB/s] 69%|██████▊   | 343M/501M [00:58<00:18, 9.04MB/s] 69%|██████▊   | 344M/501M [00:58<00:17, 9.17MB/s] 69%|██████▉   | 345M/501M [00:58<00:17, 9.32MB/s] 69%|██████▉   | 346M/501M [00:58<00:17, 9.12MB/s] 69%|██████▉   | 347M/501M [00:58<00:18, 8.90MB/s] 69%|██████▉   | 348M/501M [00:58<00:17, 9.08MB/s] 70%|██████▉   | 348M/501M [00:59<00:17, 9.21MB/s] 70%|██████▉   | 349M/501M [00:59<00:16, 9.37MB/s] 70%|███████   | 350M/501M [00:59<00:16, 9.47MB/s] 70%|███████   | 351M/501M [00:59<00:16, 9.58MB/s] 70%|███████   | 352M/501M [00:59<00:15, 9.72MB/s] 71%|███████   | 353M/501M [00:59<00:15, 9.82MB/s] 71%|███████   | 354M/501M [00:59<00:15, 9.82MB/s] 71%|███████   | 355M/501M [00:59<00:18, 8.11MB/s] 71%|███████   | 356M/501M [00:59<00:17, 8.65MB/s] 71%|███████▏  | 357M/501M [01:00<00:18, 8.03MB/s] 71%|███████▏  | 358M/501M [01:00<00:18, 8.01MB/s] 72%|███████▏  | 359M/501M [01:00<00:18, 7.95MB/s] 72%|███████▏  | 359M/501M [01:00<00:18, 7.83MB/s] 72%|███████▏  | 360M/501M [01:00<00:20, 7.09MB/s] 72%|███████▏  | 361M/501M [01:00<00:22, 6.42MB/s] 72%|███████▏  | 361M/501M [01:00<00:26, 5.57MB/s] 72%|███████▏  | 362M/501M [01:00<00:26, 5.58MB/s] 72%|███████▏  | 363M/501M [01:01<00:24, 5.88MB/s] 73%|███████▎  | 363M/501M [01:01<00:24, 5.84MB/s] 73%|███████▎  | 364M/501M [01:01<00:24, 5.81MB/s] 73%|███████▎  | 364M/501M [01:01<00:25, 5.50MB/s] 73%|███████▎  | 365M/501M [01:01<00:29, 4.88MB/s] 73%|███████▎  | 365M/501M [01:01<00:31, 4.53MB/s] 73%|███████▎  | 366M/501M [01:01<00:34, 4.14MB/s] 73%|███████▎  | 366M/501M [01:01<00:36, 3.81MB/s] 73%|███████▎  | 367M/501M [01:02<00:36, 3.83MB/s] 73%|███████▎  | 367M/501M [01:02<00:38, 3.64MB/s] 73%|███████▎  | 367M/501M [01:02<00:36, 3.83MB/s] 74%|███████▎  | 368M/501M [01:02<00:33, 4.17MB/s] 74%|███████▎  | 369M/501M [01:02<00:29, 4.67MB/s] 74%|███████▍  | 369M/501M [01:02<00:27, 5.08MB/s] 74%|███████▍  | 370M/501M [01:02<00:24, 5.50MB/s] 74%|███████▍  | 370M/501M [01:02<00:26, 5.13MB/s] 74%|███████▍  | 371M/501M [01:02<00:21, 6.27MB/s] 74%|███████▍  | 372M/501M [01:03<00:20, 6.51MB/s] 74%|███████▍  | 373M/501M [01:03<00:19, 6.70MB/s] 75%|███████▍  | 373M/501M [01:03<00:19, 6.89MB/s] 75%|███████▍  | 374M/501M [01:03<00:18, 7.01MB/s] 75%|███████▍  | 375M/501M [01:03<00:18, 7.14MB/s] 75%|███████▌  | 376M/501M [01:03<00:17, 7.30MB/s] 75%|███████▌  | 376M/501M [01:03<00:17, 7.40MB/s] 75%|███████▌  | 377M/501M [01:03<00:17, 7.31MB/s] 75%|███████▌  | 378M/501M [01:03<00:18, 7.04MB/s] 76%|███████▌  | 378M/501M [01:03<00:19, 6.42MB/s] 76%|███████▌  | 379M/501M [01:04<00:20, 6.34MB/s] 76%|███████▌  | 380M/501M [01:04<00:20, 6.12MB/s] 76%|███████▌  | 380M/501M [01:04<00:20, 6.03MB/s] 76%|███████▌  | 381M/501M [01:04<00:20, 5.98MB/s] 76%|███████▌  | 381M/501M [01:04<00:21, 5.93MB/s] 76%|███████▋  | 382M/501M [01:04<00:21, 5.89MB/s] 76%|███████▋  | 383M/501M [01:04<00:20, 5.89MB/s] 77%|███████▋  | 383M/501M [01:04<00:20, 5.95MB/s] 77%|███████▋  | 384M/501M [01:04<00:20, 5.95MB/s] 77%|███████▋  | 384M/501M [01:05<00:20, 5.99MB/s] 77%|███████▋  | 385M/501M [01:05<00:20, 6.06MB/s] 77%|███████▋  | 386M/501M [01:05<00:19, 6.14MB/s] 77%|███████▋  | 386M/501M [01:05<00:19, 6.18MB/s] 77%|███████▋  | 387M/501M [01:05<00:19, 6.25MB/s] 77%|███████▋  | 387M/501M [01:05<00:18, 6.31MB/s] 78%|███████▊  | 388M/501M [01:05<00:18, 6.39MB/s] 78%|███████▊  | 389M/501M [01:05<00:18, 6.36MB/s] 78%|███████▊  | 389M/501M [01:05<00:19, 5.94MB/s] 78%|███████▊  | 390M/501M [01:05<00:20, 5.66MB/s] 78%|███████▊  | 390M/501M [01:06<00:22, 5.22MB/s] 78%|███████▊  | 391M/501M [01:06<00:23, 4.86MB/s] 78%|███████▊  | 391M/501M [01:06<00:25, 4.46MB/s] 78%|███████▊  | 392M/501M [01:06<00:31, 3.66MB/s] 78%|███████▊  | 392M/501M [01:06<00:33, 3.38MB/s] 78%|███████▊  | 393M/501M [01:06<00:37, 3.00MB/s] 78%|███████▊  | 393M/501M [01:06<00:38, 2.93MB/s] 79%|███████▊  | 393M/501M [01:07<00:42, 2.63MB/s] 79%|███████▊  | 393M/501M [01:07<00:43, 2.57MB/s] 79%|███████▊  | 394M/501M [01:07<00:48, 2.33MB/s] 79%|███████▊  | 394M/501M [01:07<00:50, 2.23MB/s] 79%|███████▊  | 394M/501M [01:07<00:51, 2.15MB/s] 79%|███████▉  | 394M/501M [01:07<00:54, 2.06MB/s] 79%|███████▉  | 394M/501M [01:07<00:58, 1.91MB/s] 79%|███████▉  | 395M/501M [01:07<00:59, 1.85MB/s] 79%|███████▉  | 395M/501M [01:08<00:59, 1.88MB/s] 79%|███████▉  | 395M/501M [01:08<00:58, 1.89MB/s] 79%|███████▉  | 395M/501M [01:08<00:56, 1.95MB/s] 79%|███████▉  | 395M/501M [01:08<00:54, 2.01MB/s] 79%|███████▉  | 396M/501M [01:08<00:52, 2.10MB/s] 79%|███████▉  | 396M/501M [01:08<00:51, 2.12MB/s] 79%|███████▉  | 396M/501M [01:08<00:51, 2.12MB/s] 79%|███████▉  | 396M/501M [01:08<00:53, 2.03MB/s] 79%|███████▉  | 397M/501M [01:08<00:54, 2.00MB/s] 79%|███████▉  | 397M/501M [01:09<00:54, 2.00MB/s] 79%|███████▉  | 397M/501M [01:09<00:54, 1.98MB/s] 79%|███████▉  | 397M/501M [01:09<00:54, 1.99MB/s] 79%|███████▉  | 397M/501M [01:09<00:53, 2.01MB/s] 79%|███████▉  | 398M/501M [01:09<00:55, 1.93MB/s] 79%|███████▉  | 398M/501M [01:09<01:00, 1.79MB/s] 80%|███████▉  | 398M/501M [01:09<00:57, 1.88MB/s] 80%|███████▉  | 398M/501M [01:09<01:00, 1.78MB/s] 80%|███████▉  | 398M/501M [01:09<01:02, 1.72MB/s] 80%|███████▉  | 398M/501M [01:10<01:02, 1.71MB/s] 80%|███████▉  | 399M/501M [01:10<01:05, 1.64MB/s] 80%|███████▉  | 399M/501M [01:10<01:01, 1.72MB/s] 80%|███████▉  | 399M/501M [01:10<00:57, 1.86MB/s] 80%|███████▉  | 399M/501M [01:10<00:54, 1.96MB/s] 80%|███████▉  | 400M/501M [01:10<00:46, 2.27MB/s] 80%|███████▉  | 400M/501M [01:10<00:48, 2.19MB/s] 80%|████████  | 401M/501M [01:10<00:29, 3.53MB/s] 80%|████████  | 401M/501M [01:11<00:33, 3.12MB/s] 80%|████████  | 401M/501M [01:11<00:34, 3.02MB/s] 80%|████████  | 402M/501M [01:11<00:31, 3.28MB/s] 80%|████████  | 402M/501M [01:11<00:30, 3.35MB/s] 80%|████████  | 402M/501M [01:11<00:31, 3.26MB/s] 80%|████████  | 403M/501M [01:11<00:33, 3.02MB/s] 81%|████████  | 403M/501M [01:11<00:35, 2.90MB/s] 81%|████████  | 403M/501M [01:11<00:35, 2.88MB/s] 81%|████████  | 404M/501M [01:12<00:35, 2.85MB/s] 81%|████████  | 404M/501M [01:12<00:33, 3.00MB/s] 81%|████████  | 405M/501M [01:12<00:32, 3.09MB/s] 81%|████████  | 405M/501M [01:12<00:30, 3.30MB/s] 81%|████████  | 405M/501M [01:12<00:29, 3.43MB/s] 81%|████████  | 406M/501M [01:12<00:28, 3.52MB/s] 81%|████████  | 406M/501M [01:12<00:29, 3.37MB/s] 81%|████████  | 407M/501M [01:12<00:30, 3.22MB/s] 81%|████████▏ | 407M/501M [01:13<00:30, 3.26MB/s] 81%|████████▏ | 407M/501M [01:13<00:30, 3.18MB/s] 81%|████████▏ | 408M/501M [01:13<00:28, 3.42MB/s] 82%|████████▏ | 408M/501M [01:13<00:26, 3.64MB/s] 82%|████████▏ | 409M/501M [01:13<00:22, 4.22MB/s] 82%|████████▏ | 410M/501M [01:13<00:20, 4.68MB/s] 82%|████████▏ | 410M/501M [01:13<00:19, 4.97MB/s] 82%|████████▏ | 411M/501M [01:13<00:18, 4.97MB/s] 82%|████████▏ | 411M/501M [01:13<00:17, 5.32MB/s] 82%|████████▏ | 412M/501M [01:14<00:16, 5.63MB/s] 82%|████████▏ | 413M/501M [01:14<00:15, 5.92MB/s] 83%|████████▎ | 413M/501M [01:14<00:15, 5.89MB/s] 83%|████████▎ | 414M/501M [01:14<00:17, 5.24MB/s] 83%|████████▎ | 414M/501M [01:14<00:18, 4.84MB/s] 83%|████████▎ | 415M/501M [01:14<00:21, 4.12MB/s] 83%|████████▎ | 415M/501M [01:14<00:22, 3.94MB/s] 83%|████████▎ | 416M/501M [01:14<00:24, 3.57MB/s] 83%|████████▎ | 416M/501M [01:15<00:28, 3.12MB/s] 83%|████████▎ | 416M/501M [01:15<00:27, 3.16MB/s] 83%|████████▎ | 417M/501M [01:15<00:28, 3.12MB/s] 83%|████████▎ | 417M/501M [01:15<00:25, 3.39MB/s] 83%|████████▎ | 418M/501M [01:15<00:21, 4.05MB/s] 84%|████████▎ | 418M/501M [01:15<00:20, 4.21MB/s] 84%|████████▎ | 419M/501M [01:15<00:19, 4.46MB/s] 84%|████████▎ | 419M/501M [01:15<00:18, 4.64MB/s] 84%|████████▍ | 420M/501M [01:16<00:16, 5.02MB/s] 84%|████████▍ | 420M/501M [01:16<00:16, 5.00MB/s] 84%|████████▍ | 421M/501M [01:16<00:15, 5.38MB/s] 84%|████████▍ | 421M/501M [01:16<00:16, 5.01MB/s] 84%|████████▍ | 422M/501M [01:16<00:16, 5.05MB/s] 84%|████████▍ | 422M/501M [01:16<00:16, 4.86MB/s] 84%|████████▍ | 423M/501M [01:16<00:18, 4.47MB/s] 85%|████████▍ | 423M/501M [01:16<00:19, 4.16MB/s] 85%|████████▍ | 424M/501M [01:16<00:17, 4.57MB/s] 85%|████████▍ | 424M/501M [01:17<00:15, 5.25MB/s] 85%|████████▍ | 425M/501M [01:17<00:13, 5.80MB/s] 85%|████████▌ | 426M/501M [01:17<00:12, 6.19MB/s] 85%|████████▌ | 426M/501M [01:17<00:12, 6.07MB/s] 85%|████████▌ | 427M/501M [01:17<00:12, 6.09MB/s] 85%|████████▌ | 428M/501M [01:17<00:13, 5.65MB/s] 86%|████████▌ | 428M/501M [01:17<00:14, 5.20MB/s] 86%|████████▌ | 429M/501M [01:17<00:13, 5.40MB/s] 86%|████████▌ | 429M/501M [01:17<00:13, 5.39MB/s] 86%|████████▌ | 430M/501M [01:18<00:12, 5.70MB/s] 86%|████████▌ | 431M/501M [01:18<00:11, 6.14MB/s] 86%|████████▌ | 431M/501M [01:18<00:11, 6.42MB/s] 86%|████████▋ | 432M/501M [01:18<00:11, 6.32MB/s] 86%|████████▋ | 433M/501M [01:18<00:11, 6.02MB/s] 87%|████████▋ | 433M/501M [01:18<00:12, 5.78MB/s] 87%|████████▋ | 434M/501M [01:18<00:12, 5.69MB/s] 87%|████████▋ | 434M/501M [01:18<00:11, 5.99MB/s] 87%|████████▋ | 435M/501M [01:18<00:10, 6.59MB/s] 87%|████████▋ | 436M/501M [01:18<00:09, 7.06MB/s] 87%|████████▋ | 437M/501M [01:19<00:09, 7.03MB/s] 87%|████████▋ | 437M/501M [01:19<00:10, 6.46MB/s] 87%|████████▋ | 438M/501M [01:19<00:10, 6.06MB/s] 88%|████████▊ | 439M/501M [01:19<00:12, 5.40MB/s] 88%|████████▊ | 439M/501M [01:19<00:13, 4.65MB/s] 88%|████████▊ | 440M/501M [01:19<00:15, 4.07MB/s] 88%|████████▊ | 440M/501M [01:19<00:16, 3.96MB/s] 88%|████████▊ | 440M/501M [01:20<00:17, 3.51MB/s] 88%|████████▊ | 441M/501M [01:20<00:17, 3.58MB/s] 88%|████████▊ | 441M/501M [01:20<00:16, 3.73MB/s] 88%|████████▊ | 442M/501M [01:20<00:16, 3.81MB/s] 88%|████████▊ | 442M/501M [01:20<00:14, 4.33MB/s] 88%|████████▊ | 443M/501M [01:20<00:14, 4.14MB/s] 89%|████████▊ | 443M/501M [01:20<00:15, 3.91MB/s] 89%|████████▊ | 444M/501M [01:20<00:14, 4.15MB/s] 89%|████████▊ | 444M/501M [01:21<00:12, 4.67MB/s] 89%|████████▉ | 445M/501M [01:21<00:11, 4.94MB/s] 89%|████████▉ | 445M/501M [01:21<00:12, 4.83MB/s] 89%|████████▉ | 446M/501M [01:21<00:11, 5.07MB/s] 89%|████████▉ | 446M/501M [01:21<00:11, 5.11MB/s] 89%|████████▉ | 447M/501M [01:21<00:11, 4.90MB/s] 89%|████████▉ | 447M/501M [01:21<00:10, 5.09MB/s] 90%|████████▉ | 448M/501M [01:21<00:10, 5.38MB/s] 90%|████████▉ | 449M/501M [01:21<00:09, 5.47MB/s] 90%|████████▉ | 449M/501M [01:22<00:10, 5.22MB/s] 90%|████████▉ | 450M/501M [01:22<00:10, 5.31MB/s] 90%|████████▉ | 450M/501M [01:22<00:09, 5.47MB/s] 90%|█████████ | 451M/501M [01:22<00:08, 5.90MB/s] 90%|█████████ | 452M/501M [01:22<00:08, 6.19MB/s] 90%|█████████ | 452M/501M [01:22<00:08, 6.29MB/s] 90%|█████████ | 453M/501M [01:22<00:07, 6.49MB/s] 91%|█████████ | 454M/501M [01:22<00:06, 7.47MB/s] 91%|█████████ | 455M/501M [01:22<00:05, 8.71MB/s] 91%|█████████ | 456M/501M [01:22<00:04, 9.88MB/s] 91%|█████████▏| 457M/501M [01:23<00:04, 10.5MB/s] 92%|█████████▏| 459M/501M [01:23<00:03, 11.3MB/s] 92%|█████████▏| 460M/501M [01:23<00:03, 12.0MB/s] 92%|█████████▏| 461M/501M [01:23<00:03, 12.7MB/s] 92%|█████████▏| 463M/501M [01:23<00:03, 12.9MB/s] 93%|█████████▎| 464M/501M [01:23<00:02, 13.4MB/s] 93%|█████████▎| 465M/501M [01:23<00:02, 13.7MB/s] 93%|█████████▎| 467M/501M [01:23<00:02, 13.6MB/s] 94%|█████████▎| 468M/501M [01:23<00:02, 14.2MB/s] 94%|█████████▍| 470M/501M [01:23<00:02, 14.5MB/s] 94%|█████████▍| 471M/501M [01:24<00:02, 14.8MB/s] 95%|█████████▍| 473M/501M [01:24<00:01, 15.3MB/s] 95%|█████████▍| 475M/501M [01:24<00:01, 15.6MB/s] 95%|█████████▌| 476M/501M [01:24<00:01, 16.1MB/s] 96%|█████████▌| 478M/501M [01:24<00:01, 16.5MB/s] 96%|█████████▌| 480M/501M [01:24<00:01, 16.9MB/s] 96%|█████████▌| 482M/501M [01:24<00:01, 17.4MB/s] 97%|█████████▋| 483M/501M [01:24<00:01, 15.5MB/s] 97%|█████████▋| 485M/501M [01:24<00:01, 15.1MB/s] 97%|█████████▋| 486M/501M [01:25<00:01, 13.7MB/s] 97%|█████████▋| 488M/501M [01:25<00:01, 12.9MB/s] 98%|█████████▊| 489M/501M [01:25<00:00, 12.3MB/s] 98%|█████████▊| 490M/501M [01:25<00:00, 12.1MB/s] 98%|█████████▊| 491M/501M [01:25<00:00, 11.9MB/s] 98%|█████████▊| 492M/501M [01:25<00:00, 11.7MB/s] 99%|█████████▊| 493M/501M [01:25<00:00, 10.2MB/s] 99%|█████████▉| 495M/501M [01:25<00:00, 11.6MB/s] 99%|█████████▉| 496M/501M [01:26<00:00, 11.3MB/s] 99%|█████████▉| 497M/501M [01:26<00:00, 11.2MB/s]100%|█████████▉| 498M/501M [01:26<00:00, 11.1MB/s]100%|█████████▉| 499M/501M [01:26<00:00, 11.1MB/s]100%|█████████▉| 500M/501M [01:26<00:00, 11.1MB/s]100%|██████████| 501M/501M [01:26<00:00, 6.07MB/s]
[32m[2023-12-21 10:25:23,860] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/bert-base-cased/model_state.pdparams[0m
[32m[2023-12-21 10:25:24,259] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 10:25:24.264708  1171 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 10:25:24,905] [ WARNING][0m - Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder_bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 10:25:24,906] [ WARNING][0m - Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 10:25:25,632] [    INFO][0m - global step 10 / 0, loss: 4.774534, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.07202 sec, avg_samples: 512.00000, ips: 7108.99454 words/sec,  [0m
[32m[2023-12-21 10:25:26,318] [    INFO][0m - global step 20 / 0, loss: 0.057436, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06857 sec, avg_samples: 512.00000, ips: 7466.58404 words/sec,  [0m
[32m[2023-12-21 10:25:27,007] [    INFO][0m - global step 30 / 0, loss: 0.005375, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06888 sec, avg_samples: 512.00000, ips: 7432.72990 words/sec,  [0m
[32m[2023-12-21 10:25:27,696] [    INFO][0m - global step 40 / 0, loss: 0.001878, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06886 sec, avg_samples: 512.00000, ips: 7434.86832 words/sec,  [0m
[32m[2023-12-21 10:25:28,386] [    INFO][0m - global step 50 / 0, loss: 0.001099, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06893 sec, avg_samples: 512.00000, ips: 7427.68595 words/sec,  [0m
[32m[2023-12-21 10:25:29,076] [    INFO][0m - global step 60 / 0, loss: 0.001145, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06900 sec, avg_samples: 512.00000, ips: 7419.99952 words/sec,  [0m
[32m[2023-12-21 10:25:29,763] [    INFO][0m - global step 70 / 0, loss: 0.002608, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06862 sec, avg_samples: 512.00000, ips: 7461.18555 words/sec,  [0m
[32m[2023-12-21 10:25:30,450] [    INFO][0m - global step 80 / 0, loss: 0.002284, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06868 sec, avg_samples: 512.00000, ips: 7454.33520 words/sec,  [0m
[32m[2023-12-21 10:25:31,135] [    INFO][0m - global step 90 / 0, loss: 0.000858, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06851 sec, avg_samples: 512.00000, ips: 7473.75603 words/sec,  [0m
[32m[2023-12-21 10:25:31,824] [    INFO][0m - global step 100 / 0, loss: 0.001065, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06882 sec, avg_samples: 512.00000, ips: 7440.22104 words/sec,  [0m
[32m[2023-12-21 10:25:32,509] [    INFO][0m - global step 110 / 0, loss: 0.000612, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06853 sec, avg_samples: 512.00000, ips: 7470.86739 words/sec,  [0m
[32m[2023-12-21 10:25:33,197] [    INFO][0m - global step 120 / 0, loss: 0.000796, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06878 sec, avg_samples: 512.00000, ips: 7444.22903 words/sec,  [0m
[32m[2023-12-21 10:25:33,883] [    INFO][0m - global step 130 / 0, loss: 0.000553, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06855 sec, avg_samples: 512.00000, ips: 7469.36285 words/sec,  [0m
[32m[2023-12-21 10:25:34,570] [    INFO][0m - global step 140 / 0, loss: 0.000566, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.06866 sec, avg_samples: 512.00000, ips: 7456.68544 words/sec,  [0m
No XPU Memory Leak
[33m Run successfully with command - bert_for_question_answering - python3.9 test_tipc/train.py --model bert_for_question_answering --optimizer sgd --max_seq_len 512 --generated_inputs --learning_rate 0.01 --max_steps 150 --device=xpu                >/workspace/PaddleNLP/tests/test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 10:25:37,109 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 10:25:37,109 auto_parallel_config: None
LAUNCH INFO 2023-12-21 10:25:37,109 auto_tuner_json: None
LAUNCH INFO 2023-12-21 10:25:37,109 devices: 0,1
LAUNCH INFO 2023-12-21 10:25:37,109 elastic_level: -1
LAUNCH INFO 2023-12-21 10:25:37,110 elastic_timeout: 30
LAUNCH INFO 2023-12-21 10:25:37,110 enable_gpu_log: True
LAUNCH INFO 2023-12-21 10:25:37,110 gloo_port: 6767
LAUNCH INFO 2023-12-21 10:25:37,110 host: None
LAUNCH INFO 2023-12-21 10:25:37,110 ips: None
LAUNCH INFO 2023-12-21 10:25:37,110 job_id: default
LAUNCH INFO 2023-12-21 10:25:37,110 legacy: False
LAUNCH INFO 2023-12-21 10:25:37,110 log_dir: log
LAUNCH INFO 2023-12-21 10:25:37,110 log_level: INFO
LAUNCH INFO 2023-12-21 10:25:37,110 log_overwrite: False
LAUNCH INFO 2023-12-21 10:25:37,110 master: None
LAUNCH INFO 2023-12-21 10:25:37,110 max_restart: 3
LAUNCH INFO 2023-12-21 10:25:37,110 nnodes: 1
LAUNCH INFO 2023-12-21 10:25:37,110 nproc_per_node: None
LAUNCH INFO 2023-12-21 10:25:37,110 rank: -1
LAUNCH INFO 2023-12-21 10:25:37,110 run_mode: collective
LAUNCH INFO 2023-12-21 10:25:37,110 server_num: None
LAUNCH INFO 2023-12-21 10:25:37,110 servers: 
LAUNCH INFO 2023-12-21 10:25:37,110 sort_ip: False
LAUNCH INFO 2023-12-21 10:25:37,110 start_port: 6070
LAUNCH INFO 2023-12-21 10:25:37,110 trainer_num: None
LAUNCH INFO 2023-12-21 10:25:37,110 trainers: 
LAUNCH INFO 2023-12-21 10:25:37,110 training_script: test_tipc/train.py
LAUNCH INFO 2023-12-21 10:25:37,110 training_script_args: ['--model', 'bert_for_question_answering', '--optimizer', 'sgd', '--max_seq_len', '512', '--generated_inputs', '--learning_rate', '0.01', '--max_steps', '150', '--device=xpu']
LAUNCH INFO 2023-12-21 10:25:37,110 with_gloo: 1
LAUNCH INFO 2023-12-21 10:25:37,110 --------------------------------------------------
LAUNCH INFO 2023-12-21 10:25:37,111 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 10:25:37,112 Run Pod: bkwwts, replicas 2, status ready
LAUNCH INFO 2023-12-21 10:25:37,126 Watching Pod: bkwwts, replicas 2, status running
LAUNCH INFO 2023-12-21 10:26:31,190 Pod completed
LAUNCH INFO 2023-12-21 10:26:31,190 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='bert_for_question_answering', logging_steps=10, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=True, num_workers=4, profiler_options=None, save_model=None, batch_size=1, max_seq_len=512, data_dir=None, pad_to_max_seq_len=False, optimizer='sgd', learning_rate=0.01, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=None, model_name_or_path='bert-base-cased')
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 10:25:40.508461  1295 tcp_utils.cc:181] The server starts to listen on IP_ANY:44714
I1221 10:25:40.508745  1295 tcp_utils.cc:130] Successfully connected to 127.0.0.1:44714
[32m[2023-12-21 10:25:43,537] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-cased/model_state.pdparams[0m
[32m[2023-12-21 10:25:43,537] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/bert-base-cased/model_state.pdparams[0m
[32m[2023-12-21 10:25:43,990] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 10:25:43.999356  1295 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 10:25:44,618] [ WARNING][0m - Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder_bias', 'cls.predictions.decoder_weight', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 10:25:44,618] [ WARNING][0m - Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
[32m[2023-12-21 10:25:47,722] [    INFO][0m - global step 10 / 0, loss: 1.560508, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.29476 sec, avg_samples: 512.00000, ips: 1737.02246 words/sec,  [0m
[32m[2023-12-21 10:25:50,757] [    INFO][0m - global step 20 / 0, loss: 0.032347, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30342 sec, avg_samples: 512.00000, ips: 1687.43728 words/sec,  [0m
[32m[2023-12-21 10:25:53,787] [    INFO][0m - global step 30 / 0, loss: 0.014379, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30300 sec, avg_samples: 512.00000, ips: 1689.75915 words/sec,  [0m
[32m[2023-12-21 10:25:56,817] [    INFO][0m - global step 40 / 0, loss: 0.004597, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30290 sec, avg_samples: 512.00000, ips: 1690.31817 words/sec,  [0m
[32m[2023-12-21 10:25:59,846] [    INFO][0m - global step 50 / 0, loss: 0.003495, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30288 sec, avg_samples: 512.00000, ips: 1690.45309 words/sec,  [0m
[32m[2023-12-21 10:26:02,876] [    INFO][0m - global step 60 / 0, loss: 0.001866, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30296 sec, avg_samples: 512.00000, ips: 1689.98282 words/sec,  [0m
[32m[2023-12-21 10:26:05,901] [    INFO][0m - global step 70 / 0, loss: 0.001653, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30249 sec, avg_samples: 512.00000, ips: 1692.61382 words/sec,  [0m
[32m[2023-12-21 10:26:08,930] [    INFO][0m - global step 80 / 0, loss: 0.001710, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30292 sec, avg_samples: 512.00000, ips: 1690.23449 words/sec,  [0m
[32m[2023-12-21 10:26:11,957] [    INFO][0m - global step 90 / 0, loss: 0.000399, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30259 sec, avg_samples: 512.00000, ips: 1692.07449 words/sec,  [0m
[32m[2023-12-21 10:26:14,985] [    INFO][0m - global step 100 / 0, loss: 0.001701, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30278 sec, avg_samples: 512.00000, ips: 1690.97408 words/sec,  [0m
[32m[2023-12-21 10:26:18,013] [    INFO][0m - global step 110 / 0, loss: 0.000700, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30282 sec, avg_samples: 512.00000, ips: 1690.78583 words/sec,  [0m
[32m[2023-12-21 10:26:21,039] [    INFO][0m - global step 120 / 0, loss: 0.001299, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30257 sec, avg_samples: 512.00000, ips: 1692.15622 words/sec,  [0m
[32m[2023-12-21 10:26:24,064] [    INFO][0m - global step 130 / 0, loss: 0.001326, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30245 sec, avg_samples: 512.00000, ips: 1692.86681 words/sec,  [0m
[32m[2023-12-21 10:26:27,085] [    INFO][0m - global step 140 / 0, loss: 0.001061, avg_reader_cost: 0.00000 sec, avg_batch_cost: 0.30206 sec, avg_samples: 512.00000, ips: 1695.02703 words/sec,  [0m
I1221 10:26:30.607678  1329 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - bert_for_question_answering - python3.9 -m paddle.distributed.launch --gpus=0,1 test_tipc/train.py --model bert_for_question_answering --optimizer sgd --max_seq_len 512 --generated_inputs --learning_rate 0.01 --max_steps 150 --device=xpu               - /workspace/PaddleNLP/tests/test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log [0m
+ watchcat=201
+ kill -9 1060
+ sleep 10
==END==test_tipc/configs/bert_for_question_answering/train_infer_python.txt
run.sh: line 334:  1060 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/bert_for_question_answering/train_infer_python.txt
++ date +%s
+ end=1703125601
++ echo 1703125422 1703125601
++ awk '{print $2-$1-2}'
+ time=177
test_tipc/configs/bert_for_question_answering/train_infer_python.txt spend time seconds 177
+ echo 'test_tipc/configs/bert_for_question_answering/train_infer_python.txt spend time seconds 177'
+ read config_file
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703125601
+ echo ==START==test_tipc/configs/bigru_crf/train_infer_python.txt
==START==test_tipc/configs/bigru_crf/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/bigru_crf/train_infer_python.txt
+ dataline='===========================train_params===========================
model_name:bigru_crf
python:python3.7
gpu_list:0|0,1
--device:gpu|gpu
null:null
--epoch:lite_train_lite_infer=1|lite_train_whole_infer=1|whole_train_whole_infer=100
--save_model:./test_tipc/output/
--batch_size:lite_train_lite_infer=16|lite_train_whole_infer=16|whole_train_whole_infer=32
null:null
train_model_name:model.pdparams
train_infer_img_dir:./data/lexical_analysis_dataset_tiny
null:null
##
trainer:norm_train
norm_train:test_tipc/train.py --model lac --optimizer adam --max_seq_len 64 --data_dir ./data/lexical_analysis_dataset_tiny/ --seed 102
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
--output_path:./test_tipc/bigru_crf/infer_model/
--params_path:./test_tipc/output/bigru_crf/model.pdparams
norm_export:test_tipc/bigru_crf/export_model.py --data_dir ./data/lexical_analysis_dataset_tiny/
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:bigru_crf
++ strs=model_name:bigru_crf
++ IFS=:
++ array=(${strs})
++ tmp=bigru_crf
++ echo bigru_crf
+ model_name=bigru_crf
+ sleep 10
+ run run_model test_tipc/configs/bigru_crf/train_infer_python.txt lite_train_lite_infer 3600 bigru_crf
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ waitfor=7200
+ command='run_model
test_tipc/configs/bigru_crf/train_infer_python.txt
lite_train_lite_infer
3600
bigru_crf'
+ commandpid=1366
+ run_model test_tipc/configs/bigru_crf/train_infer_python.txt lite_train_lite_infer 3600 bigru_crf
+ config_file=test_tipc/configs/bigru_crf/train_infer_python.txt
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/bigru_crf/train_infer_python.txt lite_train_lite_infer
+ watchdog=1367
+ wait 1366
+ sleep 7200
2023-12-21 10:26:52 URL:https://bj.bcebos.com/paddlenlp/datasets/lexical_analysis_dataset_tiny.tar.gz [207570/207570] -> "./data/lexical_analysis_dataset_tiny.tar.gz" [1]
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/bigru_crf/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/bigru_crf/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 10:27:01,024] [    INFO][0m - global step 10 / 62, loss: 0.000000, avg_reader_cost: 0.00041 sec, avg_batch_cost: 0.48624 sec, avg_samples: 16.00000, ips: 32.90537 sequences/sec,  [0m
[32m[2023-12-21 10:27:05,421] [    INFO][0m - global step 20 / 62, loss: 0.000000, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.43955 sec, avg_samples: 16.00000, ips: 36.40077 sequences/sec,  [0m
[32m[2023-12-21 10:27:09,471] [    INFO][0m - global step 30 / 62, loss: 0.000000, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.40496 sec, avg_samples: 16.00000, ips: 39.51003 sequences/sec,  [0m
[32m[2023-12-21 10:27:13,658] [    INFO][0m - global step 40 / 62, loss: 0.000000, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.41864 sec, avg_samples: 16.00000, ips: 38.21932 sequences/sec,  [0m
[32m[2023-12-21 10:27:18,152] [    INFO][0m - global step 50 / 62, loss: 0.000000, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.44936 sec, avg_samples: 16.00000, ips: 35.60642 sequences/sec,  [0m
[32m[2023-12-21 10:27:22,441] [    INFO][0m - global step 60 / 62, loss: 0.000000, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.42885 sec, avg_samples: 16.00000, ips: 37.30868 sequences/sec,  [0m
Namespace(device='xpu', model='lac', logging_steps=10, seed=102, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=None, epoch=1, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1', batch_size=16, max_seq_len=64, data_dir='./data/lexical_analysis_dataset_tiny/', pad_to_max_seq_len=False, optimizer='adam', learning_rate=0.25, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=None, base_lr=0.001, crf_lr=0.2, emb_dim=128, hidden_size=128)
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 test_tipc/train.py --model lac --optimizer adam --max_seq_len 64 --data_dir ./data/lexical_analysis_dataset_tiny/ --seed 102 --device=xpu  --save_model=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1 --epoch=1     --batch_size=16     >/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
W1221 10:27:27.190493  1547 xpu_context.cc:151] Please NOTE: xpu device: 0
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/base/framework.py:3044: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/tensor/creation.py:2414: UserWarning: paddle.assign doesn't support float64 input now due to current platform protobuf data limitation, we convert it to float32
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (1,), (-1, -1)
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, 59) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1, -1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1, 1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape () into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/base/framework.py:3044: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/tensor/creation.py:2414: UserWarning: paddle.assign doesn't support float64 input now due to current platform protobuf data limitation, we convert it to float32
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (-1,), (-1, -1)
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (-1, -1), (1,)
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (-1, -1, -1), (-1, -1)
  warnings.warn(
I1221 10:27:28.349469  1547 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 test_tipc/bigru_crf/export_model.py --data_dir ./data/lexical_analysis_dataset_tiny/ --params_path=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1/model.pdparams --output_path=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:27:32.270113  1615 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:27:32.275094  1615 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:27:32.296332  1615 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:27:32.297758  1615 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:27:32.303431  1615 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:27:32.312494  1615 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:27:32.357831  1615 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:27:32.359766  1615 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:27:32.359788  1615 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:27:32.360472  1615 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:27:32.412703  1615 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:27:32.414754  1615 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:27:32.414856  1615 program_interpreter.cc:214] New Executor is Running.
I1221 10:27:32.419265  1673 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1 --batch_size=1   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:27:56.113739  1697 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:27:56.118597  1697 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:27:56.140442  1697 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:27:56.141906  1697 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:27:56.147487  1697 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:27:56.156497  1697 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:27:56.202108  1697 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:27:56.204061  1697 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:27:56.204078  1697 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:27:56.204783  1697 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:27:56.256348  1697 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:27:56.258632  1697 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:27:56.258739  1697 program_interpreter.cc:214] New Executor is Running.
I1221 10:27:56.263214  1755 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1 --batch_size=8   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:28:09.300299  1779 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:28:09.305157  1779 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:28:09.325242  1779 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:28:09.326685  1779 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:28:09.331611  1779 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:28:09.341070  1779 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:28:09.387939  1779 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:28:09.389876  1779 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:28:09.389897  1779 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:28:09.390450  1779 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:28:09.442795  1779 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:28:09.444675  1779 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:28:09.444772  1779 program_interpreter.cc:214] New Executor is Running.
I1221 10:28:09.448746  1841 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1 --batch_size=1   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:28:31.058522  1866 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:28:31.068063  1866 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:28:31.089409  1866 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:28:31.090859  1866 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:28:31.096470  1866 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:28:31.105494  1866 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:28:31.151094  1866 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:28:31.153028  1866 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:28:31.153046  1866 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:28:31.153735  1866 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:28:31.204592  1866 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:28:31.206809  1866 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:28:31.206920  1866 program_interpreter.cc:214] New Executor is Running.
I1221 10:28:31.212164  1929 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1 --batch_size=8   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log [0m
Does not support hardware other than CPU and GPU Currently!
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 10:28:42,554 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 10:28:42,555 auto_parallel_config: None
LAUNCH INFO 2023-12-21 10:28:42,555 auto_tuner_json: None
LAUNCH INFO 2023-12-21 10:28:42,555 devices: 0,1
LAUNCH INFO 2023-12-21 10:28:42,555 elastic_level: -1
LAUNCH INFO 2023-12-21 10:28:42,555 elastic_timeout: 30
LAUNCH INFO 2023-12-21 10:28:42,555 enable_gpu_log: True
LAUNCH INFO 2023-12-21 10:28:42,555 gloo_port: 6767
LAUNCH INFO 2023-12-21 10:28:42,555 host: None
LAUNCH INFO 2023-12-21 10:28:42,555 ips: None
LAUNCH INFO 2023-12-21 10:28:42,555 job_id: default
LAUNCH INFO 2023-12-21 10:28:42,555 legacy: False
LAUNCH INFO 2023-12-21 10:28:42,555 log_dir: log
LAUNCH INFO 2023-12-21 10:28:42,555 log_level: INFO
LAUNCH INFO 2023-12-21 10:28:42,555 log_overwrite: False
LAUNCH INFO 2023-12-21 10:28:42,555 master: None
LAUNCH INFO 2023-12-21 10:28:42,555 max_restart: 3
LAUNCH INFO 2023-12-21 10:28:42,555 nnodes: 1
LAUNCH INFO 2023-12-21 10:28:42,555 nproc_per_node: None
LAUNCH INFO 2023-12-21 10:28:42,555 rank: -1
LAUNCH INFO 2023-12-21 10:28:42,555 run_mode: collective
LAUNCH INFO 2023-12-21 10:28:42,555 server_num: None
LAUNCH INFO 2023-12-21 10:28:42,555 servers: 
LAUNCH INFO 2023-12-21 10:28:42,555 sort_ip: False
LAUNCH INFO 2023-12-21 10:28:42,555 start_port: 6070
LAUNCH INFO 2023-12-21 10:28:42,555 trainer_num: None
LAUNCH INFO 2023-12-21 10:28:42,555 trainers: 
LAUNCH INFO 2023-12-21 10:28:42,555 training_script: test_tipc/train.py
LAUNCH INFO 2023-12-21 10:28:42,555 training_script_args: ['--model', 'lac', '--optimizer', 'adam', '--max_seq_len', '64', '--data_dir', './data/lexical_analysis_dataset_tiny/', '--seed', '102', '--device=xpu', '--save_model=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', '--epoch=1', '--batch_size=16']
LAUNCH INFO 2023-12-21 10:28:42,555 with_gloo: 1
LAUNCH INFO 2023-12-21 10:28:42,555 --------------------------------------------------
LAUNCH INFO 2023-12-21 10:28:42,556 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 10:28:42,557 Run Pod: eduptu, replicas 2, status ready
LAUNCH INFO 2023-12-21 10:28:42,570 Watching Pod: eduptu, replicas 2, status running
LAUNCH INFO 2023-12-21 10:29:00,591 Pod completed
LAUNCH INFO 2023-12-21 10:29:00,591 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='lac', logging_steps=10, seed=102, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=None, epoch=1, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=16, max_seq_len=64, data_dir='./data/lexical_analysis_dataset_tiny/', pad_to_max_seq_len=False, optimizer='adam', learning_rate=0.25, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=None, base_lr=0.001, crf_lr=0.2, emb_dim=128, hidden_size=128)
[32m[2023-12-21 10:28:50,862] [    INFO][0m - global step 10 / 31, loss: 0.000000, avg_reader_cost: 0.00042 sec, avg_batch_cost: 0.48630 sec, avg_samples: 16.00000, ips: 32.90183 sequences/sec,  [0m
[32m[2023-12-21 10:28:54,795] [    INFO][0m - global step 20 / 31, loss: 0.000000, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.39319 sec, avg_samples: 16.00000, ips: 40.69276 sequences/sec,  [0m
[32m[2023-12-21 10:28:59,123] [    INFO][0m - global step 30 / 31, loss: 0.000000, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.43277 sec, avg_samples: 16.00000, ips: 36.97110 sequences/sec,  [0m
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 -m paddle.distributed.launch --gpus=0,1 test_tipc/train.py --model lac --optimizer adam --max_seq_len 64 --data_dir ./data/lexical_analysis_dataset_tiny/ --seed 102 --device=xpu --save_model=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1 --epoch=1     --batch_size=16     - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
W1221 10:29:04.121368  2052 xpu_context.cc:151] Please NOTE: xpu device: 0
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/base/framework.py:3044: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/tensor/creation.py:2414: UserWarning: paddle.assign doesn't support float64 input now due to current platform protobuf data limitation, we convert it to float32
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (1,), (-1, -1)
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, 59) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1, -1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1, 1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape () into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/base/framework.py:3044: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/tensor/creation.py:2414: UserWarning: paddle.assign doesn't support float64 input now due to current platform protobuf data limitation, we convert it to float32
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:607: UserWarning: In dy2static mode, we attemp to assign a variable with shape (-1, -1) into a variable with shape(1,), which is not always right.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (-1,), (-1, -1)
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (-1, -1), (1,)
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddle/static/nn/control_flow.py:1477: UserWarning: the input shapes of select_input should have the same rank, but get (-1, -1, -1), (-1, -1)
  warnings.warn(
I1221 10:29:05.280858  2052 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 test_tipc/bigru_crf/export_model.py --data_dir ./data/lexical_analysis_dataset_tiny/ --params_path=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1/model.pdparams --output_path=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:29:09.235989  2120 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:29:09.241092  2120 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:29:09.261693  2120 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:29:09.263139  2120 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:29:09.268056  2120 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:29:09.276985  2120 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:29:09.322708  2120 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:29:09.324656  2120 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:29:09.324676  2120 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:29:09.325351  2120 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:29:09.378129  2120 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:29:09.380136  2120 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:29:09.380237  2120 program_interpreter.cc:214] New Executor is Running.
I1221 10:29:09.385246  2177 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1 --batch_size=1   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:29:32.391778  2202 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:29:32.396772  2202 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:29:32.417660  2202 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:29:32.419131  2202 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:29:32.424827  2202 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:29:32.433868  2202 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:29:32.479249  2202 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:29:32.481181  2202 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:29:32.481200  2202 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:29:32.481877  2202 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:29:32.534523  2202 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:29:32.536792  2202 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:29:32.536892  2202 program_interpreter.cc:214] New Executor is Running.
I1221 10:29:32.541476  2259 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1 --batch_size=8   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:29:45.938891  2284 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:29:45.943797  2284 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:29:45.964779  2284 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:29:45.966166  2284 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:29:45.971148  2284 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:29:45.980129  2284 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:29:46.025410  2284 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:29:46.027323  2284 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:29:46.027343  2284 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:29:46.028055  2284 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:29:46.080242  2284 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:29:46.082695  2284 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:29:46.082823  2284 program_interpreter.cc:214] New Executor is Running.
I1221 10:29:46.087553  2346 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1 --batch_size=1   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 10:30:08.521783  2371 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 10:30:08.526737  2371 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 10:30:08.548321  2371 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 10:30:08.549754  2371 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 10:30:08.554795  2371 fuse_pass_base.cc:59] ---  detected 9 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 10:30:08.563874  2371 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    enabled FC MKL-DNN for 1 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 10:30:08.609745  2371 analysis_predictor.cc:1851] ======= optimize end =======
I1221 10:30:08.611694  2371 naive_executor.cc:200] ---  skip [feed], feed -> length
I1221 10:30:08.611712  2371 naive_executor.cc:200] ---  skip [feed], feed -> token_ids
I1221 10:30:08.612399  2371 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 10:30:08.663749  2371 onednn_context.cc:81] oneDNN v3.2.1
I1221 10:30:08.666054  2371 while_op.cc:171] [ControlFlow][WhileOp] New Executor is Running.
I1221 10:30:08.666170  2371 program_interpreter.cc:214] New Executor is Running.
I1221 10:30:08.670986  2432 conditional_block_op.cc:98] [ControlFlow][ConditionalBlock] New Executor is Running.
0	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
1	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2	[0, 0, 0, 0, 0, 0, 0, 0, 0]
3	[0, 0, 0, 0]
4	[0, 0, 0, 0, 0, 0, 0, 0]
5	[0, 0, 0, 0, 0, 0, 0, 0, 0]
6	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
7	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
8	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
9	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
12	[0, 0, 0, 0, 0, 0, 0, 0]
13	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
14	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
15	[0, 0, 0, 0, 0]
16	[0, 0, 0, 0, 0, 0, 0, 0, 0]
17	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
18	[0, 0, 0, 0, 0, 0, 0, 0, 0]
19	[0, 0, 0, 0, 0, 0]
20	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
21	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
22	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
23	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
24	[0, 0, 0, 0, 0, 0, 0, 0, 0]
25	[0, 0, 0, 0, 0]
26	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
27	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
28	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
29	[0, 0, 0, 0]
30	[0, 0, 0, 0, 0]
31	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
32	[0, 0, 0, 0, 0, 0]
33	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
34	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
35	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
36	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
37	[0, 0, 0, 0, 0, 0, 0]
38	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
39	[0, 0, 0, 0, 0, 0, 0]
40	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
41	[0, 0, 0, 0, 0, 0, 0, 0, 0]
42	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
43	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
44	[0, 0, 0, 0, 0, 0, 0, 0]
45	[0, 0, 0, 0, 0, 0, 0, 0, 0]
46	[0, 0, 0, 0, 0, 0, 0, 0]
47	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
48	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
49	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
50	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
51	[0, 0, 0, 0, 0]
52	[0, 0, 0, 0, 0, 0]
53	[0, 0, 0, 0, 0, 0, 0, 0, 0]
54	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
55	[0, 0, 0, 0, 0, 0, 0, 0]
56	[0, 0, 0, 0, 0]
57	[0, 0, 0, 0, 0, 0, 0]
58	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59	[0, 0, 0, 0]
60	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
61	[0, 0, 0, 0, 0, 0, 0, 0]
62	[0, 0, 0, 0, 0, 0, 0, 0]
63	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
64	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
65	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
66	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
67	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
68	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
69	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
70	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
71	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
72	[0, 0, 0, 0, 0, 0]
73	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
74	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
75	[0, 0, 0, 0, 0, 0]
76	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
77	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
78	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
79	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
80	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
81	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
82	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
83	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
84	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
85	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
86	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
87	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
88	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
89	[0, 0, 0, 0, 0, 0, 0, 0, 0]
90	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
91	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
92	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
93	[0, 0, 0, 0, 0, 0, 0, 0]
94	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
95	[0, 0, 0, 0, 0, 0, 0, 0]
96	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
97	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
98	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
99	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
100	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
101	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
102	[0, 0, 0, 0, 0, 0, 0, 0, 0]
103	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
104	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
105	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
106	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
107	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
108	[0, 0, 0, 0, 0, 0, 0, 0, 0]
109	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
110	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
111	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
112	[0, 0, 0, 0]
113	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
114	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
115	[0, 0, 0, 0, 0, 0, 0, 0, 0]
116	[0, 0, 0, 0, 0, 0, 0, 0]
117	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
118	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
119	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
120	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
121	[0, 0, 0, 0, 0, 0, 0]
122	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
123	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
124	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
125	[0, 0, 0, 0, 0, 0, 0, 0]
126	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
127	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
128	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
129	[0, 0, 0, 0, 0, 0, 0, 0]
130	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
131	[0, 0, 0, 0, 0, 0, 0, 0, 0]
132	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
133	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
134	[0, 0, 0, 0, 0, 0]
135	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
136	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
137	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
138	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
139	[0, 0, 0, 0, 0, 0, 0, 0]
140	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
141	[0, 0, 0, 0, 0, 0, 0]
142	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
143	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
144	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
145	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
146	[0, 0, 0, 0, 0, 0]
147	[0, 0, 0, 0, 0, 0]
148	[0, 0, 0, 0, 0, 0]
149	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
150	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
151	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
152	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
153	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
154	[0, 0, 0, 0, 0, 0, 0, 0, 0]
155	[0, 0, 0, 0, 0, 0, 0, 0, 0]
156	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
157	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
158	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
159	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
160	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
161	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
162	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
163	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
164	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
165	[0, 0, 0, 0, 0, 0, 0]
166	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
167	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
168	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
169	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
170	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
171	[0, 0, 0, 0, 0, 0, 0]
172	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
173	[0, 0, 0, 0, 0, 0, 0, 0, 0]
174	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
175	[0, 0, 0, 0, 0, 0, 0, 0]
176	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
177	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
178	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
179	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
180	[0, 0, 0, 0, 0, 0, 0, 0]
181	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
182	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
183	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
184	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
185	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
186	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
187	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
188	[0, 0, 0, 0, 0, 0, 0, 0]
189	[0, 0, 0, 0, 0, 0, 0, 0]
190	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
191	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
192	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
193	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
194	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
195	[0, 0, 0, 0, 0, 0, 0]
196	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
197	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
198	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
199	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
200	[0, 0, 0, 0, 0, 0]
201	[0, 0, 0, 0, 0, 0, 0, 0]
202	[0, 0, 0, 0, 0, 0]
203	[0, 0, 0, 0, 0, 0, 0, 0, 0]
204	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
205	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
206	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
207	[0, 0, 0, 0, 0, 0, 0, 0, 0]
208	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
209	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
210	[0, 0, 0, 0, 0, 0, 0, 0, 0]
211	[0, 0, 0, 0, 0, 0, 0]
212	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
213	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
214	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
215	[0, 0, 0, 0, 0, 0, 0, 0, 0]
216	[0, 0, 0]
217	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
218	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
219	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
220	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
221	[0, 0, 0, 0]
222	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
223	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
224	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
225	[0, 0, 0, 0, 0, 0, 0, 0, 0]
226	[0, 0, 0, 0, 0, 0, 0, 0]
227	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
228	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
229	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
230	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
231	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
232	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
233	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
234	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
235	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
236	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
237	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
238	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
239	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
240	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
241	[0, 0, 0, 0, 0, 0, 0, 0]
242	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
243	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
244	[0, 0, 0, 0, 0, 0]
245	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
246	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
247	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
248	[0, 0, 0, 0, 0, 0, 0]
249	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
250	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
251	[0, 0, 0, 0, 0, 0, 0]
252	[0, 0, 0, 0, 0, 0, 0, 0, 0]
253	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
254	[0, 0, 0, 0, 0, 0, 0, 0, 0]
255	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
256	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
257	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
258	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
259	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
260	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
261	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
262	[0, 0, 0, 0, 0, 0]
263	[0, 0, 0, 0, 0, 0, 0, 0, 0]
264	[0, 0, 0, 0, 0, 0, 0, 0]
265	[0, 0, 0, 0]
266	[0, 0, 0, 0, 0, 0, 0]
267	[0, 0, 0, 0, 0, 0, 0]
268	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
269	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
270	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
271	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
272	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
273	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
274	[0, 0, 0, 0, 0, 0, 0]
275	[0, 0, 0, 0, 0]
276	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
277	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
278	[0, 0, 0, 0, 0, 0, 0, 0]
279	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
280	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
281	[0, 0, 0, 0, 0, 0, 0, 0, 0]
282	[0, 0, 0, 0, 0]
283	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
284	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
285	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
286	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
287	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
288	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
289	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
290	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
291	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
292	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
293	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
294	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
295	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
296	[0, 0, 0, 0, 0, 0, 0, 0]
297	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
298	[0, 0, 0, 0, 0, 0, 0, 0]
299	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
300	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
301	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
302	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
303	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
304	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
305	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
306	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
307	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
308	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
309	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
310	[0, 0, 0, 0, 0, 0]
311	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
312	[0, 0, 0, 0, 0, 0, 0, 0]
313	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
314	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
315	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
316	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
317	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
318	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
319	[0, 0, 0, 0, 0, 0, 0]
320	[0, 0, 0, 0, 0, 0, 0, 0, 0]
321	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
322	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
323	[0, 0, 0, 0, 0, 0, 0, 0]
324	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
325	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
326	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
327	[0, 0, 0, 0, 0, 0, 0, 0]
328	[0, 0, 0, 0, 0]
329	[0, 0, 0, 0, 0]
330	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
331	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
332	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
333	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
334	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
335	[0, 0, 0, 0, 0]
336	[0, 0, 0, 0, 0]
337	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
338	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
339	[0, 0, 0, 0, 0]
340	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
341	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
342	[0, 0, 0, 0, 0, 0, 0]
343	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
344	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
345	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
346	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
347	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
348	[0, 0, 0, 0, 0]
349	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
350	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
351	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
352	[0, 0, 0, 0, 0, 0, 0, 0, 0]
353	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
354	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
355	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
356	[0, 0, 0, 0, 0, 0]
357	[0, 0, 0, 0, 0, 0, 0]
358	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
359	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
360	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
361	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
362	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
363	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
364	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
365	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
366	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
367	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
368	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
369	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
370	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
371	[0, 0, 0, 0, 0, 0, 0, 0]
372	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
373	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
374	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
375	[0, 0, 0, 0, 0, 0, 0, 0]
376	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
377	[0, 0, 0, 0, 0, 0, 0]
378	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
379	[0, 0, 0, 0, 0, 0, 0, 0, 0]
380	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
381	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
382	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
383	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
384	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
385	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
386	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
387	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
388	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
389	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
390	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
391	[0, 0, 0, 0, 0, 0, 0, 0, 0]
392	[0, 0, 0, 0, 0, 0, 0, 0]
393	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
394	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
395	[0, 0, 0, 0]
396	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
397	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
398	[0, 0, 0, 0, 0]
399	[0, 0, 0, 0, 0, 0, 0]
400	[0, 0, 0, 0, 0, 0, 0, 0, 0]
401	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
402	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
403	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
404	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
405	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
406	[0, 0, 0, 0, 0, 0, 0, 0]
407	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
408	[0, 0, 0, 0, 0, 0]
409	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
410	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
411	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
412	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
413	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
414	[0, 0, 0]
415	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
416	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
417	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
418	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
419	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
420	[0, 0, 0, 0, 0, 0, 0, 0]
421	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
422	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
423	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
424	[0, 0, 0, 0, 0, 0, 0]
425	[0, 0, 0, 0, 0]
426	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
427	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
428	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
429	[0, 0, 0, 0, 0, 0, 0, 0, 0]
430	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
431	[0, 0, 0, 0, 0, 0, 0, 0, 0]
432	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
433	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
434	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
435	[0, 0, 0, 0, 0, 0, 0, 0]
436	[0, 0, 0, 0, 0, 0, 0, 0]
437	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
438	[0, 0, 0, 0, 0, 0, 0]
439	[0, 0, 0, 0, 0, 0, 0, 0]
440	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
441	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
442	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
443	[0, 0, 0, 0, 0]
444	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
445	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
446	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
447	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
448	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
449	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
450	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
451	[0, 0, 0, 0, 0]
452	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
453	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
454	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
455	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
456	[0, 0, 0, 0, 0, 0, 0, 0, 0]
457	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
458	[0, 0, 0, 0, 0, 0, 0, 0, 0]
459	[0, 0, 0, 0, 0, 0, 0, 0, 0]
460	[0, 0, 0, 0]
461	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
462	[0, 0, 0, 0, 0]
463	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
464	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
465	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
466	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
467	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
468	[0, 0, 0, 0, 0, 0, 0, 0, 0]
469	[0, 0, 0, 0, 0, 0, 0, 0]
470	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
471	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
472	[0, 0, 0, 0, 0]
473	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
474	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
475	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
476	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
477	[0, 0, 0, 0, 0, 0, 0, 0]
478	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
479	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
480	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
481	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
482	[0, 0, 0]
483	[0, 0, 0, 0, 0, 0, 0, 0, 0]
484	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
485	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
486	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
487	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
488	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
489	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
490	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
491	[0, 0, 0, 0, 0, 0, 0, 0]
492	[0, 0, 0, 0, 0, 0]
493	[0, 0, 0, 0, 0, 0]
494	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
495	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
496	[0, 0, 0, 0, 0, 0, 0, 0]
497	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
498	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
499	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
500	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
501	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
502	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
503	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
504	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
505	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
506	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
507	[0, 0, 0, 0, 0, 0, 0]
508	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
509	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
510	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
511	[0, 0, 0, 0, 0, 0, 0, 0]
512	[0, 0, 0, 0, 0]
513	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
514	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
515	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
516	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
517	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
518	[0, 0, 0, 0, 0, 0, 0]
519	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
520	[0, 0, 0]
521	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
522	[0, 0, 0, 0]
523	[0, 0, 0, 0, 0, 0]
524	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
525	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
526	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
527	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
528	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
529	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
530	[0, 0, 0, 0, 0, 0, 0, 0]
531	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
532	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
533	[0, 0, 0, 0, 0]
534	[0, 0, 0, 0, 0, 0, 0, 0]
535	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
536	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
537	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
538	[0, 0, 0, 0, 0, 0, 0, 0, 0]
539	[0, 0, 0, 0, 0, 0, 0]
540	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
541	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
542	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
543	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
544	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
545	[0, 0, 0, 0, 0, 0, 0]
546	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
547	[0, 0, 0, 0, 0, 0, 0]
548	[0, 0, 0, 0, 0, 0]
549	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
550	[0, 0, 0, 0, 0, 0, 0]
551	[0, 0, 0, 0, 0, 0, 0, 0, 0]
552	[0, 0, 0, 0, 0, 0, 0, 0, 0]
553	[0, 0, 0, 0, 0, 0]
554	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
555	[0, 0, 0, 0]
556	[0, 0, 0, 0]
557	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
558	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
559	[0, 0, 0, 0, 0, 0, 0, 0]
560	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
561	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
562	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
563	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
564	[0, 0, 0, 0, 0, 0]
565	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
566	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
567	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
568	[0, 0, 0, 0, 0, 0]
569	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
570	[0, 0, 0, 0, 0]
571	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
572	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
573	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
574	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
575	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
576	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
577	[0, 0, 0, 0, 0, 0, 0, 0, 0]
578	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
579	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
580	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
581	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
582	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
583	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
584	[0, 0, 0, 0, 0, 0, 0, 0, 0]
585	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
586	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
587	[0, 0, 0, 0, 0, 0, 0]
588	[0, 0, 0, 0, 0, 0]
589	[0, 0, 0, 0, 0]
590	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
591	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
592	[0, 0, 0, 0, 0, 0, 0, 0]
593	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
594	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
595	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
596	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
597	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
598	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
599	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
600	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
601	[0, 0, 0, 0, 0, 0, 0, 0]
602	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
603	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
604	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
605	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
606	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
607	[0, 0, 0, 0, 0, 0, 0, 0]
608	[0, 0, 0, 0]
609	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
610	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
611	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
612	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
613	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
614	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
615	[0, 0, 0, 0, 0, 0]
616	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
617	[0, 0, 0, 0, 0]
618	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
619	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
620	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
621	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
622	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
623	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
624	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
625	[0, 0, 0, 0, 0, 0, 0, 0, 0]
626	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
627	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
628	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
629	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
630	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
631	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
632	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
633	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
634	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
635	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
636	[0, 0, 0, 0, 0, 0, 0, 0]
637	[0, 0, 0, 0, 0, 0, 0, 0]
638	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
639	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
640	[0, 0, 0, 0, 0]
641	[0, 0, 0, 0, 0, 0, 0, 0]
642	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
643	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
644	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
645	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
646	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
647	[0, 0, 0, 0, 0, 0]
648	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
649	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
650	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
651	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
652	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
653	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
654	[0, 0, 0, 0]
655	[0, 0, 0, 0, 0]
656	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
657	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
658	[0, 0, 0, 0, 0, 0, 0]
659	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
660	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
661	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
662	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
663	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
664	[0, 0, 0, 0, 0, 0, 0, 0, 0]
665	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
666	[0, 0, 0]
667	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
668	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
669	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
670	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
671	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
672	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
673	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
674	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
675	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
676	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
677	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
678	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
679	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
680	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
681	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
682	[0, 0, 0, 0, 0]
683	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
684	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
685	[0, 0, 0, 0, 0, 0, 0, 0]
686	[0, 0, 0, 0, 0, 0, 0, 0, 0]
687	[0, 0, 0, 0, 0, 0, 0, 0]
688	[0, 0, 0, 0, 0, 0, 0, 0]
689	[0, 0, 0, 0, 0]
690	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
691	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
692	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
693	[0, 0, 0, 0, 0, 0, 0, 0, 0]
694	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
695	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
696	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
697	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
698	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
699	[0, 0, 0, 0, 0, 0, 0, 0, 0]
700	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
701	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
702	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
703	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
704	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
705	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
706	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
707	[0, 0, 0, 0, 0, 0]
708	[0, 0, 0, 0, 0, 0, 0, 0, 0]
709	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
710	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
711	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
712	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
713	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
714	[0, 0, 0, 0, 0, 0, 0, 0]
715	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
716	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
717	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
718	[0, 0, 0, 0, 0, 0, 0, 0]
719	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
720	[0, 0, 0]
721	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
722	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
723	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
724	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
725	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
726	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
727	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
728	[0, 0, 0, 0, 0, 0, 0, 0]
729	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
730	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
731	[0, 0, 0, 0, 0, 0]
732	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
733	[0, 0, 0, 0, 0, 0, 0, 0, 0]
734	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
735	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
736	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
737	[0, 0, 0, 0, 0, 0, 0, 0, 0]
738	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
739	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
740	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
741	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
742	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
743	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
744	[0, 0, 0, 0, 0, 0, 0, 0, 0]
745	[0, 0, 0, 0, 0, 0]
746	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
747	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
748	[0, 0, 0, 0, 0, 0]
749	[0, 0, 0, 0, 0, 0, 0, 0, 0]
750	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
751	[0, 0, 0, 0, 0, 0, 0, 0]
752	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
753	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
754	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
755	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
756	[0, 0, 0, 0]
757	[0, 0, 0, 0, 0, 0, 0, 0, 0]
758	[0, 0, 0, 0, 0, 0, 0, 0]
759	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
760	[0, 0, 0]
761	[0, 0, 0, 0, 0, 0, 0]
762	[0, 0, 0, 0, 0, 0, 0, 0]
763	[0, 0, 0, 0, 0, 0]
764	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
765	[0, 0, 0, 0, 0, 0, 0]
766	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
767	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
768	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
769	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
770	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
771	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
772	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
773	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
774	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
775	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
776	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
777	[0, 0, 0, 0, 0]
778	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
779	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
780	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
781	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
782	[0, 0, 0, 0, 0, 0, 0]
783	[0, 0, 0, 0, 0, 0, 0, 0, 0]
784	[0, 0, 0, 0, 0, 0]
785	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
786	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
787	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
788	[0, 0, 0]
789	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
790	[0, 0, 0, 0, 0, 0]
791	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
792	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
793	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
794	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
795	[0, 0, 0, 0, 0, 0, 0, 0]
796	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
797	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
798	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
799	[0, 0, 0, 0, 0, 0, 0, 0]
800	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
801	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
802	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
803	[0, 0, 0, 0, 0]
804	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
805	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
806	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
807	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
808	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
809	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
810	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
811	[0, 0, 0, 0, 0, 0, 0, 0, 0]
812	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
813	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
814	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
815	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
816	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
817	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
818	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
819	[0, 0, 0, 0, 0, 0, 0, 0, 0]
820	[0, 0, 0, 0, 0]
821	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
822	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
823	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
824	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
825	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
826	[0, 0, 0, 0]
827	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
828	[0, 0, 0, 0, 0, 0, 0]
829	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
830	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
831	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
832	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
833	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
834	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
835	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
836	[0, 0, 0, 0, 0, 0]
837	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
838	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
839	[0, 0, 0, 0, 0, 0, 0, 0]
840	[0, 0, 0, 0, 0, 0, 0, 0]
841	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
842	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
843	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
844	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
845	[0, 0, 0]
846	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
847	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
848	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
849	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
850	[0, 0, 0, 0, 0, 0, 0, 0]
851	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
852	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
853	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
854	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
855	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
856	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
857	[0, 0, 0, 0, 0, 0]
858	[0, 0, 0, 0, 0, 0, 0, 0, 0]
859	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
860	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
861	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
862	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
863	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
864	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
865	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
866	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
867	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
868	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
869	[0, 0, 0, 0, 0, 0, 0, 0, 0]
870	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
871	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
872	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
873	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
874	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
875	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
876	[0, 0, 0, 0, 0, 0, 0]
877	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
878	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
879	[0, 0, 0, 0, 0, 0, 0]
880	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
881	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
882	[0, 0, 0, 0, 0, 0, 0]
883	[0, 0, 0, 0, 0, 0, 0, 0, 0]
884	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
885	[0, 0, 0, 0, 0]
886	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
887	[0, 0, 0, 0, 0, 0, 0, 0, 0]
888	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
889	[0, 0, 0, 0, 0, 0]
890	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
891	[0, 0, 0, 0, 0]
892	[0, 0, 0, 0]
893	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
894	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
895	[0, 0, 0, 0, 0, 0, 0]
896	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
897	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
898	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
899	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
900	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
901	[0, 0, 0, 0, 0, 0, 0, 0]
902	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
903	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
904	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
905	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
906	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
907	[0, 0, 0, 0, 0, 0, 0]
908	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
909	[0, 0, 0, 0]
910	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
911	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
912	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
913	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
914	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
915	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
916	[0, 0, 0, 0, 0, 0, 0, 0]
917	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
918	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
919	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
920	[0, 0, 0, 0, 0, 0, 0, 0]
921	[0, 0, 0, 0, 0, 0, 0]
922	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
923	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
924	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
925	[0, 0, 0, 0, 0, 0]
926	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
927	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
928	[0, 0, 0, 0, 0, 0, 0]
929	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
930	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
931	[0, 0, 0, 0, 0, 0, 0, 0]
932	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
933	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
934	[0, 0, 0, 0, 0, 0, 0, 0, 0]
935	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
936	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
937	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
938	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
939	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
940	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
941	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
942	[0, 0, 0, 0, 0, 0, 0]
943	[0, 0, 0, 0, 0]
944	[0, 0, 0, 0, 0, 0, 0, 0]
945	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
946	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
947	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
948	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
949	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
950	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
951	[0, 0, 0, 0]
952	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
953	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
954	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
955	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
956	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
957	[0, 0, 0, 0, 0, 0, 0, 0, 0]
958	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
959	[0, 0, 0, 0, 0, 0, 0]
960	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
961	[0, 0, 0, 0, 0, 0, 0, 0, 0]
962	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
963	[0, 0, 0, 0, 0, 0, 0]
964	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
965	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
966	[0, 0, 0, 0, 0, 0, 0, 0, 0]
967	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
968	[0, 0, 0]
969	[0, 0, 0, 0, 0, 0, 0, 0, 0]
970	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
971	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
972	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
973	[0, 0, 0, 0]
974	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
975	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
976	[0, 0, 0, 0, 0, 0]
977	[0, 0, 0, 0, 0, 0, 0]
978	[0, 0, 0, 0, 0]
979	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
980	[0, 0, 0, 0, 0, 0]
981	[0, 0, 0, 0, 0, 0]
982	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
983	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
984	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
985	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
986	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
987	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
988	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
989	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
990	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
991	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
992	[0, 0, 0, 0, 0, 0, 0, 0]
993	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
994	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
995	[0, 0, 0, 0]
996	[0, 0, 0, 0, 0, 0, 0, 0]
997	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
998	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
999	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
No XPU Memory Leak
[33m Run successfully with command - bigru_crf - python3.9 ./test_tipc/bigru_crf/deploy/predict.py --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1 --batch_size=8   --data_dir=./data/lexical_analysis_dataset_tiny --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log [0m
Does not support hardware other than CPU and GPU Currently!
+ watchcat=1060
+ kill -9 1367
+ sleep 10
==END==test_tipc/configs/bigru_crf/train_infer_python.txt
run.sh: line 334:  1367 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/bigru_crf/train_infer_python.txt
++ date +%s
+ end=1703125828
++ echo 1703125601 1703125828
++ awk '{print $2-$1-2}'
+ time=225
+ echo 'test_tipc/configs/bigru_crf/train_infer_python.txt spend time seconds 225'
+ read config_file
test_tipc/configs/bigru_crf/train_infer_python.txt spend time seconds 225
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703125828
+ echo ==START==test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
==START==test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
+ dataline='===========================train_params===========================
model_name:ernie3_for_sequence_classification
python:python3.7
gpu_list:0|0,1
--device:gpu|gpu
--use_amp:null
--max_steps:null
null:null
--batch_size:null
null:null
null:null
null:null
null:null
##
trainer:norm_train
norm_train:test_tipc/train.py --model ernie3_for_sequence_classification --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path ernie-3.0-base-zh --pad_to_max_seq_len --max_seq_len 512 --logging_steps 10 --seed 42 --task_name tnews --max_steps 150
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
null:null
null:null
norm_export:null
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:ernie3_for_sequence_classification
++ strs=model_name:ernie3_for_sequence_classification
++ IFS=:
++ array=(${strs})
++ tmp=ernie3_for_sequence_classification
++ echo ernie3_for_sequence_classification
+ model_name=ernie3_for_sequence_classification
+ sleep 10
+ run run_model test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt lite_train_lite_infer 3600 ernie3_for_sequence_classification
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
+ waitfor=7200
+ command='run_model
test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
lite_train_lite_infer
3600
ernie3_for_sequence_classification'
+ commandpid=2470
+ run_model test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt lite_train_lite_infer 3600 ernie3_for_sequence_classification
+ config_file=test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt lite_train_lite_infer
+ watchdog=2471
+ wait 2470
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 10:30:42,000] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt and saved to /root/.paddlenlp/models/ernie-3.0-base-zh[0m
[32m[2023-12-21 10:30:42,206] [    INFO][0m - Downloading ernie_3.0_base_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt[0m
Namespace(device='xpu', model='ernie3_for_sequence_classification', logging_steps=10, seed=42, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model=None, batch_size=1, max_seq_len=512, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='ernie-3.0-base-zh', task_name='tnews', max_seq_length=512, warmup_steps=0, warmup_proportion=0.1)
  0%|          | 0.00/182k [00:00<?, ?B/s] 10%|█         | 19.0k/182k [00:00<00:00, 176kB/s] 28%|██▊       | 51.0k/182k [00:00<00:00, 245kB/s] 54%|█████▍    | 99.0k/182k [00:00<00:00, 333kB/s] 81%|████████  | 147k/182k [00:00<00:00, 375kB/s] 100%|██████████| 182k/182k [00:00<00:00, 386kB/s]
[32m[2023-12-21 10:30:42,899] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2023-12-21 10:30:42,900] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
  0%|          | 0/5004 [00:00<?, ?it/s]  1%|          | 35/5004 [00:00<00:18, 268.34it/s]  2%|▏         | 83/5004 [00:00<00:14, 330.08it/s]  3%|▎         | 163/5004 [00:00<00:10, 460.15it/s]  4%|▍         | 211/5004 [00:00<00:10, 456.91it/s]  5%|▌         | 275/5004 [00:00<00:09, 494.88it/s]  7%|▋         | 355/5004 [00:00<00:08, 528.96it/s]  8%|▊         | 419/5004 [00:00<00:08, 517.90it/s] 10%|▉         | 483/5004 [00:01<00:09, 477.79it/s] 11%|█         | 532/5004 [00:01<00:11, 396.85it/s] 12%|█▏        | 595/5004 [00:01<00:11, 389.31it/s] 13%|█▎        | 659/5004 [00:01<00:10, 411.77it/s] 14%|█▍        | 723/5004 [00:01<00:10, 424.90it/s] 15%|█▌        | 767/5004 [00:01<00:10, 410.37it/s] 16%|█▌        | 809/5004 [00:01<00:10, 395.86it/s] 17%|█▋        | 849/5004 [00:02<00:11, 375.97it/s] 18%|█▊        | 887/5004 [00:02<00:10, 375.99it/s] 19%|█▊        | 931/5004 [00:02<00:12, 328.91it/s] 20%|█▉        | 979/5004 [00:02<00:11, 363.67it/s] 21%|██        | 1042/5004 [00:02<00:09, 407.37it/s] 22%|██▏       | 1122/5004 [00:02<00:08, 477.49it/s] 24%|██▍       | 1218/5004 [00:02<00:07, 492.25it/s] 25%|██▌       | 1268/5004 [00:02<00:07, 485.32it/s] 26%|██▋       | 1317/5004 [00:03<00:08, 459.13it/s] 27%|██▋       | 1363/5004 [00:03<00:08, 441.37it/s] 28%|██▊       | 1410/5004 [00:03<00:08, 437.30it/s] 29%|██▉       | 1474/5004 [00:03<00:07, 444.25it/s] 31%|███▏      | 1570/5004 [00:03<00:06, 537.00it/s] 33%|███▎      | 1666/5004 [00:03<00:05, 616.94it/s] 36%|███▌      | 1778/5004 [00:03<00:04, 680.94it/s] 37%|███▋      | 1874/5004 [00:03<00:04, 672.00it/s] 39%|███▉      | 1954/5004 [00:04<00:04, 702.96it/s] 41%|████      | 2034/5004 [00:04<00:04, 700.81it/s] 43%|████▎     | 2146/5004 [00:04<00:03, 716.04it/s] 44%|████▍     | 2226/5004 [00:04<00:03, 731.06it/s] 46%|████▋     | 2322/5004 [00:04<00:03, 722.49it/s] 48%|████▊     | 2418/5004 [00:04<00:03, 782.33it/s] 50%|█████     | 2514/5004 [00:04<00:03, 816.49it/s] 52%|█████▏    | 2626/5004 [00:04<00:02, 869.38it/s] 55%|█████▌    | 2754/5004 [00:05<00:02, 859.45it/s] 58%|█████▊    | 2882/5004 [00:05<00:02, 850.94it/s] 59%|█████▉    | 2968/5004 [00:05<00:02, 833.75it/s] 61%|██████▏   | 3074/5004 [00:05<00:02, 816.59it/s] 64%|██████▎   | 3186/5004 [00:05<00:02, 888.54it/s] 67%|██████▋   | 3330/5004 [00:05<00:01, 1031.00it/s] 70%|███████   | 3506/5004 [00:05<00:01, 1219.64it/s] 73%|███████▎  | 3650/5004 [00:05<00:01, 1279.11it/s] 76%|███████▌  | 3781/5004 [00:05<00:00, 1287.68it/s] 78%|███████▊  | 3912/5004 [00:06<00:00, 1262.43it/s] 81%|████████  | 4040/5004 [00:06<00:00, 1177.62it/s] 83%|████████▎ | 4178/5004 [00:06<00:00, 1209.75it/s] 87%|████████▋ | 4338/5004 [00:06<00:00, 1309.13it/s] 92%|█████████▏| 4610/5004 [00:06<00:00, 1701.13it/s] 97%|█████████▋| 4850/5004 [00:06<00:00, 1883.96it/s]100%|██████████| 5004/5004 [00:06<00:00, 753.13it/s] 
[32m[2023-12-21 10:31:09,524] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/ernie-3.0-base-zh/config.json[0m
[32m[2023-12-21 10:31:09,697] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams[0m
[32m[2023-12-21 10:31:09,697] [    INFO][0m - Downloading ernie_3.0_base_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams[0m
  0%|          | 0.00/452M [00:00<?, ?B/s]  0%|          | 35.0k/452M [00:00<27:00, 293kB/s]  0%|          | 83.0k/452M [00:00<22:38, 349kB/s]  0%|          | 131k/452M [00:00<20:14, 390kB/s]   0%|          | 195k/452M [00:00<16:32, 478kB/s]  0%|          | 259k/452M [00:00<15:12, 520kB/s]  0%|          | 339k/452M [00:00<12:57, 610kB/s]  0%|          | 435k/452M [00:00<11:23, 694kB/s]  0%|          | 531k/452M [00:00<10:13, 772kB/s]  0%|          | 643k/452M [00:01<09:28, 833kB/s]  0%|          | 771k/452M [00:01<08:31, 926kB/s]  0%|          | 883k/452M [00:01<08:09, 968kB/s]  0%|          | 995k/452M [00:01<07:48, 1.01MB/s]  0%|          | 1.13M/452M [00:01<07:04, 1.12MB/s]  0%|          | 1.30M/452M [00:01<06:25, 1.23MB/s]  0%|          | 1.47M/452M [00:01<05:54, 1.33MB/s]  0%|          | 1.63M/452M [00:01<05:35, 1.41MB/s]  0%|          | 1.82M/452M [00:01<05:13, 1.51MB/s]  0%|          | 1.99M/452M [00:02<04:59, 1.57MB/s]  0%|          | 2.21M/452M [00:02<04:37, 1.70MB/s]  1%|          | 2.38M/452M [00:02<04:35, 1.71MB/s]  1%|          | 2.55M/452M [00:02<04:31, 1.74MB/s]  1%|          | 2.80M/452M [00:02<04:12, 1.87MB/s]  1%|          | 3.05M/452M [00:02<03:46, 2.08MB/s]  1%|          | 3.27M/452M [00:02<03:50, 2.04MB/s]  1%|          | 3.53M/452M [00:02<03:42, 2.12MB/s]  1%|          | 3.82M/452M [00:02<03:21, 2.34MB/s]  1%|          | 4.05M/452M [00:03<03:26, 2.27MB/s]  1%|          | 4.33M/452M [00:03<03:10, 2.46MB/s]  1%|          | 4.60M/452M [00:03<03:13, 2.42MB/s]  1%|          | 4.85M/452M [00:03<03:18, 2.36MB/s]  1%|          | 5.10M/452M [00:03<03:35, 2.18MB/s]  1%|          | 5.31M/452M [00:03<03:39, 2.13MB/s]  1%|          | 5.51M/452M [00:03<03:39, 2.14MB/s]  1%|▏         | 5.72M/452M [00:03<04:07, 1.90MB/s]  1%|▏         | 5.91M/452M [00:03<04:09, 1.88MB/s]  1%|▏         | 6.09M/452M [00:04<04:28, 1.74MB/s]  1%|▏         | 6.26M/452M [00:04<04:55, 1.58MB/s]  1%|▏         | 6.42M/452M [00:04<05:14, 1.49MB/s]  1%|▏         | 6.56M/452M [00:04<05:25, 1.43MB/s]  1%|▏         | 6.71M/452M [00:04<06:07, 1.27MB/s]  2%|▏         | 6.83M/452M [00:04<06:36, 1.18MB/s]  2%|▏         | 6.95M/452M [00:04<06:37, 1.18MB/s]  2%|▏         | 7.06M/452M [00:04<06:38, 1.17MB/s]  2%|▏         | 7.19M/452M [00:05<07:16, 1.07MB/s]  2%|▏         | 7.32M/452M [00:05<07:03, 1.10MB/s]  2%|▏         | 7.50M/452M [00:05<06:27, 1.20MB/s]  2%|▏         | 7.71M/452M [00:05<05:56, 1.31MB/s]  2%|▏         | 7.92M/452M [00:05<05:17, 1.47MB/s]  2%|▏         | 8.13M/452M [00:05<04:50, 1.61MB/s]  2%|▏         | 8.35M/452M [00:05<04:36, 1.68MB/s]  2%|▏         | 8.57M/452M [00:05<04:26, 1.75MB/s]  2%|▏         | 8.78M/452M [00:06<04:41, 1.65MB/s]  2%|▏         | 8.97M/452M [00:06<04:27, 1.73MB/s]  2%|▏         | 9.14M/452M [00:06<05:00, 1.55MB/s]  2%|▏         | 9.33M/452M [00:06<05:11, 1.49MB/s]  2%|▏         | 9.57M/452M [00:06<04:53, 1.58MB/s]  2%|▏         | 9.80M/452M [00:06<04:41, 1.65MB/s]  2%|▏         | 10.0M/452M [00:06<04:31, 1.71MB/s]  2%|▏         | 10.3M/452M [00:07<04:32, 1.70MB/s]  2%|▏         | 10.5M/452M [00:07<04:24, 1.75MB/s]  2%|▏         | 10.7M/452M [00:07<04:42, 1.64MB/s]  2%|▏         | 10.8M/452M [00:07<04:55, 1.57MB/s]  2%|▏         | 11.0M/452M [00:07<04:55, 1.56MB/s]  2%|▏         | 11.2M/452M [00:07<04:22, 1.76MB/s]  3%|▎         | 11.4M/452M [00:07<04:16, 1.80MB/s]  3%|▎         | 11.6M/452M [00:07<04:17, 1.79MB/s]  3%|▎         | 11.8M/452M [00:08<04:25, 1.74MB/s]  3%|▎         | 11.9M/452M [00:08<04:28, 1.72MB/s]  3%|▎         | 12.1M/452M [00:08<04:46, 1.61MB/s]  3%|▎         | 12.3M/452M [00:08<05:17, 1.45MB/s]  3%|▎         | 12.4M/452M [00:08<05:02, 1.53MB/s]  3%|▎         | 12.6M/452M [00:08<05:08, 1.49MB/s]  3%|▎         | 12.7M/452M [00:08<06:21, 1.21MB/s]  3%|▎         | 12.9M/452M [00:08<05:48, 1.32MB/s]  3%|▎         | 13.0M/452M [00:09<06:28, 1.19MB/s]  3%|▎         | 13.2M/452M [00:09<06:29, 1.18MB/s]  3%|▎         | 13.3M/452M [00:09<06:33, 1.17MB/s]  3%|▎         | 13.4M/452M [00:09<06:37, 1.16MB/s]  3%|▎         | 13.5M/452M [00:09<06:34, 1.17MB/s]  3%|▎         | 13.6M/452M [00:09<06:47, 1.13MB/s]  3%|▎         | 13.8M/452M [00:09<06:49, 1.12MB/s]  3%|▎         | 13.9M/452M [00:09<06:37, 1.16MB/s]  3%|▎         | 14.0M/452M [00:09<06:38, 1.15MB/s]  3%|▎         | 14.1M/452M [00:10<06:44, 1.14MB/s]  3%|▎         | 14.2M/452M [00:10<06:33, 1.17MB/s]  3%|▎         | 14.4M/452M [00:10<05:31, 1.39MB/s]  3%|▎         | 14.6M/452M [00:10<05:54, 1.29MB/s]  3%|▎         | 14.7M/452M [00:10<06:09, 1.24MB/s]  3%|▎         | 14.8M/452M [00:10<06:09, 1.24MB/s]  3%|▎         | 15.0M/452M [00:10<05:47, 1.32MB/s]  3%|▎         | 15.1M/452M [00:10<05:50, 1.31MB/s]  3%|▎         | 15.3M/452M [00:10<05:51, 1.30MB/s]  3%|▎         | 15.4M/452M [00:11<05:59, 1.27MB/s]  3%|▎         | 15.6M/452M [00:11<05:32, 1.38MB/s]  3%|▎         | 15.7M/452M [00:11<05:18, 1.44MB/s]  4%|▎         | 16.1M/452M [00:11<03:34, 2.13MB/s]  4%|▎         | 16.5M/452M [00:11<02:51, 2.67MB/s]  4%|▍         | 17.0M/452M [00:11<02:21, 3.24MB/s]  4%|▍         | 17.4M/452M [00:11<02:14, 3.39MB/s]  4%|▍         | 17.8M/452M [00:11<02:00, 3.77MB/s]  4%|▍         | 18.3M/452M [00:11<01:58, 3.83MB/s]  4%|▍         | 18.7M/452M [00:12<01:59, 3.79MB/s]  4%|▍         | 19.0M/452M [00:12<02:14, 3.38MB/s]  4%|▍         | 19.4M/452M [00:12<02:24, 3.14MB/s]  4%|▍         | 19.7M/452M [00:12<02:41, 2.81MB/s]  4%|▍         | 20.0M/452M [00:12<02:52, 2.63MB/s]  4%|▍         | 20.2M/452M [00:12<03:11, 2.36MB/s]  5%|▍         | 20.4M/452M [00:12<03:11, 2.37MB/s]  5%|▍         | 20.7M/452M [00:12<03:19, 2.26MB/s]  5%|▍         | 20.9M/452M [00:13<03:15, 2.32MB/s]  5%|▍         | 21.2M/452M [00:13<03:04, 2.45MB/s]  5%|▍         | 21.5M/452M [00:13<02:53, 2.60MB/s]  5%|▍         | 21.8M/452M [00:13<02:50, 2.65MB/s]  5%|▍         | 22.1M/452M [00:13<02:41, 2.80MB/s]  5%|▍         | 22.5M/452M [00:13<02:17, 3.29MB/s]  5%|▌         | 23.0M/452M [00:13<01:59, 3.75MB/s]  5%|▌         | 23.5M/452M [00:13<01:48, 4.15MB/s]  5%|▌         | 24.1M/452M [00:13<01:38, 4.54MB/s]  5%|▌         | 24.5M/452M [00:14<01:39, 4.52MB/s]  6%|▌         | 25.0M/452M [00:14<01:46, 4.19MB/s]  6%|▌         | 25.4M/452M [00:14<02:20, 3.18MB/s]  6%|▌         | 25.9M/452M [00:14<02:04, 3.59MB/s]  6%|▌         | 26.3M/452M [00:14<01:57, 3.79MB/s]  6%|▌         | 26.7M/452M [00:14<01:57, 3.79MB/s]  6%|▌         | 27.1M/452M [00:14<01:53, 3.94MB/s]  6%|▌         | 27.5M/452M [00:14<02:01, 3.68MB/s]  6%|▌         | 27.9M/452M [00:15<02:22, 3.13MB/s]  6%|▌         | 28.2M/452M [00:15<02:31, 2.94MB/s]  6%|▋         | 28.6M/452M [00:15<02:17, 3.22MB/s]  6%|▋         | 29.1M/452M [00:15<01:56, 3.82MB/s]  7%|▋         | 29.6M/452M [00:15<01:52, 3.94MB/s]  7%|▋         | 30.1M/452M [00:15<01:42, 4.32MB/s]  7%|▋         | 30.5M/452M [00:15<01:41, 4.34MB/s]  7%|▋         | 31.0M/452M [00:15<01:58, 3.72MB/s]  7%|▋         | 31.4M/452M [00:16<02:00, 3.66MB/s]  7%|▋         | 31.8M/452M [00:16<01:52, 3.91MB/s]  7%|▋         | 32.3M/452M [00:16<01:46, 4.12MB/s]  7%|▋         | 32.7M/452M [00:16<01:42, 4.29MB/s]  7%|▋         | 33.1M/452M [00:16<01:44, 4.21MB/s]  7%|▋         | 33.5M/452M [00:16<02:01, 3.62MB/s]  7%|▋         | 33.9M/452M [00:16<02:16, 3.23MB/s]  8%|▊         | 34.2M/452M [00:16<02:32, 2.88MB/s]  8%|▊         | 34.5M/452M [00:17<02:39, 2.75MB/s]  8%|▊         | 34.8M/452M [00:17<02:48, 2.59MB/s]  8%|▊         | 35.1M/452M [00:17<02:50, 2.57MB/s]  8%|▊         | 35.3M/452M [00:17<03:03, 2.38MB/s]  8%|▊         | 35.5M/452M [00:17<03:19, 2.19MB/s]  8%|▊         | 35.7M/452M [00:17<03:57, 1.84MB/s]  8%|▊         | 35.9M/452M [00:17<03:56, 1.85MB/s]  8%|▊         | 36.2M/452M [00:17<03:43, 1.96MB/s]  8%|▊         | 36.4M/452M [00:18<03:53, 1.87MB/s]  8%|▊         | 36.6M/452M [00:18<03:54, 1.86MB/s]  8%|▊         | 36.8M/452M [00:18<03:59, 1.82MB/s]  8%|▊         | 36.9M/452M [00:18<03:59, 1.82MB/s]  8%|▊         | 37.2M/452M [00:18<03:44, 1.94MB/s]  8%|▊         | 37.5M/452M [00:18<03:02, 2.38MB/s]  8%|▊         | 37.9M/452M [00:18<02:39, 2.73MB/s]  8%|▊         | 38.2M/452M [00:18<02:35, 2.80MB/s]  9%|▊         | 38.5M/452M [00:18<02:39, 2.72MB/s]  9%|▊         | 38.8M/452M [00:19<02:33, 2.83MB/s]  9%|▊         | 39.1M/452M [00:19<03:02, 2.38MB/s]  9%|▊         | 39.3M/452M [00:19<03:18, 2.18MB/s]  9%|▉         | 39.7M/452M [00:19<02:54, 2.47MB/s]  9%|▉         | 40.0M/452M [00:19<02:52, 2.51MB/s]  9%|▉         | 40.3M/452M [00:19<02:59, 2.40MB/s]  9%|▉         | 40.5M/452M [00:19<03:14, 2.22MB/s]  9%|▉         | 40.8M/452M [00:20<03:14, 2.22MB/s]  9%|▉         | 41.1M/452M [00:20<03:27, 2.08MB/s]  9%|▉         | 41.4M/452M [00:20<03:41, 1.95MB/s]  9%|▉         | 41.7M/452M [00:20<03:18, 2.17MB/s]  9%|▉         | 42.0M/452M [00:20<02:55, 2.45MB/s]  9%|▉         | 42.4M/452M [00:20<02:46, 2.59MB/s]  9%|▉         | 42.7M/452M [00:20<02:53, 2.47MB/s]  9%|▉         | 42.9M/452M [00:20<02:52, 2.49MB/s] 10%|▉         | 43.2M/452M [00:21<03:23, 2.10MB/s] 10%|▉         | 43.4M/452M [00:21<03:22, 2.12MB/s] 10%|▉         | 43.7M/452M [00:21<03:36, 1.98MB/s] 10%|▉         | 44.0M/452M [00:21<03:31, 2.03MB/s] 10%|▉         | 44.4M/452M [00:21<03:07, 2.28MB/s] 10%|▉         | 44.9M/452M [00:21<02:35, 2.74MB/s] 10%|█         | 45.3M/452M [00:21<02:26, 2.92MB/s] 10%|█         | 45.6M/452M [00:22<02:39, 2.67MB/s] 10%|█         | 45.9M/452M [00:22<02:42, 2.62MB/s] 10%|█         | 46.2M/452M [00:22<02:52, 2.46MB/s] 10%|█         | 46.5M/452M [00:22<02:36, 2.72MB/s] 10%|█         | 46.8M/452M [00:22<02:35, 2.74MB/s] 10%|█         | 47.0M/452M [00:22<02:37, 2.69MB/s] 10%|█         | 47.4M/452M [00:22<02:24, 2.95MB/s] 11%|█         | 48.0M/452M [00:22<01:54, 3.71MB/s] 11%|█         | 48.6M/452M [00:23<01:31, 4.63MB/s] 11%|█         | 49.2M/452M [00:23<01:25, 4.93MB/s] 11%|█         | 49.7M/452M [00:23<01:25, 4.94MB/s] 11%|█         | 50.1M/452M [00:23<02:02, 3.44MB/s] 11%|█         | 50.5M/452M [00:23<02:18, 3.04MB/s] 11%|█         | 50.9M/452M [00:23<02:45, 2.54MB/s] 11%|█▏        | 51.1M/452M [00:24<02:57, 2.36MB/s] 11%|█▏        | 51.4M/452M [00:24<03:00, 2.33MB/s] 11%|█▏        | 51.8M/452M [00:24<02:39, 2.63MB/s] 12%|█▏        | 52.1M/452M [00:24<02:27, 2.85MB/s] 12%|█▏        | 52.5M/452M [00:24<02:16, 3.07MB/s] 12%|█▏        | 52.8M/452M [00:24<02:40, 2.61MB/s] 12%|█▏        | 53.1M/452M [00:24<02:55, 2.38MB/s] 12%|█▏        | 53.3M/452M [00:24<03:18, 2.11MB/s] 12%|█▏        | 53.5M/452M [00:25<03:18, 2.11MB/s] 12%|█▏        | 53.7M/452M [00:25<03:31, 1.97MB/s] 12%|█▏        | 53.9M/452M [00:25<03:55, 1.77MB/s] 12%|█▏        | 54.2M/452M [00:25<03:28, 2.00MB/s] 12%|█▏        | 54.4M/452M [00:25<03:59, 1.74MB/s] 12%|█▏        | 54.6M/452M [00:25<04:53, 1.42MB/s] 12%|█▏        | 54.7M/452M [00:25<05:18, 1.31MB/s] 12%|█▏        | 54.9M/452M [00:26<05:57, 1.17MB/s] 12%|█▏        | 55.0M/452M [00:26<06:09, 1.13MB/s] 12%|█▏        | 55.1M/452M [00:26<06:56, 1.00MB/s] 12%|█▏        | 55.2M/452M [00:26<06:50, 1.01MB/s] 12%|█▏        | 55.3M/452M [00:26<06:46, 1.02MB/s] 12%|█▏        | 55.4M/452M [00:26<06:19, 1.10MB/s] 12%|█▏        | 55.6M/452M [00:26<05:49, 1.19MB/s] 12%|█▏        | 55.8M/452M [00:26<05:07, 1.35MB/s] 12%|█▏        | 55.9M/452M [00:27<05:36, 1.24MB/s] 12%|█▏        | 56.0M/452M [00:27<05:46, 1.20MB/s] 12%|█▏        | 56.2M/452M [00:27<05:44, 1.20MB/s] 12%|█▏        | 56.3M/452M [00:27<05:48, 1.19MB/s] 12%|█▏        | 56.4M/452M [00:27<05:52, 1.18MB/s] 12%|█▏        | 56.5M/452M [00:27<06:13, 1.11MB/s] 13%|█▎        | 56.6M/452M [00:27<06:36, 1.05MB/s] 13%|█▎        | 56.7M/452M [00:27<06:56, 997kB/s]  13%|█▎        | 56.8M/452M [00:28<07:28, 925kB/s] 13%|█▎        | 56.9M/452M [00:28<07:50, 882kB/s] 13%|█▎        | 57.0M/452M [00:28<08:11, 843kB/s] 13%|█▎        | 57.1M/452M [00:28<08:10, 845kB/s] 13%|█▎        | 57.2M/452M [00:28<07:44, 891kB/s] 13%|█▎        | 57.3M/452M [00:28<07:36, 907kB/s] 13%|█▎        | 57.4M/452M [00:28<07:28, 924kB/s] 13%|█▎        | 57.5M/452M [00:28<07:47, 885kB/s] 13%|█▎        | 57.6M/452M [00:28<08:03, 857kB/s] 13%|█▎        | 57.7M/452M [00:29<08:07, 848kB/s] 13%|█▎        | 57.8M/452M [00:29<07:49, 881kB/s] 13%|█▎        | 57.9M/452M [00:29<07:42, 895kB/s] 13%|█▎        | 58.0M/452M [00:29<07:19, 941kB/s] 13%|█▎        | 58.2M/452M [00:29<06:07, 1.12MB/s] 13%|█▎        | 58.3M/452M [00:29<06:46, 1.02MB/s] 13%|█▎        | 58.4M/452M [00:29<06:20, 1.08MB/s] 13%|█▎        | 58.5M/452M [00:29<06:37, 1.04MB/s] 13%|█▎        | 58.6M/452M [00:30<06:47, 1.01MB/s] 13%|█▎        | 58.8M/452M [00:30<06:44, 1.02MB/s] 13%|█▎        | 58.9M/452M [00:30<07:02, 976kB/s]  13%|█▎        | 59.0M/452M [00:30<07:05, 970kB/s] 13%|█▎        | 59.1M/452M [00:30<06:50, 1.01MB/s] 13%|█▎        | 59.2M/452M [00:30<06:58, 986kB/s]  13%|█▎        | 59.3M/452M [00:30<07:20, 936kB/s] 13%|█▎        | 59.5M/452M [00:30<07:13, 950kB/s] 13%|█▎        | 59.6M/452M [00:31<07:04, 971kB/s] 13%|█▎        | 59.7M/452M [00:31<07:09, 958kB/s] 13%|█▎        | 59.8M/452M [00:31<06:51, 1.00MB/s] 13%|█▎        | 60.0M/452M [00:31<06:42, 1.02MB/s] 13%|█▎        | 60.1M/452M [00:31<06:30, 1.05MB/s] 13%|█▎        | 60.2M/452M [00:31<06:12, 1.11MB/s] 13%|█▎        | 60.3M/452M [00:31<06:28, 1.06MB/s] 13%|█▎        | 60.5M/452M [00:31<06:43, 1.02MB/s] 13%|█▎        | 60.6M/452M [00:32<06:56, 987kB/s]  13%|█▎        | 60.7M/452M [00:32<06:42, 1.02MB/s] 13%|█▎        | 61.0M/452M [00:32<05:00, 1.37MB/s] 14%|█▎        | 61.2M/452M [00:32<04:16, 1.60MB/s] 14%|█▎        | 61.5M/452M [00:32<03:49, 1.79MB/s] 14%|█▎        | 61.8M/452M [00:32<03:35, 1.90MB/s] 14%|█▎        | 62.1M/452M [00:32<03:32, 1.93MB/s] 14%|█▍        | 62.4M/452M [00:32<03:09, 2.15MB/s] 14%|█▍        | 62.6M/452M [00:33<03:10, 2.15MB/s] 14%|█▍        | 62.9M/452M [00:33<02:55, 2.32MB/s] 14%|█▍        | 63.2M/452M [00:33<02:40, 2.54MB/s] 14%|█▍        | 63.5M/452M [00:33<02:20, 2.89MB/s] 14%|█▍        | 64.0M/452M [00:33<02:05, 3.25MB/s] 14%|█▍        | 64.3M/452M [00:33<02:04, 3.26MB/s] 14%|█▍        | 64.7M/452M [00:33<02:04, 3.27MB/s] 14%|█▍        | 65.0M/452M [00:33<02:14, 3.01MB/s] 14%|█▍        | 65.3M/452M [00:33<02:17, 2.96MB/s] 14%|█▍        | 65.6M/452M [00:34<02:19, 2.91MB/s] 15%|█▍        | 66.0M/452M [00:34<02:00, 3.36MB/s] 15%|█▍        | 66.3M/452M [00:34<02:00, 3.36MB/s] 15%|█▍        | 66.7M/452M [00:34<02:06, 3.21MB/s] 15%|█▍        | 67.0M/452M [00:34<02:28, 2.72MB/s] 15%|█▍        | 67.3M/452M [00:34<02:34, 2.62MB/s] 15%|█▍        | 67.5M/452M [00:34<02:41, 2.49MB/s] 15%|█▍        | 67.8M/452M [00:34<02:41, 2.50MB/s] 15%|█▌        | 68.0M/452M [00:35<02:59, 2.24MB/s] 15%|█▌        | 68.3M/452M [00:35<02:55, 2.29MB/s] 15%|█▌        | 68.6M/452M [00:35<02:35, 2.59MB/s] 15%|█▌        | 69.0M/452M [00:35<02:13, 3.02MB/s] 15%|█▌        | 69.6M/452M [00:35<01:46, 3.75MB/s] 16%|█▌        | 70.2M/452M [00:35<01:30, 4.44MB/s] 16%|█▌        | 70.8M/452M [00:35<01:19, 5.01MB/s] 16%|█▌        | 71.3M/452M [00:35<01:29, 4.45MB/s] 16%|█▌        | 71.7M/452M [00:36<01:38, 4.05MB/s] 16%|█▌        | 72.1M/452M [00:36<01:46, 3.75MB/s] 16%|█▌        | 72.5M/452M [00:36<01:55, 3.45MB/s] 16%|█▌        | 72.8M/452M [00:36<01:55, 3.44MB/s] 16%|█▌        | 73.4M/452M [00:36<01:39, 3.99MB/s] 16%|█▋        | 73.9M/452M [00:36<01:29, 4.45MB/s] 16%|█▋        | 74.4M/452M [00:36<01:35, 4.14MB/s] 17%|█▋        | 74.8M/452M [00:36<01:48, 3.64MB/s] 17%|█▋        | 75.2M/452M [00:36<01:48, 3.66MB/s] 17%|█▋        | 75.5M/452M [00:37<02:03, 3.21MB/s] 17%|█▋        | 75.8M/452M [00:37<02:18, 2.85MB/s] 17%|█▋        | 76.1M/452M [00:37<04:12, 1.56MB/s] 17%|█▋        | 76.9M/452M [00:37<02:43, 2.41MB/s] 17%|█▋        | 77.3M/452M [00:37<02:29, 2.63MB/s] 17%|█▋        | 77.7M/452M [00:38<02:22, 2.76MB/s] 17%|█▋        | 78.1M/452M [00:38<02:21, 2.77MB/s] 17%|█▋        | 78.6M/452M [00:38<02:11, 2.98MB/s] 18%|█▊        | 79.2M/452M [00:38<02:02, 3.19MB/s] 18%|█▊        | 79.8M/452M [00:38<01:45, 3.71MB/s] 18%|█▊        | 80.8M/452M [00:38<01:16, 5.08MB/s] 18%|█▊        | 81.9M/452M [00:38<01:00, 6.47MB/s] 18%|█▊        | 82.9M/452M [00:39<00:52, 7.42MB/s] 18%|█▊        | 83.7M/452M [00:39<00:57, 6.77MB/s] 19%|█▊        | 84.3M/452M [00:39<01:07, 5.72MB/s] 19%|█▉        | 85.0M/452M [00:39<01:17, 4.96MB/s] 19%|█▉        | 85.5M/452M [00:39<01:25, 4.49MB/s] 19%|█▉        | 86.1M/452M [00:39<01:20, 4.77MB/s] 19%|█▉        | 86.7M/452M [00:39<01:17, 4.96MB/s] 19%|█▉        | 87.2M/452M [00:40<01:23, 4.56MB/s] 19%|█▉        | 87.7M/452M [00:40<01:22, 4.61MB/s] 19%|█▉        | 88.1M/452M [00:40<01:38, 3.89MB/s] 20%|█▉        | 88.5M/452M [00:40<01:57, 3.25MB/s] 20%|█▉        | 88.9M/452M [00:40<02:11, 2.90MB/s] 20%|█▉        | 89.1M/452M [00:40<02:13, 2.86MB/s] 20%|█▉        | 89.5M/452M [00:40<02:05, 3.03MB/s] 20%|█▉        | 89.8M/452M [00:41<02:07, 2.99MB/s] 20%|█▉        | 90.1M/452M [00:41<02:29, 2.54MB/s] 20%|█▉        | 90.4M/452M [00:41<02:32, 2.49MB/s] 20%|██        | 90.6M/452M [00:41<02:36, 2.42MB/s] 20%|██        | 90.9M/452M [00:41<02:42, 2.33MB/s] 20%|██        | 91.1M/452M [00:41<02:56, 2.14MB/s] 20%|██        | 91.4M/452M [00:41<02:47, 2.26MB/s] 20%|██        | 91.7M/452M [00:41<02:31, 2.50MB/s] 20%|██        | 92.1M/452M [00:42<02:10, 2.89MB/s] 20%|██        | 92.5M/452M [00:42<01:54, 3.29MB/s] 21%|██        | 92.9M/452M [00:42<02:01, 3.10MB/s] 21%|██        | 93.3M/452M [00:42<01:51, 3.37MB/s] 21%|██        | 93.6M/452M [00:42<02:36, 2.40MB/s] 21%|██        | 93.9M/452M [00:42<02:45, 2.27MB/s] 21%|██        | 94.2M/452M [00:42<03:03, 2.05MB/s] 21%|██        | 94.4M/452M [00:43<03:08, 1.99MB/s] 21%|██        | 94.6M/452M [00:43<03:09, 1.98MB/s] 21%|██        | 94.9M/452M [00:43<02:49, 2.21MB/s] 21%|██        | 95.2M/452M [00:43<02:27, 2.55MB/s] 21%|██        | 95.6M/452M [00:43<02:16, 2.74MB/s] 21%|██        | 95.9M/452M [00:43<02:10, 2.86MB/s] 21%|██▏       | 96.1M/452M [00:43<02:42, 2.29MB/s] 21%|██▏       | 96.4M/452M [00:43<02:40, 2.33MB/s] 21%|██▏       | 96.6M/452M [00:44<02:50, 2.19MB/s] 21%|██▏       | 96.9M/452M [00:44<02:43, 2.28MB/s] 21%|██▏       | 97.1M/452M [00:44<02:56, 2.12MB/s] 22%|██▏       | 97.3M/452M [00:44<02:55, 2.12MB/s] 22%|██▏       | 97.5M/452M [00:44<03:29, 1.78MB/s] 22%|██▏       | 97.7M/452M [00:44<03:42, 1.67MB/s] 22%|██▏       | 97.9M/452M [00:44<03:50, 1.62MB/s] 22%|██▏       | 98.1M/452M [00:44<03:23, 1.82MB/s] 22%|██▏       | 98.4M/452M [00:45<02:56, 2.10MB/s] 22%|██▏       | 98.6M/452M [00:45<03:31, 1.75MB/s] 22%|██▏       | 98.8M/452M [00:45<03:36, 1.71MB/s] 22%|██▏       | 99.0M/452M [00:45<03:50, 1.61MB/s] 22%|██▏       | 99.2M/452M [00:45<04:17, 1.44MB/s] 22%|██▏       | 99.3M/452M [00:45<04:09, 1.48MB/s] 22%|██▏       | 99.5M/452M [00:45<04:39, 1.32MB/s] 22%|██▏       | 99.6M/452M [00:46<04:53, 1.26MB/s] 22%|██▏       | 99.8M/452M [00:46<04:36, 1.34MB/s] 22%|██▏       | 100M/452M [00:46<04:19, 1.42MB/s]  22%|██▏       | 100M/452M [00:46<04:23, 1.40MB/s] 22%|██▏       | 100M/452M [00:46<04:29, 1.37MB/s] 22%|██▏       | 100M/452M [00:46<04:43, 1.30MB/s] 22%|██▏       | 101M/452M [00:46<04:57, 1.24MB/s] 22%|██▏       | 101M/452M [00:46<04:07, 1.49MB/s] 22%|██▏       | 101M/452M [00:47<03:45, 1.63MB/s] 22%|██▏       | 101M/452M [00:47<04:09, 1.47MB/s] 22%|██▏       | 101M/452M [00:47<04:09, 1.47MB/s] 22%|██▏       | 102M/452M [00:47<04:09, 1.47MB/s] 22%|██▏       | 102M/452M [00:47<04:37, 1.33MB/s] 23%|██▎       | 102M/452M [00:47<03:38, 1.68MB/s] 23%|██▎       | 102M/452M [00:47<03:34, 1.71MB/s] 23%|██▎       | 102M/452M [00:47<03:22, 1.81MB/s] 23%|██▎       | 103M/452M [00:48<04:03, 1.51MB/s] 23%|██▎       | 103M/452M [00:48<04:25, 1.38MB/s] 23%|██▎       | 103M/452M [00:48<05:08, 1.19MB/s] 23%|██▎       | 103M/452M [00:48<05:07, 1.19MB/s] 23%|██▎       | 103M/452M [00:48<05:27, 1.12MB/s] 23%|██▎       | 103M/452M [00:48<05:46, 1.06MB/s] 23%|██▎       | 103M/452M [00:48<06:00, 1.02MB/s] 23%|██▎       | 103M/452M [00:49<05:51, 1.04MB/s] 23%|██▎       | 104M/452M [00:49<06:11, 984kB/s]  23%|██▎       | 104M/452M [00:49<06:21, 959kB/s] 23%|██▎       | 104M/452M [00:49<06:38, 918kB/s] 23%|██▎       | 104M/452M [00:49<06:14, 975kB/s] 23%|██▎       | 104M/452M [00:49<05:21, 1.14MB/s] 23%|██▎       | 104M/452M [00:49<04:47, 1.27MB/s] 23%|██▎       | 104M/452M [00:49<04:54, 1.24MB/s] 23%|██▎       | 105M/452M [00:50<04:36, 1.32MB/s] 23%|██▎       | 105M/452M [00:50<04:59, 1.22MB/s] 23%|██▎       | 105M/452M [00:50<05:08, 1.18MB/s] 23%|██▎       | 105M/452M [00:50<05:19, 1.14MB/s] 23%|██▎       | 105M/452M [00:50<05:04, 1.19MB/s] 23%|██▎       | 105M/452M [00:50<04:33, 1.33MB/s] 23%|██▎       | 105M/452M [00:50<04:19, 1.40MB/s] 23%|██▎       | 106M/452M [00:50<03:54, 1.55MB/s] 23%|██▎       | 106M/452M [00:50<03:55, 1.55MB/s] 23%|██▎       | 106M/452M [00:51<04:33, 1.33MB/s] 23%|██▎       | 106M/452M [00:51<04:27, 1.36MB/s] 23%|██▎       | 106M/452M [00:51<04:00, 1.51MB/s] 24%|██▎       | 106M/452M [00:51<03:59, 1.51MB/s] 24%|██▎       | 107M/452M [00:51<03:36, 1.67MB/s] 24%|██▎       | 107M/452M [00:51<03:39, 1.65MB/s] 24%|██▎       | 107M/452M [00:51<03:31, 1.71MB/s] 24%|██▎       | 107M/452M [00:51<03:39, 1.65MB/s] 24%|██▎       | 107M/452M [00:52<03:50, 1.57MB/s] 24%|██▍       | 108M/452M [00:52<03:57, 1.52MB/s] 24%|██▍       | 108M/452M [00:52<04:02, 1.49MB/s] 24%|██▍       | 108M/452M [00:52<04:03, 1.49MB/s] 24%|██▍       | 108M/452M [00:52<03:55, 1.54MB/s] 24%|██▍       | 108M/452M [00:52<03:40, 1.64MB/s] 24%|██▍       | 109M/452M [00:52<03:01, 1.99MB/s] 24%|██▍       | 109M/452M [00:52<03:19, 1.81MB/s] 24%|██▍       | 109M/452M [00:52<03:27, 1.73MB/s] 24%|██▍       | 109M/452M [00:53<03:35, 1.67MB/s] 24%|██▍       | 109M/452M [00:53<03:47, 1.58MB/s] 24%|██▍       | 109M/452M [00:53<03:52, 1.55MB/s] 24%|██▍       | 110M/452M [00:53<03:52, 1.54MB/s] 24%|██▍       | 110M/452M [00:53<03:44, 1.60MB/s] 24%|██▍       | 110M/452M [00:53<03:43, 1.61MB/s] 24%|██▍       | 110M/452M [00:53<03:27, 1.73MB/s] 24%|██▍       | 111M/452M [00:54<03:10, 1.89MB/s] 25%|██▍       | 111M/452M [00:54<02:32, 2.34MB/s] 25%|██▍       | 111M/452M [00:54<02:19, 2.57MB/s] 25%|██▍       | 112M/452M [00:54<02:41, 2.21MB/s] 25%|██▍       | 112M/452M [00:54<02:06, 2.83MB/s] 25%|██▍       | 112M/452M [00:54<02:07, 2.79MB/s] 25%|██▍       | 113M/452M [00:54<02:10, 2.72MB/s] 25%|██▍       | 113M/452M [00:54<02:18, 2.57MB/s] 25%|██▌       | 113M/452M [00:54<02:16, 2.61MB/s] 25%|██▌       | 114M/452M [00:55<02:12, 2.67MB/s] 25%|██▌       | 114M/452M [00:55<02:05, 2.83MB/s] 25%|██▌       | 114M/452M [00:55<02:02, 2.90MB/s] 25%|██▌       | 114M/452M [00:55<02:04, 2.85MB/s] 25%|██▌       | 115M/452M [00:55<01:56, 3.04MB/s] 25%|██▌       | 115M/452M [00:55<02:04, 2.85MB/s] 26%|██▌       | 115M/452M [00:55<02:32, 2.32MB/s] 26%|██▌       | 116M/452M [00:55<02:26, 2.41MB/s] 26%|██▌       | 116M/452M [00:56<02:08, 2.75MB/s] 26%|██▌       | 117M/452M [00:56<01:59, 2.94MB/s] 26%|██▌       | 117M/452M [00:56<01:55, 3.05MB/s] 26%|██▌       | 117M/452M [00:56<01:52, 3.12MB/s] 26%|██▌       | 118M/452M [00:56<01:49, 3.20MB/s] 26%|██▌       | 118M/452M [00:56<01:53, 3.08MB/s] 26%|██▌       | 118M/452M [00:56<01:53, 3.09MB/s] 26%|██▋       | 119M/452M [00:57<01:53, 3.07MB/s] 26%|██▋       | 119M/452M [00:57<01:48, 3.21MB/s] 26%|██▋       | 120M/452M [00:57<02:09, 2.70MB/s] 26%|██▋       | 120M/452M [00:57<02:10, 2.68MB/s] 27%|██▋       | 120M/452M [00:57<01:39, 3.51MB/s] 27%|██▋       | 121M/452M [00:57<01:37, 3.58MB/s] 27%|██▋       | 121M/452M [00:57<01:48, 3.19MB/s] 27%|██▋       | 121M/452M [00:57<01:55, 3.01MB/s] 27%|██▋       | 122M/452M [00:58<01:57, 2.95MB/s] 27%|██▋       | 122M/452M [00:58<02:06, 2.75MB/s] 27%|██▋       | 122M/452M [00:58<02:02, 2.83MB/s] 27%|██▋       | 123M/452M [00:58<02:12, 2.61MB/s] 27%|██▋       | 123M/452M [00:58<02:19, 2.48MB/s] 27%|██▋       | 123M/452M [00:58<02:16, 2.54MB/s] 27%|██▋       | 124M/452M [00:58<02:11, 2.62MB/s] 27%|██▋       | 124M/452M [00:58<02:03, 2.78MB/s] 27%|██▋       | 124M/452M [00:58<02:05, 2.74MB/s] 28%|██▊       | 124M/452M [00:59<01:59, 2.88MB/s] 28%|██▊       | 125M/452M [00:59<01:58, 2.89MB/s] 28%|██▊       | 125M/452M [00:59<01:56, 2.96MB/s] 28%|██▊       | 125M/452M [00:59<01:59, 2.88MB/s] 28%|██▊       | 126M/452M [00:59<01:54, 2.99MB/s] 28%|██▊       | 126M/452M [00:59<01:55, 2.97MB/s] 28%|██▊       | 126M/452M [00:59<02:01, 2.82MB/s] 28%|██▊       | 127M/452M [00:59<02:10, 2.62MB/s] 28%|██▊       | 127M/452M [01:00<02:28, 2.30MB/s] 28%|██▊       | 127M/452M [01:00<02:51, 1.99MB/s] 28%|██▊       | 127M/452M [01:00<03:03, 1.85MB/s] 28%|██▊       | 128M/452M [01:00<03:18, 1.71MB/s] 28%|██▊       | 128M/452M [01:00<03:15, 1.74MB/s] 28%|██▊       | 128M/452M [01:00<03:27, 1.64MB/s] 28%|██▊       | 128M/452M [01:00<03:31, 1.61MB/s] 28%|██▊       | 128M/452M [01:00<03:19, 1.70MB/s] 28%|██▊       | 129M/452M [01:01<03:16, 1.72MB/s] 28%|██▊       | 129M/452M [01:01<03:23, 1.67MB/s] 29%|██▊       | 129M/452M [01:01<03:18, 1.71MB/s] 29%|██▊       | 129M/452M [01:01<04:01, 1.40MB/s] 29%|██▊       | 129M/452M [01:01<04:04, 1.39MB/s] 29%|██▊       | 129M/452M [01:01<04:14, 1.33MB/s] 29%|██▊       | 129M/452M [01:01<04:21, 1.29MB/s] 29%|██▊       | 130M/452M [01:02<04:20, 1.30MB/s] 29%|██▊       | 130M/452M [01:02<04:12, 1.34MB/s] 29%|██▊       | 130M/452M [01:02<03:59, 1.41MB/s] 29%|██▉       | 130M/452M [01:02<03:16, 1.72MB/s] 29%|██▉       | 130M/452M [01:02<02:57, 1.91MB/s] 29%|██▉       | 131M/452M [01:02<02:16, 2.48MB/s] 29%|██▉       | 131M/452M [01:02<02:02, 2.76MB/s] 29%|██▉       | 132M/452M [01:02<02:05, 2.68MB/s] 29%|██▉       | 132M/452M [01:02<02:07, 2.63MB/s] 29%|██▉       | 132M/452M [01:03<02:10, 2.58MB/s] 29%|██▉       | 132M/452M [01:03<02:26, 2.29MB/s] 29%|██▉       | 133M/452M [01:03<03:11, 1.75MB/s] 29%|██▉       | 133M/452M [01:03<02:53, 1.93MB/s] 29%|██▉       | 133M/452M [01:03<03:00, 1.85MB/s] 29%|██▉       | 133M/452M [01:03<03:19, 1.68MB/s] 29%|██▉       | 133M/452M [01:03<03:16, 1.70MB/s] 30%|██▉       | 134M/452M [01:04<03:28, 1.60MB/s] 30%|██▉       | 134M/452M [01:04<03:44, 1.49MB/s] 30%|██▉       | 134M/452M [01:04<03:26, 1.62MB/s] 30%|██▉       | 134M/452M [01:04<03:34, 1.56MB/s] 30%|██▉       | 134M/452M [01:04<03:31, 1.58MB/s] 30%|██▉       | 135M/452M [01:04<03:08, 1.76MB/s] 30%|██▉       | 135M/452M [01:04<03:12, 1.73MB/s] 30%|██▉       | 135M/452M [01:04<02:31, 2.20MB/s] 30%|██▉       | 135M/452M [01:04<02:15, 2.44MB/s] 30%|███       | 136M/452M [01:05<01:56, 2.86MB/s] 30%|███       | 136M/452M [01:05<02:23, 2.31MB/s] 30%|███       | 136M/452M [01:05<02:20, 2.36MB/s] 30%|███       | 137M/452M [01:05<02:34, 2.14MB/s] 30%|███       | 137M/452M [01:05<02:47, 1.98MB/s] 30%|███       | 137M/452M [01:05<02:49, 1.95MB/s] 30%|███       | 137M/452M [01:05<02:26, 2.26MB/s] 30%|███       | 138M/452M [01:06<01:52, 2.93MB/s] 31%|███       | 138M/452M [01:06<01:41, 3.25MB/s] 31%|███       | 139M/452M [01:06<01:34, 3.48MB/s] 31%|███       | 139M/452M [01:06<01:49, 3.01MB/s] 31%|███       | 139M/452M [01:06<01:47, 3.04MB/s] 31%|███       | 140M/452M [01:06<02:17, 2.38MB/s] 31%|███       | 140M/452M [01:06<02:28, 2.20MB/s] 31%|███       | 140M/452M [01:07<02:44, 1.99MB/s] 31%|███       | 140M/452M [01:07<02:49, 1.92MB/s] 31%|███       | 141M/452M [01:07<02:48, 1.94MB/s] 31%|███       | 141M/452M [01:07<02:46, 1.96MB/s] 31%|███       | 141M/452M [01:07<02:40, 2.04MB/s] 31%|███       | 141M/452M [01:07<03:02, 1.78MB/s] 31%|███▏      | 141M/452M [01:07<03:23, 1.60MB/s] 31%|███▏      | 142M/452M [01:07<03:23, 1.60MB/s] 31%|███▏      | 142M/452M [01:08<03:35, 1.51MB/s] 31%|███▏      | 142M/452M [01:08<03:43, 1.46MB/s] 31%|███▏      | 142M/452M [01:08<03:58, 1.37MB/s] 31%|███▏      | 142M/452M [01:08<03:56, 1.38MB/s] 31%|███▏      | 142M/452M [01:08<03:32, 1.53MB/s] 32%|███▏      | 143M/452M [01:08<03:32, 1.53MB/s] 32%|███▏      | 143M/452M [01:08<03:32, 1.53MB/s] 32%|███▏      | 143M/452M [01:08<03:12, 1.68MB/s] 32%|███▏      | 143M/452M [01:08<02:53, 1.87MB/s] 32%|███▏      | 143M/452M [01:09<02:53, 1.87MB/s] 32%|███▏      | 144M/452M [01:09<02:46, 1.94MB/s] 32%|███▏      | 144M/452M [01:09<02:40, 2.01MB/s] 32%|███▏      | 144M/452M [01:09<02:37, 2.06MB/s] 32%|███▏      | 144M/452M [01:09<02:30, 2.15MB/s] 32%|███▏      | 145M/452M [01:09<01:57, 2.74MB/s] 32%|███▏      | 145M/452M [01:09<01:58, 2.73MB/s] 32%|███▏      | 145M/452M [01:09<02:05, 2.57MB/s] 32%|███▏      | 146M/452M [01:10<02:09, 2.48MB/s] 32%|███▏      | 146M/452M [01:10<02:38, 2.03MB/s] 32%|███▏      | 146M/452M [01:10<02:58, 1.80MB/s] 32%|███▏      | 146M/452M [01:10<03:04, 1.74MB/s] 32%|███▏      | 146M/452M [01:10<03:05, 1.73MB/s] 32%|███▏      | 147M/452M [01:10<03:11, 1.67MB/s] 32%|███▏      | 147M/452M [01:10<03:17, 1.63MB/s] 32%|███▏      | 147M/452M [01:10<03:51, 1.39MB/s] 33%|███▎      | 147M/452M [01:11<03:43, 1.43MB/s] 33%|███▎      | 147M/452M [01:11<03:46, 1.41MB/s] 33%|███▎      | 147M/452M [01:11<03:47, 1.40MB/s] 33%|███▎      | 147M/452M [01:11<03:53, 1.37MB/s] 33%|███▎      | 148M/452M [01:11<04:15, 1.25MB/s] 33%|███▎      | 148M/452M [01:11<04:37, 1.15MB/s] 33%|███▎      | 148M/452M [01:11<05:09, 1.03MB/s] 33%|███▎      | 148M/452M [01:12<05:01, 1.06MB/s] 33%|███▎      | 148M/452M [01:12<05:08, 1.03MB/s] 33%|███▎      | 148M/452M [01:12<05:24, 981kB/s]  33%|███▎      | 149M/452M [01:12<05:12, 1.02MB/s] 33%|███▎      | 149M/452M [01:12<05:18, 1.00MB/s] 33%|███▎      | 149M/452M [01:12<05:28, 969kB/s]  33%|███▎      | 149M/452M [01:13<05:29, 965kB/s] 33%|███▎      | 149M/452M [01:13<05:06, 1.04MB/s] 33%|███▎      | 149M/452M [01:13<04:44, 1.12MB/s] 33%|███▎      | 149M/452M [01:13<04:53, 1.08MB/s] 33%|███▎      | 150M/452M [01:13<03:49, 1.38MB/s] 33%|███▎      | 150M/452M [01:13<03:26, 1.54MB/s] 33%|███▎      | 150M/452M [01:13<03:06, 1.70MB/s] 33%|███▎      | 150M/452M [01:13<02:44, 1.92MB/s] 33%|███▎      | 150M/452M [01:13<02:45, 1.91MB/s] 33%|███▎      | 151M/452M [01:14<02:51, 1.84MB/s] 33%|███▎      | 151M/452M [01:14<02:57, 1.78MB/s] 33%|███▎      | 151M/452M [01:14<03:04, 1.71MB/s] 33%|███▎      | 151M/452M [01:14<02:44, 1.92MB/s] 34%|███▎      | 152M/452M [01:14<02:26, 2.15MB/s] 34%|███▎      | 152M/452M [01:14<02:20, 2.25MB/s] 34%|███▎      | 152M/452M [01:14<02:29, 2.11MB/s] 34%|███▎      | 152M/452M [01:14<02:31, 2.08MB/s] 34%|███▎      | 153M/452M [01:15<02:32, 2.06MB/s] 34%|███▍      | 153M/452M [01:15<02:30, 2.09MB/s] 34%|███▍      | 153M/452M [01:15<02:20, 2.24MB/s] 34%|███▍      | 154M/452M [01:15<02:31, 2.07MB/s] 34%|███▍      | 154M/452M [01:15<02:10, 2.39MB/s] 34%|███▍      | 154M/452M [01:15<02:30, 2.07MB/s] 34%|███▍      | 154M/452M [01:15<02:44, 1.90MB/s] 34%|███▍      | 155M/452M [01:16<02:49, 1.84MB/s] 34%|███▍      | 155M/452M [01:16<02:47, 1.86MB/s] 34%|███▍      | 155M/452M [01:16<03:35, 1.45MB/s] 34%|███▍      | 155M/452M [01:16<03:34, 1.45MB/s] 34%|███▍      | 155M/452M [01:16<03:31, 1.47MB/s] 34%|███▍      | 155M/452M [01:16<03:57, 1.31MB/s] 34%|███▍      | 155M/452M [01:16<03:55, 1.32MB/s] 34%|███▍      | 156M/452M [01:16<03:50, 1.35MB/s] 34%|███▍      | 156M/452M [01:17<03:39, 1.42MB/s] 34%|███▍      | 156M/452M [01:17<03:36, 1.43MB/s] 35%|███▍      | 156M/452M [01:17<03:39, 1.42MB/s] 35%|███▍      | 156M/452M [01:17<03:59, 1.30MB/s] 35%|███▍      | 156M/452M [01:17<03:38, 1.42MB/s] 35%|███▍      | 157M/452M [01:17<03:37, 1.42MB/s] 35%|███▍      | 157M/452M [01:17<03:43, 1.39MB/s] 35%|███▍      | 157M/452M [01:17<03:45, 1.38MB/s] 35%|███▍      | 157M/452M [01:18<04:10, 1.23MB/s] 35%|███▍      | 157M/452M [01:18<03:45, 1.37MB/s] 35%|███▍      | 157M/452M [01:18<03:50, 1.34MB/s] 35%|███▍      | 158M/452M [01:18<03:52, 1.33MB/s] 35%|███▍      | 158M/452M [01:18<03:57, 1.30MB/s] 35%|███▍      | 158M/452M [01:18<04:06, 1.25MB/s] 35%|███▍      | 158M/452M [01:18<04:23, 1.17MB/s] 35%|███▍      | 158M/452M [01:18<04:32, 1.13MB/s] 35%|███▍      | 158M/452M [01:18<04:27, 1.15MB/s] 35%|███▍      | 158M/452M [01:19<04:18, 1.19MB/s] 35%|███▌      | 158M/452M [01:19<04:04, 1.26MB/s] 35%|███▌      | 159M/452M [01:19<03:31, 1.46MB/s] 35%|███▌      | 159M/452M [01:19<03:25, 1.50MB/s] 35%|███▌      | 159M/452M [01:19<03:06, 1.65MB/s] 35%|███▌      | 159M/452M [01:19<02:50, 1.80MB/s] 35%|███▌      | 160M/452M [01:19<02:54, 1.76MB/s] 35%|███▌      | 160M/452M [01:20<02:50, 1.80MB/s] 35%|███▌      | 160M/452M [01:20<02:50, 1.80MB/s] 35%|███▌      | 160M/452M [01:20<02:53, 1.77MB/s] 35%|███▌      | 160M/452M [01:20<03:08, 1.63MB/s] 36%|███▌      | 161M/452M [01:20<03:00, 1.69MB/s] 36%|███▌      | 161M/452M [01:20<02:41, 1.89MB/s] 36%|███▌      | 161M/452M [01:20<02:20, 2.17MB/s] 36%|███▌      | 162M/452M [01:20<01:57, 2.59MB/s] 36%|███▌      | 162M/452M [01:20<02:02, 2.48MB/s] 36%|███▌      | 162M/452M [01:21<02:09, 2.35MB/s] 36%|███▌      | 162M/452M [01:21<02:08, 2.37MB/s] 36%|███▌      | 163M/452M [01:21<02:02, 2.47MB/s] 36%|███▌      | 163M/452M [01:21<02:05, 2.42MB/s] 36%|███▌      | 163M/452M [01:21<02:17, 2.20MB/s] 36%|███▌      | 163M/452M [01:21<02:10, 2.32MB/s] 36%|███▌      | 164M/452M [01:21<02:34, 1.96MB/s] 36%|███▌      | 164M/452M [01:21<02:33, 1.97MB/s] 36%|███▋      | 164M/452M [01:22<02:33, 1.96MB/s] 36%|███▋      | 164M/452M [01:22<02:35, 1.94MB/s] 36%|███▋      | 164M/452M [01:22<03:51, 1.30MB/s] 36%|███▋      | 165M/452M [01:22<03:09, 1.59MB/s] 36%|███▋      | 165M/452M [01:22<03:10, 1.58MB/s] 37%|███▋      | 165M/452M [01:22<02:49, 1.78MB/s] 37%|███▋      | 166M/452M [01:22<02:22, 2.11MB/s] 37%|███▋      | 166M/452M [01:23<02:08, 2.34MB/s] 37%|███▋      | 166M/452M [01:23<02:17, 2.18MB/s] 37%|███▋      | 166M/452M [01:23<02:16, 2.20MB/s] 37%|███▋      | 167M/452M [01:23<02:37, 1.90MB/s] 37%|███▋      | 167M/452M [01:23<02:48, 1.78MB/s] 37%|███▋      | 167M/452M [01:23<03:02, 1.64MB/s] 37%|███▋      | 167M/452M [01:23<02:46, 1.79MB/s] 37%|███▋      | 167M/452M [01:24<03:14, 1.54MB/s] 37%|███▋      | 168M/452M [01:24<02:45, 1.80MB/s] 37%|███▋      | 168M/452M [01:24<02:43, 1.82MB/s] 37%|███▋      | 168M/452M [01:24<02:23, 2.08MB/s] 37%|███▋      | 168M/452M [01:24<02:24, 2.06MB/s] 37%|███▋      | 169M/452M [01:24<02:22, 2.08MB/s] 37%|███▋      | 169M/452M [01:24<02:45, 1.80MB/s] 37%|███▋      | 169M/452M [01:24<02:38, 1.88MB/s] 37%|███▋      | 169M/452M [01:25<02:38, 1.88MB/s] 37%|███▋      | 169M/452M [01:25<02:57, 1.67MB/s] 37%|███▋      | 170M/452M [01:25<02:56, 1.68MB/s] 38%|███▊      | 170M/452M [01:25<02:34, 1.91MB/s] 38%|███▊      | 170M/452M [01:25<02:41, 1.84MB/s] 38%|███▊      | 170M/452M [01:25<02:44, 1.80MB/s] 38%|███▊      | 170M/452M [01:25<02:44, 1.79MB/s] 38%|███▊      | 171M/452M [01:25<02:31, 1.95MB/s] 38%|███▊      | 171M/452M [01:25<02:32, 1.93MB/s] 38%|███▊      | 171M/452M [01:26<02:37, 1.87MB/s] 38%|███▊      | 171M/452M [01:26<02:38, 1.86MB/s] 38%|███▊      | 171M/452M [01:26<02:51, 1.71MB/s] 38%|███▊      | 172M/452M [01:26<02:58, 1.65MB/s] 38%|███▊      | 172M/452M [01:26<03:11, 1.54MB/s] 38%|███▊      | 172M/452M [01:26<03:24, 1.44MB/s] 38%|███▊      | 172M/452M [01:26<03:25, 1.43MB/s] 38%|███▊      | 172M/452M [01:26<03:19, 1.47MB/s] 38%|███▊      | 172M/452M [01:26<03:22, 1.45MB/s] 38%|███▊      | 172M/452M [01:27<03:23, 1.44MB/s] 38%|███▊      | 173M/452M [01:27<03:21, 1.45MB/s] 38%|███▊      | 173M/452M [01:27<03:15, 1.50MB/s] 38%|███▊      | 173M/452M [01:27<03:01, 1.62MB/s] 38%|███▊      | 173M/452M [01:27<02:33, 1.91MB/s] 38%|███▊      | 173M/452M [01:27<02:45, 1.77MB/s] 38%|███▊      | 174M/452M [01:27<02:20, 2.07MB/s] 39%|███▊      | 174M/452M [01:27<01:44, 2.80MB/s] 39%|███▊      | 174M/452M [01:28<01:51, 2.62MB/s] 39%|███▊      | 175M/452M [01:28<02:14, 2.16MB/s] 39%|███▊      | 175M/452M [01:28<02:08, 2.26MB/s] 39%|███▊      | 175M/452M [01:28<02:26, 1.98MB/s] 39%|███▉      | 175M/452M [01:28<02:35, 1.87MB/s] 39%|███▉      | 176M/452M [01:28<02:36, 1.85MB/s] 39%|███▉      | 176M/452M [01:28<02:35, 1.87MB/s] 39%|███▉      | 176M/452M [01:28<02:36, 1.85MB/s] 39%|███▉      | 176M/452M [01:29<02:34, 1.87MB/s] 39%|███▉      | 176M/452M [01:29<02:16, 2.12MB/s] 39%|███▉      | 177M/452M [01:29<02:18, 2.09MB/s] 39%|███▉      | 177M/452M [01:29<02:29, 1.93MB/s] 39%|███▉      | 177M/452M [01:29<02:37, 1.84MB/s] 39%|███▉      | 177M/452M [01:29<02:44, 1.75MB/s] 39%|███▉      | 177M/452M [01:29<02:34, 1.86MB/s] 39%|███▉      | 178M/452M [01:29<02:43, 1.76MB/s] 39%|███▉      | 178M/452M [01:29<02:52, 1.67MB/s] 39%|███▉      | 178M/452M [01:30<02:51, 1.68MB/s] 39%|███▉      | 178M/452M [01:30<03:10, 1.51MB/s] 39%|███▉      | 178M/452M [01:30<03:11, 1.50MB/s] 39%|███▉      | 179M/452M [01:30<02:49, 1.69MB/s] 40%|███▉      | 179M/452M [01:30<02:53, 1.65MB/s] 40%|███▉      | 179M/452M [01:30<02:54, 1.64MB/s] 40%|███▉      | 179M/452M [01:30<02:37, 1.82MB/s] 40%|███▉      | 179M/452M [01:30<02:27, 1.95MB/s] 40%|███▉      | 180M/452M [01:30<02:24, 1.98MB/s] 40%|███▉      | 180M/452M [01:31<02:33, 1.86MB/s] 40%|███▉      | 180M/452M [01:31<02:30, 1.90MB/s] 40%|███▉      | 180M/452M [01:31<02:21, 2.02MB/s] 40%|███▉      | 181M/452M [01:31<02:02, 2.33MB/s] 40%|███▉      | 181M/452M [01:31<02:00, 2.37MB/s] 40%|████      | 181M/452M [01:31<01:42, 2.78MB/s] 40%|████      | 182M/452M [01:31<01:43, 2.74MB/s] 40%|████      | 182M/452M [01:31<01:56, 2.43MB/s] 40%|████      | 182M/452M [01:32<02:05, 2.26MB/s] 40%|████      | 182M/452M [01:32<02:07, 2.22MB/s] 40%|████      | 183M/452M [01:32<02:05, 2.26MB/s] 40%|████      | 183M/452M [01:32<01:49, 2.58MB/s] 41%|████      | 183M/452M [01:32<01:34, 2.98MB/s] 41%|████      | 184M/452M [01:32<01:32, 3.04MB/s] 41%|████      | 184M/452M [01:32<02:09, 2.17MB/s] 41%|████      | 184M/452M [01:33<02:38, 1.77MB/s] 41%|████      | 185M/452M [01:33<03:03, 1.53MB/s] 41%|████      | 185M/452M [01:33<03:28, 1.34MB/s] 41%|████      | 185M/452M [01:33<03:42, 1.26MB/s] 41%|████      | 185M/452M [01:33<03:36, 1.29MB/s] 41%|████      | 185M/452M [01:34<03:41, 1.26MB/s] 41%|████      | 185M/452M [01:34<03:23, 1.38MB/s] 41%|████      | 186M/452M [01:34<03:22, 1.38MB/s] 41%|████      | 186M/452M [01:34<02:35, 1.80MB/s] 41%|████      | 186M/452M [01:34<02:46, 1.67MB/s] 41%|████      | 186M/452M [01:34<03:11, 1.46MB/s] 41%|████      | 186M/452M [01:34<03:40, 1.26MB/s] 41%|████▏     | 187M/452M [01:35<03:52, 1.20MB/s] 41%|████▏     | 187M/452M [01:35<03:35, 1.29MB/s] 41%|████▏     | 187M/452M [01:35<03:19, 1.39MB/s] 41%|████▏     | 187M/452M [01:35<02:39, 1.74MB/s] 41%|████▏     | 187M/452M [01:35<02:32, 1.82MB/s] 41%|████▏     | 188M/452M [01:35<02:36, 1.78MB/s] 42%|████▏     | 188M/452M [01:35<02:46, 1.66MB/s] 42%|████▏     | 188M/452M [01:35<02:58, 1.56MB/s] 42%|████▏     | 188M/452M [01:35<02:35, 1.78MB/s] 42%|████▏     | 189M/452M [01:36<02:10, 2.12MB/s] 42%|████▏     | 189M/452M [01:36<02:00, 2.29MB/s] 42%|████▏     | 189M/452M [01:36<01:52, 2.45MB/s] 42%|████▏     | 189M/452M [01:36<01:54, 2.42MB/s] 42%|████▏     | 190M/452M [01:36<01:47, 2.57MB/s] 42%|████▏     | 190M/452M [01:36<01:44, 2.64MB/s] 42%|████▏     | 190M/452M [01:36<01:49, 2.52MB/s] 42%|████▏     | 191M/452M [01:36<01:48, 2.54MB/s] 42%|████▏     | 191M/452M [01:36<01:37, 2.80MB/s] 42%|████▏     | 191M/452M [01:37<01:31, 2.99MB/s] 42%|████▏     | 192M/452M [01:37<01:34, 2.91MB/s] 42%|████▏     | 192M/452M [01:37<01:40, 2.73MB/s] 42%|████▏     | 192M/452M [01:37<01:46, 2.57MB/s] 43%|████▎     | 192M/452M [01:37<01:47, 2.54MB/s] 43%|████▎     | 193M/452M [01:37<01:50, 2.46MB/s] 43%|████▎     | 193M/452M [01:37<02:02, 2.21MB/s] 43%|████▎     | 193M/452M [01:38<02:10, 2.08MB/s] 43%|████▎     | 193M/452M [01:38<02:05, 2.17MB/s] 43%|████▎     | 194M/452M [01:38<02:22, 1.91MB/s] 43%|████▎     | 194M/452M [01:38<02:25, 1.87MB/s] 43%|████▎     | 194M/452M [01:38<02:18, 1.96MB/s] 43%|████▎     | 194M/452M [01:38<02:03, 2.20MB/s] 43%|████▎     | 195M/452M [01:38<01:37, 2.78MB/s] 43%|████▎     | 195M/452M [01:38<01:21, 3.32MB/s] 43%|████▎     | 196M/452M [01:38<01:14, 3.61MB/s] 43%|████▎     | 196M/452M [01:39<01:18, 3.42MB/s] 43%|████▎     | 196M/452M [01:39<01:29, 3.01MB/s] 43%|████▎     | 197M/452M [01:39<01:49, 2.44MB/s] 44%|████▎     | 197M/452M [01:39<02:00, 2.22MB/s] 44%|████▎     | 197M/452M [01:39<02:00, 2.22MB/s] 44%|████▎     | 197M/452M [01:39<02:12, 2.01MB/s] 44%|████▎     | 198M/452M [01:39<02:06, 2.12MB/s] 44%|████▍     | 198M/452M [01:40<01:51, 2.38MB/s] 44%|████▍     | 198M/452M [01:40<01:43, 2.58MB/s] 44%|████▍     | 199M/452M [01:40<01:43, 2.57MB/s] 44%|████▍     | 199M/452M [01:40<01:49, 2.43MB/s] 44%|████▍     | 199M/452M [01:40<01:50, 2.40MB/s] 44%|████▍     | 199M/452M [01:40<01:56, 2.27MB/s] 44%|████▍     | 200M/452M [01:40<01:59, 2.22MB/s] 44%|████▍     | 200M/452M [01:40<01:51, 2.37MB/s] 44%|████▍     | 200M/452M [01:40<01:50, 2.39MB/s] 44%|████▍     | 200M/452M [01:41<01:50, 2.40MB/s] 44%|████▍     | 201M/452M [01:41<01:54, 2.30MB/s] 44%|████▍     | 201M/452M [01:41<01:50, 2.38MB/s] 44%|████▍     | 201M/452M [01:41<01:43, 2.54MB/s] 45%|████▍     | 201M/452M [01:41<01:38, 2.66MB/s] 45%|████▍     | 202M/452M [01:41<01:39, 2.64MB/s] 45%|████▍     | 202M/452M [01:41<01:41, 2.58MB/s] 45%|████▍     | 202M/452M [01:41<01:35, 2.75MB/s] 45%|████▍     | 203M/452M [01:41<01:27, 2.98MB/s] 45%|████▍     | 203M/452M [01:42<01:26, 3.03MB/s] 45%|████▍     | 203M/452M [01:42<01:29, 2.91MB/s] 45%|████▌     | 204M/452M [01:42<01:32, 2.81MB/s] 45%|████▌     | 204M/452M [01:42<01:44, 2.50MB/s] 45%|████▌     | 204M/452M [01:42<01:35, 2.72MB/s] 45%|████▌     | 205M/452M [01:42<01:37, 2.66MB/s] 45%|████▌     | 205M/452M [01:42<01:37, 2.67MB/s] 45%|████▌     | 205M/452M [01:42<01:56, 2.23MB/s] 45%|████▌     | 205M/452M [01:43<02:02, 2.12MB/s] 45%|████▌     | 205M/452M [01:43<02:17, 1.89MB/s] 45%|████▌     | 206M/452M [01:43<02:41, 1.60MB/s] 45%|████▌     | 206M/452M [01:43<03:13, 1.33MB/s] 46%|████▌     | 206M/452M [01:43<03:37, 1.19MB/s] 46%|████▌     | 206M/452M [01:43<03:08, 1.37MB/s] 46%|████▌     | 206M/452M [01:44<03:15, 1.32MB/s] 46%|████▌     | 207M/452M [01:44<04:08, 1.04MB/s] 46%|████▌     | 207M/452M [01:44<03:10, 1.35MB/s] 46%|████▌     | 207M/452M [01:44<02:35, 1.65MB/s] 46%|████▌     | 207M/452M [01:44<02:11, 1.96MB/s] 46%|████▌     | 208M/452M [01:44<01:53, 2.27MB/s] 46%|████▌     | 208M/452M [01:44<01:40, 2.54MB/s] 46%|████▌     | 209M/452M [01:44<01:08, 3.71MB/s] 46%|████▋     | 209M/452M [01:45<00:56, 4.55MB/s] 47%|████▋     | 210M/452M [01:45<00:40, 6.28MB/s] 47%|████▋     | 211M/452M [01:45<00:39, 6.47MB/s] 47%|████▋     | 212M/452M [01:45<00:37, 6.75MB/s] 47%|████▋     | 213M/452M [01:45<00:39, 6.34MB/s] 47%|████▋     | 213M/452M [01:45<00:38, 6.51MB/s] 47%|████▋     | 214M/452M [01:45<00:43, 5.79MB/s] 47%|████▋     | 215M/452M [01:45<00:44, 5.61MB/s] 48%|████▊     | 215M/452M [01:45<00:45, 5.44MB/s] 48%|████▊     | 216M/452M [01:46<00:43, 5.64MB/s] 48%|████▊     | 216M/452M [01:46<00:41, 5.91MB/s] 48%|████▊     | 217M/452M [01:46<00:41, 5.89MB/s] 48%|████▊     | 218M/452M [01:46<00:41, 5.87MB/s] 48%|████▊     | 218M/452M [01:46<00:41, 5.98MB/s] 48%|████▊     | 219M/452M [01:46<00:40, 6.12MB/s] 48%|████▊     | 219M/452M [01:46<00:43, 5.68MB/s] 49%|████▊     | 220M/452M [01:46<00:45, 5.39MB/s] 49%|████▉     | 221M/452M [01:46<00:40, 5.95MB/s] 49%|████▉     | 221M/452M [01:47<00:38, 6.27MB/s] 49%|████▉     | 222M/452M [01:47<00:37, 6.42MB/s] 49%|████▉     | 223M/452M [01:47<00:37, 6.38MB/s] 49%|████▉     | 223M/452M [01:47<00:41, 5.81MB/s] 49%|████▉     | 224M/452M [01:47<00:49, 4.89MB/s] 50%|████▉     | 224M/452M [01:47<01:00, 3.95MB/s] 50%|████▉     | 225M/452M [01:47<01:05, 3.64MB/s] 50%|████▉     | 225M/452M [01:48<01:01, 3.86MB/s] 50%|████▉     | 226M/452M [01:48<00:53, 4.42MB/s] 50%|█████     | 226M/452M [01:48<00:52, 4.52MB/s] 50%|█████     | 227M/452M [01:48<00:49, 4.80MB/s] 50%|█████     | 227M/452M [01:48<00:44, 5.28MB/s] 50%|█████     | 228M/452M [01:48<00:44, 5.28MB/s] 51%|█████     | 229M/452M [01:48<00:43, 5.45MB/s] 51%|█████     | 229M/452M [01:48<00:41, 5.61MB/s] 51%|█████     | 230M/452M [01:48<00:40, 5.77MB/s] 51%|█████     | 230M/452M [01:48<00:40, 5.76MB/s] 51%|█████     | 231M/452M [01:49<00:41, 5.61MB/s] 51%|█████     | 231M/452M [01:49<00:42, 5.43MB/s] 51%|█████▏    | 232M/452M [01:49<00:50, 4.53MB/s] 51%|█████▏    | 232M/452M [01:49<01:02, 3.70MB/s] 51%|█████▏    | 233M/452M [01:49<01:10, 3.28MB/s] 52%|█████▏    | 233M/452M [01:49<01:16, 2.99MB/s] 52%|█████▏    | 234M/452M [01:50<01:29, 2.57MB/s] 52%|█████▏    | 234M/452M [01:50<01:27, 2.61MB/s] 52%|█████▏    | 234M/452M [01:50<01:38, 2.31MB/s] 52%|█████▏    | 234M/452M [01:50<01:35, 2.40MB/s] 52%|█████▏    | 235M/452M [01:50<01:24, 2.69MB/s] 52%|█████▏    | 235M/452M [01:50<01:42, 2.21MB/s] 52%|█████▏    | 235M/452M [01:50<01:42, 2.22MB/s] 52%|█████▏    | 236M/452M [01:51<01:36, 2.35MB/s] 52%|█████▏    | 236M/452M [01:51<01:47, 2.12MB/s] 52%|█████▏    | 236M/452M [01:51<01:32, 2.46MB/s] 52%|█████▏    | 236M/452M [01:51<01:32, 2.46MB/s] 52%|█████▏    | 237M/452M [01:51<01:34, 2.39MB/s] 52%|█████▏    | 237M/452M [01:51<01:22, 2.73MB/s] 53%|█████▎    | 238M/452M [01:51<01:03, 3.55MB/s] 53%|█████▎    | 238M/452M [01:51<00:48, 4.67MB/s] 53%|█████▎    | 239M/452M [01:51<00:46, 4.81MB/s] 53%|█████▎    | 239M/452M [01:52<00:46, 4.81MB/s] 53%|█████▎    | 240M/452M [01:52<00:47, 4.72MB/s] 53%|█████▎    | 240M/452M [01:52<00:57, 3.89MB/s] 53%|█████▎    | 241M/452M [01:52<01:05, 3.38MB/s] 53%|█████▎    | 241M/452M [01:52<01:10, 3.14MB/s] 53%|█████▎    | 241M/452M [01:52<01:16, 2.89MB/s] 53%|█████▎    | 242M/452M [01:52<01:11, 3.08MB/s] 54%|█████▎    | 242M/452M [01:53<01:03, 3.49MB/s] 54%|█████▎    | 243M/452M [01:53<00:53, 4.09MB/s] 54%|█████▍    | 244M/452M [01:53<00:47, 4.59MB/s] 54%|█████▍    | 244M/452M [01:53<00:48, 4.48MB/s] 54%|█████▍    | 245M/452M [01:53<00:54, 3.98MB/s] 54%|█████▍    | 245M/452M [01:53<01:00, 3.58MB/s] 54%|█████▍    | 245M/452M [01:53<01:07, 3.23MB/s] 54%|█████▍    | 246M/452M [01:53<01:07, 3.22MB/s] 54%|█████▍    | 246M/452M [01:54<01:06, 3.23MB/s] 54%|█████▍    | 246M/452M [01:54<01:07, 3.20MB/s] 55%|█████▍    | 247M/452M [01:54<01:09, 3.12MB/s] 55%|█████▍    | 247M/452M [01:54<01:02, 3.47MB/s] 55%|█████▍    | 247M/452M [01:54<00:57, 3.74MB/s] 55%|█████▍    | 248M/452M [01:54<00:58, 3.66MB/s] 55%|█████▍    | 248M/452M [01:54<00:58, 3.64MB/s] 55%|█████▍    | 249M/452M [01:54<00:58, 3.66MB/s] 55%|█████▌    | 249M/452M [01:54<01:02, 3.40MB/s] 55%|█████▌    | 249M/452M [01:55<01:03, 3.36MB/s] 55%|█████▌    | 250M/452M [01:55<01:15, 2.81MB/s] 55%|█████▌    | 250M/452M [01:55<01:17, 2.75MB/s] 55%|█████▌    | 250M/452M [01:55<01:18, 2.70MB/s] 55%|█████▌    | 251M/452M [01:55<01:10, 3.01MB/s] 56%|█████▌    | 251M/452M [01:55<00:53, 3.94MB/s] 56%|█████▌    | 252M/452M [01:55<00:46, 4.52MB/s] 56%|█████▌    | 252M/452M [01:55<00:44, 4.66MB/s] 56%|█████▌    | 253M/452M [01:56<00:48, 4.31MB/s] 56%|█████▌    | 253M/452M [01:56<00:52, 3.98MB/s] 56%|█████▌    | 254M/452M [01:56<00:54, 3.85MB/s] 56%|█████▌    | 254M/452M [01:56<00:55, 3.73MB/s] 56%|█████▋    | 255M/452M [01:56<01:01, 3.37MB/s] 56%|█████▋    | 255M/452M [01:56<01:04, 3.20MB/s] 56%|█████▋    | 255M/452M [01:56<01:03, 3.26MB/s] 57%|█████▋    | 256M/452M [01:56<01:04, 3.20MB/s] 57%|█████▋    | 256M/452M [01:57<01:04, 3.17MB/s] 57%|█████▋    | 256M/452M [01:57<01:13, 2.80MB/s] 57%|█████▋    | 257M/452M [01:57<01:16, 2.69MB/s] 57%|█████▋    | 257M/452M [01:57<01:11, 2.86MB/s] 57%|█████▋    | 257M/452M [01:57<01:23, 2.45MB/s] 57%|█████▋    | 257M/452M [01:57<01:23, 2.46MB/s] 57%|█████▋    | 258M/452M [01:57<01:18, 2.61MB/s] 57%|█████▋    | 258M/452M [01:57<01:17, 2.64MB/s] 57%|█████▋    | 258M/452M [01:58<01:24, 2.41MB/s] 57%|█████▋    | 259M/452M [01:58<01:23, 2.42MB/s] 57%|█████▋    | 259M/452M [01:58<01:13, 2.78MB/s] 57%|█████▋    | 260M/452M [01:58<00:54, 3.69MB/s] 57%|█████▋    | 260M/452M [01:58<00:51, 3.92MB/s] 58%|█████▊    | 260M/452M [01:58<00:49, 4.10MB/s] 58%|█████▊    | 261M/452M [01:58<00:48, 4.10MB/s] 58%|█████▊    | 261M/452M [01:58<00:57, 3.46MB/s] 58%|█████▊    | 262M/452M [01:58<00:55, 3.63MB/s] 58%|█████▊    | 262M/452M [01:59<01:02, 3.22MB/s] 58%|█████▊    | 262M/452M [01:59<01:02, 3.21MB/s] 58%|█████▊    | 263M/452M [01:59<01:15, 2.65MB/s] 58%|█████▊    | 263M/452M [01:59<01:16, 2.59MB/s] 58%|█████▊    | 263M/452M [01:59<01:17, 2.57MB/s] 58%|█████▊    | 264M/452M [01:59<01:20, 2.46MB/s] 58%|█████▊    | 264M/452M [01:59<01:18, 2.53MB/s] 58%|█████▊    | 264M/452M [02:00<01:04, 3.04MB/s] 58%|█████▊    | 265M/452M [02:00<01:02, 3.16MB/s] 59%|█████▊    | 265M/452M [02:00<00:53, 3.64MB/s] 59%|█████▊    | 266M/452M [02:00<00:50, 3.90MB/s] 59%|█████▉    | 266M/452M [02:00<00:46, 4.24MB/s] 59%|█████▉    | 267M/452M [02:00<00:46, 4.20MB/s] 59%|█████▉    | 267M/452M [02:00<00:45, 4.28MB/s] 59%|█████▉    | 267M/452M [02:00<00:44, 4.35MB/s] 59%|█████▉    | 268M/452M [02:00<00:46, 4.18MB/s] 59%|█████▉    | 268M/452M [02:01<00:52, 3.66MB/s] 59%|█████▉    | 269M/452M [02:01<00:53, 3.63MB/s] 59%|█████▉    | 269M/452M [02:01<00:51, 3.74MB/s] 60%|█████▉    | 270M/452M [02:01<00:47, 4.06MB/s] 60%|█████▉    | 270M/452M [02:01<00:43, 4.43MB/s] 60%|█████▉    | 271M/452M [02:01<00:46, 4.12MB/s] 60%|█████▉    | 271M/452M [02:01<00:49, 3.84MB/s] 60%|█████▉    | 271M/452M [02:01<00:54, 3.46MB/s] 60%|██████    | 272M/452M [02:02<00:53, 3.52MB/s] 60%|██████    | 272M/452M [02:02<00:55, 3.43MB/s] 60%|██████    | 272M/452M [02:02<00:59, 3.16MB/s] 60%|██████    | 273M/452M [02:02<01:06, 2.82MB/s] 60%|██████    | 273M/452M [02:02<01:07, 2.79MB/s] 60%|██████    | 273M/452M [02:02<01:09, 2.68MB/s] 60%|██████    | 274M/452M [02:02<01:15, 2.50MB/s] 61%|██████    | 274M/452M [02:02<01:17, 2.42MB/s] 61%|██████    | 274M/452M [02:03<01:10, 2.66MB/s] 61%|██████    | 274M/452M [02:03<01:09, 2.67MB/s] 61%|██████    | 275M/452M [02:03<01:21, 2.28MB/s] 61%|██████    | 275M/452M [02:03<01:22, 2.25MB/s] 61%|██████    | 275M/452M [02:03<01:25, 2.17MB/s] 61%|██████    | 275M/452M [02:03<01:24, 2.21MB/s] 61%|██████    | 276M/452M [02:03<01:29, 2.08MB/s] 61%|██████    | 276M/452M [02:03<01:29, 2.07MB/s] 61%|██████    | 276M/452M [02:03<01:26, 2.13MB/s] 61%|██████    | 276M/452M [02:04<01:28, 2.08MB/s] 61%|██████    | 277M/452M [02:04<01:05, 2.82MB/s] 61%|██████▏   | 277M/452M [02:04<01:06, 2.74MB/s] 61%|██████▏   | 277M/452M [02:04<01:06, 2.76MB/s] 61%|██████▏   | 278M/452M [02:04<00:53, 3.41MB/s] 62%|██████▏   | 279M/452M [02:04<00:38, 4.69MB/s] 62%|██████▏   | 280M/452M [02:04<00:31, 5.75MB/s] 62%|██████▏   | 280M/452M [02:04<00:33, 5.36MB/s] 62%|██████▏   | 281M/452M [02:05<00:37, 4.79MB/s] 62%|██████▏   | 281M/452M [02:05<00:38, 4.71MB/s] 62%|██████▏   | 282M/452M [02:05<00:46, 3.83MB/s] 62%|██████▏   | 282M/452M [02:05<00:51, 3.49MB/s] 62%|██████▏   | 282M/452M [02:05<00:48, 3.69MB/s] 63%|██████▎   | 283M/452M [02:05<00:47, 3.72MB/s] 63%|██████▎   | 283M/452M [02:05<00:45, 3.92MB/s] 63%|██████▎   | 284M/452M [02:05<00:40, 4.32MB/s] 63%|██████▎   | 285M/452M [02:06<00:41, 4.20MB/s] 63%|██████▎   | 285M/452M [02:06<00:42, 4.11MB/s] 63%|██████▎   | 286M/452M [02:06<00:40, 4.30MB/s] 63%|██████▎   | 286M/452M [02:06<00:36, 4.83MB/s] 63%|██████▎   | 287M/452M [02:06<00:33, 5.12MB/s] 64%|██████▎   | 288M/452M [02:06<00:28, 5.96MB/s] 64%|██████▍   | 289M/452M [02:06<00:27, 6.34MB/s] 64%|██████▍   | 290M/452M [02:06<00:23, 7.14MB/s] 64%|██████▍   | 290M/452M [02:07<00:23, 7.26MB/s] 64%|██████▍   | 291M/452M [02:07<00:23, 7.26MB/s] 65%|██████▍   | 292M/452M [02:07<00:27, 6.15MB/s] 65%|██████▍   | 292M/452M [02:07<00:27, 6.05MB/s] 65%|██████▍   | 293M/452M [02:07<00:35, 4.72MB/s] 65%|██████▍   | 293M/452M [02:07<00:34, 4.78MB/s] 65%|██████▍   | 294M/452M [02:07<00:35, 4.72MB/s] 65%|██████▌   | 294M/452M [02:08<00:37, 4.37MB/s] 65%|██████▌   | 295M/452M [02:08<00:37, 4.35MB/s] 65%|██████▌   | 295M/452M [02:08<00:41, 3.94MB/s] 65%|██████▌   | 296M/452M [02:08<00:39, 4.12MB/s] 66%|██████▌   | 296M/452M [02:08<00:37, 4.38MB/s] 66%|██████▌   | 297M/452M [02:08<00:32, 5.04MB/s] 66%|██████▌   | 298M/452M [02:08<00:31, 5.09MB/s] 66%|██████▌   | 298M/452M [02:08<00:29, 5.44MB/s] 66%|██████▌   | 299M/452M [02:08<00:29, 5.47MB/s] 66%|██████▌   | 299M/452M [02:09<00:30, 5.32MB/s] 66%|██████▋   | 300M/452M [02:09<00:31, 5.13MB/s] 66%|██████▋   | 300M/452M [02:09<00:31, 5.06MB/s] 67%|██████▋   | 301M/452M [02:09<00:31, 5.09MB/s] 67%|██████▋   | 301M/452M [02:09<00:29, 5.31MB/s] 67%|██████▋   | 302M/452M [02:09<00:27, 5.82MB/s] 67%|██████▋   | 303M/452M [02:09<00:26, 5.97MB/s] 67%|██████▋   | 303M/452M [02:09<00:25, 6.04MB/s] 67%|██████▋   | 304M/452M [02:09<00:24, 6.34MB/s] 67%|██████▋   | 305M/452M [02:09<00:24, 6.44MB/s] 68%|██████▊   | 305M/452M [02:10<00:23, 6.46MB/s] 68%|██████▊   | 306M/452M [02:10<00:22, 6.74MB/s] 68%|██████▊   | 307M/452M [02:10<00:22, 6.74MB/s] 68%|██████▊   | 307M/452M [02:10<00:22, 6.77MB/s] 68%|██████▊   | 308M/452M [02:10<00:21, 6.94MB/s] 68%|██████▊   | 309M/452M [02:10<00:21, 6.93MB/s] 68%|██████▊   | 309M/452M [02:10<00:23, 6.27MB/s] 69%|██████▊   | 310M/452M [02:10<00:25, 5.79MB/s] 69%|██████▊   | 311M/452M [02:10<00:24, 6.09MB/s] 69%|██████▉   | 312M/452M [02:11<00:22, 6.57MB/s] 69%|██████▉   | 312M/452M [02:11<00:21, 6.79MB/s] 69%|██████▉   | 313M/452M [02:11<00:22, 6.37MB/s] 69%|██████▉   | 314M/452M [02:11<00:22, 6.51MB/s] 69%|██████▉   | 314M/452M [02:11<00:26, 5.55MB/s] 70%|██████▉   | 315M/452M [02:11<00:30, 4.80MB/s] 70%|██████▉   | 315M/452M [02:11<00:33, 4.23MB/s] 70%|██████▉   | 316M/452M [02:12<00:37, 3.84MB/s] 70%|██████▉   | 316M/452M [02:12<00:40, 3.50MB/s] 70%|██████▉   | 317M/452M [02:12<00:47, 3.02MB/s] 70%|███████   | 317M/452M [02:12<00:50, 2.79MB/s] 70%|███████   | 317M/452M [02:12<00:49, 2.88MB/s] 70%|███████   | 318M/452M [02:12<00:44, 3.19MB/s] 70%|███████   | 318M/452M [02:12<00:49, 2.87MB/s] 70%|███████   | 318M/452M [02:13<00:48, 2.88MB/s] 70%|███████   | 319M/452M [02:13<00:48, 2.90MB/s] 71%|███████   | 319M/452M [02:13<00:50, 2.75MB/s] 71%|███████   | 319M/452M [02:13<00:48, 2.90MB/s] 71%|███████   | 320M/452M [02:13<00:44, 3.15MB/s] 71%|███████   | 320M/452M [02:13<00:40, 3.40MB/s] 71%|███████   | 321M/452M [02:13<00:38, 3.60MB/s] 71%|███████   | 321M/452M [02:13<00:37, 3.69MB/s] 71%|███████   | 322M/452M [02:14<00:42, 3.22MB/s] 71%|███████   | 322M/452M [02:14<00:37, 3.63MB/s] 71%|███████▏  | 322M/452M [02:14<00:43, 3.10MB/s] 71%|███████▏  | 323M/452M [02:14<00:48, 2.83MB/s] 71%|███████▏  | 323M/452M [02:14<00:53, 2.54MB/s] 71%|███████▏  | 323M/452M [02:14<01:00, 2.25MB/s] 72%|███████▏  | 324M/452M [02:14<01:03, 2.12MB/s] 72%|███████▏  | 324M/452M [02:15<01:08, 1.97MB/s] 72%|███████▏  | 324M/452M [02:15<01:08, 1.96MB/s] 72%|███████▏  | 324M/452M [02:15<01:05, 2.04MB/s] 72%|███████▏  | 324M/452M [02:15<01:03, 2.11MB/s] 72%|███████▏  | 325M/452M [02:15<01:01, 2.18MB/s] 72%|███████▏  | 325M/452M [02:15<01:02, 2.15MB/s] 72%|███████▏  | 325M/452M [02:15<00:58, 2.28MB/s] 72%|███████▏  | 325M/452M [02:15<01:02, 2.12MB/s] 72%|███████▏  | 326M/452M [02:16<01:06, 2.01MB/s] 72%|███████▏  | 326M/452M [02:16<01:03, 2.08MB/s] 72%|███████▏  | 326M/452M [02:16<01:11, 1.84MB/s] 72%|███████▏  | 327M/452M [02:16<00:56, 2.32MB/s] 72%|███████▏  | 327M/452M [02:16<00:50, 2.63MB/s] 72%|███████▏  | 327M/452M [02:16<00:46, 2.83MB/s] 72%|███████▏  | 327M/452M [02:16<00:49, 2.64MB/s] 72%|███████▏  | 328M/452M [02:16<00:49, 2.65MB/s] 73%|███████▎  | 328M/452M [02:17<00:53, 2.44MB/s] 73%|███████▎  | 328M/452M [02:17<00:57, 2.25MB/s] 73%|███████▎  | 328M/452M [02:17<01:10, 1.85MB/s] 73%|███████▎  | 329M/452M [02:17<01:28, 1.47MB/s] 73%|███████▎  | 329M/452M [02:17<01:24, 1.52MB/s] 73%|███████▎  | 329M/452M [02:17<01:39, 1.31MB/s] 73%|███████▎  | 329M/452M [02:17<01:27, 1.47MB/s] 73%|███████▎  | 329M/452M [02:18<01:34, 1.37MB/s] 73%|███████▎  | 330M/452M [02:18<01:23, 1.55MB/s] 73%|███████▎  | 330M/452M [02:18<01:13, 1.74MB/s] 73%|███████▎  | 330M/452M [02:18<01:10, 1.81MB/s] 73%|███████▎  | 330M/452M [02:18<01:05, 1.95MB/s] 73%|███████▎  | 330M/452M [02:18<01:11, 1.79MB/s] 73%|███████▎  | 331M/452M [02:18<01:11, 1.78MB/s] 73%|███████▎  | 331M/452M [02:18<01:05, 1.95MB/s] 73%|███████▎  | 331M/452M [02:19<01:00, 2.10MB/s] 73%|███████▎  | 331M/452M [02:19<01:01, 2.07MB/s] 73%|███████▎  | 332M/452M [02:19<01:03, 1.98MB/s] 73%|███████▎  | 332M/452M [02:19<01:02, 2.03MB/s] 73%|███████▎  | 332M/452M [02:19<01:00, 2.08MB/s] 73%|███████▎  | 332M/452M [02:19<00:57, 2.19MB/s] 74%|███████▎  | 333M/452M [02:19<00:57, 2.17MB/s] 74%|███████▎  | 333M/452M [02:19<00:56, 2.21MB/s] 74%|███████▎  | 333M/452M [02:19<00:57, 2.19MB/s] 74%|███████▎  | 333M/452M [02:20<00:57, 2.18MB/s] 74%|███████▍  | 334M/452M [02:20<00:55, 2.23MB/s] 74%|███████▍  | 334M/452M [02:20<00:55, 2.23MB/s] 74%|███████▍  | 334M/452M [02:20<01:02, 1.99MB/s] 74%|███████▍  | 334M/452M [02:20<01:04, 1.93MB/s] 74%|███████▍  | 335M/452M [02:20<01:09, 1.79MB/s] 74%|███████▍  | 335M/452M [02:20<01:19, 1.55MB/s] 74%|███████▍  | 335M/452M [02:21<01:18, 1.56MB/s] 74%|███████▍  | 335M/452M [02:21<01:12, 1.69MB/s] 74%|███████▍  | 335M/452M [02:21<01:05, 1.87MB/s] 74%|███████▍  | 336M/452M [02:21<01:05, 1.88MB/s] 74%|███████▍  | 336M/452M [02:21<00:55, 2.22MB/s] 74%|███████▍  | 336M/452M [02:21<00:46, 2.61MB/s] 74%|███████▍  | 337M/452M [02:21<00:48, 2.49MB/s] 74%|███████▍  | 337M/452M [02:21<00:48, 2.48MB/s] 75%|███████▍  | 337M/452M [02:21<00:49, 2.44MB/s] 75%|███████▍  | 337M/452M [02:22<00:50, 2.39MB/s] 75%|███████▍  | 338M/452M [02:22<00:50, 2.38MB/s] 75%|███████▍  | 338M/452M [02:22<00:57, 2.10MB/s] 75%|███████▍  | 338M/452M [02:22<01:02, 1.90MB/s] 75%|███████▍  | 338M/452M [02:22<01:04, 1.87MB/s] 75%|███████▍  | 338M/452M [02:22<01:06, 1.81MB/s] 75%|███████▍  | 339M/452M [02:22<01:06, 1.78MB/s] 75%|███████▍  | 339M/452M [02:22<01:02, 1.91MB/s] 75%|███████▍  | 339M/452M [02:23<01:04, 1.84MB/s] 75%|███████▍  | 339M/452M [02:23<01:11, 1.67MB/s] 75%|███████▌  | 339M/452M [02:23<01:12, 1.64MB/s] 75%|███████▌  | 339M/452M [02:23<01:13, 1.62MB/s] 75%|███████▌  | 340M/452M [02:23<01:13, 1.62MB/s] 75%|███████▌  | 340M/452M [02:23<01:19, 1.49MB/s] 75%|███████▌  | 340M/452M [02:23<01:06, 1.77MB/s] 75%|███████▌  | 340M/452M [02:23<01:14, 1.57MB/s] 75%|███████▌  | 340M/452M [02:24<01:15, 1.57MB/s] 75%|███████▌  | 341M/452M [02:24<01:16, 1.53MB/s] 75%|███████▌  | 341M/452M [02:24<01:20, 1.46MB/s] 75%|███████▌  | 341M/452M [02:24<01:25, 1.37MB/s] 75%|███████▌  | 341M/452M [02:24<01:30, 1.29MB/s] 75%|███████▌  | 341M/452M [02:24<01:32, 1.26MB/s] 75%|███████▌  | 341M/452M [02:24<01:29, 1.30MB/s] 75%|███████▌  | 341M/452M [02:24<01:27, 1.33MB/s] 75%|███████▌  | 341M/452M [02:24<01:28, 1.32MB/s] 76%|███████▌  | 342M/452M [02:25<01:31, 1.26MB/s] 76%|███████▌  | 342M/452M [02:25<01:35, 1.21MB/s] 76%|███████▌  | 342M/452M [02:25<01:35, 1.21MB/s] 76%|███████▌  | 342M/452M [02:25<01:34, 1.23MB/s] 76%|███████▌  | 342M/452M [02:25<01:32, 1.25MB/s] 76%|███████▌  | 342M/452M [02:25<01:33, 1.23MB/s] 76%|███████▌  | 342M/452M [02:25<01:30, 1.27MB/s] 76%|███████▌  | 343M/452M [02:25<01:22, 1.39MB/s] 76%|███████▌  | 343M/452M [02:26<01:11, 1.60MB/s] 76%|███████▌  | 343M/452M [02:26<01:12, 1.58MB/s] 76%|███████▌  | 343M/452M [02:26<01:12, 1.59MB/s] 76%|███████▌  | 343M/452M [02:26<01:00, 1.90MB/s] 76%|███████▌  | 344M/452M [02:26<00:56, 2.01MB/s] 76%|███████▌  | 344M/452M [02:26<00:53, 2.12MB/s] 76%|███████▌  | 344M/452M [02:26<00:49, 2.31MB/s] 76%|███████▌  | 345M/452M [02:26<00:46, 2.44MB/s] 76%|███████▋  | 345M/452M [02:26<00:38, 2.90MB/s] 76%|███████▋  | 346M/452M [02:27<00:32, 3.40MB/s] 76%|███████▋  | 346M/452M [02:27<00:31, 3.53MB/s] 77%|███████▋  | 346M/452M [02:27<00:36, 3.06MB/s] 77%|███████▋  | 347M/452M [02:27<00:37, 2.99MB/s] 77%|███████▋  | 347M/452M [02:27<00:44, 2.46MB/s] 77%|███████▋  | 347M/452M [02:27<00:48, 2.26MB/s] 77%|███████▋  | 347M/452M [02:27<00:50, 2.18MB/s] 77%|███████▋  | 348M/452M [02:27<00:49, 2.24MB/s] 77%|███████▋  | 348M/452M [02:28<00:43, 2.49MB/s] 77%|███████▋  | 348M/452M [02:28<00:42, 2.58MB/s] 77%|███████▋  | 349M/452M [02:28<00:40, 2.70MB/s] 77%|███████▋  | 349M/452M [02:28<00:38, 2.80MB/s] 77%|███████▋  | 349M/452M [02:28<00:36, 2.94MB/s] 77%|███████▋  | 350M/452M [02:28<00:37, 2.91MB/s] 77%|███████▋  | 350M/452M [02:28<00:34, 3.08MB/s] 77%|███████▋  | 350M/452M [02:28<00:35, 2.99MB/s] 77%|███████▋  | 351M/452M [02:28<00:34, 3.11MB/s] 78%|███████▊  | 351M/452M [02:29<00:34, 3.10MB/s] 78%|███████▊  | 351M/452M [02:29<00:33, 3.15MB/s] 78%|███████▊  | 352M/452M [02:29<00:32, 3.21MB/s] 78%|███████▊  | 352M/452M [02:29<00:32, 3.26MB/s] 78%|███████▊  | 352M/452M [02:29<00:32, 3.26MB/s] 78%|███████▊  | 353M/452M [02:29<00:39, 2.68MB/s] 78%|███████▊  | 353M/452M [02:29<00:41, 2.50MB/s] 78%|███████▊  | 353M/452M [02:29<00:38, 2.69MB/s] 78%|███████▊  | 354M/452M [02:30<00:34, 3.02MB/s] 78%|███████▊  | 354M/452M [02:30<00:34, 2.95MB/s] 78%|███████▊  | 354M/452M [02:30<00:33, 3.11MB/s] 78%|███████▊  | 355M/452M [02:30<00:30, 3.33MB/s] 78%|███████▊  | 355M/452M [02:30<00:28, 3.56MB/s] 79%|███████▊  | 355M/452M [02:30<00:27, 3.73MB/s] 79%|███████▊  | 356M/452M [02:30<00:25, 3.90MB/s] 79%|███████▉  | 356M/452M [02:30<00:25, 4.01MB/s] 79%|███████▉  | 357M/452M [02:30<00:23, 4.21MB/s] 79%|███████▉  | 357M/452M [02:30<00:23, 4.32MB/s] 79%|███████▉  | 358M/452M [02:31<00:22, 4.33MB/s] 79%|███████▉  | 358M/452M [02:31<00:21, 4.49MB/s] 79%|███████▉  | 359M/452M [02:31<00:25, 3.88MB/s] 79%|███████▉  | 359M/452M [02:31<00:27, 3.58MB/s] 79%|███████▉  | 359M/452M [02:31<00:28, 3.38MB/s] 79%|███████▉  | 360M/452M [02:31<00:29, 3.33MB/s] 80%|███████▉  | 360M/452M [02:31<00:28, 3.44MB/s] 80%|███████▉  | 360M/452M [02:31<00:27, 3.51MB/s] 80%|███████▉  | 361M/452M [02:32<00:26, 3.66MB/s] 80%|███████▉  | 361M/452M [02:32<00:25, 3.72MB/s] 80%|███████▉  | 362M/452M [02:32<00:25, 3.70MB/s] 80%|███████▉  | 362M/452M [02:32<00:33, 2.81MB/s] 80%|████████  | 362M/452M [02:32<00:35, 2.69MB/s] 80%|████████  | 362M/452M [02:32<00:37, 2.55MB/s] 80%|████████  | 363M/452M [02:32<00:38, 2.44MB/s] 80%|████████  | 363M/452M [02:32<00:38, 2.41MB/s] 80%|████████  | 363M/452M [02:33<00:36, 2.55MB/s] 80%|████████  | 364M/452M [02:33<00:32, 2.87MB/s] 80%|████████  | 364M/452M [02:33<00:29, 3.17MB/s] 81%|████████  | 364M/452M [02:33<00:29, 3.13MB/s] 81%|████████  | 365M/452M [02:33<00:28, 3.25MB/s] 81%|████████  | 365M/452M [02:33<00:26, 3.44MB/s] 81%|████████  | 366M/452M [02:33<00:24, 3.70MB/s] 81%|████████  | 366M/452M [02:33<00:21, 4.26MB/s] 81%|████████  | 367M/452M [02:33<00:21, 4.14MB/s] 81%|████████  | 367M/452M [02:34<00:20, 4.39MB/s] 81%|████████▏ | 368M/452M [02:34<00:19, 4.66MB/s] 81%|████████▏ | 368M/452M [02:34<00:19, 4.65MB/s] 82%|████████▏ | 369M/452M [02:34<00:17, 4.88MB/s] 82%|████████▏ | 369M/452M [02:34<00:18, 4.71MB/s] 82%|████████▏ | 370M/452M [02:34<00:17, 4.99MB/s] 82%|████████▏ | 370M/452M [02:34<00:17, 4.91MB/s] 82%|████████▏ | 371M/452M [02:34<00:17, 4.91MB/s] 82%|████████▏ | 371M/452M [02:34<00:18, 4.69MB/s] 82%|████████▏ | 372M/452M [02:35<00:18, 4.66MB/s] 82%|████████▏ | 372M/452M [02:35<00:17, 4.80MB/s] 82%|████████▏ | 373M/452M [02:35<00:16, 5.06MB/s] 83%|████████▎ | 373M/452M [02:35<00:16, 5.16MB/s] 83%|████████▎ | 374M/452M [02:35<00:15, 5.34MB/s] 83%|████████▎ | 374M/452M [02:35<00:14, 5.56MB/s] 83%|████████▎ | 375M/452M [02:35<00:14, 5.68MB/s] 83%|████████▎ | 376M/452M [02:35<00:15, 5.35MB/s] 83%|████████▎ | 376M/452M [02:35<00:18, 4.41MB/s] 83%|████████▎ | 377M/452M [02:36<00:19, 4.16MB/s] 83%|████████▎ | 377M/452M [02:36<00:19, 4.06MB/s] 83%|████████▎ | 377M/452M [02:36<00:18, 4.19MB/s] 84%|████████▎ | 378M/452M [02:36<00:16, 4.66MB/s] 84%|████████▎ | 379M/452M [02:36<00:14, 5.25MB/s] 84%|████████▍ | 379M/452M [02:36<00:13, 5.48MB/s] 84%|████████▍ | 380M/452M [02:36<00:12, 6.01MB/s] 84%|████████▍ | 381M/452M [02:36<00:11, 6.28MB/s] 84%|████████▍ | 381M/452M [02:36<00:13, 5.71MB/s] 84%|████████▍ | 382M/452M [02:37<00:13, 5.36MB/s] 85%|████████▍ | 382M/452M [02:37<00:16, 4.46MB/s] 85%|████████▍ | 383M/452M [02:37<00:18, 3.90MB/s] 85%|████████▍ | 383M/452M [02:37<00:20, 3.59MB/s] 85%|████████▍ | 384M/452M [02:37<00:19, 3.62MB/s] 85%|████████▍ | 384M/452M [02:37<00:19, 3.66MB/s] 85%|████████▍ | 384M/452M [02:37<00:20, 3.42MB/s] 85%|████████▌ | 385M/452M [02:38<00:21, 3.35MB/s] 85%|████████▌ | 385M/452M [02:38<00:22, 3.18MB/s] 85%|████████▌ | 385M/452M [02:38<00:22, 3.07MB/s] 85%|████████▌ | 386M/452M [02:38<00:23, 3.03MB/s] 85%|████████▌ | 386M/452M [02:38<00:22, 3.08MB/s] 85%|████████▌ | 386M/452M [02:38<00:21, 3.23MB/s] 86%|████████▌ | 387M/452M [02:38<00:19, 3.54MB/s] 86%|████████▌ | 387M/452M [02:38<00:17, 3.80MB/s] 86%|████████▌ | 388M/452M [02:38<00:14, 4.65MB/s] 86%|████████▌ | 389M/452M [02:39<00:11, 5.78MB/s] 86%|████████▌ | 390M/452M [02:39<00:10, 6.19MB/s] 86%|████████▋ | 390M/452M [02:39<00:10, 6.49MB/s] 86%|████████▋ | 391M/452M [02:39<00:09, 7.01MB/s] 87%|████████▋ | 392M/452M [02:39<00:08, 7.14MB/s] 87%|████████▋ | 393M/452M [02:39<00:08, 7.60MB/s] 87%|████████▋ | 394M/452M [02:39<00:08, 7.51MB/s] 87%|████████▋ | 394M/452M [02:39<00:07, 7.85MB/s] 87%|████████▋ | 395M/452M [02:39<00:07, 7.77MB/s] 88%|████████▊ | 396M/452M [02:40<00:07, 8.08MB/s] 88%|████████▊ | 397M/452M [02:40<00:07, 8.14MB/s] 88%|████████▊ | 398M/452M [02:40<00:08, 7.14MB/s] 88%|████████▊ | 398M/452M [02:40<00:08, 6.82MB/s] 88%|████████▊ | 399M/452M [02:40<00:08, 6.78MB/s] 88%|████████▊ | 400M/452M [02:40<00:08, 6.59MB/s] 89%|████████▊ | 400M/452M [02:40<00:08, 6.61MB/s] 89%|████████▊ | 401M/452M [02:40<00:07, 6.84MB/s] 89%|████████▉ | 402M/452M [02:40<00:08, 6.63MB/s] 89%|████████▉ | 402M/452M [02:41<00:07, 6.82MB/s] 89%|████████▉ | 403M/452M [02:41<00:07, 6.75MB/s] 89%|████████▉ | 404M/452M [02:41<00:07, 6.90MB/s] 89%|████████▉ | 405M/452M [02:41<00:06, 7.21MB/s] 90%|████████▉ | 405M/452M [02:41<00:07, 6.93MB/s] 90%|████████▉ | 406M/452M [02:41<00:06, 7.31MB/s] 90%|████████▉ | 407M/452M [02:41<00:06, 7.13MB/s] 90%|█████████ | 408M/452M [02:41<00:06, 7.31MB/s] 90%|█████████ | 408M/452M [02:41<00:06, 7.41MB/s] 90%|█████████ | 409M/452M [02:41<00:06, 7.44MB/s] 91%|█████████ | 410M/452M [02:42<00:05, 7.71MB/s] 91%|█████████ | 411M/452M [02:42<00:05, 7.53MB/s] 91%|█████████ | 411M/452M [02:42<00:06, 6.16MB/s] 91%|█████████ | 412M/452M [02:42<00:07, 5.92MB/s] 91%|█████████ | 413M/452M [02:42<00:08, 4.65MB/s] 91%|█████████▏| 413M/452M [02:42<00:10, 3.96MB/s] 91%|█████████▏| 414M/452M [02:43<00:12, 3.18MB/s] 92%|█████████▏| 414M/452M [02:43<00:14, 2.73MB/s] 92%|█████████▏| 414M/452M [02:43<00:16, 2.48MB/s] 92%|█████████▏| 414M/452M [02:43<00:16, 2.42MB/s] 92%|█████████▏| 415M/452M [02:43<00:15, 2.47MB/s] 92%|█████████▏| 415M/452M [02:43<00:15, 2.56MB/s] 92%|█████████▏| 415M/452M [02:43<00:15, 2.55MB/s] 92%|█████████▏| 416M/452M [02:44<00:15, 2.48MB/s] 92%|█████████▏| 416M/452M [02:44<00:16, 2.34MB/s] 92%|█████████▏| 416M/452M [02:44<00:16, 2.27MB/s] 92%|█████████▏| 416M/452M [02:44<00:16, 2.23MB/s] 92%|█████████▏| 416M/452M [02:44<00:17, 2.20MB/s] 92%|█████████▏| 417M/452M [02:44<00:17, 2.16MB/s] 92%|█████████▏| 417M/452M [02:44<00:17, 2.14MB/s] 92%|█████████▏| 417M/452M [02:44<00:16, 2.18MB/s] 92%|█████████▏| 417M/452M [02:44<00:16, 2.26MB/s] 92%|█████████▏| 418M/452M [02:45<00:14, 2.50MB/s] 92%|█████████▏| 418M/452M [02:45<00:12, 2.93MB/s] 93%|█████████▎| 419M/452M [02:45<00:10, 3.23MB/s] 93%|█████████▎| 419M/452M [02:45<00:10, 3.20MB/s] 93%|█████████▎| 419M/452M [02:45<00:11, 3.14MB/s] 93%|█████████▎| 419M/452M [02:45<00:12, 2.74MB/s] 93%|█████████▎| 420M/452M [02:45<00:12, 2.69MB/s] 93%|█████████▎| 420M/452M [02:45<00:14, 2.42MB/s] 93%|█████████▎| 420M/452M [02:46<00:14, 2.33MB/s] 93%|█████████▎| 420M/452M [02:46<00:14, 2.36MB/s] 93%|█████████▎| 421M/452M [02:46<00:19, 1.73MB/s] 93%|█████████▎| 421M/452M [02:46<00:15, 2.12MB/s] 93%|█████████▎| 421M/452M [02:46<00:15, 2.07MB/s] 93%|█████████▎| 422M/452M [02:46<00:16, 1.98MB/s] 93%|█████████▎| 422M/452M [02:46<00:17, 1.82MB/s] 93%|█████████▎| 422M/452M [02:47<00:17, 1.78MB/s] 93%|█████████▎| 422M/452M [02:47<00:18, 1.69MB/s] 93%|█████████▎| 422M/452M [02:47<00:20, 1.56MB/s] 93%|█████████▎| 422M/452M [02:47<00:22, 1.42MB/s] 93%|█████████▎| 423M/452M [02:47<00:23, 1.35MB/s] 93%|█████████▎| 423M/452M [02:47<00:23, 1.33MB/s] 94%|█████████▎| 423M/452M [02:47<00:23, 1.32MB/s] 94%|█████████▎| 423M/452M [02:48<00:22, 1.38MB/s] 94%|█████████▎| 423M/452M [02:48<00:21, 1.41MB/s] 94%|█████████▎| 424M/452M [02:48<00:19, 1.51MB/s] 94%|█████████▎| 424M/452M [02:48<00:14, 2.02MB/s] 94%|█████████▍| 424M/452M [02:48<00:09, 3.02MB/s] 94%|█████████▍| 425M/452M [02:48<00:07, 3.78MB/s] 94%|█████████▍| 426M/452M [02:48<00:05, 4.74MB/s] 94%|█████████▍| 426M/452M [02:48<00:05, 4.98MB/s] 94%|█████████▍| 427M/452M [02:48<00:04, 5.58MB/s] 95%|█████████▍| 428M/452M [02:49<00:04, 5.65MB/s] 95%|█████████▍| 428M/452M [02:49<00:04, 6.03MB/s] 95%|█████████▍| 429M/452M [02:49<00:04, 6.01MB/s] 95%|█████████▍| 430M/452M [02:49<00:04, 5.92MB/s] 95%|█████████▌| 430M/452M [02:49<00:03, 6.41MB/s] 95%|█████████▌| 431M/452M [02:49<00:03, 6.20MB/s] 95%|█████████▌| 432M/452M [02:49<00:03, 6.64MB/s] 96%|█████████▌| 432M/452M [02:49<00:03, 6.35MB/s] 96%|█████████▌| 433M/452M [02:49<00:02, 6.73MB/s] 96%|█████████▌| 434M/452M [02:50<00:03, 6.43MB/s] 96%|█████████▌| 435M/452M [02:50<00:02, 6.79MB/s] 96%|█████████▌| 435M/452M [02:50<00:02, 6.46MB/s] 96%|█████████▋| 436M/452M [02:50<00:02, 6.23MB/s] 96%|█████████▋| 436M/452M [02:50<00:02, 5.84MB/s] 97%|█████████▋| 437M/452M [02:50<00:02, 5.50MB/s] 97%|█████████▋| 438M/452M [02:50<00:02, 5.31MB/s] 97%|█████████▋| 438M/452M [02:50<00:03, 4.74MB/s] 97%|█████████▋| 438M/452M [02:50<00:03, 4.50MB/s] 97%|█████████▋| 439M/452M [02:51<00:03, 4.44MB/s] 97%|█████████▋| 439M/452M [02:51<00:03, 4.44MB/s] 97%|█████████▋| 440M/452M [02:51<00:03, 4.26MB/s] 97%|█████████▋| 440M/452M [02:51<00:03, 4.19MB/s] 97%|█████████▋| 441M/452M [02:51<00:02, 4.21MB/s] 97%|█████████▋| 441M/452M [02:51<00:02, 4.00MB/s] 98%|█████████▊| 441M/452M [02:51<00:03, 3.64MB/s] 98%|█████████▊| 442M/452M [02:51<00:02, 4.02MB/s] 98%|█████████▊| 443M/452M [02:51<00:02, 4.84MB/s] 98%|█████████▊| 443M/452M [02:52<00:01, 5.02MB/s] 98%|█████████▊| 444M/452M [02:52<00:01, 4.88MB/s] 98%|█████████▊| 444M/452M [02:52<00:02, 3.64MB/s] 98%|█████████▊| 445M/452M [02:52<00:02, 3.44MB/s] 98%|█████████▊| 445M/452M [02:52<00:02, 3.40MB/s] 98%|█████████▊| 445M/452M [02:52<00:02, 3.50MB/s] 99%|█████████▊| 446M/452M [02:52<00:01, 3.59MB/s] 99%|█████████▊| 446M/452M [02:53<00:01, 3.71MB/s] 99%|█████████▊| 447M/452M [02:53<00:01, 4.30MB/s] 99%|█████████▉| 447M/452M [02:53<00:01, 5.05MB/s] 99%|█████████▉| 448M/452M [02:53<00:00, 5.45MB/s] 99%|█████████▉| 449M/452M [02:53<00:00, 5.78MB/s] 99%|█████████▉| 449M/452M [02:53<00:00, 6.07MB/s]100%|█████████▉| 450M/452M [02:53<00:00, 6.24MB/s]100%|█████████▉| 451M/452M [02:53<00:00, 6.37MB/s]100%|█████████▉| 451M/452M [02:53<00:00, 6.27MB/s]100%|█████████▉| 452M/452M [02:53<00:00, 6.47MB/s]100%|██████████| 452M/452M [02:54<00:00, 2.73MB/s]
[32m[2023-12-21 10:34:04,246] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-base-zh/model_state.pdparams[0m
[32m[2023-12-21 10:34:04,604] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[33m[2023-12-21 10:34:12,115] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-base-zh were not used when initializing ErnieForSequenceClassification: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']
- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 10:34:12,116] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-base-zh and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 10:40:35,534] [    INFO][0m - global step 10 / 5336000, loss: 2.389522, avg_reader_cost: 0.00988 sec, avg_batch_cost: 38.33649 sec, avg_samples: 512.00000, ips: 13.35542 words/sec,  [0m
[32m[2023-12-21 10:45:50,780] [    INFO][0m - global step 20 / 5336000, loss: 2.725381, avg_reader_cost: 0.00053 sec, avg_batch_cost: 31.52454 sec, avg_samples: 512.00000, ips: 16.24132 words/sec,  [0m
[32m[2023-12-21 10:52:05,462] [    INFO][0m - global step 30 / 5336000, loss: 2.948388, avg_reader_cost: 0.00037 sec, avg_batch_cost: 37.46810 sec, avg_samples: 512.00000, ips: 13.66496 words/sec,  [0m
[32m[2023-12-21 10:57:43,929] [    INFO][0m - global step 40 / 5336000, loss: 3.223535, avg_reader_cost: 0.00036 sec, avg_batch_cost: 33.84662 sec, avg_samples: 512.00000, ips: 15.12706 words/sec,  [0m
[32m[2023-12-21 11:03:54,880] [    INFO][0m - global step 50 / 5336000, loss: 1.870736, avg_reader_cost: 0.00037 sec, avg_batch_cost: 37.09509 sec, avg_samples: 512.00000, ips: 13.80237 words/sec,  [0m
[32m[2023-12-21 11:10:08,311] [    INFO][0m - global step 60 / 5336000, loss: 3.146376, avg_reader_cost: 0.00038 sec, avg_batch_cost: 37.34300 sec, avg_samples: 512.00000, ips: 13.71073 words/sec,  [0m
[32m[2023-12-21 11:16:35,183] [    INFO][0m - global step 70 / 5336000, loss: 3.227182, avg_reader_cost: 0.00037 sec, avg_batch_cost: 38.68712 sec, avg_samples: 512.00000, ips: 13.23438 words/sec,  [0m
[32m[2023-12-21 11:22:09,074] [    INFO][0m - global step 80 / 5336000, loss: 2.423183, avg_reader_cost: 0.00036 sec, avg_batch_cost: 33.38905 sec, avg_samples: 512.00000, ips: 15.33437 words/sec,  [0m
[32m[2023-12-21 11:28:13,319] [    INFO][0m - global step 90 / 5336000, loss: 2.612578, avg_reader_cost: 0.00037 sec, avg_batch_cost: 36.42447 sec, avg_samples: 512.00000, ips: 14.05648 words/sec,  [0m
[32m[2023-12-21 11:34:23,861] [    INFO][0m - global step 100 / 5336000, loss: 2.318558, avg_reader_cost: 0.00037 sec, avg_batch_cost: 37.05406 sec, avg_samples: 512.00000, ips: 13.81765 words/sec,  [0m
[32m[2023-12-21 11:40:14,540] [    INFO][0m - global step 110 / 5336000, loss: 2.047007, avg_reader_cost: 0.00036 sec, avg_batch_cost: 35.06789 sec, avg_samples: 512.00000, ips: 14.60025 words/sec,  [0m
[32m[2023-12-21 11:46:07,932] [    INFO][0m - global step 120 / 5336000, loss: 2.549383, avg_reader_cost: 0.00037 sec, avg_batch_cost: 35.33908 sec, avg_samples: 512.00000, ips: 14.48821 words/sec,  [0m
[32m[2023-12-21 11:52:09,263] [    INFO][0m - global step 130 / 5336000, loss: 1.880372, avg_reader_cost: 0.00036 sec, avg_batch_cost: 36.13307 sec, avg_samples: 512.00000, ips: 14.16984 words/sec,  [0m
[32m[2023-12-21 11:57:41,332] [    INFO][0m - global step 140 / 5336000, loss: 3.119580, avg_reader_cost: 0.00037 sec, avg_batch_cost: 33.20683 sec, avg_samples: 512.00000, ips: 15.41851 words/sec,  [0m
No XPU Memory Leak
[33m Run successfully with command - ernie3_for_sequence_classification - python3.9 test_tipc/train.py --model ernie3_for_sequence_classification --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path ernie-3.0-base-zh --pad_to_max_seq_len --max_seq_len 512 --logging_steps 10 --seed 42 --task_name tnews --max_steps 150 --device=xpu                >/workspace/PaddleNLP/tests/test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:03:30,401 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 12:03:30,401 auto_parallel_config: None
LAUNCH INFO 2023-12-21 12:03:30,401 auto_tuner_json: None
LAUNCH INFO 2023-12-21 12:03:30,401 devices: 0,1
LAUNCH INFO 2023-12-21 12:03:30,401 elastic_level: -1
LAUNCH INFO 2023-12-21 12:03:30,401 elastic_timeout: 30
LAUNCH INFO 2023-12-21 12:03:30,401 enable_gpu_log: True
LAUNCH INFO 2023-12-21 12:03:30,401 gloo_port: 6767
LAUNCH INFO 2023-12-21 12:03:30,401 host: None
LAUNCH INFO 2023-12-21 12:03:30,401 ips: None
LAUNCH INFO 2023-12-21 12:03:30,401 job_id: default
LAUNCH INFO 2023-12-21 12:03:30,401 legacy: False
LAUNCH INFO 2023-12-21 12:03:30,401 log_dir: log
LAUNCH INFO 2023-12-21 12:03:30,401 log_level: INFO
LAUNCH INFO 2023-12-21 12:03:30,401 log_overwrite: False
LAUNCH INFO 2023-12-21 12:03:30,401 master: None
LAUNCH INFO 2023-12-21 12:03:30,401 max_restart: 3
LAUNCH INFO 2023-12-21 12:03:30,401 nnodes: 1
LAUNCH INFO 2023-12-21 12:03:30,401 nproc_per_node: None
LAUNCH INFO 2023-12-21 12:03:30,401 rank: -1
LAUNCH INFO 2023-12-21 12:03:30,401 run_mode: collective
LAUNCH INFO 2023-12-21 12:03:30,401 server_num: None
LAUNCH INFO 2023-12-21 12:03:30,401 servers: 
LAUNCH INFO 2023-12-21 12:03:30,401 sort_ip: False
LAUNCH INFO 2023-12-21 12:03:30,402 start_port: 6070
LAUNCH INFO 2023-12-21 12:03:30,402 trainer_num: None
LAUNCH INFO 2023-12-21 12:03:30,402 trainers: 
LAUNCH INFO 2023-12-21 12:03:30,402 training_script: test_tipc/train.py
LAUNCH INFO 2023-12-21 12:03:30,402 training_script_args: ['--model', 'ernie3_for_sequence_classification', '--optimizer', 'adamw', '--lr_scheduler', 'linear_decay_with_warmup', '--learning_rate', '2e-5', '--max_grad_norm', '1.0', '--model_name_or_path', 'ernie-3.0-base-zh', '--pad_to_max_seq_len', '--max_seq_len', '512', '--logging_steps', '10', '--seed', '42', '--task_name', 'tnews', '--max_steps', '150', '--device=xpu']
LAUNCH INFO 2023-12-21 12:03:30,402 with_gloo: 1
LAUNCH INFO 2023-12-21 12:03:30,402 --------------------------------------------------
LAUNCH INFO 2023-12-21 12:03:30,402 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 12:03:30,403 Run Pod: bycbks, replicas 2, status ready
LAUNCH INFO 2023-12-21 12:03:30,691 Watching Pod: bycbks, replicas 2, status running
+ kill -9 2470
+ printmsg ernie3_for_sequence_classification test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
+ model_name=ernie3_for_sequence_classification
+ config_file=test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
+ msg='ernie3_for_sequence_classification test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt time cost > 3600seconds'
+ echo 'ernie3_for_sequence_classification test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt time cost > 3600seconds'
+ kill -9 2471
+ sleep 10
==END==test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
+ echo ==END==test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt
++ date +%s
+ end=1703133048
++ echo 1703125828 1703133048
++ awk '{print $2-$1-2}'
+ time=7218
+ echo 'test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt spend time seconds 7218'
+ read config_file
test_tipc/configs/ernie3_for_sequence_classification/train_infer_python.txt spend time seconds 7218
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703133048
+ echo ==START==test_tipc/configs/ernie_information_extraction/train_infer_python.txt
==START==test_tipc/configs/ernie_information_extraction/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/ernie_information_extraction/train_infer_python.txt
+ dataline='===========================train_params=========================== 
model_name:ernie_information_extraction
python:python
gpu_list:0|0,1
--device:gpu|gpu
null:null
--epoch:lite_train_lite_infer=10|lite_train_whole_infer=10|whole_train_whole_infer=10
--save_dir:null
--batch_size:lite_train_lite_infer=32|lite_train_whole_infer=32|whole_train_whole_infer=32
null:null
null:model
null:null
--data_dir:./waybill_ie/data
##
trainer:norm
norm_train:./test_tipc/ernie_information_extraction/train.py --max_steps 150
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
--output_path:null
--params_path:null
norm_export:./test_tipc/ernie_information_extraction/export_model.py
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:ernie_information_extraction
++ strs=model_name:ernie_information_extraction
++ IFS=:
++ array=(${strs})
++ tmp=ernie_information_extraction
++ echo ernie_information_extraction
+ model_name=ernie_information_extraction
+ sleep 10
+ run run_model test_tipc/configs/ernie_information_extraction/train_infer_python.txt lite_train_lite_infer 3600 ernie_information_extraction
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ waitfor=7200
+ command='run_model
test_tipc/configs/ernie_information_extraction/train_infer_python.txt
lite_train_lite_infer
3600
ernie_information_extraction'
+ commandpid=2821
+ run_model test_tipc/configs/ernie_information_extraction/train_infer_python.txt lite_train_lite_infer 3600 ernie_information_extraction
+ watchdog=2822
+ config_file=test_tipc/configs/ernie_information_extraction/train_infer_python.txt
+ mode=lite_train_lite_infer
+ wait 2821
+ bash test_tipc/prepare.sh test_tipc/configs/ernie_information_extraction/train_infer_python.txt lite_train_lite_infer
+ sleep 7200
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
  0%|          | 0/154 [00:00<?, ?it/s] 10%|█         | 16/154 [00:00<00:01, 121.23it/s] 21%|██        | 32/154 [00:00<00:01, 117.61it/s] 42%|████▏     | 64/154 [00:00<00:00, 156.55it/s] 52%|█████▏    | 80/154 [00:00<00:00, 137.39it/s] 62%|██████▏   | 96/154 [00:00<00:00, 140.54it/s] 73%|███████▎  | 112/154 [00:00<00:00, 129.42it/s] 83%|████████▎ | 128/154 [00:01<00:00, 114.94it/s]100%|██████████| 154/154 [00:01<00:00, 143.42it/s]
No XPU Memory Leak
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/ernie_information_extraction/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/ernie_information_extraction/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:31:05,883] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-1.0'.[0m
[32m[2023-12-21 12:31:05,883] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt and saved to /root/.paddlenlp/models/ernie-1.0[0m
[32m[2023-12-21 12:31:13,412] [    INFO][0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie/vocab.txt[0m
  0%|          | 0.00/89.5k [00:00<?, ?B/s] 21%|██        | 19.0k/89.5k [00:00<00:00, 91.0kB/s] 57%|█████▋    | 51.0k/89.5k [00:00<00:00, 172kB/s]  93%|█████████▎| 83.0k/89.5k [00:00<00:00, 225kB/s]100%|██████████| 89.5k/89.5k [00:00<00:00, 211kB/s]
[32m[2023-12-21 12:31:14,254] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:31:14,254] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
[32m[2023-12-21 12:31:14,255] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification'> to load 'ernie-1.0'.[0m
[32m[2023-12-21 12:31:14,256] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/ernie-1.0/config.json[0m
[32m[2023-12-21 12:31:14,493] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie/ernie_v1_chn_base.pdparams[0m
[32m[2023-12-21 12:31:14,494] [    INFO][0m - Downloading ernie_v1_chn_base.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie/ernie_v1_chn_base.pdparams[0m
  0%|          | 0.00/383M [00:00<?, ?B/s]  0%|          | 19.0k/383M [00:00<50:01, 134kB/s]  0%|          | 67.0k/383M [00:00<24:30, 273kB/s]  0%|          | 99.0k/383M [00:00<24:14, 276kB/s]  0%|          | 147k/383M [00:00<19:37, 341kB/s]   0%|          | 182k/383M [00:00<25:03, 267kB/s]  0%|          | 211k/383M [00:00<29:33, 227kB/s]  0%|          | 259k/383M [00:01<27:01, 248kB/s]  0%|          | 307k/383M [00:01<24:07, 278kB/s]  0%|          | 355k/383M [00:01<21:12, 316kB/s]  0%|          | 403k/383M [00:01<19:43, 339kB/s]  0%|          | 451k/383M [00:01<19:44, 339kB/s]  0%|          | 486k/383M [00:01<20:48, 321kB/s]  0%|          | 518k/383M [00:01<21:27, 312kB/s]  0%|          | 563k/383M [00:01<22:38, 295kB/s]  0%|          | 627k/383M [00:02<20:09, 332kB/s]  0%|          | 691k/383M [00:02<18:04, 370kB/s]  0%|          | 755k/383M [00:02<17:09, 390kB/s]  0%|          | 803k/383M [00:02<16:15, 411kB/s]  0%|          | 844k/383M [00:02<17:24, 384kB/s]  0%|          | 882k/383M [00:02<19:20, 345kB/s]  0%|          | 917k/383M [00:02<21:36, 309kB/s]  0%|          | 948k/383M [00:03<22:48, 293kB/s]  0%|          | 979k/383M [00:03<22:27, 298kB/s]  0%|          | 1.02M/383M [00:03<17:52, 374kB/s]  0%|          | 1.07M/383M [00:03<17:22, 384kB/s]  0%|          | 1.11M/383M [00:03<16:44, 399kB/s]  0%|          | 1.16M/383M [00:03<18:59, 352kB/s]  0%|          | 1.19M/383M [00:03<19:05, 350kB/s]  0%|          | 1.24M/383M [00:03<18:41, 357kB/s]  0%|          | 1.32M/383M [00:04<14:45, 452kB/s]  0%|          | 1.38M/383M [00:04<13:14, 504kB/s]  0%|          | 1.47M/383M [00:04<10:51, 615kB/s]  0%|          | 1.57M/383M [00:04<09:48, 680kB/s]  0%|          | 1.64M/383M [00:04<10:08, 658kB/s]  0%|          | 1.71M/383M [00:04<11:33, 577kB/s]  0%|          | 1.77M/383M [00:04<12:36, 529kB/s]  0%|          | 1.82M/383M [00:04<12:39, 527kB/s]  0%|          | 1.89M/383M [00:05<11:36, 574kB/s]  1%|          | 1.96M/383M [00:05<12:07, 550kB/s]  1%|          | 2.02M/383M [00:05<12:34, 530kB/s]  1%|          | 2.08M/383M [00:05<13:01, 511kB/s]  1%|          | 2.13M/383M [00:05<13:45, 484kB/s]  1%|          | 2.19M/383M [00:05<15:43, 424kB/s]  1%|          | 2.25M/383M [00:05<15:44, 423kB/s]  1%|          | 2.30M/383M [00:06<15:35, 427kB/s]  1%|          | 2.36M/383M [00:06<14:29, 459kB/s]  1%|          | 2.42M/383M [00:06<13:29, 494kB/s]  1%|          | 2.49M/383M [00:06<13:00, 511kB/s]  1%|          | 2.57M/383M [00:06<11:52, 560kB/s]  1%|          | 2.63M/383M [00:06<12:40, 525kB/s]  1%|          | 2.68M/383M [00:06<12:49, 519kB/s]  1%|          | 2.75M/383M [00:06<13:29, 493kB/s]  1%|          | 2.85M/383M [00:07<12:40, 525kB/s]  1%|          | 2.94M/383M [00:07<11:41, 569kB/s]  1%|          | 3.05M/383M [00:07<10:03, 660kB/s]  1%|          | 3.16M/383M [00:07<08:49, 753kB/s]  1%|          | 3.27M/383M [00:07<08:56, 743kB/s]  1%|          | 3.39M/383M [00:07<09:05, 730kB/s]  1%|          | 3.53M/383M [00:07<08:14, 805kB/s]  1%|          | 3.67M/383M [00:08<07:36, 872kB/s]  1%|          | 3.82M/383M [00:08<07:06, 934kB/s]  1%|          | 3.99M/383M [00:08<06:27, 1.03MB/s]  1%|          | 4.16M/383M [00:08<06:10, 1.07MB/s]  1%|          | 4.33M/383M [00:08<06:45, 979kB/s]   1%|          | 4.53M/383M [00:08<06:40, 992kB/s]  1%|          | 4.75M/383M [00:09<05:56, 1.11MB/s]  1%|▏         | 4.99M/383M [00:09<05:15, 1.26MB/s]  1%|▏         | 5.24M/383M [00:09<04:38, 1.43MB/s]  1%|▏         | 5.50M/383M [00:09<04:06, 1.61MB/s]  2%|▏         | 5.78M/383M [00:09<04:04, 1.62MB/s]  2%|▏         | 5.99M/383M [00:09<03:51, 1.71MB/s]  2%|▏         | 6.24M/383M [00:09<03:50, 1.72MB/s]  2%|▏         | 6.57M/383M [00:10<03:26, 1.91MB/s]  2%|▏         | 6.89M/383M [00:10<03:16, 2.01MB/s]  2%|▏         | 7.19M/383M [00:10<02:55, 2.25MB/s]  2%|▏         | 7.44M/383M [00:10<03:00, 2.19MB/s]  2%|▏         | 7.82M/383M [00:10<02:59, 2.20MB/s]  2%|▏         | 8.22M/383M [00:10<03:14, 2.02MB/s]  2%|▏         | 8.63M/383M [00:11<02:39, 2.46MB/s]  2%|▏         | 8.89M/383M [00:11<03:01, 2.17MB/s]  2%|▏         | 9.14M/383M [00:11<03:29, 1.87MB/s]  3%|▎         | 9.64M/383M [00:11<02:59, 2.18MB/s]  3%|▎         | 10.2M/383M [00:11<02:25, 2.69MB/s]  3%|▎         | 10.7M/383M [00:11<02:04, 3.14MB/s]  3%|▎         | 11.1M/383M [00:11<02:00, 3.25MB/s]  3%|▎         | 11.7M/383M [00:12<01:40, 3.86MB/s]  3%|▎         | 12.3M/383M [00:12<01:29, 4.36MB/s]  3%|▎         | 13.1M/383M [00:12<01:10, 5.51MB/s]  4%|▎         | 14.0M/383M [00:12<01:02, 6.18MB/s]  4%|▍         | 15.0M/383M [00:12<00:52, 7.40MB/s]  4%|▍         | 15.8M/383M [00:12<00:54, 7.03MB/s]  4%|▍         | 16.7M/383M [00:12<00:52, 7.35MB/s]  5%|▍         | 17.4M/383M [00:12<01:04, 5.98MB/s]  5%|▍         | 18.1M/383M [00:13<01:04, 5.90MB/s]  5%|▍         | 18.7M/383M [00:13<01:01, 6.23MB/s]  5%|▌         | 19.6M/383M [00:13<00:54, 6.95MB/s]  5%|▌         | 20.7M/383M [00:13<00:48, 7.92MB/s]  6%|▌         | 21.5M/383M [00:13<00:50, 7.47MB/s]  6%|▌         | 22.2M/383M [00:13<00:52, 7.26MB/s]  6%|▌         | 22.9M/383M [00:13<01:04, 5.87MB/s]  6%|▌         | 23.5M/383M [00:13<01:08, 5.50MB/s]  6%|▋         | 24.1M/383M [00:14<01:09, 5.44MB/s]  6%|▋         | 24.6M/383M [00:14<01:10, 5.33MB/s]  7%|▋         | 25.3M/383M [00:14<01:06, 5.61MB/s]  7%|▋         | 26.1M/383M [00:14<01:02, 6.03MB/s]  7%|▋         | 26.9M/383M [00:14<00:57, 6.51MB/s]  7%|▋         | 27.8M/383M [00:14<00:52, 7.16MB/s]  7%|▋         | 28.5M/383M [00:14<01:05, 5.66MB/s]  8%|▊         | 29.0M/383M [00:14<01:09, 5.33MB/s]  8%|▊         | 29.6M/383M [00:15<01:08, 5.40MB/s]  8%|▊         | 30.2M/383M [00:15<01:26, 4.28MB/s]  8%|▊         | 30.6M/383M [00:15<01:26, 4.26MB/s]  8%|▊         | 31.1M/383M [00:15<01:26, 4.28MB/s]  8%|▊         | 31.5M/383M [00:15<01:25, 4.34MB/s]  8%|▊         | 31.9M/383M [00:15<01:25, 4.32MB/s]  8%|▊         | 32.4M/383M [00:15<01:40, 3.66MB/s]  9%|▊         | 32.7M/383M [00:15<01:46, 3.45MB/s]  9%|▊         | 33.1M/383M [00:16<02:20, 2.61MB/s]  9%|▉         | 33.8M/383M [00:16<01:49, 3.36MB/s]  9%|▉         | 34.2M/383M [00:16<02:06, 2.90MB/s]  9%|▉         | 34.5M/383M [00:16<02:12, 2.77MB/s]  9%|▉         | 34.7M/383M [00:16<02:21, 2.58MB/s]  9%|▉         | 35.0M/383M [00:16<02:36, 2.33MB/s]  9%|▉         | 35.3M/383M [00:17<02:32, 2.40MB/s]  9%|▉         | 35.6M/383M [00:17<02:32, 2.39MB/s]  9%|▉         | 35.9M/383M [00:17<02:26, 2.48MB/s]  9%|▉         | 36.3M/383M [00:17<02:25, 2.50MB/s] 10%|▉         | 36.6M/383M [00:17<02:14, 2.70MB/s] 10%|▉         | 36.9M/383M [00:17<02:08, 2.83MB/s] 10%|▉         | 37.2M/383M [00:17<02:10, 2.78MB/s] 10%|▉         | 37.5M/383M [00:17<02:13, 2.71MB/s] 10%|▉         | 37.7M/383M [00:18<02:26, 2.47MB/s] 10%|▉         | 38.0M/383M [00:18<02:26, 2.47MB/s] 10%|▉         | 38.3M/383M [00:18<02:19, 2.59MB/s] 10%|█         | 38.9M/383M [00:18<01:49, 3.28MB/s] 10%|█         | 39.5M/383M [00:18<01:25, 4.21MB/s] 10%|█         | 39.9M/383M [00:18<01:41, 3.53MB/s] 11%|█         | 40.3M/383M [00:18<01:55, 3.11MB/s] 11%|█         | 40.6M/383M [00:19<02:12, 2.72MB/s] 11%|█         | 40.9M/383M [00:19<02:10, 2.74MB/s] 11%|█         | 41.5M/383M [00:19<01:42, 3.50MB/s] 11%|█         | 42.0M/383M [00:19<01:35, 3.76MB/s] 11%|█         | 42.6M/383M [00:19<01:21, 4.38MB/s] 11%|█▏        | 43.3M/383M [00:19<01:38, 3.61MB/s] 11%|█▏        | 43.9M/383M [00:19<01:25, 4.17MB/s] 12%|█▏        | 44.3M/383M [00:20<01:44, 3.41MB/s] 12%|█▏        | 44.7M/383M [00:20<02:12, 2.68MB/s] 12%|█▏        | 45.0M/383M [00:20<02:12, 2.68MB/s] 12%|█▏        | 45.3M/383M [00:20<02:09, 2.73MB/s] 12%|█▏        | 45.7M/383M [00:20<02:06, 2.80MB/s] 12%|█▏        | 46.0M/383M [00:20<02:10, 2.72MB/s] 12%|█▏        | 46.3M/383M [00:20<02:24, 2.45MB/s] 12%|█▏        | 46.5M/383M [00:21<02:43, 2.16MB/s] 12%|█▏        | 46.8M/383M [00:21<02:48, 2.09MB/s] 12%|█▏        | 47.1M/383M [00:21<02:35, 2.26MB/s] 12%|█▏        | 47.4M/383M [00:21<02:24, 2.43MB/s] 13%|█▎        | 48.1M/383M [00:21<01:51, 3.14MB/s] 13%|█▎        | 48.7M/383M [00:21<01:41, 3.45MB/s] 13%|█▎        | 49.0M/383M [00:21<01:59, 2.93MB/s] 13%|█▎        | 49.3M/383M [00:22<01:59, 2.92MB/s] 13%|█▎        | 49.6M/383M [00:22<02:00, 2.91MB/s] 13%|█▎        | 50.0M/383M [00:22<02:02, 2.85MB/s] 13%|█▎        | 50.3M/383M [00:22<02:01, 2.87MB/s] 13%|█▎        | 50.6M/383M [00:22<02:00, 2.90MB/s] 13%|█▎        | 51.0M/383M [00:22<01:55, 3.02MB/s] 13%|█▎        | 51.3M/383M [00:22<01:51, 3.13MB/s] 13%|█▎        | 51.6M/383M [00:22<01:59, 2.91MB/s] 14%|█▎        | 52.0M/383M [00:23<02:05, 2.77MB/s] 14%|█▎        | 52.3M/383M [00:23<02:02, 2.83MB/s] 14%|█▎        | 52.6M/383M [00:23<02:00, 2.87MB/s] 14%|█▍        | 53.0M/383M [00:23<01:53, 3.06MB/s] 14%|█▍        | 53.5M/383M [00:23<01:35, 3.61MB/s] 14%|█▍        | 54.3M/383M [00:23<01:12, 4.79MB/s] 14%|█▍        | 54.8M/383M [00:23<01:10, 4.85MB/s] 14%|█▍        | 55.2M/383M [00:23<01:26, 3.98MB/s] 15%|█▍        | 55.6M/383M [00:24<01:42, 3.36MB/s] 15%|█▍        | 56.0M/383M [00:24<01:59, 2.87MB/s] 15%|█▍        | 56.3M/383M [00:24<01:58, 2.90MB/s] 15%|█▍        | 56.7M/383M [00:24<01:47, 3.17MB/s] 15%|█▍        | 57.2M/383M [00:24<01:37, 3.51MB/s] 15%|█▌        | 58.0M/383M [00:24<01:14, 4.58MB/s] 15%|█▌        | 58.5M/383M [00:24<01:16, 4.46MB/s] 15%|█▌        | 58.9M/383M [00:25<01:51, 3.05MB/s] 15%|█▌        | 59.3M/383M [00:25<02:03, 2.75MB/s] 16%|█▌        | 59.7M/383M [00:25<02:06, 2.67MB/s] 16%|█▌        | 60.1M/383M [00:25<02:02, 2.76MB/s] 16%|█▌        | 60.5M/383M [00:25<01:52, 3.02MB/s] 16%|█▌        | 61.0M/383M [00:25<01:34, 3.59MB/s] 16%|█▌        | 61.4M/383M [00:25<01:44, 3.24MB/s] 16%|█▌        | 62.0M/383M [00:26<01:25, 3.96MB/s] 16%|█▋        | 62.5M/383M [00:26<01:23, 4.05MB/s] 16%|█▋        | 63.0M/383M [00:26<01:22, 4.05MB/s] 17%|█▋        | 63.4M/383M [00:26<01:23, 4.00MB/s] 17%|█▋        | 63.8M/383M [00:26<01:31, 3.67MB/s] 17%|█▋        | 64.2M/383M [00:26<01:27, 3.82MB/s] 17%|█▋        | 64.6M/383M [00:26<01:27, 3.82MB/s] 17%|█▋        | 65.0M/383M [00:26<01:30, 3.67MB/s] 17%|█▋        | 65.3M/383M [00:26<01:33, 3.56MB/s] 17%|█▋        | 65.7M/383M [00:27<01:34, 3.51MB/s] 17%|█▋        | 66.0M/383M [00:27<01:49, 3.04MB/s] 17%|█▋        | 66.3M/383M [00:27<02:00, 2.76MB/s] 17%|█▋        | 66.6M/383M [00:27<02:10, 2.54MB/s] 17%|█▋        | 66.9M/383M [00:27<02:10, 2.54MB/s] 18%|█▊        | 67.2M/383M [00:27<02:05, 2.64MB/s] 18%|█▊        | 67.5M/383M [00:27<02:39, 2.08MB/s] 18%|█▊        | 67.7M/383M [00:28<02:43, 2.03MB/s] 18%|█▊        | 67.9M/383M [00:28<02:53, 1.91MB/s] 18%|█▊        | 68.1M/383M [00:28<02:57, 1.86MB/s] 18%|█▊        | 68.3M/383M [00:28<02:57, 1.86MB/s] 18%|█▊        | 68.5M/383M [00:28<02:39, 2.07MB/s] 18%|█▊        | 68.7M/383M [00:28<03:05, 1.78MB/s] 18%|█▊        | 69.1M/383M [00:28<02:58, 1.85MB/s] 18%|█▊        | 69.3M/383M [00:29<03:04, 1.78MB/s] 18%|█▊        | 69.5M/383M [00:29<03:05, 1.78MB/s] 18%|█▊        | 69.6M/383M [00:29<03:08, 1.75MB/s] 18%|█▊        | 69.8M/383M [00:29<03:10, 1.72MB/s] 18%|█▊        | 70.0M/383M [00:29<03:07, 1.75MB/s] 18%|█▊        | 70.4M/383M [00:29<02:20, 2.33MB/s] 18%|█▊        | 70.6M/383M [00:29<02:36, 2.10MB/s] 19%|█▊        | 70.9M/383M [00:29<02:38, 2.07MB/s] 19%|█▊        | 71.3M/383M [00:30<02:13, 2.46MB/s] 19%|█▊        | 71.5M/383M [00:30<02:35, 2.10MB/s] 19%|█▊        | 71.8M/383M [00:30<02:40, 2.03MB/s] 19%|█▉        | 72.0M/383M [00:30<02:54, 1.87MB/s] 19%|█▉        | 72.2M/383M [00:30<03:16, 1.66MB/s] 19%|█▉        | 72.3M/383M [00:30<03:33, 1.53MB/s] 19%|█▉        | 72.5M/383M [00:30<03:31, 1.54MB/s] 19%|█▉        | 72.7M/383M [00:30<03:18, 1.64MB/s] 19%|█▉        | 72.9M/383M [00:31<03:08, 1.72MB/s] 19%|█▉        | 73.2M/383M [00:31<02:29, 2.17MB/s] 19%|█▉        | 73.4M/383M [00:31<02:29, 2.17MB/s] 19%|█▉        | 73.6M/383M [00:31<02:36, 2.07MB/s] 19%|█▉        | 73.8M/383M [00:31<02:55, 1.85MB/s] 19%|█▉        | 74.0M/383M [00:31<03:00, 1.80MB/s] 19%|█▉        | 74.2M/383M [00:31<03:08, 1.72MB/s] 19%|█▉        | 74.4M/383M [00:31<03:01, 1.78MB/s] 19%|█▉        | 74.6M/383M [00:32<03:00, 1.79MB/s] 20%|█▉        | 74.8M/383M [00:32<02:55, 1.84MB/s] 20%|█▉        | 75.0M/383M [00:32<02:55, 1.84MB/s] 20%|█▉        | 75.3M/383M [00:32<03:16, 1.65MB/s] 20%|█▉        | 75.5M/383M [00:32<03:24, 1.58MB/s] 20%|█▉        | 75.7M/383M [00:32<03:29, 1.54MB/s] 20%|█▉        | 75.9M/383M [00:32<03:20, 1.61MB/s] 20%|█▉        | 76.1M/383M [00:32<03:09, 1.70MB/s] 20%|█▉        | 76.3M/383M [00:33<03:06, 1.72MB/s] 20%|█▉        | 76.5M/383M [00:33<02:58, 1.80MB/s] 20%|██        | 76.7M/383M [00:33<03:11, 1.68MB/s] 20%|██        | 76.9M/383M [00:33<03:15, 1.65MB/s] 20%|██        | 77.1M/383M [00:33<03:27, 1.54MB/s] 20%|██        | 77.2M/383M [00:33<03:51, 1.39MB/s] 20%|██        | 77.4M/383M [00:33<03:50, 1.39MB/s] 20%|██        | 77.6M/383M [00:33<03:32, 1.51MB/s] 20%|██        | 77.8M/383M [00:34<03:23, 1.58MB/s] 20%|██        | 78.0M/383M [00:34<03:09, 1.69MB/s] 20%|██        | 78.2M/383M [00:34<03:16, 1.62MB/s] 20%|██        | 78.3M/383M [00:34<03:50, 1.39MB/s] 20%|██        | 78.5M/383M [00:34<03:40, 1.45MB/s] 21%|██        | 78.7M/383M [00:34<03:19, 1.60MB/s] 21%|██        | 79.0M/383M [00:34<03:07, 1.71MB/s] 21%|██        | 79.2M/383M [00:34<02:50, 1.87MB/s] 21%|██        | 79.4M/383M [00:35<02:51, 1.85MB/s] 21%|██        | 79.7M/383M [00:35<02:23, 2.22MB/s] 21%|██        | 80.0M/383M [00:35<02:13, 2.39MB/s] 21%|██        | 80.5M/383M [00:35<01:44, 3.05MB/s] 21%|██        | 80.9M/383M [00:35<01:44, 3.04MB/s] 21%|██        | 81.2M/383M [00:35<01:45, 3.01MB/s] 21%|██▏       | 81.7M/383M [00:35<01:24, 3.72MB/s] 21%|██▏       | 82.2M/383M [00:35<01:20, 3.90MB/s] 22%|██▏       | 82.6M/383M [00:35<01:18, 4.00MB/s] 22%|██▏       | 82.9M/383M [00:36<01:21, 3.87MB/s] 22%|██▏       | 83.4M/383M [00:36<01:17, 4.05MB/s] 22%|██▏       | 83.9M/383M [00:36<01:09, 4.53MB/s] 22%|██▏       | 84.5M/383M [00:36<01:02, 4.98MB/s] 22%|██▏       | 85.2M/383M [00:36<00:57, 5.47MB/s] 22%|██▏       | 85.9M/383M [00:36<00:53, 5.82MB/s] 23%|██▎       | 86.6M/383M [00:36<00:51, 6.09MB/s] 23%|██▎       | 87.3M/383M [00:36<00:49, 6.29MB/s] 23%|██▎       | 88.0M/383M [00:36<00:47, 6.51MB/s] 23%|██▎       | 88.8M/383M [00:37<00:46, 6.67MB/s] 23%|██▎       | 89.5M/383M [00:37<00:45, 6.81MB/s] 24%|██▎       | 90.3M/383M [00:37<00:45, 6.76MB/s] 24%|██▎       | 90.9M/383M [00:37<00:46, 6.58MB/s] 24%|██▍       | 91.6M/383M [00:37<00:51, 5.92MB/s] 24%|██▍       | 92.1M/383M [00:37<00:55, 5.47MB/s] 24%|██▍       | 92.7M/383M [00:37<00:59, 5.14MB/s] 24%|██▍       | 93.2M/383M [00:37<01:00, 4.99MB/s] 24%|██▍       | 93.7M/383M [00:38<01:07, 4.47MB/s] 25%|██▍       | 94.1M/383M [00:38<01:12, 4.19MB/s] 25%|██▍       | 94.6M/383M [00:38<01:19, 3.82MB/s] 25%|██▍       | 95.1M/383M [00:38<01:23, 3.64MB/s] 25%|██▍       | 95.5M/383M [00:38<01:23, 3.61MB/s] 25%|██▌       | 95.9M/383M [00:38<01:24, 3.58MB/s] 25%|██▌       | 96.2M/383M [00:38<01:25, 3.51MB/s] 25%|██▌       | 96.5M/383M [00:38<01:27, 3.43MB/s] 25%|██▌       | 96.9M/383M [00:39<01:37, 3.07MB/s] 25%|██▌       | 97.2M/383M [00:39<01:40, 2.99MB/s] 25%|██▌       | 97.4M/383M [00:39<01:45, 2.85MB/s] 25%|██▌       | 97.7M/383M [00:39<01:48, 2.75MB/s] 26%|██▌       | 98.0M/383M [00:39<01:47, 2.78MB/s] 26%|██▌       | 98.3M/383M [00:39<01:42, 2.91MB/s] 26%|██▌       | 98.8M/383M [00:39<01:26, 3.47MB/s] 26%|██▌       | 99.1M/383M [00:39<01:25, 3.47MB/s] 26%|██▌       | 99.5M/383M [00:39<01:41, 2.92MB/s] 26%|██▌       | 99.8M/383M [00:40<01:35, 3.12MB/s] 26%|██▌       | 100M/383M [00:40<01:30, 3.30MB/s]  26%|██▌       | 101M/383M [00:40<01:36, 3.06MB/s] 26%|██▋       | 101M/383M [00:40<01:28, 3.34MB/s] 26%|██▋       | 101M/383M [00:40<01:27, 3.36MB/s] 27%|██▋       | 102M/383M [00:40<01:34, 3.13MB/s] 27%|██▋       | 102M/383M [00:40<01:23, 3.52MB/s] 27%|██▋       | 103M/383M [00:40<01:15, 3.88MB/s] 27%|██▋       | 103M/383M [00:41<01:14, 3.96MB/s] 27%|██▋       | 104M/383M [00:41<01:13, 3.98MB/s] 27%|██▋       | 104M/383M [00:41<01:16, 3.84MB/s] 27%|██▋       | 105M/383M [00:41<01:17, 3.78MB/s] 27%|██▋       | 105M/383M [00:41<01:37, 3.00MB/s] 27%|██▋       | 105M/383M [00:41<01:37, 2.98MB/s] 28%|██▊       | 106M/383M [00:41<01:36, 3.01MB/s] 28%|██▊       | 106M/383M [00:42<01:56, 2.50MB/s] 28%|██▊       | 107M/383M [00:42<01:28, 3.28MB/s] 28%|██▊       | 107M/383M [00:42<01:32, 3.13MB/s] 28%|██▊       | 107M/383M [00:42<01:46, 2.72MB/s] 28%|██▊       | 108M/383M [00:42<01:49, 2.63MB/s] 28%|██▊       | 108M/383M [00:42<02:17, 2.11MB/s] 28%|██▊       | 108M/383M [00:42<02:17, 2.09MB/s] 28%|██▊       | 108M/383M [00:43<02:39, 1.80MB/s] 28%|██▊       | 109M/383M [00:43<02:39, 1.80MB/s] 28%|██▊       | 109M/383M [00:43<02:38, 1.81MB/s] 28%|██▊       | 109M/383M [00:43<02:37, 1.83MB/s] 28%|██▊       | 109M/383M [00:43<02:21, 2.03MB/s] 29%|██▊       | 109M/383M [00:43<02:33, 1.87MB/s] 29%|██▊       | 110M/383M [00:43<02:26, 1.95MB/s] 29%|██▊       | 110M/383M [00:43<02:12, 2.16MB/s] 29%|██▉       | 110M/383M [00:44<01:41, 2.81MB/s] 29%|██▉       | 111M/383M [00:44<01:25, 3.36MB/s] 29%|██▉       | 111M/383M [00:44<01:17, 3.69MB/s] 29%|██▉       | 112M/383M [00:44<01:14, 3.81MB/s] 29%|██▉       | 112M/383M [00:44<01:23, 3.39MB/s] 29%|██▉       | 113M/383M [00:44<01:23, 3.38MB/s] 29%|██▉       | 113M/383M [00:44<01:37, 2.91MB/s] 30%|██▉       | 113M/383M [00:45<02:01, 2.33MB/s] 30%|██▉       | 114M/383M [00:45<02:12, 2.14MB/s] 30%|██▉       | 114M/383M [00:45<02:10, 2.17MB/s] 30%|██▉       | 114M/383M [00:45<02:10, 2.17MB/s] 30%|██▉       | 114M/383M [00:45<02:09, 2.19MB/s] 30%|██▉       | 115M/383M [00:45<02:07, 2.21MB/s] 30%|██▉       | 115M/383M [00:45<02:20, 2.00MB/s] 30%|███       | 115M/383M [00:46<02:03, 2.27MB/s] 30%|███       | 116M/383M [00:46<01:54, 2.45MB/s] 30%|███       | 116M/383M [00:46<01:53, 2.46MB/s] 30%|███       | 116M/383M [00:46<01:54, 2.46MB/s] 30%|███       | 117M/383M [00:46<01:51, 2.50MB/s] 31%|███       | 117M/383M [00:46<01:46, 2.62MB/s] 31%|███       | 117M/383M [00:46<01:38, 2.84MB/s] 31%|███       | 118M/383M [00:46<01:28, 3.15MB/s] 31%|███       | 118M/383M [00:47<01:23, 3.31MB/s] 31%|███       | 119M/383M [00:47<01:15, 3.68MB/s] 31%|███       | 119M/383M [00:47<01:14, 3.70MB/s] 31%|███       | 119M/383M [00:47<01:17, 3.55MB/s] 31%|███       | 120M/383M [00:47<01:18, 3.50MB/s] 31%|███▏      | 120M/383M [00:47<01:26, 3.18MB/s] 31%|███▏      | 120M/383M [00:47<01:43, 2.66MB/s] 31%|███▏      | 121M/383M [00:47<01:59, 2.31MB/s] 32%|███▏      | 121M/383M [00:48<01:58, 2.33MB/s] 32%|███▏      | 121M/383M [00:48<02:20, 1.96MB/s] 32%|███▏      | 121M/383M [00:48<02:18, 1.98MB/s] 32%|███▏      | 121M/383M [00:48<02:26, 1.88MB/s] 32%|███▏      | 122M/383M [00:48<02:33, 1.79MB/s] 32%|███▏      | 122M/383M [00:48<02:37, 1.74MB/s] 32%|███▏      | 122M/383M [00:48<02:48, 1.62MB/s] 32%|███▏      | 122M/383M [00:48<02:54, 1.57MB/s] 32%|███▏      | 122M/383M [00:49<02:55, 1.56MB/s] 32%|███▏      | 122M/383M [00:49<03:02, 1.50MB/s] 32%|███▏      | 123M/383M [00:49<03:12, 1.42MB/s] 32%|███▏      | 123M/383M [00:49<03:06, 1.46MB/s] 32%|███▏      | 123M/383M [00:49<02:53, 1.58MB/s] 32%|███▏      | 123M/383M [00:49<02:30, 1.81MB/s] 32%|███▏      | 123M/383M [00:49<02:17, 1.98MB/s] 32%|███▏      | 124M/383M [00:49<02:19, 1.96MB/s] 32%|███▏      | 124M/383M [00:49<02:26, 1.86MB/s] 32%|███▏      | 124M/383M [00:50<02:35, 1.75MB/s] 32%|███▏      | 124M/383M [00:50<02:38, 1.71MB/s] 32%|███▏      | 124M/383M [00:50<02:45, 1.64MB/s] 32%|███▏      | 124M/383M [00:50<02:52, 1.58MB/s] 33%|███▎      | 125M/383M [00:50<02:42, 1.66MB/s] 33%|███▎      | 125M/383M [00:50<02:18, 1.95MB/s] 33%|███▎      | 125M/383M [00:50<01:58, 2.28MB/s] 33%|███▎      | 126M/383M [00:50<01:41, 2.66MB/s] 33%|███▎      | 126M/383M [00:50<01:47, 2.51MB/s] 33%|███▎      | 126M/383M [00:51<01:53, 2.38MB/s] 33%|███▎      | 126M/383M [00:51<01:50, 2.43MB/s] 33%|███▎      | 127M/383M [00:51<02:09, 2.08MB/s] 33%|███▎      | 127M/383M [00:51<02:18, 1.95MB/s] 33%|███▎      | 127M/383M [00:51<02:33, 1.76MB/s] 33%|███▎      | 127M/383M [00:51<02:49, 1.59MB/s] 33%|███▎      | 127M/383M [00:51<02:43, 1.64MB/s] 33%|███▎      | 128M/383M [00:52<02:40, 1.68MB/s] 33%|███▎      | 128M/383M [00:52<02:31, 1.77MB/s] 33%|███▎      | 128M/383M [00:52<02:21, 1.89MB/s] 33%|███▎      | 128M/383M [00:52<02:15, 1.97MB/s] 34%|███▎      | 129M/383M [00:52<02:04, 2.14MB/s] 34%|███▎      | 129M/383M [00:52<01:57, 2.28MB/s] 34%|███▎      | 129M/383M [00:52<02:06, 2.11MB/s] 34%|███▎      | 129M/383M [00:52<02:05, 2.12MB/s] 34%|███▍      | 129M/383M [00:52<02:04, 2.14MB/s] 34%|███▍      | 130M/383M [00:53<02:06, 2.10MB/s] 34%|███▍      | 130M/383M [00:53<02:05, 2.12MB/s] 34%|███▍      | 130M/383M [00:53<02:12, 2.01MB/s] 34%|███▍      | 130M/383M [00:53<02:24, 1.83MB/s] 34%|███▍      | 130M/383M [00:53<02:36, 1.70MB/s] 34%|███▍      | 131M/383M [00:53<02:44, 1.61MB/s] 34%|███▍      | 131M/383M [00:53<02:44, 1.61MB/s] 34%|███▍      | 131M/383M [00:53<02:41, 1.64MB/s] 34%|███▍      | 131M/383M [00:53<02:33, 1.72MB/s] 34%|███▍      | 131M/383M [00:54<02:57, 1.49MB/s] 34%|███▍      | 131M/383M [00:54<03:16, 1.35MB/s] 34%|███▍      | 132M/383M [00:54<03:17, 1.34MB/s] 34%|███▍      | 132M/383M [00:54<03:14, 1.36MB/s] 34%|███▍      | 132M/383M [00:54<03:08, 1.39MB/s] 34%|███▍      | 132M/383M [00:54<02:56, 1.49MB/s] 35%|███▍      | 132M/383M [00:54<03:08, 1.39MB/s] 35%|███▍      | 132M/383M [00:54<03:03, 1.43MB/s] 35%|███▍      | 133M/383M [00:55<02:58, 1.47MB/s] 35%|███▍      | 133M/383M [00:55<02:45, 1.59MB/s] 35%|███▍      | 133M/383M [00:55<02:36, 1.68MB/s] 35%|███▍      | 133M/383M [00:55<03:10, 1.38MB/s] 35%|███▍      | 133M/383M [00:55<02:44, 1.59MB/s] 35%|███▍      | 134M/383M [00:55<03:06, 1.41MB/s] 35%|███▍      | 134M/383M [00:55<03:22, 1.29MB/s] 35%|███▍      | 134M/383M [00:56<03:24, 1.28MB/s] 35%|███▍      | 134M/383M [00:56<03:46, 1.15MB/s] 35%|███▍      | 134M/383M [00:56<04:17, 1.02MB/s] 35%|███▌      | 134M/383M [00:56<03:57, 1.10MB/s] 35%|███▌      | 134M/383M [00:56<04:11, 1.04MB/s] 35%|███▌      | 134M/383M [00:56<05:00, 867kB/s]  35%|███▌      | 135M/383M [00:56<04:34, 950kB/s] 35%|███▌      | 135M/383M [00:57<04:40, 929kB/s] 35%|███▌      | 135M/383M [00:57<04:27, 973kB/s] 35%|███▌      | 135M/383M [00:57<04:41, 926kB/s] 35%|███▌      | 135M/383M [00:57<04:49, 899kB/s] 35%|███▌      | 135M/383M [00:57<04:01, 1.08MB/s] 35%|███▌      | 135M/383M [00:57<03:40, 1.18MB/s] 35%|███▌      | 135M/383M [00:57<04:39, 930kB/s]  35%|███▌      | 136M/383M [00:57<04:17, 1.01MB/s] 35%|███▌      | 136M/383M [00:58<03:42, 1.17MB/s] 35%|███▌      | 136M/383M [00:58<03:55, 1.10MB/s] 35%|███▌      | 136M/383M [00:58<04:00, 1.08MB/s] 36%|███▌      | 136M/383M [00:58<03:29, 1.23MB/s] 36%|███▌      | 137M/383M [00:58<03:20, 1.29MB/s] 36%|███▌      | 137M/383M [00:58<03:02, 1.41MB/s] 36%|███▌      | 137M/383M [00:58<02:53, 1.49MB/s] 36%|███▌      | 137M/383M [00:59<02:46, 1.55MB/s] 36%|███▌      | 137M/383M [00:59<02:35, 1.66MB/s] 36%|███▌      | 138M/383M [00:59<02:34, 1.67MB/s] 36%|███▌      | 138M/383M [00:59<02:23, 1.79MB/s] 36%|███▌      | 138M/383M [00:59<02:09, 1.99MB/s] 36%|███▌      | 138M/383M [00:59<01:47, 2.40MB/s] 36%|███▌      | 139M/383M [00:59<01:53, 2.26MB/s] 36%|███▌      | 139M/383M [00:59<01:55, 2.22MB/s] 36%|███▋      | 139M/383M [00:59<01:57, 2.18MB/s] 36%|███▋      | 139M/383M [01:00<01:50, 2.32MB/s] 36%|███▋      | 140M/383M [01:00<01:35, 2.69MB/s] 37%|███▋      | 140M/383M [01:00<01:27, 2.92MB/s] 37%|███▋      | 140M/383M [01:00<01:45, 2.42MB/s] 37%|███▋      | 141M/383M [01:00<01:41, 2.49MB/s] 37%|███▋      | 141M/383M [01:00<01:41, 2.50MB/s] 37%|███▋      | 141M/383M [01:00<02:01, 2.10MB/s] 37%|███▋      | 142M/383M [01:01<01:58, 2.15MB/s] 37%|███▋      | 142M/383M [01:01<01:59, 2.13MB/s] 37%|███▋      | 142M/383M [01:01<01:58, 2.13MB/s] 37%|███▋      | 142M/383M [01:01<01:52, 2.25MB/s] 37%|███▋      | 142M/383M [01:01<01:59, 2.11MB/s] 37%|███▋      | 143M/383M [01:01<02:05, 2.02MB/s] 37%|███▋      | 143M/383M [01:01<01:33, 2.68MB/s] 37%|███▋      | 143M/383M [01:01<01:53, 2.21MB/s] 37%|███▋      | 144M/383M [01:02<02:02, 2.06MB/s] 38%|███▊      | 144M/383M [01:02<02:08, 1.96MB/s] 38%|███▊      | 144M/383M [01:02<02:17, 1.83MB/s] 38%|███▊      | 144M/383M [01:02<02:12, 1.89MB/s] 38%|███▊      | 145M/383M [01:02<01:59, 2.10MB/s] 38%|███▊      | 145M/383M [01:02<01:49, 2.28MB/s] 38%|███▊      | 145M/383M [01:02<01:53, 2.20MB/s] 38%|███▊      | 145M/383M [01:02<02:01, 2.05MB/s] 38%|███▊      | 146M/383M [01:03<01:59, 2.08MB/s] 38%|███▊      | 146M/383M [01:03<02:01, 2.06MB/s] 38%|███▊      | 146M/383M [01:03<02:05, 1.98MB/s] 38%|███▊      | 146M/383M [01:03<01:51, 2.23MB/s] 38%|███▊      | 147M/383M [01:03<01:49, 2.28MB/s] 38%|███▊      | 147M/383M [01:03<01:49, 2.27MB/s] 38%|███▊      | 147M/383M [01:03<01:52, 2.21MB/s] 38%|███▊      | 148M/383M [01:03<01:52, 2.20MB/s] 39%|███▊      | 148M/383M [01:04<01:49, 2.26MB/s] 39%|███▊      | 148M/383M [01:04<01:49, 2.25MB/s] 39%|███▊      | 148M/383M [01:04<01:54, 2.16MB/s] 39%|███▊      | 148M/383M [01:04<02:00, 2.04MB/s] 39%|███▉      | 149M/383M [01:04<02:02, 2.02MB/s] 39%|███▉      | 149M/383M [01:04<02:30, 1.63MB/s] 39%|███▉      | 149M/383M [01:04<02:41, 1.52MB/s] 39%|███▉      | 149M/383M [01:04<02:41, 1.52MB/s] 39%|███▉      | 149M/383M [01:05<02:44, 1.49MB/s] 39%|███▉      | 149M/383M [01:05<02:45, 1.48MB/s] 39%|███▉      | 150M/383M [01:05<02:50, 1.44MB/s] 39%|███▉      | 150M/383M [01:05<02:55, 1.39MB/s] 39%|███▉      | 150M/383M [01:05<02:52, 1.42MB/s] 39%|███▉      | 150M/383M [01:05<02:40, 1.52MB/s] 39%|███▉      | 150M/383M [01:05<02:14, 1.82MB/s] 39%|███▉      | 150M/383M [01:05<02:19, 1.75MB/s] 39%|███▉      | 151M/383M [01:06<02:34, 1.58MB/s] 39%|███▉      | 151M/383M [01:06<02:27, 1.66MB/s] 39%|███▉      | 151M/383M [01:06<02:35, 1.57MB/s] 39%|███▉      | 151M/383M [01:06<02:46, 1.46MB/s] 39%|███▉      | 151M/383M [01:06<02:53, 1.40MB/s] 40%|███▉      | 152M/383M [01:06<02:52, 1.41MB/s] 40%|███▉      | 152M/383M [01:06<02:51, 1.42MB/s] 40%|███▉      | 152M/383M [01:06<03:09, 1.28MB/s] 40%|███▉      | 152M/383M [01:07<03:07, 1.29MB/s] 40%|███▉      | 152M/383M [01:07<03:01, 1.34MB/s] 40%|███▉      | 152M/383M [01:07<02:53, 1.40MB/s] 40%|███▉      | 153M/383M [01:07<02:43, 1.48MB/s] 40%|███▉      | 153M/383M [01:07<02:41, 1.49MB/s] 40%|███▉      | 153M/383M [01:07<02:19, 1.74MB/s] 40%|███▉      | 153M/383M [01:07<02:15, 1.79MB/s] 40%|████      | 153M/383M [01:07<02:14, 1.79MB/s] 40%|████      | 154M/383M [01:08<01:54, 2.10MB/s] 40%|████      | 154M/383M [01:08<01:51, 2.16MB/s] 40%|████      | 154M/383M [01:08<01:54, 2.09MB/s] 40%|████      | 155M/383M [01:08<01:55, 2.07MB/s] 40%|████      | 155M/383M [01:08<01:57, 2.05MB/s] 40%|████      | 155M/383M [01:08<01:54, 2.09MB/s] 40%|████      | 155M/383M [01:08<01:55, 2.07MB/s] 41%|████      | 155M/383M [01:08<01:58, 2.01MB/s] 41%|████      | 156M/383M [01:09<01:49, 2.17MB/s] 41%|████      | 156M/383M [01:09<01:32, 2.57MB/s] 41%|████      | 156M/383M [01:09<01:29, 2.66MB/s] 41%|████      | 157M/383M [01:09<01:35, 2.48MB/s] 41%|████      | 157M/383M [01:09<01:29, 2.65MB/s] 41%|████      | 157M/383M [01:09<01:24, 2.80MB/s] 41%|████      | 158M/383M [01:09<01:17, 3.07MB/s] 41%|████      | 158M/383M [01:09<01:13, 3.19MB/s] 41%|████▏     | 158M/383M [01:09<01:16, 3.07MB/s] 41%|████▏     | 159M/383M [01:10<01:15, 3.11MB/s] 41%|████▏     | 159M/383M [01:10<01:14, 3.17MB/s] 42%|████▏     | 159M/383M [01:10<01:17, 3.03MB/s] 42%|████▏     | 160M/383M [01:10<01:19, 2.93MB/s] 42%|████▏     | 160M/383M [01:10<01:13, 3.18MB/s] 42%|████▏     | 161M/383M [01:10<01:08, 3.39MB/s] 42%|████▏     | 161M/383M [01:10<01:15, 3.11MB/s] 42%|████▏     | 161M/383M [01:10<01:19, 2.93MB/s] 42%|████▏     | 162M/383M [01:11<01:21, 2.84MB/s] 42%|████▏     | 162M/383M [01:11<01:25, 2.72MB/s] 42%|████▏     | 162M/383M [01:11<01:35, 2.42MB/s] 42%|████▏     | 162M/383M [01:11<01:41, 2.28MB/s] 42%|████▏     | 163M/383M [01:11<01:43, 2.23MB/s] 43%|████▎     | 163M/383M [01:11<01:15, 3.04MB/s] 43%|████▎     | 164M/383M [01:11<01:21, 2.81MB/s] 43%|████▎     | 164M/383M [01:11<01:08, 3.35MB/s] 43%|████▎     | 165M/383M [01:12<01:05, 3.51MB/s] 43%|████▎     | 165M/383M [01:12<01:08, 3.33MB/s] 43%|████▎     | 166M/383M [01:12<01:08, 3.34MB/s] 43%|████▎     | 166M/383M [01:12<01:06, 3.44MB/s] 43%|████▎     | 166M/383M [01:12<01:07, 3.38MB/s] 43%|████▎     | 167M/383M [01:12<01:05, 3.47MB/s] 44%|████▎     | 167M/383M [01:12<01:05, 3.46MB/s] 44%|████▎     | 167M/383M [01:12<01:04, 3.49MB/s] 44%|████▍     | 168M/383M [01:13<01:06, 3.41MB/s] 44%|████▍     | 168M/383M [01:13<01:05, 3.44MB/s] 44%|████▍     | 169M/383M [01:13<01:12, 3.09MB/s] 44%|████▍     | 169M/383M [01:13<01:20, 2.78MB/s] 44%|████▍     | 169M/383M [01:13<01:21, 2.74MB/s] 44%|████▍     | 170M/383M [01:13<01:13, 3.07MB/s] 44%|████▍     | 170M/383M [01:13<01:31, 2.45MB/s] 45%|████▍     | 171M/383M [01:14<01:06, 3.34MB/s] 45%|████▍     | 171M/383M [01:14<01:09, 3.18MB/s] 45%|████▍     | 171M/383M [01:14<01:12, 3.07MB/s] 45%|████▍     | 172M/383M [01:14<01:11, 3.10MB/s] 45%|████▍     | 172M/383M [01:14<01:16, 2.89MB/s] 45%|████▍     | 172M/383M [01:14<01:20, 2.75MB/s] 45%|████▌     | 173M/383M [01:14<01:16, 2.87MB/s] 45%|████▌     | 173M/383M [01:14<01:13, 3.01MB/s] 45%|████▌     | 173M/383M [01:15<01:10, 3.13MB/s] 45%|████▌     | 174M/383M [01:15<01:06, 3.33MB/s] 45%|████▌     | 174M/383M [01:15<01:04, 3.42MB/s] 45%|████▌     | 174M/383M [01:15<01:04, 3.42MB/s] 46%|████▌     | 175M/383M [01:15<01:12, 3.03MB/s] 46%|████▌     | 175M/383M [01:15<01:20, 2.72MB/s] 46%|████▌     | 175M/383M [01:15<01:14, 2.92MB/s] 46%|████▌     | 176M/383M [01:15<01:13, 2.96MB/s] 46%|████▌     | 176M/383M [01:16<01:13, 2.97MB/s] 46%|████▌     | 177M/383M [01:16<01:09, 3.13MB/s] 46%|████▌     | 177M/383M [01:16<01:25, 2.52MB/s] 46%|████▋     | 178M/383M [01:16<01:13, 2.92MB/s] 46%|████▋     | 178M/383M [01:16<01:13, 2.91MB/s] 46%|████▋     | 178M/383M [01:16<01:13, 2.95MB/s] 47%|████▋     | 178M/383M [01:16<01:17, 2.76MB/s] 47%|████▋     | 179M/383M [01:17<01:20, 2.65MB/s] 47%|████▋     | 179M/383M [01:17<01:18, 2.72MB/s] 47%|████▋     | 180M/383M [01:17<01:18, 2.72MB/s] 47%|████▋     | 180M/383M [01:17<01:13, 2.89MB/s] 47%|████▋     | 180M/383M [01:17<00:57, 3.69MB/s] 47%|████▋     | 181M/383M [01:17<00:58, 3.62MB/s] 47%|████▋     | 182M/383M [01:17<00:47, 4.45MB/s] 47%|████▋     | 182M/383M [01:17<00:49, 4.29MB/s] 48%|████▊     | 183M/383M [01:18<00:50, 4.18MB/s] 48%|████▊     | 183M/383M [01:18<00:50, 4.17MB/s] 48%|████▊     | 183M/383M [01:18<00:51, 4.05MB/s] 48%|████▊     | 184M/383M [01:18<00:57, 3.63MB/s] 48%|████▊     | 184M/383M [01:18<01:01, 3.39MB/s] 48%|████▊     | 184M/383M [01:18<01:04, 3.24MB/s] 48%|████▊     | 185M/383M [01:18<01:04, 3.23MB/s] 48%|████▊     | 185M/383M [01:18<01:07, 3.06MB/s] 48%|████▊     | 186M/383M [01:19<01:11, 2.89MB/s] 48%|████▊     | 186M/383M [01:19<01:37, 2.13MB/s] 49%|████▊     | 186M/383M [01:19<01:25, 2.41MB/s] 49%|████▊     | 187M/383M [01:19<01:27, 2.36MB/s] 49%|████▊     | 187M/383M [01:19<01:39, 2.06MB/s] 49%|████▉     | 187M/383M [01:19<01:50, 1.87MB/s] 49%|████▉     | 187M/383M [01:19<01:53, 1.82MB/s] 49%|████▉     | 187M/383M [01:20<02:02, 1.67MB/s] 49%|████▉     | 188M/383M [01:20<02:00, 1.70MB/s] 49%|████▉     | 188M/383M [01:20<02:04, 1.64MB/s] 49%|████▉     | 188M/383M [01:20<02:01, 1.68MB/s] 49%|████▉     | 188M/383M [01:20<02:01, 1.68MB/s] 49%|████▉     | 188M/383M [01:20<02:11, 1.55MB/s] 49%|████▉     | 189M/383M [01:20<02:08, 1.59MB/s] 49%|████▉     | 189M/383M [01:21<02:05, 1.63MB/s] 49%|████▉     | 189M/383M [01:21<02:11, 1.55MB/s] 49%|████▉     | 190M/383M [01:21<02:03, 1.65MB/s] 50%|████▉     | 190M/383M [01:21<01:48, 1.87MB/s] 50%|████▉     | 190M/383M [01:21<01:28, 2.30MB/s] 50%|████▉     | 191M/383M [01:21<01:29, 2.25MB/s] 50%|████▉     | 191M/383M [01:22<01:22, 2.46MB/s] 50%|████▉     | 192M/383M [01:22<01:02, 3.21MB/s] 50%|█████     | 192M/383M [01:22<00:56, 3.53MB/s] 50%|█████     | 193M/383M [01:22<00:49, 4.06MB/s] 51%|█████     | 194M/383M [01:22<00:36, 5.39MB/s] 51%|█████     | 194M/383M [01:22<00:34, 5.76MB/s] 51%|█████     | 195M/383M [01:22<00:43, 4.59MB/s] 51%|█████     | 195M/383M [01:23<00:51, 3.85MB/s] 51%|█████     | 196M/383M [01:23<00:55, 3.53MB/s] 51%|█████     | 196M/383M [01:23<00:59, 3.32MB/s] 51%|█████▏    | 197M/383M [01:23<01:03, 3.08MB/s] 51%|█████▏    | 197M/383M [01:23<01:00, 3.21MB/s] 52%|█████▏    | 198M/383M [01:23<00:58, 3.33MB/s] 52%|█████▏    | 198M/383M [01:23<01:01, 3.18MB/s] 52%|█████▏    | 198M/383M [01:24<01:05, 2.97MB/s] 52%|█████▏    | 199M/383M [01:24<00:58, 3.33MB/s] 52%|█████▏    | 199M/383M [01:24<01:01, 3.13MB/s] 52%|█████▏    | 200M/383M [01:24<01:00, 3.20MB/s] 52%|█████▏    | 200M/383M [01:24<00:57, 3.33MB/s] 52%|█████▏    | 201M/383M [01:24<00:52, 3.64MB/s] 52%|█████▏    | 201M/383M [01:24<00:51, 3.69MB/s] 53%|█████▎    | 202M/383M [01:24<00:46, 4.11MB/s] 53%|█████▎    | 202M/383M [01:25<00:51, 3.67MB/s] 53%|█████▎    | 202M/383M [01:25<00:50, 3.75MB/s] 53%|█████▎    | 203M/383M [01:25<01:01, 3.08MB/s] 53%|█████▎    | 203M/383M [01:25<00:56, 3.37MB/s] 53%|█████▎    | 204M/383M [01:25<00:58, 3.23MB/s] 53%|█████▎    | 204M/383M [01:25<00:46, 4.08MB/s] 53%|█████▎    | 205M/383M [01:25<00:51, 3.63MB/s] 54%|█████▎    | 205M/383M [01:26<00:50, 3.72MB/s] 54%|█████▎    | 206M/383M [01:26<00:50, 3.69MB/s] 54%|█████▎    | 206M/383M [01:26<01:03, 2.94MB/s] 54%|█████▍    | 206M/383M [01:26<01:05, 2.84MB/s] 54%|█████▍    | 206M/383M [01:26<01:13, 2.51MB/s] 54%|█████▍    | 207M/383M [01:26<01:15, 2.45MB/s] 54%|█████▍    | 207M/383M [01:26<01:13, 2.52MB/s] 54%|█████▍    | 207M/383M [01:27<01:24, 2.17MB/s] 54%|█████▍    | 208M/383M [01:27<01:30, 2.03MB/s] 54%|█████▍    | 208M/383M [01:27<01:32, 1.99MB/s] 54%|█████▍    | 208M/383M [01:27<01:37, 1.89MB/s] 54%|█████▍    | 208M/383M [01:27<01:45, 1.75MB/s] 54%|█████▍    | 208M/383M [01:27<01:39, 1.85MB/s] 54%|█████▍    | 209M/383M [01:27<01:41, 1.81MB/s] 55%|█████▍    | 209M/383M [01:28<01:44, 1.74MB/s] 55%|█████▍    | 209M/383M [01:28<01:48, 1.68MB/s] 55%|█████▍    | 209M/383M [01:28<01:51, 1.64MB/s] 55%|█████▍    | 210M/383M [01:28<01:46, 1.70MB/s] 55%|█████▍    | 210M/383M [01:28<01:44, 1.74MB/s] 55%|█████▍    | 210M/383M [01:28<01:41, 1.80MB/s] 55%|█████▍    | 210M/383M [01:28<01:33, 1.93MB/s] 55%|█████▍    | 211M/383M [01:29<01:38, 1.85MB/s] 55%|█████▌    | 211M/383M [01:29<01:32, 1.95MB/s] 55%|█████▌    | 211M/383M [01:29<01:33, 1.92MB/s] 55%|█████▌    | 212M/383M [01:29<01:27, 2.05MB/s] 55%|█████▌    | 212M/383M [01:29<01:25, 2.10MB/s] 55%|█████▌    | 212M/383M [01:29<01:21, 2.21MB/s] 55%|█████▌    | 212M/383M [01:29<01:16, 2.35MB/s] 55%|█████▌    | 213M/383M [01:29<01:15, 2.38MB/s] 56%|█████▌    | 213M/383M [01:30<01:05, 2.74MB/s] 56%|█████▌    | 213M/383M [01:30<00:56, 3.15MB/s] 56%|█████▌    | 214M/383M [01:30<00:55, 3.20MB/s] 56%|█████▌    | 214M/383M [01:30<00:51, 3.45MB/s] 56%|█████▌    | 215M/383M [01:30<00:45, 3.93MB/s] 56%|█████▌    | 215M/383M [01:30<00:47, 3.73MB/s] 56%|█████▌    | 216M/383M [01:30<00:50, 3.46MB/s] 56%|█████▋    | 216M/383M [01:30<01:00, 2.90MB/s] 56%|█████▋    | 216M/383M [01:31<01:00, 2.91MB/s] 56%|█████▋    | 216M/383M [01:31<01:06, 2.64MB/s] 57%|█████▋    | 217M/383M [01:31<01:10, 2.47MB/s] 57%|█████▋    | 217M/383M [01:31<01:11, 2.42MB/s] 57%|█████▋    | 217M/383M [01:31<01:12, 2.39MB/s] 57%|█████▋    | 217M/383M [01:31<01:13, 2.35MB/s] 57%|█████▋    | 218M/383M [01:31<01:16, 2.28MB/s] 57%|█████▋    | 218M/383M [01:31<01:18, 2.21MB/s] 57%|█████▋    | 218M/383M [01:32<01:26, 2.00MB/s] 57%|█████▋    | 218M/383M [01:32<01:27, 1.97MB/s] 57%|█████▋    | 219M/383M [01:32<01:25, 2.03MB/s] 57%|█████▋    | 219M/383M [01:32<01:30, 1.91MB/s] 57%|█████▋    | 219M/383M [01:32<01:31, 1.88MB/s] 57%|█████▋    | 219M/383M [01:32<01:27, 1.97MB/s] 57%|█████▋    | 219M/383M [01:32<01:31, 1.88MB/s] 57%|█████▋    | 220M/383M [01:32<01:26, 1.98MB/s] 57%|█████▋    | 220M/383M [01:33<01:35, 1.80MB/s] 57%|█████▋    | 220M/383M [01:33<01:37, 1.75MB/s] 57%|█████▋    | 220M/383M [01:33<01:28, 1.94MB/s] 58%|█████▊    | 220M/383M [01:33<01:25, 1.99MB/s] 58%|█████▊    | 221M/383M [01:33<01:21, 2.10MB/s] 58%|█████▊    | 221M/383M [01:33<01:21, 2.08MB/s] 58%|█████▊    | 221M/383M [01:33<01:22, 2.06MB/s] 58%|█████▊    | 222M/383M [01:33<01:03, 2.68MB/s] 58%|█████▊    | 222M/383M [01:33<00:58, 2.91MB/s] 58%|█████▊    | 222M/383M [01:33<00:57, 2.92MB/s] 58%|█████▊    | 222M/383M [01:34<00:58, 2.90MB/s] 58%|█████▊    | 223M/383M [01:34<01:02, 2.71MB/s] 58%|█████▊    | 223M/383M [01:34<00:54, 3.06MB/s] 58%|█████▊    | 224M/383M [01:34<00:51, 3.28MB/s] 58%|█████▊    | 224M/383M [01:34<00:44, 3.72MB/s] 59%|█████▊    | 225M/383M [01:34<00:39, 4.22MB/s] 59%|█████▉    | 225M/383M [01:34<00:31, 5.23MB/s] 59%|█████▉    | 226M/383M [01:34<00:30, 5.49MB/s] 59%|█████▉    | 227M/383M [01:34<00:26, 6.11MB/s] 59%|█████▉    | 228M/383M [01:35<00:24, 6.56MB/s] 60%|█████▉    | 228M/383M [01:35<00:23, 6.83MB/s] 60%|█████▉    | 229M/383M [01:35<00:30, 5.34MB/s] 60%|█████▉    | 230M/383M [01:35<00:31, 5.18MB/s] 60%|██████    | 230M/383M [01:35<00:27, 5.79MB/s] 60%|██████    | 231M/383M [01:35<00:25, 6.28MB/s] 60%|██████    | 232M/383M [01:35<00:25, 6.21MB/s] 61%|██████    | 232M/383M [01:35<00:27, 5.76MB/s] 61%|██████    | 233M/383M [01:36<00:25, 6.13MB/s] 61%|██████    | 234M/383M [01:36<00:26, 5.83MB/s] 61%|██████    | 234M/383M [01:36<00:38, 4.08MB/s] 61%|██████    | 235M/383M [01:36<00:42, 3.66MB/s] 61%|██████▏   | 235M/383M [01:36<00:46, 3.37MB/s] 61%|██████▏   | 235M/383M [01:36<00:49, 3.13MB/s] 62%|██████▏   | 236M/383M [01:37<00:54, 2.86MB/s] 62%|██████▏   | 236M/383M [01:37<00:56, 2.73MB/s] 62%|██████▏   | 236M/383M [01:37<01:14, 2.06MB/s] 62%|██████▏   | 237M/383M [01:37<01:13, 2.08MB/s] 62%|██████▏   | 237M/383M [01:37<01:15, 2.04MB/s] 62%|██████▏   | 237M/383M [01:37<01:12, 2.10MB/s] 62%|██████▏   | 237M/383M [01:38<01:13, 2.09MB/s] 62%|██████▏   | 238M/383M [01:38<01:12, 2.09MB/s] 62%|██████▏   | 238M/383M [01:38<01:15, 2.01MB/s] 62%|██████▏   | 238M/383M [01:38<01:24, 1.81MB/s] 62%|██████▏   | 238M/383M [01:38<01:30, 1.67MB/s] 62%|██████▏   | 239M/383M [01:38<01:37, 1.56MB/s] 62%|██████▏   | 239M/383M [01:38<01:39, 1.53MB/s] 62%|██████▏   | 239M/383M [01:39<01:39, 1.52MB/s] 62%|██████▏   | 239M/383M [01:39<01:33, 1.63MB/s] 62%|██████▏   | 239M/383M [01:39<01:33, 1.62MB/s] 62%|██████▏   | 239M/383M [01:39<01:55, 1.31MB/s] 63%|██████▎   | 240M/383M [01:39<02:08, 1.18MB/s] 63%|██████▎   | 240M/383M [01:39<02:06, 1.19MB/s] 63%|██████▎   | 240M/383M [01:39<02:00, 1.25MB/s] 63%|██████▎   | 240M/383M [01:40<01:56, 1.29MB/s] 63%|██████▎   | 240M/383M [01:40<01:52, 1.33MB/s] 63%|██████▎   | 241M/383M [01:40<01:58, 1.26MB/s] 63%|██████▎   | 241M/383M [01:40<01:57, 1.27MB/s] 63%|██████▎   | 241M/383M [01:40<01:50, 1.35MB/s] 63%|██████▎   | 241M/383M [01:40<01:47, 1.39MB/s] 63%|██████▎   | 241M/383M [01:40<01:39, 1.49MB/s] 63%|██████▎   | 242M/383M [01:41<01:37, 1.53MB/s] 63%|██████▎   | 242M/383M [01:41<01:33, 1.59MB/s] 63%|██████▎   | 242M/383M [01:41<01:39, 1.49MB/s] 63%|██████▎   | 242M/383M [01:41<01:48, 1.37MB/s] 63%|██████▎   | 242M/383M [01:41<01:53, 1.30MB/s] 63%|██████▎   | 243M/383M [01:41<01:54, 1.28MB/s] 63%|██████▎   | 243M/383M [01:42<01:55, 1.28MB/s] 63%|██████▎   | 243M/383M [01:42<01:55, 1.28MB/s] 63%|██████▎   | 243M/383M [01:42<01:56, 1.26MB/s] 63%|██████▎   | 243M/383M [01:42<02:05, 1.17MB/s] 64%|██████▎   | 244M/383M [01:42<02:00, 1.22MB/s] 64%|██████▎   | 244M/383M [01:42<01:54, 1.27MB/s] 64%|██████▎   | 244M/383M [01:43<01:42, 1.43MB/s] 64%|██████▎   | 244M/383M [01:43<01:26, 1.69MB/s] 64%|██████▍   | 245M/383M [01:43<01:12, 1.99MB/s] 64%|██████▍   | 245M/383M [01:43<01:26, 1.68MB/s] 64%|██████▍   | 245M/383M [01:43<01:30, 1.61MB/s] 64%|██████▍   | 245M/383M [01:43<01:31, 1.59MB/s] 64%|██████▍   | 245M/383M [01:43<01:28, 1.63MB/s] 64%|██████▍   | 246M/383M [01:43<01:25, 1.69MB/s] 64%|██████▍   | 246M/383M [01:44<01:22, 1.74MB/s] 64%|██████▍   | 246M/383M [01:44<01:20, 1.79MB/s] 64%|██████▍   | 246M/383M [01:44<01:16, 1.89MB/s] 64%|██████▍   | 246M/383M [01:44<01:16, 1.88MB/s] 64%|██████▍   | 247M/383M [01:44<01:18, 1.84MB/s] 64%|██████▍   | 247M/383M [01:44<01:29, 1.60MB/s] 64%|██████▍   | 247M/383M [01:44<01:36, 1.48MB/s] 64%|██████▍   | 247M/383M [01:44<01:37, 1.47MB/s] 65%|██████▍   | 247M/383M [01:45<01:30, 1.58MB/s] 65%|██████▍   | 248M/383M [01:45<01:24, 1.69MB/s] 65%|██████▍   | 248M/383M [01:45<01:23, 1.69MB/s] 65%|██████▍   | 248M/383M [01:45<01:20, 1.75MB/s] 65%|██████▍   | 248M/383M [01:45<01:31, 1.54MB/s] 65%|██████▍   | 248M/383M [01:45<01:56, 1.22MB/s] 65%|██████▍   | 249M/383M [01:46<01:27, 1.61MB/s] 65%|██████▍   | 249M/383M [01:46<01:22, 1.70MB/s] 65%|██████▌   | 249M/383M [01:46<01:12, 1.95MB/s] 65%|██████▌   | 249M/383M [01:46<01:11, 1.95MB/s] 65%|██████▌   | 250M/383M [01:46<01:10, 1.99MB/s] 65%|██████▌   | 250M/383M [01:46<01:10, 2.00MB/s] 65%|██████▌   | 250M/383M [01:46<00:58, 2.38MB/s] 65%|██████▌   | 251M/383M [01:46<00:44, 3.11MB/s] 65%|██████▌   | 251M/383M [01:46<00:44, 3.14MB/s] 66%|██████▌   | 252M/383M [01:46<00:34, 3.98MB/s] 66%|██████▌   | 252M/383M [01:47<00:30, 4.55MB/s] 66%|██████▌   | 253M/383M [01:47<00:27, 4.96MB/s] 66%|██████▌   | 254M/383M [01:47<00:25, 5.31MB/s] 66%|██████▋   | 254M/383M [01:47<00:27, 4.95MB/s] 66%|██████▋   | 255M/383M [01:47<00:31, 4.30MB/s] 67%|██████▋   | 255M/383M [01:47<00:33, 3.96MB/s] 67%|██████▋   | 255M/383M [01:47<00:35, 3.79MB/s] 67%|██████▋   | 256M/383M [01:47<00:35, 3.79MB/s] 67%|██████▋   | 256M/383M [01:48<00:39, 3.41MB/s] 67%|██████▋   | 256M/383M [01:48<00:38, 3.42MB/s] 67%|██████▋   | 257M/383M [01:48<00:45, 2.93MB/s] 67%|██████▋   | 257M/383M [01:48<00:56, 2.34MB/s] 67%|██████▋   | 257M/383M [01:48<00:58, 2.26MB/s] 67%|██████▋   | 258M/383M [01:48<01:12, 1.81MB/s] 67%|██████▋   | 258M/383M [01:49<01:11, 1.84MB/s] 67%|██████▋   | 258M/383M [01:49<01:11, 1.84MB/s] 67%|██████▋   | 258M/383M [01:49<01:12, 1.80MB/s] 67%|██████▋   | 258M/383M [01:49<01:17, 1.70MB/s] 67%|██████▋   | 258M/383M [01:49<01:25, 1.53MB/s] 67%|██████▋   | 259M/383M [01:49<01:32, 1.41MB/s] 68%|██████▊   | 259M/383M [01:49<01:29, 1.46MB/s] 68%|██████▊   | 259M/383M [01:49<01:20, 1.62MB/s] 68%|██████▊   | 259M/383M [01:50<01:10, 1.84MB/s] 68%|██████▊   | 260M/383M [01:50<01:03, 2.04MB/s] 68%|██████▊   | 260M/383M [01:50<00:57, 2.27MB/s] 68%|██████▊   | 260M/383M [01:50<00:56, 2.30MB/s] 68%|██████▊   | 260M/383M [01:50<01:06, 1.95MB/s] 68%|██████▊   | 261M/383M [01:50<01:06, 1.92MB/s] 68%|██████▊   | 261M/383M [01:50<01:08, 1.88MB/s] 68%|██████▊   | 261M/383M [01:50<01:07, 1.91MB/s] 68%|██████▊   | 261M/383M [01:51<01:02, 2.03MB/s] 68%|██████▊   | 261M/383M [01:51<01:02, 2.04MB/s] 68%|██████▊   | 262M/383M [01:51<00:56, 2.26MB/s] 68%|██████▊   | 262M/383M [01:51<01:00, 2.11MB/s] 68%|██████▊   | 262M/383M [01:51<01:02, 2.02MB/s] 68%|██████▊   | 262M/383M [01:51<01:06, 1.89MB/s] 69%|██████▊   | 263M/383M [01:51<01:08, 1.84MB/s] 69%|██████▊   | 263M/383M [01:51<01:08, 1.83MB/s] 69%|██████▊   | 263M/383M [01:51<01:06, 1.91MB/s] 69%|██████▊   | 263M/383M [01:52<01:07, 1.87MB/s] 69%|██████▊   | 263M/383M [01:52<01:05, 1.91MB/s] 69%|██████▉   | 264M/383M [01:52<01:03, 1.99MB/s] 69%|██████▉   | 264M/383M [01:52<00:58, 2.15MB/s] 69%|██████▉   | 264M/383M [01:52<00:55, 2.25MB/s] 69%|██████▉   | 264M/383M [01:52<00:56, 2.19MB/s] 69%|██████▉   | 265M/383M [01:52<00:59, 2.10MB/s] 69%|██████▉   | 265M/383M [01:52<00:58, 2.14MB/s] 69%|██████▉   | 265M/383M [01:52<00:55, 2.24MB/s] 69%|██████▉   | 265M/383M [01:53<00:50, 2.44MB/s] 69%|██████▉   | 266M/383M [01:53<00:46, 2.65MB/s] 69%|██████▉   | 266M/383M [01:53<00:43, 2.85MB/s] 70%|██████▉   | 267M/383M [01:53<00:43, 2.82MB/s] 70%|██████▉   | 267M/383M [01:53<00:45, 2.67MB/s] 70%|██████▉   | 267M/383M [01:53<00:46, 2.63MB/s] 70%|██████▉   | 267M/383M [01:53<00:52, 2.33MB/s] 70%|██████▉   | 268M/383M [01:54<00:47, 2.55MB/s] 70%|██████▉   | 268M/383M [01:54<00:44, 2.71MB/s] 70%|███████   | 269M/383M [01:54<00:40, 2.95MB/s] 70%|███████   | 269M/383M [01:54<00:41, 2.91MB/s] 70%|███████   | 269M/383M [01:54<00:43, 2.74MB/s] 70%|███████   | 270M/383M [01:54<00:48, 2.48MB/s] 70%|███████   | 270M/383M [01:54<00:56, 2.10MB/s] 70%|███████   | 270M/383M [01:54<00:57, 2.08MB/s] 71%|███████   | 270M/383M [01:55<00:50, 2.37MB/s] 71%|███████   | 271M/383M [01:55<00:47, 2.49MB/s] 71%|███████   | 271M/383M [01:55<00:48, 2.43MB/s] 71%|███████   | 271M/383M [01:55<00:46, 2.52MB/s] 71%|███████   | 271M/383M [01:55<00:47, 2.48MB/s] 71%|███████   | 272M/383M [01:55<00:52, 2.22MB/s] 71%|███████   | 272M/383M [01:55<00:57, 2.04MB/s] 71%|███████   | 272M/383M [01:55<00:59, 1.95MB/s] 71%|███████   | 272M/383M [01:56<01:01, 1.91MB/s] 71%|███████   | 272M/383M [01:56<00:57, 2.03MB/s] 71%|███████   | 273M/383M [01:56<00:52, 2.19MB/s] 71%|███████   | 273M/383M [01:56<00:50, 2.30MB/s] 71%|███████▏  | 273M/383M [01:56<00:50, 2.28MB/s] 71%|███████▏  | 274M/383M [01:56<00:52, 2.20MB/s] 71%|███████▏  | 274M/383M [01:56<00:51, 2.22MB/s] 71%|███████▏  | 274M/383M [01:56<00:48, 2.38MB/s] 72%|███████▏  | 274M/383M [01:56<00:47, 2.39MB/s] 72%|███████▏  | 275M/383M [01:57<00:43, 2.60MB/s] 72%|███████▏  | 275M/383M [01:57<00:41, 2.71MB/s] 72%|███████▏  | 275M/383M [01:57<00:42, 2.69MB/s] 72%|███████▏  | 276M/383M [01:57<00:37, 3.01MB/s] 72%|███████▏  | 276M/383M [01:57<00:40, 2.78MB/s] 72%|███████▏  | 276M/383M [01:57<00:47, 2.35MB/s] 72%|███████▏  | 277M/383M [01:57<00:46, 2.38MB/s] 72%|███████▏  | 277M/383M [01:58<00:47, 2.34MB/s] 72%|███████▏  | 277M/383M [01:58<00:52, 2.11MB/s] 72%|███████▏  | 277M/383M [01:58<00:53, 2.09MB/s] 72%|███████▏  | 278M/383M [01:58<00:57, 1.91MB/s] 72%|███████▏  | 278M/383M [01:58<01:07, 1.64MB/s] 73%|███████▎  | 278M/383M [01:58<01:12, 1.52MB/s] 73%|███████▎  | 278M/383M [01:58<01:15, 1.47MB/s] 73%|███████▎  | 278M/383M [01:58<01:17, 1.41MB/s] 73%|███████▎  | 278M/383M [01:59<01:18, 1.40MB/s] 73%|███████▎  | 279M/383M [01:59<01:20, 1.36MB/s] 73%|███████▎  | 279M/383M [01:59<01:07, 1.63MB/s] 73%|███████▎  | 279M/383M [01:59<01:06, 1.64MB/s] 73%|███████▎  | 279M/383M [01:59<01:08, 1.60MB/s] 73%|███████▎  | 279M/383M [01:59<01:12, 1.50MB/s] 73%|███████▎  | 279M/383M [01:59<01:17, 1.40MB/s] 73%|███████▎  | 280M/383M [01:59<01:24, 1.29MB/s] 73%|███████▎  | 280M/383M [02:00<01:27, 1.24MB/s] 73%|███████▎  | 280M/383M [02:00<01:22, 1.32MB/s] 73%|███████▎  | 280M/383M [02:00<01:18, 1.39MB/s] 73%|███████▎  | 280M/383M [02:00<01:09, 1.55MB/s] 73%|███████▎  | 280M/383M [02:00<01:06, 1.61MB/s] 73%|███████▎  | 280M/383M [02:00<01:10, 1.52MB/s] 73%|███████▎  | 281M/383M [02:00<01:12, 1.48MB/s] 73%|███████▎  | 281M/383M [02:00<01:13, 1.46MB/s] 73%|███████▎  | 281M/383M [02:00<01:15, 1.41MB/s] 73%|███████▎  | 281M/383M [02:01<01:15, 1.42MB/s] 73%|███████▎  | 281M/383M [02:01<01:15, 1.42MB/s] 73%|███████▎  | 281M/383M [02:01<01:11, 1.50MB/s] 73%|███████▎  | 282M/383M [02:01<01:09, 1.53MB/s] 74%|███████▎  | 282M/383M [02:01<01:11, 1.49MB/s] 74%|███████▎  | 282M/383M [02:01<01:17, 1.38MB/s] 74%|███████▎  | 282M/383M [02:01<01:14, 1.42MB/s] 74%|███████▎  | 282M/383M [02:01<01:25, 1.24MB/s] 74%|███████▎  | 282M/383M [02:02<01:21, 1.30MB/s] 74%|███████▎  | 283M/383M [02:02<01:18, 1.35MB/s] 74%|███████▍  | 283M/383M [02:02<01:12, 1.45MB/s] 74%|███████▍  | 283M/383M [02:02<01:12, 1.45MB/s] 74%|███████▍  | 283M/383M [02:02<01:14, 1.41MB/s] 74%|███████▍  | 283M/383M [02:02<01:22, 1.27MB/s] 74%|███████▍  | 283M/383M [02:02<01:23, 1.26MB/s] 74%|███████▍  | 284M/383M [02:02<01:25, 1.22MB/s] 74%|███████▍  | 284M/383M [02:03<01:21, 1.28MB/s] 74%|███████▍  | 284M/383M [02:03<01:18, 1.33MB/s] 74%|███████▍  | 284M/383M [02:03<01:17, 1.33MB/s] 74%|███████▍  | 284M/383M [02:03<01:20, 1.30MB/s] 74%|███████▍  | 284M/383M [02:03<01:15, 1.37MB/s] 74%|███████▍  | 285M/383M [02:03<01:14, 1.39MB/s] 74%|███████▍  | 285M/383M [02:03<01:16, 1.35MB/s] 74%|███████▍  | 285M/383M [02:03<01:23, 1.24MB/s] 74%|███████▍  | 285M/383M [02:04<01:25, 1.20MB/s] 74%|███████▍  | 285M/383M [02:04<01:20, 1.29MB/s] 74%|███████▍  | 285M/383M [02:04<01:16, 1.35MB/s] 74%|███████▍  | 285M/383M [02:04<01:12, 1.42MB/s] 75%|███████▍  | 286M/383M [02:04<00:55, 1.83MB/s] 75%|███████▍  | 286M/383M [02:04<00:56, 1.81MB/s] 75%|███████▍  | 286M/383M [02:04<00:57, 1.76MB/s] 75%|███████▍  | 286M/383M [02:04<00:57, 1.77MB/s] 75%|███████▍  | 287M/383M [02:04<00:50, 2.03MB/s] 75%|███████▍  | 287M/383M [02:05<00:49, 2.05MB/s] 75%|███████▍  | 287M/383M [02:05<00:49, 2.05MB/s] 75%|███████▍  | 287M/383M [02:05<00:51, 1.96MB/s] 75%|███████▍  | 287M/383M [02:05<00:53, 1.89MB/s] 75%|███████▌  | 288M/383M [02:05<00:52, 1.90MB/s] 75%|███████▌  | 288M/383M [02:05<00:52, 1.89MB/s] 75%|███████▌  | 288M/383M [02:05<01:00, 1.66MB/s] 75%|███████▌  | 288M/383M [02:05<01:06, 1.49MB/s] 75%|███████▌  | 288M/383M [02:06<01:10, 1.42MB/s] 75%|███████▌  | 289M/383M [02:06<01:05, 1.53MB/s] 75%|███████▌  | 289M/383M [02:06<01:02, 1.59MB/s] 75%|███████▌  | 289M/383M [02:06<00:57, 1.72MB/s] 75%|███████▌  | 289M/383M [02:06<00:56, 1.76MB/s] 75%|███████▌  | 289M/383M [02:06<00:55, 1.76MB/s] 76%|███████▌  | 290M/383M [02:06<01:03, 1.54MB/s] 76%|███████▌  | 290M/383M [02:06<01:14, 1.31MB/s] 76%|███████▌  | 290M/383M [02:07<01:03, 1.54MB/s] 76%|███████▌  | 290M/383M [02:07<00:56, 1.74MB/s] 76%|███████▌  | 290M/383M [02:07<00:50, 1.92MB/s] 76%|███████▌  | 291M/383M [02:07<00:47, 2.05MB/s] 76%|███████▌  | 291M/383M [02:07<00:46, 2.08MB/s] 76%|███████▌  | 291M/383M [02:07<00:45, 2.14MB/s] 76%|███████▌  | 291M/383M [02:07<00:51, 1.87MB/s] 76%|███████▌  | 292M/383M [02:08<01:02, 1.53MB/s] 76%|███████▌  | 292M/383M [02:08<00:57, 1.67MB/s] 76%|███████▌  | 292M/383M [02:08<00:47, 2.01MB/s] 76%|███████▋  | 292M/383M [02:08<00:42, 2.24MB/s] 76%|███████▋  | 293M/383M [02:08<00:39, 2.42MB/s] 76%|███████▋  | 293M/383M [02:08<00:37, 2.49MB/s] 77%|███████▋  | 294M/383M [02:08<00:34, 2.70MB/s] 77%|███████▋  | 294M/383M [02:08<00:39, 2.40MB/s] 77%|███████▋  | 294M/383M [02:09<00:42, 2.18MB/s] 77%|███████▋  | 294M/383M [02:09<00:56, 1.64MB/s] 77%|███████▋  | 295M/383M [02:09<00:42, 2.20MB/s] 77%|███████▋  | 295M/383M [02:09<00:44, 2.07MB/s] 77%|███████▋  | 295M/383M [02:09<00:48, 1.90MB/s] 77%|███████▋  | 296M/383M [02:10<00:50, 1.81MB/s] 77%|███████▋  | 296M/383M [02:10<00:50, 1.82MB/s] 77%|███████▋  | 296M/383M [02:10<00:51, 1.78MB/s] 77%|███████▋  | 297M/383M [02:10<00:45, 2.01MB/s] 77%|███████▋  | 297M/383M [02:10<00:54, 1.66MB/s] 77%|███████▋  | 297M/383M [02:10<00:56, 1.61MB/s] 78%|███████▊  | 297M/383M [02:11<00:59, 1.52MB/s] 78%|███████▊  | 298M/383M [02:11<00:59, 1.52MB/s] 78%|███████▊  | 298M/383M [02:11<00:56, 1.59MB/s] 78%|███████▊  | 298M/383M [02:11<00:50, 1.76MB/s] 78%|███████▊  | 299M/383M [02:11<00:45, 1.93MB/s] 78%|███████▊  | 299M/383M [02:12<00:43, 2.04MB/s] 78%|███████▊  | 300M/383M [02:12<00:39, 2.20MB/s] 78%|███████▊  | 300M/383M [02:12<00:36, 2.41MB/s] 78%|███████▊  | 300M/383M [02:12<00:34, 2.53MB/s] 78%|███████▊  | 301M/383M [02:12<00:31, 2.76MB/s] 79%|███████▊  | 301M/383M [02:12<00:28, 3.05MB/s] 79%|███████▊  | 301M/383M [02:12<00:25, 3.31MB/s] 79%|███████▊  | 302M/383M [02:12<00:24, 3.43MB/s] 79%|███████▉  | 302M/383M [02:13<00:26, 3.17MB/s] 79%|███████▉  | 303M/383M [02:13<00:28, 2.93MB/s] 79%|███████▉  | 303M/383M [02:13<00:26, 3.18MB/s] 79%|███████▉  | 303M/383M [02:13<00:24, 3.36MB/s] 79%|███████▉  | 304M/383M [02:13<00:21, 3.84MB/s] 79%|███████▉  | 304M/383M [02:13<00:19, 4.16MB/s] 80%|███████▉  | 305M/383M [02:13<00:18, 4.39MB/s] 80%|███████▉  | 305M/383M [02:13<00:18, 4.40MB/s] 80%|███████▉  | 306M/383M [02:13<00:19, 4.12MB/s] 80%|███████▉  | 306M/383M [02:14<00:20, 3.99MB/s] 80%|███████▉  | 307M/383M [02:14<00:20, 3.98MB/s] 80%|████████  | 307M/383M [02:14<00:16, 4.81MB/s] 80%|████████  | 308M/383M [02:14<00:16, 4.71MB/s] 80%|████████  | 309M/383M [02:14<00:14, 5.41MB/s] 81%|████████  | 309M/383M [02:14<00:15, 4.89MB/s] 81%|████████  | 310M/383M [02:14<00:14, 5.44MB/s] 81%|████████  | 310M/383M [02:14<00:16, 4.59MB/s] 81%|████████  | 311M/383M [02:15<00:17, 4.43MB/s] 81%|████████  | 311M/383M [02:15<00:16, 4.48MB/s] 81%|████████▏ | 312M/383M [02:15<00:15, 4.82MB/s] 81%|████████▏ | 312M/383M [02:15<00:16, 4.62MB/s] 82%|████████▏ | 313M/383M [02:15<00:15, 4.76MB/s] 82%|████████▏ | 314M/383M [02:15<00:12, 5.70MB/s] 82%|████████▏ | 314M/383M [02:15<00:11, 6.39MB/s] 82%|████████▏ | 315M/383M [02:15<00:11, 6.47MB/s] 82%|████████▏ | 316M/383M [02:16<00:14, 4.94MB/s] 83%|████████▎ | 316M/383M [02:16<00:18, 3.72MB/s] 83%|████████▎ | 317M/383M [02:16<00:18, 3.85MB/s] 83%|████████▎ | 317M/383M [02:16<00:20, 3.44MB/s] 83%|████████▎ | 317M/383M [02:16<00:22, 3.13MB/s] 83%|████████▎ | 318M/383M [02:16<00:22, 3.08MB/s] 83%|████████▎ | 318M/383M [02:16<00:22, 3.04MB/s] 83%|████████▎ | 318M/383M [02:17<00:22, 3.07MB/s] 83%|████████▎ | 319M/383M [02:17<00:23, 2.91MB/s] 83%|████████▎ | 319M/383M [02:17<00:20, 3.34MB/s] 83%|████████▎ | 320M/383M [02:17<00:22, 2.92MB/s] 83%|████████▎ | 320M/383M [02:17<00:22, 2.93MB/s] 84%|████████▎ | 320M/383M [02:17<00:21, 3.09MB/s] 84%|████████▎ | 321M/383M [02:17<00:21, 3.06MB/s] 84%|████████▎ | 321M/383M [02:17<00:22, 2.90MB/s] 84%|████████▍ | 321M/383M [02:18<00:23, 2.73MB/s] 84%|████████▍ | 321M/383M [02:18<00:24, 2.60MB/s] 84%|████████▍ | 322M/383M [02:18<00:26, 2.45MB/s] 84%|████████▍ | 322M/383M [02:18<00:27, 2.36MB/s] 84%|████████▍ | 322M/383M [02:18<00:28, 2.26MB/s] 84%|████████▍ | 322M/383M [02:18<00:29, 2.18MB/s] 84%|████████▍ | 323M/383M [02:18<00:29, 2.15MB/s] 84%|████████▍ | 323M/383M [02:18<00:28, 2.22MB/s] 84%|████████▍ | 323M/383M [02:18<00:29, 2.14MB/s] 84%|████████▍ | 323M/383M [02:19<00:29, 2.17MB/s] 84%|████████▍ | 324M/383M [02:19<00:27, 2.24MB/s] 84%|████████▍ | 324M/383M [02:19<00:28, 2.19MB/s] 85%|████████▍ | 324M/383M [02:19<00:29, 2.11MB/s] 85%|████████▍ | 324M/383M [02:19<00:31, 1.98MB/s] 85%|████████▍ | 324M/383M [02:19<00:31, 1.94MB/s] 85%|████████▍ | 325M/383M [02:19<00:29, 2.07MB/s] 85%|████████▍ | 325M/383M [02:19<00:28, 2.14MB/s] 85%|████████▍ | 325M/383M [02:20<00:28, 2.14MB/s] 85%|████████▍ | 325M/383M [02:20<00:27, 2.21MB/s] 85%|████████▍ | 326M/383M [02:20<00:26, 2.25MB/s] 85%|████████▍ | 326M/383M [02:20<00:26, 2.28MB/s] 85%|████████▌ | 326M/383M [02:20<00:25, 2.36MB/s] 85%|████████▌ | 326M/383M [02:20<00:23, 2.53MB/s] 85%|████████▌ | 327M/383M [02:20<00:19, 2.97MB/s] 85%|████████▌ | 327M/383M [02:20<00:16, 3.46MB/s] 85%|████████▌ | 328M/383M [02:20<00:17, 3.25MB/s] 86%|████████▌ | 328M/383M [02:21<00:19, 3.02MB/s] 86%|████████▌ | 328M/383M [02:21<00:20, 2.83MB/s] 86%|████████▌ | 329M/383M [02:21<00:21, 2.66MB/s] 86%|████████▌ | 329M/383M [02:21<00:22, 2.56MB/s] 86%|████████▌ | 329M/383M [02:21<00:23, 2.47MB/s] 86%|████████▌ | 329M/383M [02:21<00:23, 2.45MB/s] 86%|████████▌ | 330M/383M [02:21<00:21, 2.68MB/s] 86%|████████▌ | 330M/383M [02:21<00:22, 2.44MB/s] 86%|████████▌ | 330M/383M [02:21<00:23, 2.39MB/s] 86%|████████▌ | 330M/383M [02:22<00:22, 2.41MB/s] 86%|████████▋ | 331M/383M [02:22<00:23, 2.34MB/s] 86%|████████▋ | 331M/383M [02:22<00:24, 2.27MB/s] 86%|████████▋ | 331M/383M [02:22<00:24, 2.21MB/s] 86%|████████▋ | 331M/383M [02:22<00:23, 2.32MB/s] 87%|████████▋ | 332M/383M [02:22<00:22, 2.38MB/s] 87%|████████▋ | 332M/383M [02:22<00:22, 2.44MB/s] 87%|████████▋ | 332M/383M [02:22<00:22, 2.36MB/s] 87%|████████▋ | 332M/383M [02:23<00:22, 2.33MB/s] 87%|████████▋ | 333M/383M [02:23<00:22, 2.37MB/s] 87%|████████▋ | 333M/383M [02:23<00:24, 2.11MB/s] 87%|████████▋ | 334M/383M [02:23<00:18, 2.79MB/s] 87%|████████▋ | 334M/383M [02:23<00:18, 2.79MB/s] 87%|████████▋ | 334M/383M [02:23<00:22, 2.34MB/s] 87%|████████▋ | 334M/383M [02:23<00:24, 2.08MB/s] 87%|████████▋ | 335M/383M [02:24<00:24, 2.08MB/s] 87%|████████▋ | 335M/383M [02:24<00:26, 1.94MB/s] 87%|████████▋ | 335M/383M [02:24<00:34, 1.47MB/s] 87%|████████▋ | 335M/383M [02:24<00:32, 1.53MB/s] 88%|████████▊ | 336M/383M [02:24<00:30, 1.62MB/s] 88%|████████▊ | 336M/383M [02:24<00:29, 1.67MB/s] 88%|████████▊ | 336M/383M [02:25<00:30, 1.63MB/s] 88%|████████▊ | 336M/383M [02:25<00:31, 1.55MB/s] 88%|████████▊ | 336M/383M [02:25<00:33, 1.45MB/s] 88%|████████▊ | 337M/383M [02:25<00:34, 1.44MB/s] 88%|████████▊ | 337M/383M [02:25<00:33, 1.45MB/s] 88%|████████▊ | 337M/383M [02:25<00:33, 1.48MB/s] 88%|████████▊ | 337M/383M [02:25<00:30, 1.60MB/s] 88%|████████▊ | 337M/383M [02:25<00:29, 1.61MB/s] 88%|████████▊ | 338M/383M [02:26<00:30, 1.58MB/s] 88%|████████▊ | 338M/383M [02:26<00:32, 1.48MB/s] 88%|████████▊ | 338M/383M [02:26<00:32, 1.48MB/s] 88%|████████▊ | 338M/383M [02:26<00:29, 1.59MB/s] 88%|████████▊ | 338M/383M [02:26<00:24, 1.89MB/s] 88%|████████▊ | 339M/383M [02:26<00:22, 2.08MB/s] 88%|████████▊ | 339M/383M [02:26<00:21, 2.11MB/s] 89%|████████▊ | 339M/383M [02:27<00:22, 2.09MB/s] 89%|████████▊ | 339M/383M [02:27<00:22, 2.03MB/s] 89%|████████▊ | 340M/383M [02:27<00:24, 1.85MB/s] 89%|████████▊ | 340M/383M [02:27<00:25, 1.81MB/s] 89%|████████▊ | 340M/383M [02:27<00:25, 1.78MB/s] 89%|████████▉ | 340M/383M [02:27<00:27, 1.62MB/s] 89%|████████▉ | 340M/383M [02:27<00:26, 1.70MB/s] 89%|████████▉ | 341M/383M [02:27<00:23, 1.94MB/s] 89%|████████▉ | 341M/383M [02:27<00:23, 1.92MB/s] 89%|████████▉ | 341M/383M [02:28<00:24, 1.83MB/s] 89%|████████▉ | 341M/383M [02:28<00:26, 1.69MB/s] 89%|████████▉ | 341M/383M [02:28<00:27, 1.57MB/s] 89%|████████▉ | 342M/383M [02:28<00:28, 1.56MB/s] 89%|████████▉ | 342M/383M [02:28<00:29, 1.47MB/s] 89%|████████▉ | 342M/383M [02:28<00:25, 1.72MB/s] 89%|████████▉ | 342M/383M [02:28<00:24, 1.74MB/s] 89%|████████▉ | 342M/383M [02:28<00:25, 1.71MB/s] 89%|████████▉ | 343M/383M [02:29<00:25, 1.68MB/s] 89%|████████▉ | 343M/383M [02:29<00:26, 1.59MB/s] 89%|████████▉ | 343M/383M [02:29<00:26, 1.60MB/s] 90%|████████▉ | 343M/383M [02:29<00:22, 1.88MB/s] 90%|████████▉ | 344M/383M [02:29<00:19, 2.09MB/s] 90%|████████▉ | 344M/383M [02:29<00:18, 2.29MB/s] 90%|████████▉ | 344M/383M [02:29<00:18, 2.25MB/s] 90%|████████▉ | 344M/383M [02:30<00:18, 2.17MB/s] 90%|████████▉ | 345M/383M [02:30<00:20, 1.99MB/s] 90%|████████▉ | 345M/383M [02:30<00:23, 1.73MB/s] 90%|█████████ | 345M/383M [02:30<00:24, 1.64MB/s] 90%|█████████ | 345M/383M [02:30<00:24, 1.63MB/s] 90%|█████████ | 346M/383M [02:30<00:22, 1.75MB/s] 90%|█████████ | 346M/383M [02:30<00:20, 1.88MB/s] 90%|█████████ | 346M/383M [02:30<00:20, 1.88MB/s] 90%|█████████ | 346M/383M [02:31<00:21, 1.84MB/s] 90%|█████████ | 346M/383M [02:31<00:22, 1.73MB/s] 90%|█████████ | 346M/383M [02:31<00:22, 1.71MB/s] 90%|█████████ | 347M/383M [02:31<00:23, 1.64MB/s] 90%|█████████ | 347M/383M [02:31<00:23, 1.62MB/s] 91%|█████████ | 347M/383M [02:31<00:22, 1.66MB/s] 91%|█████████ | 347M/383M [02:31<00:20, 1.81MB/s] 91%|█████████ | 347M/383M [02:31<00:19, 1.92MB/s] 91%|█████████ | 348M/383M [02:31<00:19, 1.96MB/s] 91%|█████████ | 348M/383M [02:32<00:19, 1.94MB/s] 91%|█████████ | 348M/383M [02:32<00:19, 1.87MB/s] 91%|█████████ | 348M/383M [02:32<00:19, 1.89MB/s] 91%|█████████ | 349M/383M [02:32<00:19, 1.87MB/s] 91%|█████████ | 349M/383M [02:32<00:19, 1.87MB/s] 91%|█████████ | 349M/383M [02:32<00:19, 1.86MB/s] 91%|█████████ | 349M/383M [02:32<00:18, 1.93MB/s] 91%|█████████ | 350M/383M [02:32<00:18, 1.89MB/s] 91%|█████████▏| 350M/383M [02:33<00:19, 1.81MB/s] 91%|█████████▏| 350M/383M [02:33<00:19, 1.83MB/s] 91%|█████████▏| 350M/383M [02:33<00:17, 2.01MB/s] 91%|█████████▏| 351M/383M [02:33<00:16, 2.03MB/s] 92%|█████████▏| 351M/383M [02:33<00:15, 2.25MB/s] 92%|█████████▏| 351M/383M [02:33<00:16, 2.07MB/s] 92%|█████████▏| 351M/383M [02:33<00:18, 1.86MB/s] 92%|█████████▏| 351M/383M [02:34<00:17, 1.87MB/s] 92%|█████████▏| 352M/383M [02:34<00:17, 1.87MB/s] 92%|█████████▏| 352M/383M [02:34<00:20, 1.58MB/s] 92%|█████████▏| 352M/383M [02:34<00:17, 1.85MB/s] 92%|█████████▏| 352M/383M [02:34<00:18, 1.80MB/s] 92%|█████████▏| 352M/383M [02:34<00:19, 1.65MB/s] 92%|█████████▏| 353M/383M [02:34<00:16, 1.99MB/s] 92%|█████████▏| 353M/383M [02:34<00:16, 1.93MB/s] 92%|█████████▏| 353M/383M [02:35<00:17, 1.78MB/s] 92%|█████████▏| 354M/383M [02:35<00:15, 2.00MB/s] 92%|█████████▏| 354M/383M [02:35<00:16, 1.93MB/s] 92%|█████████▏| 354M/383M [02:35<00:15, 2.00MB/s] 92%|█████████▏| 354M/383M [02:35<00:13, 2.32MB/s] 93%|█████████▎| 355M/383M [02:35<00:12, 2.41MB/s] 93%|█████████▎| 355M/383M [02:35<00:13, 2.15MB/s] 93%|█████████▎| 355M/383M [02:35<00:10, 2.73MB/s] 93%|█████████▎| 356M/383M [02:36<00:10, 2.69MB/s] 93%|█████████▎| 356M/383M [02:36<00:11, 2.59MB/s] 93%|█████████▎| 356M/383M [02:36<00:09, 3.13MB/s] 93%|█████████▎| 357M/383M [02:36<00:08, 3.44MB/s] 93%|█████████▎| 358M/383M [02:36<00:07, 3.66MB/s] 93%|█████████▎| 358M/383M [02:36<00:05, 4.57MB/s] 94%|█████████▎| 359M/383M [02:36<00:05, 5.04MB/s] 94%|█████████▍| 360M/383M [02:36<00:03, 6.52MB/s] 94%|█████████▍| 361M/383M [02:37<00:03, 6.32MB/s] 94%|█████████▍| 361M/383M [02:37<00:03, 6.20MB/s] 94%|█████████▍| 362M/383M [02:37<00:04, 5.48MB/s] 95%|█████████▍| 363M/383M [02:37<00:03, 5.99MB/s] 95%|█████████▍| 363M/383M [02:37<00:03, 6.04MB/s] 95%|█████████▌| 364M/383M [02:37<00:02, 7.52MB/s] 95%|█████████▌| 365M/383M [02:37<00:02, 8.10MB/s] 96%|█████████▌| 366M/383M [02:37<00:01, 9.14MB/s] 96%|█████████▌| 367M/383M [02:37<00:01, 8.93MB/s] 96%|█████████▌| 368M/383M [02:38<00:01, 8.92MB/s] 96%|█████████▋| 369M/383M [02:38<00:01, 7.96MB/s] 97%|█████████▋| 370M/383M [02:38<00:01, 7.11MB/s] 97%|█████████▋| 371M/383M [02:38<00:01, 6.99MB/s] 97%|█████████▋| 372M/383M [02:38<00:01, 7.65MB/s] 97%|█████████▋| 372M/383M [02:38<00:01, 8.16MB/s] 97%|█████████▋| 373M/383M [02:38<00:01, 8.22MB/s] 98%|█████████▊| 374M/383M [02:38<00:01, 8.52MB/s] 98%|█████████▊| 375M/383M [02:38<00:01, 7.32MB/s] 98%|█████████▊| 376M/383M [02:39<00:01, 6.99MB/s] 98%|█████████▊| 376M/383M [02:39<00:01, 6.74MB/s] 98%|█████████▊| 377M/383M [02:39<00:01, 6.38MB/s] 99%|█████████▊| 378M/383M [02:39<00:00, 7.26MB/s] 99%|█████████▉| 379M/383M [02:39<00:00, 7.89MB/s] 99%|█████████▉| 380M/383M [02:39<00:00, 8.81MB/s] 99%|█████████▉| 381M/383M [02:39<00:00, 9.65MB/s]100%|█████████▉| 382M/383M [02:39<00:00, 9.82MB/s]100%|██████████| 383M/383M [02:39<00:00, 2.51MB/s]
[32m[2023-12-21 12:33:55,336] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-1.0/model_state.pdparams[0m
[32m[2023-12-21 12:33:55,929] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:33:55.934901  2986 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:34:00,076] [ WARNING][0m - Some weights of the model checkpoint at ernie-1.0 were not used when initializing ErnieForTokenClassification: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']
- This IS expected if you are initializing ErnieForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:34:00,076] [ WARNING][0m - Some weights of ErnieForTokenClassification were not initialized from the model checkpoint at ernie-1.0 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:34:16,759] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:34:18,209] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:34:18,210] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:34:18,210] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 10, epoch: 0, batch: 9, loss: 1.53562, speed: 5.35 step/s
global step 20, epoch: 0, batch: 19, loss: 0.97692, speed: 6.24 step/s
global step 30, epoch: 0, batch: 29, loss: 0.57764, speed: 6.09 step/s
global step 40, epoch: 0, batch: 39, loss: 0.31076, speed: 6.24 step/s
global step 50, epoch: 0, batch: 49, loss: 0.18845, speed: 7.69 step/s
global step 60, epoch: 1, batch: 9, loss: 0.11065, speed: 5.31 step/s
global step 70, epoch: 1, batch: 19, loss: 0.04230, speed: 6.46 step/s
global step 80, epoch: 1, batch: 29, loss: 0.04517, speed: 6.35 step/s
global step 90, epoch: 1, batch: 39, loss: 0.02685, speed: 6.15 step/s
global step 100, epoch: 1, batch: 49, loss: 0.04233, speed: 7.53 step/s
[EVAL] Precision: 0.968439 - Recall: 0.980656 - F1: 0.974509
global step 110, epoch: 2, batch: 9, loss: 0.01468, speed: 2.54 step/s
global step 120, epoch: 2, batch: 19, loss: 0.01219, speed: 6.29 step/s
global step 130, epoch: 2, batch: 29, loss: 0.01373, speed: 6.19 step/s
global step 140, epoch: 2, batch: 39, loss: 0.01800, speed: 6.36 step/s
global step 150, epoch: 2, batch: 49, loss: 0.02784, speed: 7.72 step/s
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/train.py --max_steps 150 --device=xpu  --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 --epoch=10     --batch_size=32 --data_dir=./waybill_ie/data   >/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:34:31,650] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification'> to load '/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model'.[0m
[32m[2023-12-21 12:34:31,650] [    INFO][0m - Loading configuration file /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:34:31,651] [    INFO][0m - Loading weights file /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:34:32,025] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:34:32.034236  3069 xpu_context.cc:151] Please NOTE: xpu device: 0
[32m[2023-12-21 12:34:36,022] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForTokenClassification.
[0m
[32m[2023-12-21 12:34:36,022] [    INFO][0m - All the weights of ErnieForTokenClassification were initialized from the model checkpoint at /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieForTokenClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 12:34:37.750058  3069 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:34:43,214] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt[0m
[32m[2023-12-21 12:34:43,226] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:34:43,226] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
I1221 12:34:43.242640  3137 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:34:43.247700  3137 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:34:43.571852  3137 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:34:43.575865  3137 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:34:43.626883  3137 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:34:43.670960  3137 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:34:43.700789  3137 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:34:43.720839  3137 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:34:43.734234  3137 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:34:43.760980  3137 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[37m---    enabled FC MKL-DNN for 73 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:34:43.787019  3137 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:34:43.823200  3137 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:34:43.824823  3137 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:34:43.824841  3137 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:34:43.826237  3137 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 12:34:43.880373  3137 onednn_context.cc:81] oneDNN v3.2.1
('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')
('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')
('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区', 'A4')('公园路32号', 'A4')('赵本山', 'P')
('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')
('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区', 'A4')('锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')
('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')
('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')
('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')
('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')
('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')
('湖南省', 'A1')('娄底市', 'A2')('娄星区', 'A3')('乐坪大道48号', 'A4')('潘苹', 'P')('18500039123', 'T')
('韩泽涛', 'P')('山东省', 'A1')('威海市', 'A2')('文登区', 'A3')('横山路天润小区39号', 'A4')('18600274912', 'T')
('15712917351', 'T')('宗忆珍', 'P')('山东省', 'A1')('青岛市', 'A2')('崂山区', 'A3')('秦岭路18号', 'A4')
('程云燊', 'P')('河南省', 'A1')('商丘市', 'A2')('柘城县', 'A3')('未来大道与和谐大街交叉口', 'A4')('13366971006', 'T')
('秋庆龙', 'P')('13051428666', 'T')('台湾', 'A1')('新北市', 'A2')('深坑区', 'A3')('北深路三段265号', 'A4')
('江西省', 'A1')('萍乡市', 'A2')('湘东区', 'A3')('泉湖北路', 'A4')('18511082664', 'T')('焦博', 'P')
('邵春雨', 'P')('15611120818', 'T')('四川省', 'A1')('德阳市', 'A2')('旌阳区', 'A3')('岷江西路550号', 'A4')
('13520006391', 'T')('海南省', 'A1')('琼中黎族苗族自治县', 'A2')('澜湖路3号', 'A4')('仲崇文', 'P')
('勾雪睿', 'P')('17610370000', 'T')('黑龙江省', 'A1')('伊春市', 'A2')('乌伊岭区', 'A3')('学府路', 'A4')
('林绰刈', 'P')('湖南省', 'A1')('郴州市', 'A2')('嘉禾县', 'A3')('晋屏北路98号', 'A4')('18810210100', 'T')
('寇津铭', 'P')('云南省', 'A1')('昆明市', 'A2')('五华区', 'A3')('护国路81号云坤大厦', 'A4')('18518522228', 'T')
('宁赞辉', 'P')('13701356291', 'T')('新疆维吾尔自治区', 'A1')('伊犁哈萨克自治州', 'A2')('昭苏县', 'A3')('天马国际旅游文化广场7栋2层', 'A4')
('福建省', 'A1')('三明市', 'A2')('清流县', 'A3')('龙津镇上坪路1幢', 'A4')('邱雯嘉', 'P')('18513171209', 'T')
('宋胭哲', 'P')('13001227066', 'T')('山西省', 'A1')('运城市', 'A2')('垣曲县', 'A3')('历山镇同善村', 'A4')
('扶鑫宇', 'P')('13910650370', 'T')('山东省', 'A1')('潍坊市', 'A2')('寿光市', 'A3')('开发区', 'A4')('渤海路龙泉街764号渤海路商业步行街', 'A4')
('18511185846', 'T')('江苏省', 'A1')('南通市', 'A2')('启东市', 'A3')('恒大路1号', 'A4')('恒大海上威尼斯707栋', 'A4')('孙薇', 'P')
('18518491696', 'T')('松欣城', 'P')('江苏省', 'A1')('盐城市', 'A2')('滨海县', 'A3')('东坎镇学前路翰林苑7号', 'A4')
('童依伊', 'P')('13263130338', 'T')('贵州省', 'A1')('遵义市', 'A2')('正安县', 'A3')('凤仪街道和平街五小区43号', 'A4')
('蒋海芳', 'P')('17601682370', 'T')('山东省', 'A1')('济宁市', 'A2')('兖州区', 'A3')('建设路银座广场5号楼', 'A4')
('查嘉明', 'P')('江西省', 'A1')('宜春市', 'A2')('奉新县', 'A3')('冯川镇奉新大道', 'A4')('13720067215', 'T')
('诸信不', 'P')('浙江省', 'A1')('台州市', 'A2')('三门县', 'A3')('人民路162号', 'A4')('18613837766', 'T')
('13001190949', 'T')('湖北省', 'A1')('恩施土家族苗族自治州', 'A2')('鹤峰县', 'A3')('九峰大道8号天德大厦', 'A4')('宓至诚', 'P')
('18511727471', 'T')('汲雪扉', 'P')('湖北省', 'A1')('黄冈市', 'A2')('武穴市', 'A3')('刊江大道187号', 'A4')
('15910937534', 'T')('福建省', 'A1')('福州市', 'A2')('长乐区', 'A3')('漳港街道仙岐村5区91号一3', 'A4')('詹森炜', 'P')
('符卜鹂', 'P')('13811355487', 'T')('福建省', 'A1')('泉州市', 'A2')('永春县', 'A3')('城南大街122-124号', 'A4')
('16501000691', 'T')('湖南省', 'A1')('郴州市', 'A2')('苏仙区', 'A3')('郴江路309号', 'A4')('匡政', 'P')
('江苏省', 'A1')('无锡市', 'A2')('梁溪区', 'A3')('崇安广南路万博商业广场12号', 'A4')('家万兴', 'P')('13601204307', 'T')
('13522585345', 'T')('尹利朋', 'P')('浙江省', 'A1')('丽水市', 'A2')('庆元县', 'A3')('百山祖镇车根林学路20号', 'A4')
('鄂斯如', 'P')('13263113344', 'T')('四川省', 'A1')('巴中市', 'A2')('南江县', 'A3')('公园坝小区a栋', 'A4')
('18511887966', 'T')('淳于雅雯', 'P')('内蒙古自治区', 'A1')('鄂尔多斯市', 'A2')('伊金霍洛旗', 'A3')('伊金霍洛镇', 'A4')
('13263300708', 'T')('祁东平', 'P')('台湾', 'A1')('台北市', 'A2')('大安区', 'A3')('大安区', 'A4')('复兴南路二段41号', 'A4')
('四川省', 'A1')('广安市', 'A2')('广安区', 'A3')('建安中路183号,紧邻思源广场,图书馆', 'A4')('18510554664', 'T')('诸葛炫', 'P')
('冉家祺', 'P')('河南省', 'A1')('焦作市', 'A2')('中站区', 'A3')('焦克路1号', 'A4')('18513929212', 'T')
('湖南省', 'A1')('长沙市', 'A2')('岳麓区', 'A3')('桐梓坡西路187号', 'A4')('韶华', 'P')('18611563237', 'T')
('13269212222', 'T')('广东省', 'A1')('阳江市', 'A2')('阳东区', 'A3')('新华路52号', 'A4')('冉二铵', 'P')
('广东省', 'A1')('河源市', 'A2')('龙川县', 'A3')('老隆镇下泡水南村新龙花园祥龙阁c座第一层', 'A4')('101,102', 'T')('蒲黎', 'P')('18310786073', 'T')
('仲彤彤', 'P')('17111265088', 'T')('福建省', 'A1')('宁德市', 'A2')('福安市', 'A3')('新华中路143号', 'A4')
('18515672403', 'T')('青海省', 'A1')('海东市', 'A2')('化隆回族自治县', 'A3')('巴燕镇大什字', 'A4')('岑稳', 'P')
('18510037977', 'T')('河南省', 'A1')('安阳市', 'A2')('安阳县', 'A3')('水冶镇松涛路与一号路交口东北角', 'A4')('缪一菲', 'P')
('13366934338', 'T')('陶政吾', 'P')('广东省', 'A1')('江门市', 'A2')('开平市', 'A3')('长沙西郊路33号', 'A4')
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32 --data_dir=./waybill_ie/data > /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:34:56,464] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt[0m
[32m[2023-12-21 12:34:56,476] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:34:56,476] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
I1221 12:34:56.493029  3204 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:34:56.498150  3204 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:34:56.794337  3204 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:34:56.798305  3204 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:34:56.849496  3204 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:34:56.894189  3204 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:34:56.923833  3204 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:34:56.943810  3204 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:34:56.957393  3204 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:34:56.983944  3204 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[37m---    enabled FC MKL-DNN for 73 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:34:57.010311  3204 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:34:57.046692  3204 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:34:57.048287  3204 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:34:57.048305  3204 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:34:57.049715  3204 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 12:34:57.103708  3204 onednn_context.cc:81] oneDNN v3.2.1
('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')
('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')
('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区', 'A4')('公园路32号', 'A4')('赵本山', 'P')
('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')
('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区', 'A4')('锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')
('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')
('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')
('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')
('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')
('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')
('湖南省', 'A1')('娄底市', 'A2')('娄星区', 'A3')('乐坪大道48号', 'A4')('潘苹', 'P')('18500039123', 'T')
('韩泽涛', 'P')('山东省', 'A1')('威海市', 'A2')('文登区', 'A3')('横山路天润小区39号', 'A4')('18600274912', 'T')
('15712917351', 'T')('宗忆珍', 'P')('山东省', 'A1')('青岛市', 'A2')('崂山区', 'A3')('秦岭路18号', 'A4')
('程云燊', 'P')('河南省', 'A1')('商丘市', 'A2')('柘城县', 'A3')('未来大道与和谐大街交叉口', 'A4')('13366971006', 'T')
('秋庆龙', 'P')('13051428666', 'T')('台湾', 'A1')('新北市', 'A2')('深坑区', 'A3')('北深路三段265号', 'A4')
('江西省', 'A1')('萍乡市', 'A2')('湘东区', 'A3')('泉湖北路', 'A4')('18511082664', 'T')('焦博', 'P')
('邵春雨', 'P')('15611120818', 'T')('四川省', 'A1')('德阳市', 'A2')('旌阳区', 'A3')('岷江西路550号', 'A4')
('13520006391', 'T')('海南省', 'A1')('琼中黎族苗族自治县', 'A2')('澜湖路3号', 'A4')('仲崇文', 'P')
('勾雪睿', 'P')('17610370000', 'T')('黑龙江省', 'A1')('伊春市', 'A2')('乌伊岭区', 'A3')('学府路', 'A4')
('林绰刈', 'P')('湖南省', 'A1')('郴州市', 'A2')('嘉禾县', 'A3')('晋屏北路98号', 'A4')('18810210100', 'T')
('寇津铭', 'P')('云南省', 'A1')('昆明市', 'A2')('五华区', 'A3')('护国路81号云坤大厦', 'A4')('18518522228', 'T')
('宁赞辉', 'P')('13701356291', 'T')('新疆维吾尔自治区', 'A1')('伊犁哈萨克自治州', 'A2')('昭苏县', 'A3')('天马国际旅游文化广场7栋2层', 'A4')
('福建省', 'A1')('三明市', 'A2')('清流县', 'A3')('龙津镇上坪路1幢', 'A4')('邱雯嘉', 'P')('18513171209', 'T')
('宋胭哲', 'P')('13001227066', 'T')('山西省', 'A1')('运城市', 'A2')('垣曲县', 'A3')('历山镇同善村', 'A4')
('扶鑫宇', 'P')('13910650370', 'T')('山东省', 'A1')('潍坊市', 'A2')('寿光市', 'A3')('开发区', 'A4')('渤海路龙泉街764号渤海路商业步行街', 'A4')
('18511185846', 'T')('江苏省', 'A1')('南通市', 'A2')('启东市', 'A3')('恒大路1号', 'A4')('恒大海上威尼斯707栋', 'A4')('孙薇', 'P')
('18518491696', 'T')('松欣城', 'P')('江苏省', 'A1')('盐城市', 'A2')('滨海县', 'A3')('东坎镇学前路翰林苑7号', 'A4')
('童依伊', 'P')('13263130338', 'T')('贵州省', 'A1')('遵义市', 'A2')('正安县', 'A3')('凤仪街道和平街五小区43号', 'A4')
('蒋海芳', 'P')('17601682370', 'T')('山东省', 'A1')('济宁市', 'A2')('兖州区', 'A3')('建设路银座广场5号楼', 'A4')
('查嘉明', 'P')('江西省', 'A1')('宜春市', 'A2')('奉新县', 'A3')('冯川镇奉新大道', 'A4')('13720067215', 'T')
('诸信不', 'P')('浙江省', 'A1')('台州市', 'A2')('三门县', 'A3')('人民路162号', 'A4')('18613837766', 'T')
('13001190949', 'T')('湖北省', 'A1')('恩施土家族苗族自治州', 'A2')('鹤峰县', 'A3')('九峰大道8号天德大厦', 'A4')('宓至诚', 'P')
('18511727471', 'T')('汲雪扉', 'P')('湖北省', 'A1')('黄冈市', 'A2')('武穴市', 'A3')('刊江大道187号', 'A4')
('15910937534', 'T')('福建省', 'A1')('福州市', 'A2')('长乐区', 'A3')('漳港街道仙岐村5区91号一3', 'A4')('詹森炜', 'P')
('符卜鹂', 'P')('13811355487', 'T')('福建省', 'A1')('泉州市', 'A2')('永春县', 'A3')('城南大街122-124号', 'A4')
('16501000691', 'T')('湖南省', 'A1')('郴州市', 'A2')('苏仙区', 'A3')('郴江路309号', 'A4')('匡政', 'P')
('江苏省', 'A1')('无锡市', 'A2')('梁溪区', 'A3')('崇安广南路万博商业广场12号', 'A4')('家万兴', 'P')('13601204307', 'T')
('13522585345', 'T')('尹利朋', 'P')('浙江省', 'A1')('丽水市', 'A2')('庆元县', 'A3')('百山祖镇车根林学路20号', 'A4')
('鄂斯如', 'P')('13263113344', 'T')('四川省', 'A1')('巴中市', 'A2')('南江县', 'A3')('公园坝小区a栋', 'A4')
('18511887966', 'T')('淳于雅雯', 'P')('内蒙古自治区', 'A1')('鄂尔多斯市', 'A2')('伊金霍洛旗', 'A3')('伊金霍洛镇', 'A4')
('13263300708', 'T')('祁东平', 'P')('台湾', 'A1')('台北市', 'A2')('大安区', 'A3')('大安区', 'A4')('复兴南路二段41号', 'A4')
('四川省', 'A1')('广安市', 'A2')('广安区', 'A3')('建安中路183号,紧邻思源广场,图书馆', 'A4')('18510554664', 'T')('诸葛炫', 'P')
('冉家祺', 'P')('河南省', 'A1')('焦作市', 'A2')('中站区', 'A3')('焦克路1号', 'A4')('18513929212', 'T')
('湖南省', 'A1')('长沙市', 'A2')('岳麓区', 'A3')('桐梓坡西路187号', 'A4')('韶华', 'P')('18611563237', 'T')
('13269212222', 'T')('广东省', 'A1')('阳江市', 'A2')('阳东区', 'A3')('新华路52号', 'A4')('冉二铵', 'P')
('广东省', 'A1')('河源市', 'A2')('龙川县', 'A3')('老隆镇下泡水南村新龙花园祥龙阁c座第一层', 'A4')('101,102', 'T')('蒲黎', 'P')('18310786073', 'T')
('仲彤彤', 'P')('17111265088', 'T')('福建省', 'A1')('宁德市', 'A2')('福安市', 'A3')('新华中路143号', 'A4')
('18515672403', 'T')('青海省', 'A1')('海东市', 'A2')('化隆回族自治县', 'A3')('巴燕镇大什字', 'A4')('岑稳', 'P')
('18510037977', 'T')('河南省', 'A1')('安阳市', 'A2')('安阳县', 'A3')('水冶镇松涛路与一号路交口东北角', 'A4')('缪一菲', 'P')
('13366934338', 'T')('陶政吾', 'P')('广东省', 'A1')('江门市', 'A2')('开平市', 'A3')('长沙西郊路33号', 'A4')
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32 --data_dir=./waybill_ie/data > /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
Does not support hardware other than CPU and GPU Currently!
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:35:01,272 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 12:35:01,272 auto_parallel_config: None
LAUNCH INFO 2023-12-21 12:35:01,272 auto_tuner_json: None
LAUNCH INFO 2023-12-21 12:35:01,272 devices: 0,1
LAUNCH INFO 2023-12-21 12:35:01,272 elastic_level: -1
LAUNCH INFO 2023-12-21 12:35:01,272 elastic_timeout: 30
LAUNCH INFO 2023-12-21 12:35:01,272 enable_gpu_log: True
LAUNCH INFO 2023-12-21 12:35:01,272 gloo_port: 6767
LAUNCH INFO 2023-12-21 12:35:01,272 host: None
LAUNCH INFO 2023-12-21 12:35:01,272 ips: None
LAUNCH INFO 2023-12-21 12:35:01,272 job_id: default
LAUNCH INFO 2023-12-21 12:35:01,272 legacy: False
LAUNCH INFO 2023-12-21 12:35:01,273 log_dir: log
LAUNCH INFO 2023-12-21 12:35:01,273 log_level: INFO
LAUNCH INFO 2023-12-21 12:35:01,273 log_overwrite: False
LAUNCH INFO 2023-12-21 12:35:01,273 master: None
LAUNCH INFO 2023-12-21 12:35:01,273 max_restart: 3
LAUNCH INFO 2023-12-21 12:35:01,273 nnodes: 1
LAUNCH INFO 2023-12-21 12:35:01,273 nproc_per_node: None
LAUNCH INFO 2023-12-21 12:35:01,273 rank: -1
LAUNCH INFO 2023-12-21 12:35:01,273 run_mode: collective
LAUNCH INFO 2023-12-21 12:35:01,273 server_num: None
LAUNCH INFO 2023-12-21 12:35:01,273 servers: 
LAUNCH INFO 2023-12-21 12:35:01,273 sort_ip: False
LAUNCH INFO 2023-12-21 12:35:01,273 start_port: 6070
LAUNCH INFO 2023-12-21 12:35:01,273 trainer_num: None
LAUNCH INFO 2023-12-21 12:35:01,273 trainers: 
LAUNCH INFO 2023-12-21 12:35:01,273 training_script: ./test_tipc/ernie_information_extraction/train.py
LAUNCH INFO 2023-12-21 12:35:01,273 training_script_args: ['--max_steps', '150', '--device=xpu', '--save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1', '--epoch=10', '--batch_size=32', '--data_dir=./waybill_ie/data']
LAUNCH INFO 2023-12-21 12:35:01,273 with_gloo: 1
LAUNCH INFO 2023-12-21 12:35:01,273 --------------------------------------------------
LAUNCH INFO 2023-12-21 12:35:01,274 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 12:35:01,275 Run Pod: ewgwsj, replicas 2, status ready
LAUNCH INFO 2023-12-21 12:35:01,395 Watching Pod: ewgwsj, replicas 2, status running
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='ernie3_for_sequence_classification', logging_steps=10, seed=42, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model=None, batch_size=1, max_seq_len=512, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='ernie-3.0-base-zh', task_name='tnews', max_seq_length=512, warmup_steps=0, warmup_proportion=0.1)
[32m[2023-12-21 12:03:33,949] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt[0m
[32m[2023-12-21 12:03:33,975] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:03:33,975] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json[0m
[32m[2023-12-21 12:03:53,598] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-base-zh/model_state.pdparams[0m
[32m[2023-12-21 12:03:53,599] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-base-zh/model_state.pdparams[0m
[32m[2023-12-21 12:03:53,983] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[33m[2023-12-21 12:04:01,373] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-base-zh were not used when initializing ErnieForSequenceClassification: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']
- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:04:01,373] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-base-zh and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:10:30,852] [    INFO][0m - global step 10 / 2668000, loss: 2.949987, avg_reader_cost: 0.01549 sec, avg_batch_cost: 38.94519 sec, avg_samples: 512.00000, ips: 13.14668 words/sec,  [0m
[32m[2023-12-21 12:16:07,954] [    INFO][0m - global step 20 / 2668000, loss: 2.814317, avg_reader_cost: 0.00037 sec, avg_batch_cost: 33.71014 sec, avg_samples: 512.00000, ips: 15.18831 words/sec,  [0m
[32m[2023-12-21 12:22:12,485] [    INFO][0m - global step 30 / 2668000, loss: 2.755727, avg_reader_cost: 0.00038 sec, avg_batch_cost: 36.45302 sec, avg_samples: 512.00000, ips: 14.04548 words/sec,  [0m
[32m[2023-12-21 12:28:05,888] [    INFO][0m - global step 40 / 2668000, loss: 2.509257, avg_reader_cost: 0.00036 sec, avg_batch_cost: 35.34032 sec, avg_samples: 512.00000, ips: 14.48770 words/sec,  [0m
[32m[2023-12-21 12:34:12,697] [    INFO][0m - global step 50 / 2668000, loss: 2.714228, avg_reader_cost: 0.00039 sec, avg_batch_cost: 36.68079 sec, avg_samples: 512.00000, ips: 13.95826 words/sec,  [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 12:35:04.636642  3327 tcp_utils.cc:181] The server starts to listen on IP_ANY:44632
I1221 12:35:04.636982  3327 tcp_utils.cc:130] Successfully connected to 127.0.0.1:44632
[32m[2023-12-21 12:35:04,935] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-1.0'.[0m
[32m[2023-12-21 12:35:04,935] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt[0m
[32m[2023-12-21 12:35:04,946] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:35:04,947] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
[32m[2023-12-21 12:35:04,947] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification'> to load 'ernie-1.0'.[0m
[32m[2023-12-21 12:35:04,948] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/model_state.pdparams[0m
[32m[2023-12-21 12:35:04,948] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-1.0/model_state.pdparams[0m
[32m[2023-12-21 12:35:05,433] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:35:05.437513  3327 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:35:09,438] [ WARNING][0m - Some weights of the model checkpoint at ernie-1.0 were not used when initializing ErnieForTokenClassification: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']
- This IS expected if you are initializing ErnieForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:35:09,438] [ WARNING][0m - Some weights of ErnieForTokenClassification were not initialized from the model checkpoint at ernie-1.0 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
global step 10, epoch: 0, batch: 9, loss: 1.49256, speed: 2.70 step/s
global step 20, epoch: 0, batch: 19, loss: 0.95630, speed: 2.65 step/s
global step 30, epoch: 1, batch: 4, loss: 0.53094, speed: 2.67 step/s
global step 40, epoch: 1, batch: 14, loss: 0.27912, speed: 2.69 step/s
global step 50, epoch: 1, batch: 24, loss: 0.13862, speed: 2.82 step/s
global step 60, epoch: 2, batch: 9, loss: 0.05925, speed: 2.43 step/s
global step 70, epoch: 2, batch: 19, loss: 0.03348, speed: 2.59 step/s
global step 80, epoch: 3, batch: 4, loss: 0.02033, speed: 2.65 step/s
global step 90, epoch: 3, batch: 14, loss: 0.02247, speed: 2.61 step/s
global step 100, epoch: 3, batch: 24, loss: 0.03475, speed: 2.81 step/s
[EVAL] Precision: 0.973356 - Recall: 0.983179 - F1: 0.978243
LAUNCH INFO 2023-12-21 12:36:11,474 Pod completed
LAUNCH INFO 2023-12-21 12:36:11,474 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 12:35:04.636642  3327 tcp_utils.cc:181] The server starts to listen on IP_ANY:44632
I1221 12:35:04.636982  3327 tcp_utils.cc:130] Successfully connected to 127.0.0.1:44632
[32m[2023-12-21 12:35:04,935] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-1.0'.[0m
[32m[2023-12-21 12:35:04,935] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt[0m
[32m[2023-12-21 12:35:04,946] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:35:04,947] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
[32m[2023-12-21 12:35:04,947] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification'> to load 'ernie-1.0'.[0m
[32m[2023-12-21 12:35:04,948] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/model_state.pdparams[0m
[32m[2023-12-21 12:35:04,948] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-1.0/model_state.pdparams[0m
[32m[2023-12-21 12:35:05,433] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:35:05.437513  3327 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:35:09,438] [ WARNING][0m - Some weights of the model checkpoint at ernie-1.0 were not used when initializing ErnieForTokenClassification: ['cls.predictions.decoder_bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight']
- This IS expected if you are initializing ErnieForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:35:09,438] [ WARNING][0m - Some weights of ErnieForTokenClassification were not initialized from the model checkpoint at ernie-1.0 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
global step 10, epoch: 0, batch: 9, loss: 1.49256, speed: 2.70 step/s
global step 20, epoch: 0, batch: 19, loss: 0.95630, speed: 2.65 step/s
global step 30, epoch: 1, batch: 4, loss: 0.53094, speed: 2.67 step/s
global step 40, epoch: 1, batch: 14, loss: 0.27912, speed: 2.69 step/s
global step 50, epoch: 1, batch: 24, loss: 0.13862, speed: 2.82 step/s
global step 60, epoch: 2, batch: 9, loss: 0.05925, speed: 2.43 step/s
global step 70, epoch: 2, batch: 19, loss: 0.03348, speed: 2.59 step/s
global step 80, epoch: 3, batch: 4, loss: 0.02033, speed: 2.65 step/s
global step 90, epoch: 3, batch: 14, loss: 0.02247, speed: 2.61 step/s
global step 100, epoch: 3, batch: 24, loss: 0.03475, speed: 2.81 step/s
[EVAL] Precision: 0.973356 - Recall: 0.983179 - F1: 0.978243
[32m[2023-12-21 12:35:48,088] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:35:49,163] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:35:49,164] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:35:49,164] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 4, batch: 9, loss: 0.01433, speed: 1.77 step/s
global step 120, epoch: 4, batch: 19, loss: 0.00785, speed: 2.61 step/s
global step 130, epoch: 5, batch: 4, loss: 0.00875, speed: 2.62 step/s
global step 140, epoch: 5, batch: 14, loss: 0.02638, speed: 2.59 step/s
global step 150, epoch: 5, batch: 24, loss: 0.00780, speed: 2.80 step/s
I1221 12:36:10.369403  3361 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python -m paddle.distributed.launch --gpus=0,1 ./test_tipc/ernie_information_extraction/train.py --max_steps 150 --device=xpu --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 --epoch=10     --batch_size=32 --data_dir=./waybill_ie/data   - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:36:15,153] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForTokenClassification'> to load '/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model'.[0m
[32m[2023-12-21 12:36:15,154] [    INFO][0m - Loading configuration file /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:36:15,154] [    INFO][0m - Loading weights file /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:36:15,575] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:36:15.579556  3428 xpu_context.cc:151] Please NOTE: xpu device: 0
[32m[2023-12-21 12:36:19,712] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForTokenClassification.
[0m
[32m[2023-12-21 12:36:19,713] [    INFO][0m - All the weights of ErnieForTokenClassification were initialized from the model checkpoint at /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieForTokenClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 12:36:21.510285  3428 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:36:26,823] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt[0m
[32m[2023-12-21 12:36:26,835] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:36:26,835] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
I1221 12:36:26.851675  3496 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:36:26.856817  3496 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:36:27.124881  3496 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:36:27.128815  3496 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:36:27.178575  3496 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:36:27.220638  3496 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:36:27.249325  3496 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:36:27.268766  3496 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:36:27.281592  3496 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:36:27.306864  3496 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[37m---    enabled FC MKL-DNN for 73 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:36:27.331683  3496 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:36:27.368513  3496 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:36:27.370213  3496 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:36:27.370230  3496 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:36:27.371604  3496 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 12:36:27.423384  3496 onednn_context.cc:81] oneDNN v3.2.1
('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')
('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')
('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区', 'A4')('公园路32号', 'A4')('赵本山', 'P')
('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')
('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区', 'A4')('锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')
('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')
('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')
('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')
('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')
('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')
('湖南省', 'A1')('娄底市', 'A2')('娄星区', 'A3')('乐坪大道48号', 'A4')('潘苹', 'P')('18500039123', 'T')
('韩泽涛', 'P')('山东省', 'A1')('威海市', 'A2')('文登区', 'A3')('横山路天润小区39号', 'A4')('18600274912', 'T')
('15712917351', 'T')('宗忆珍', 'P')('山东省', 'A1')('青岛市', 'A2')('崂山区', 'A3')('秦岭路18号', 'A4')
('程云燊', 'P')('河南省', 'A1')('商丘市', 'A2')('柘城县', 'A3')('未来大道与和谐大街交叉口', 'A4')('13366971006', 'T')
('秋庆龙', 'P')('13051428666', 'T')('台湾', 'A1')('新北市', 'A2')('深坑区', 'A3')('北深路三段265号', 'A4')
('江西省', 'A1')('萍乡市', 'A2')('湘东区', 'A3')('泉湖北路', 'A4')('18511082664', 'T')('焦博', 'P')
('邵春雨', 'P')('15611120818', 'T')('四川省', 'A1')('德阳市', 'A2')('旌阳区', 'A3')('岷江西路550号', 'A4')
('13520006391', 'T')('海南省', 'A1')('琼中黎族苗族自治县', 'A2')('澜湖路3号', 'A4')('仲崇文', 'P')
('勾雪睿', 'P')('17610370000', 'T')('黑龙江省', 'A1')('伊春市', 'A2')('乌伊岭区', 'A3')('学府路', 'A4')
('林绰刈', 'P')('湖南省', 'A1')('郴州市', 'A2')('嘉禾县', 'A3')('晋屏北路98号', 'A4')('18810210100', 'T')
('寇津铭', 'P')('云南省', 'A1')('昆明市', 'A2')('五华区', 'A3')('护国路81号云坤大厦', 'A4')('18518522228', 'T')
('宁赞辉', 'P')('13701356291', 'T')('新疆维吾尔自治区', 'A1')('伊犁哈萨克自治州', 'A2')('昭苏县', 'A3')('天马国际旅游文化广场7栋2层', 'A4')
('福建省', 'A1')('三明市', 'A2')('清流县', 'A3')('龙津镇上坪路1幢', 'A4')('邱雯嘉', 'P')('18513171209', 'T')
('宋胭哲', 'P')('13001227066', 'T')('山西省', 'A1')('运城市', 'A2')('垣曲县', 'A3')('历山镇同善村', 'A4')
('扶鑫宇', 'P')('13910650370', 'T')('山东省', 'A1')('潍坊市', 'A2')('寿光市', 'A3')('开发区', 'A4')('渤海路龙泉街764号渤海路商业步行街', 'A4')
('18511185846', 'T')('江苏省', 'A1')('南通市', 'A2')('启东市', 'A3')('恒大路1号', 'A4')('恒大海上威尼斯707栋', 'A4')('孙薇', 'P')
('18518491696', 'T')('松欣城', 'P')('江苏省', 'A1')('盐城市', 'A2')('滨海县', 'A3')('东坎镇学前路翰林苑7号', 'A4')
('童依伊', 'P')('13263130338', 'T')('贵州省', 'A1')('遵义市', 'A2')('正安县', 'A3')('凤仪街道和平街五小区43号', 'A4')
('蒋海芳', 'P')('17601682370', 'T')('山东省', 'A1')('济宁市', 'A2')('兖州区', 'A3')('建设路银座广场5号楼', 'A4')
('查嘉明', 'P')('江西省', 'A1')('宜春市', 'A2')('奉新县', 'A3')('冯川镇奉新大道', 'A4')('13720067215', 'T')
('诸信不', 'P')('浙江省', 'A1')('台州市', 'A2')('三门县', 'A3')('人民路162号', 'A4')('18613837766', 'T')
('13001190949', 'T')('湖北省', 'A1')('恩施土家族苗族自治州', 'A2')('鹤峰县', 'A3')('九峰大道8号天德大厦', 'A4')('宓至诚', 'P')
('18511727471', 'T')('汲雪扉', 'P')('湖北省', 'A1')('黄冈市', 'A2')('武穴市', 'A3')('刊江大道187号', 'A4')
('15910937534', 'T')('福建省', 'A1')('福州市', 'A2')('长乐区', 'A3')('漳港街道仙岐村5区91号一3', 'A4')('詹森炜', 'P')
('符卜鹂', 'P')('13811355487', 'T')('福建省', 'A1')('泉州市', 'A2')('永春县', 'A3')('城南大街122-124号', 'A4')
('16501000691', 'T')('湖南省', 'A1')('郴州市', 'A2')('苏仙区', 'A3')('郴江路309号', 'A4')('匡政', 'P')
('江苏省', 'A1')('无锡市', 'A2')('梁溪区', 'A3')('崇安广南路万博商业广场12号', 'A4')('家万兴', 'P')('13601204307', 'T')
('13522585345', 'T')('尹利朋', 'P')('浙江省', 'A1')('丽水市', 'A2')('庆元县', 'A3')('百山祖镇车根林学路20号', 'A4')
('鄂斯如', 'P')('13263113344', 'T')('四川省', 'A1')('巴中市', 'A2')('南江县', 'A3')('公园坝小区a栋', 'A4')
('18511887966', 'T')('淳于雅雯', 'P')('内蒙古自治区', 'A1')('鄂尔多斯市', 'A2')('伊金霍洛旗', 'A3')('伊金霍洛镇', 'A4')
('13263300708', 'T')('祁东平', 'P')('台湾', 'A1')('台北市', 'A2')('大安区', 'A3')('大安区', 'A4')('复兴南路二段41号', 'A4')
('四川省', 'A1')('广安市', 'A2')('广安区', 'A3')('建安中路183号,紧邻思源广场,图书馆', 'A4')('18510554664', 'T')('诸葛炫', 'P')
('冉家祺', 'P')('河南省', 'A1')('焦作市', 'A2')('中站区', 'A3')('焦克路1号', 'A4')('18513929212', 'T')
('湖南省', 'A1')('长沙市', 'A2')('岳麓区', 'A3')('桐梓坡西路187号', 'A4')('韶华', 'P')('18611563237', 'T')
('13269212222', 'T')('广东省', 'A1')('阳江市', 'A2')('阳东区', 'A3')('新华路52号', 'A4')('冉二铵', 'P')
('广东省', 'A1')('河源市', 'A2')('龙川县', 'A3')('老隆镇下泡水南村新龙花园祥龙阁c座第一层', 'A4')('101,102', 'T')('蒲黎', 'P')('18310786073', 'T')
('仲彤彤', 'P')('17111265088', 'T')('福建省', 'A1')('宁德市', 'A2')('福安市', 'A3')('新华中路143号', 'A4')
('18515672403', 'T')('青海省', 'A1')('海东市', 'A2')('化隆回族自治县', 'A3')('巴燕镇大什字', 'A4')('岑稳', 'P')
('18510037977', 'T')('河南省', 'A1')('安阳市', 'A2')('安阳县', 'A3')('水冶镇松涛路与一号路交口东北角', 'A4')('缪一菲', 'P')
('13366934338', 'T')('陶政吾', 'P')('广东省', 'A1')('江门市', 'A2')('开平市', 'A3')('长沙西郊路33号', 'A4')
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32 --data_dir=./waybill_ie/data > /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:36:39,618] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-1.0/vocab.txt[0m
[32m[2023-12-21 12:36:39,629] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-1.0/tokenizer_config.json[0m
[32m[2023-12-21 12:36:39,629] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-1.0/special_tokens_map.json[0m
I1221 12:36:39.646242  3563 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:36:39.651599  3563 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:36:39.913065  3563 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:36:39.916923  3563 fuse_pass_base.cc:59] ---  detected 24 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:36:39.965086  3563 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:36:40.006750  3563 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:36:40.035120  3563 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:36:40.054471  3563 fuse_pass_base.cc:59] ---  detected 36 subgraphs
[37m---    fused 36 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:36:40.067250  3563 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[37m---    fused 12 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:36:40.092505  3563 fuse_pass_base.cc:59] ---  detected 73 subgraphs
[37m---    enabled FC MKL-DNN for 73 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:36:40.117321  3563 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:36:40.153124  3563 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:36:40.154745  3563 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:36:40.154768  3563 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:36:40.156610  3563 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
I1221 12:36:40.208420  3563 onednn_context.cc:81] oneDNN v3.2.1
('黑龙江省', 'A1')('双鸭山市', 'A2')('尖山区', 'A3')('八马路与东平行路交叉口北40米', 'A4')('韦业涛', 'P')('18600009172', 'T')
('广西壮族自治区', 'A1')('桂林市', 'A2')('雁山区', 'A3')('雁山镇西龙村老年活动中心', 'A4')('17610348888', 'T')('羊卓卫', 'P')
('15652864561', 'T')('河南省', 'A1')('开封市', 'A2')('顺河回族区', 'A3')('顺河区', 'A4')('公园路32号', 'A4')('赵本山', 'P')
('河北省', 'A1')('唐山市', 'A2')('玉田县', 'A3')('无终大街159号', 'A4')('18614253058', 'T')('尚汉生', 'P')
('台湾', 'A1')('台中市', 'A2')('北区', 'A3')('北区', 'A4')('锦新街18号', 'A4')('18511226708', 'T')('蓟丽', 'P')
('廖梓琪', 'P')('18514743222', 'T')('湖北省', 'A1')('宜昌市', 'A2')('长阳土家族自治县', 'A3')('贺家坪镇贺家坪村一组临河1号', 'A4')
('江苏省', 'A1')('南通市', 'A2')('海门市', 'A3')('孝威村孝威路88号', 'A4')('18611840623', 'T')('计星仪', 'P')
('17601674746', 'T')('赵春丽', 'P')('内蒙古自治区', 'A1')('乌兰察布市', 'A2')('凉城县', 'A3')('新建街', 'A4')
('云南省', 'A1')('临沧市', 'A2')('耿马傣族佤族自治县', 'A3')('鑫源路法院对面', 'A4')('许贞爱', 'P')('18510566685', 'T')
('四川省', 'A1')('成都市', 'A2')('双流区', 'A3')('东升镇北仓路196号', 'A4')('耿丕岭', 'P')('18513466161', 'T')
('湖南省', 'A1')('娄底市', 'A2')('娄星区', 'A3')('乐坪大道48号', 'A4')('潘苹', 'P')('18500039123', 'T')
('韩泽涛', 'P')('山东省', 'A1')('威海市', 'A2')('文登区', 'A3')('横山路天润小区39号', 'A4')('18600274912', 'T')
('15712917351', 'T')('宗忆珍', 'P')('山东省', 'A1')('青岛市', 'A2')('崂山区', 'A3')('秦岭路18号', 'A4')
('程云燊', 'P')('河南省', 'A1')('商丘市', 'A2')('柘城县', 'A3')('未来大道与和谐大街交叉口', 'A4')('13366971006', 'T')
('秋庆龙', 'P')('13051428666', 'T')('台湾', 'A1')('新北市', 'A2')('深坑区', 'A3')('北深路三段265号', 'A4')
('江西省', 'A1')('萍乡市', 'A2')('湘东区', 'A3')('泉湖北路', 'A4')('18511082664', 'T')('焦博', 'P')
('邵春雨', 'P')('15611120818', 'T')('四川省', 'A1')('德阳市', 'A2')('旌阳区', 'A3')('岷江西路550号', 'A4')
('13520006391', 'T')('海南省', 'A1')('琼中黎族苗族自治县', 'A2')('澜湖路3号', 'A4')('仲崇文', 'P')
('勾雪睿', 'P')('17610370000', 'T')('黑龙江省', 'A1')('伊春市', 'A2')('乌伊岭区', 'A3')('学府路', 'A4')
('林绰刈', 'P')('湖南省', 'A1')('郴州市', 'A2')('嘉禾县', 'A3')('晋屏北路98号', 'A4')('18810210100', 'T')
('寇津铭', 'P')('云南省', 'A1')('昆明市', 'A2')('五华区', 'A3')('护国路81号云坤大厦', 'A4')('18518522228', 'T')
('宁赞辉', 'P')('13701356291', 'T')('新疆维吾尔自治区', 'A1')('伊犁哈萨克自治州', 'A2')('昭苏县', 'A3')('天马国际旅游文化广场7栋2层', 'A4')
('福建省', 'A1')('三明市', 'A2')('清流县', 'A3')('龙津镇上坪路1幢', 'A4')('邱雯嘉', 'P')('18513171209', 'T')
('宋胭哲', 'P')('13001227066', 'T')('山西省', 'A1')('运城市', 'A2')('垣曲县', 'A3')('历山镇同善村', 'A4')
('扶鑫宇', 'P')('13910650370', 'T')('山东省', 'A1')('潍坊市', 'A2')('寿光市', 'A3')('开发区', 'A4')('渤海路龙泉街764号渤海路商业步行街', 'A4')
('18511185846', 'T')('江苏省', 'A1')('南通市', 'A2')('启东市', 'A3')('恒大路1号', 'A4')('恒大海上威尼斯707栋', 'A4')('孙薇', 'P')
('18518491696', 'T')('松欣城', 'P')('江苏省', 'A1')('盐城市', 'A2')('滨海县', 'A3')('东坎镇学前路翰林苑7号', 'A4')
('童依伊', 'P')('13263130338', 'T')('贵州省', 'A1')('遵义市', 'A2')('正安县', 'A3')('凤仪街道和平街五小区43号', 'A4')
('蒋海芳', 'P')('17601682370', 'T')('山东省', 'A1')('济宁市', 'A2')('兖州区', 'A3')('建设路银座广场5号楼', 'A4')
('查嘉明', 'P')('江西省', 'A1')('宜春市', 'A2')('奉新县', 'A3')('冯川镇奉新大道', 'A4')('13720067215', 'T')
('诸信不', 'P')('浙江省', 'A1')('台州市', 'A2')('三门县', 'A3')('人民路162号', 'A4')('18613837766', 'T')
('13001190949', 'T')('湖北省', 'A1')('恩施土家族苗族自治州', 'A2')('鹤峰县', 'A3')('九峰大道8号天德大厦', 'A4')('宓至诚', 'P')
('18511727471', 'T')('汲雪扉', 'P')('湖北省', 'A1')('黄冈市', 'A2')('武穴市', 'A3')('刊江大道187号', 'A4')
('15910937534', 'T')('福建省', 'A1')('福州市', 'A2')('长乐区', 'A3')('漳港街道仙岐村5区91号一3', 'A4')('詹森炜', 'P')
('符卜鹂', 'P')('13811355487', 'T')('福建省', 'A1')('泉州市', 'A2')('永春县', 'A3')('城南大街122-124号', 'A4')
('16501000691', 'T')('湖南省', 'A1')('郴州市', 'A2')('苏仙区', 'A3')('郴江路309号', 'A4')('匡政', 'P')
('江苏省', 'A1')('无锡市', 'A2')('梁溪区', 'A3')('崇安广南路万博商业广场12号', 'A4')('家万兴', 'P')('13601204307', 'T')
('13522585345', 'T')('尹利朋', 'P')('浙江省', 'A1')('丽水市', 'A2')('庆元县', 'A3')('百山祖镇车根林学路20号', 'A4')
('鄂斯如', 'P')('13263113344', 'T')('四川省', 'A1')('巴中市', 'A2')('南江县', 'A3')('公园坝小区a栋', 'A4')
('18511887966', 'T')('淳于雅雯', 'P')('内蒙古自治区', 'A1')('鄂尔多斯市', 'A2')('伊金霍洛旗', 'A3')('伊金霍洛镇', 'A4')
('13263300708', 'T')('祁东平', 'P')('台湾', 'A1')('台北市', 'A2')('大安区', 'A3')('大安区', 'A4')('复兴南路二段41号', 'A4')
('四川省', 'A1')('广安市', 'A2')('广安区', 'A3')('建安中路183号,紧邻思源广场,图书馆', 'A4')('18510554664', 'T')('诸葛炫', 'P')
('冉家祺', 'P')('河南省', 'A1')('焦作市', 'A2')('中站区', 'A3')('焦克路1号', 'A4')('18513929212', 'T')
('湖南省', 'A1')('长沙市', 'A2')('岳麓区', 'A3')('桐梓坡西路187号', 'A4')('韶华', 'P')('18611563237', 'T')
('13269212222', 'T')('广东省', 'A1')('阳江市', 'A2')('阳东区', 'A3')('新华路52号', 'A4')('冉二铵', 'P')
('广东省', 'A1')('河源市', 'A2')('龙川县', 'A3')('老隆镇下泡水南村新龙花园祥龙阁c座第一层', 'A4')('101,102', 'T')('蒲黎', 'P')('18310786073', 'T')
('仲彤彤', 'P')('17111265088', 'T')('福建省', 'A1')('宁德市', 'A2')('福安市', 'A3')('新华中路143号', 'A4')
('18515672403', 'T')('青海省', 'A1')('海东市', 'A2')('化隆回族自治县', 'A3')('巴燕镇大什字', 'A4')('岑稳', 'P')
('18510037977', 'T')('河南省', 'A1')('安阳市', 'A2')('安阳县', 'A3')('水冶镇松涛路与一号路交口东北角', 'A4')('缪一菲', 'P')
('13366934338', 'T')('陶政吾', 'P')('广东省', 'A1')('江门市', 'A2')('开平市', 'A3')('长沙西郊路33号', 'A4')
No XPU Memory Leak
[33m Run successfully with command - ernie_information_extraction - python ./test_tipc/ernie_information_extraction/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32 --data_dir=./waybill_ie/data > /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
Does not support hardware other than CPU and GPU Currently!
+ watchcat=2471
+ kill -9 2822
+ sleep 10
==END==test_tipc/configs/ernie_information_extraction/train_infer_python.txt
run.sh: line 334:  2822 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/ernie_information_extraction/train_infer_python.txt
++ date +%s
+ end=1703133413
++ echo 1703133048 1703133413
++ awk '{print $2-$1-2}'
+ time=363
test_tipc/configs/ernie_information_extraction/train_infer_python.txt spend time seconds 363
+ echo 'test_tipc/configs/ernie_information_extraction/train_infer_python.txt spend time seconds 363'
+ read config_file
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703133413
+ echo ==START==test_tipc/configs/ernie_text_cls/train_infer_python.txt
==START==test_tipc/configs/ernie_text_cls/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/ernie_text_cls/train_infer_python.txt
+ dataline='===========================train_params=========================== 
model_name:ernie_text_cls
python:python
gpu_list:0|0,1
--device:gpu|gpu
null:null
--epoch:lite_train_lite_infer=1|lite_train_whole_infer=1|whole_train_whole_infer=3
--save_dir:null
--batch_size:lite_train_lite_infer=32|lite_train_whole_infer=32|whole_train_whole_infer=32
null:null
null:model
null:null
null:null
##
trainer:norm
norm_train:./test_tipc/ernie_text_cls/train.py --max_steps 150
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
--output_path:null
--params_path:null
norm_export:./test_tipc/ernie_text_cls/export_model.py
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:ernie_text_cls
++ strs=model_name:ernie_text_cls
++ IFS=:
++ array=(${strs})
++ tmp=ernie_text_cls
++ echo ernie_text_cls
+ model_name=ernie_text_cls
+ sleep 10
+ run run_model test_tipc/configs/ernie_text_cls/train_infer_python.txt lite_train_lite_infer 3600 ernie_text_cls
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ waitfor=7200
+ command='run_model
test_tipc/configs/ernie_text_cls/train_infer_python.txt
lite_train_lite_infer
3600
ernie_text_cls'
+ commandpid=3647
+ run_model test_tipc/configs/ernie_text_cls/train_infer_python.txt lite_train_lite_infer 3600 ernie_text_cls
+ config_file=test_tipc/configs/ernie_text_cls/train_infer_python.txt
+ mode=lite_train_lite_infer
+ watchdog=3648
+ bash test_tipc/prepare.sh test_tipc/configs/ernie_text_cls/train_infer_python.txt lite_train_lite_infer
+ wait 3647
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/ernie_text_cls/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/ernie_text_cls/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
  0%|          | 0/1909 [00:00<?, ?it/s]  1%|          | 19/1909 [00:00<00:30, 62.04it/s]  2%|▏         | 35/1909 [00:00<00:22, 83.71it/s]  3%|▎         | 51/1909 [00:00<00:25, 72.24it/s]  4%|▎         | 67/1909 [00:00<00:22, 82.26it/s]  4%|▍         | 83/1909 [00:01<00:21, 83.83it/s]  5%|▌         | 99/1909 [00:01<00:21, 84.68it/s]  6%|▌         | 115/1909 [00:01<00:21, 84.45it/s]  7%|▋         | 131/1909 [00:01<00:20, 88.04it/s]  8%|▊         | 147/1909 [00:01<00:19, 92.07it/s]  9%|▊         | 163/1909 [00:01<00:17, 97.40it/s]  9%|▉         | 179/1909 [00:02<00:17, 99.93it/s] 11%|█         | 211/1909 [00:02<00:14, 114.23it/s] 12%|█▏        | 227/1909 [00:02<00:13, 121.37it/s] 14%|█▎        | 259/1909 [00:02<00:12, 127.64it/s] 15%|█▌        | 291/1909 [00:02<00:11, 135.28it/s] 16%|█▌        | 307/1909 [00:02<00:12, 130.27it/s] 18%|█▊        | 339/1909 [00:03<00:10, 148.73it/s] 19%|█▉        | 371/1909 [00:03<00:09, 165.98it/s] 21%|██        | 403/1909 [00:03<00:08, 174.05it/s] 23%|██▎       | 435/1909 [00:03<00:07, 188.54it/s] 24%|██▍       | 467/1909 [00:03<00:06, 211.61it/s] 26%|██▌       | 490/1909 [00:03<00:06, 215.33it/s] 27%|██▋       | 515/1909 [00:03<00:06, 205.36it/s] 29%|██▊       | 547/1909 [00:04<00:07, 186.14it/s] 30%|███       | 579/1909 [00:04<00:06, 190.24it/s] 31%|███▏      | 599/1909 [00:04<00:06, 190.96it/s] 33%|███▎      | 627/1909 [00:04<00:06, 184.55it/s] 35%|███▍      | 659/1909 [00:04<00:06, 186.99it/s] 37%|███▋      | 707/1909 [00:04<00:05, 205.64it/s] 39%|███▊      | 739/1909 [00:05<00:05, 206.60it/s] 40%|████      | 771/1909 [00:05<00:05, 210.47it/s] 42%|████▏     | 803/1909 [00:05<00:05, 219.63it/s] 44%|████▎     | 835/1909 [00:05<00:04, 235.92it/s] 46%|████▋     | 883/1909 [00:05<00:04, 255.66it/s] 48%|████▊     | 909/1909 [00:05<00:03, 255.80it/s] 49%|████▉     | 935/1909 [00:05<00:03, 256.65it/s] 51%|█████▏    | 979/1909 [00:05<00:03, 276.70it/s] 53%|█████▎    | 1011/1909 [00:06<00:03, 277.41it/s] 55%|█████▌    | 1058/1909 [00:06<00:03, 277.27it/s] 59%|█████▉    | 1122/1909 [00:06<00:02, 309.63it/s] 62%|██████▏   | 1186/1909 [00:06<00:02, 331.28it/s] 65%|██████▌   | 1250/1909 [00:06<00:01, 359.57it/s] 69%|██████▉   | 1314/1909 [00:06<00:01, 400.04it/s] 71%|███████   | 1355/1909 [00:06<00:01, 392.05it/s] 75%|███████▍  | 1426/1909 [00:07<00:01, 413.28it/s] 78%|███████▊  | 1490/1909 [00:07<00:00, 456.36it/s] 83%|████████▎ | 1586/1909 [00:07<00:00, 535.82it/s] 86%|████████▌ | 1641/1909 [00:07<00:00, 531.44it/s] 91%|█████████ | 1730/1909 [00:07<00:00, 621.82it/s] 95%|█████████▍| 1810/1909 [00:07<00:00, 621.56it/s] 98%|█████████▊| 1874/1909 [00:07<00:00, 595.57it/s]100%|██████████| 1909/1909 [00:07<00:00, 242.79it/s]
[32m[2023-12-21 12:37:16,917] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:37:16,918] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/config.json[0m
[32m[2023-12-21 12:37:17,637] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams[0m
[32m[2023-12-21 12:37:17,637] [    INFO][0m - Downloading ernie_3.0_medium_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams[0m
  0%|          | 0.00/313M [00:00<?, ?B/s]  0%|          | 3.00k/313M [00:00<4:48:43, 18.9kB/s]  0%|          | 19.0k/313M [00:00<1:09:29, 78.6kB/s]  0%|          | 35.0k/313M [00:00<1:10:07, 77.9kB/s]  0%|          | 67.0k/313M [00:00<44:22, 123kB/s]     0%|          | 99.0k/313M [00:00<32:54, 166kB/s]  0%|          | 131k/313M [00:00<26:57, 203kB/s]   0%|          | 179k/313M [00:00<19:40, 277kB/s]  0%|          | 211k/313M [00:01<19:33, 279kB/s]  0%|          | 259k/313M [00:01<16:38, 328kB/s]  0%|          | 323k/313M [00:01<13:43, 398kB/s]  0%|          | 419k/313M [00:01<10:29, 520kB/s]  0%|          | 499k/313M [00:01<09:42, 562kB/s]  0%|          | 595k/313M [00:01<08:17, 658kB/s]  0%|          | 675k/313M [00:01<07:47, 699kB/s]  0%|          | 745k/313M [00:01<08:27, 645kB/s]  0%|          | 810k/313M [00:02<08:34, 636kB/s]  0%|          | 873k/313M [00:02<08:54, 612kB/s]  0%|          | 979k/313M [00:02<07:47, 699kB/s]  0%|          | 1.07M/313M [00:02<06:50, 796kB/s]  0%|          | 1.19M/313M [00:02<06:22, 853kB/s]  0%|          | 1.27M/313M [00:02<06:29, 838kB/s]  0%|          | 1.36M/313M [00:02<06:35, 826kB/s]  0%|          | 1.46M/313M [00:02<06:31, 833kB/s]  0%|          | 1.55M/313M [00:02<06:17, 863kB/s]  1%|          | 1.64M/313M [00:03<06:04, 894kB/s]  1%|          | 1.78M/313M [00:03<05:12, 1.04MB/s]  1%|          | 1.88M/313M [00:03<05:14, 1.04MB/s]  1%|          | 1.98M/313M [00:03<05:42, 952kB/s]   1%|          | 2.07M/313M [00:03<06:09, 882kB/s]  1%|          | 2.17M/313M [00:03<06:44, 805kB/s]  1%|          | 2.28M/313M [00:03<06:55, 782kB/s]  1%|          | 2.39M/313M [00:04<07:30, 722kB/s]  1%|          | 2.47M/313M [00:04<07:32, 719kB/s]  1%|          | 2.54M/313M [00:04<07:30, 721kB/s]  1%|          | 2.64M/313M [00:04<07:51, 689kB/s]  1%|          | 2.72M/313M [00:04<07:33, 717kB/s]  1%|          | 2.83M/313M [00:04<08:14, 657kB/s]  1%|          | 2.90M/313M [00:04<08:20, 649kB/s]  1%|          | 2.96M/313M [00:04<08:29, 637kB/s]  1%|          | 3.02M/313M [00:05<08:32, 633kB/s]  1%|          | 3.13M/313M [00:05<07:14, 747kB/s]  1%|          | 3.22M/313M [00:05<07:41, 703kB/s]  1%|          | 3.36M/313M [00:05<06:57, 776kB/s]  1%|          | 3.50M/313M [00:05<06:40, 810kB/s]  1%|          | 3.64M/313M [00:05<07:18, 738kB/s]  1%|          | 3.82M/313M [00:06<05:56, 908kB/s]  1%|▏         | 3.91M/313M [00:06<06:38, 812kB/s]  1%|▏         | 3.99M/313M [00:06<07:23, 730kB/s]  1%|▏         | 4.07M/313M [00:06<07:26, 725kB/s]  1%|▏         | 4.14M/313M [00:06<07:16, 742kB/s]  1%|▏         | 4.22M/313M [00:06<07:12, 747kB/s]  1%|▏         | 4.29M/313M [00:06<07:19, 735kB/s]  1%|▏         | 4.36M/313M [00:06<07:33, 713kB/s]  1%|▏         | 4.44M/313M [00:06<07:31, 716kB/s]  1%|▏         | 4.57M/313M [00:07<06:36, 814kB/s]  2%|▏         | 4.71M/313M [00:07<05:35, 962kB/s]  2%|▏         | 4.80M/313M [00:07<05:35, 963kB/s]  2%|▏         | 4.92M/313M [00:07<05:09, 1.04MB/s]  2%|▏         | 5.03M/313M [00:07<05:43, 938kB/s]   2%|▏         | 5.12M/313M [00:07<05:47, 927kB/s]  2%|▏         | 5.21M/313M [00:07<05:59, 896kB/s]  2%|▏         | 5.30M/313M [00:07<06:43, 798kB/s]  2%|▏         | 5.42M/313M [00:08<05:50, 919kB/s]  2%|▏         | 5.52M/313M [00:08<06:36, 811kB/s]  2%|▏         | 5.63M/313M [00:08<06:02, 887kB/s]  2%|▏         | 5.72M/313M [00:08<06:07, 875kB/s]  2%|▏         | 5.80M/313M [00:08<06:19, 848kB/s]  2%|▏         | 5.89M/313M [00:08<08:20, 642kB/s]  2%|▏         | 5.96M/313M [00:08<08:12, 653kB/s]  2%|▏         | 6.10M/313M [00:08<06:23, 838kB/s]  2%|▏         | 6.19M/313M [00:09<07:25, 721kB/s]  2%|▏         | 6.33M/313M [00:09<07:04, 757kB/s]  2%|▏         | 6.49M/313M [00:09<06:24, 834kB/s]  2%|▏         | 6.64M/313M [00:09<05:48, 919kB/s]  2%|▏         | 6.80M/313M [00:09<05:04, 1.05MB/s]  2%|▏         | 6.97M/313M [00:09<04:43, 1.13MB/s]  2%|▏         | 7.22M/313M [00:09<03:38, 1.47MB/s]  2%|▏         | 7.49M/313M [00:10<03:15, 1.63MB/s]  2%|▏         | 7.66M/313M [00:10<03:31, 1.51MB/s]  3%|▎         | 7.83M/313M [00:10<03:47, 1.40MB/s]  3%|▎         | 8.02M/313M [00:10<04:14, 1.26MB/s]  3%|▎         | 8.14M/313M [00:10<04:14, 1.25MB/s]  3%|▎         | 8.27M/313M [00:10<04:13, 1.26MB/s]  3%|▎         | 8.39M/313M [00:11<05:46, 919kB/s]   3%|▎         | 8.49M/313M [00:11<06:03, 878kB/s]  3%|▎         | 8.59M/313M [00:11<06:24, 830kB/s]  3%|▎         | 8.67M/313M [00:11<07:07, 746kB/s]  3%|▎         | 8.77M/313M [00:11<07:33, 703kB/s]  3%|▎         | 8.86M/313M [00:11<07:38, 694kB/s]  3%|▎         | 8.96M/313M [00:11<07:34, 700kB/s]  3%|▎         | 9.05M/313M [00:12<07:42, 687kB/s]  3%|▎         | 9.16M/313M [00:12<07:25, 713kB/s]  3%|▎         | 9.25M/313M [00:12<07:47, 680kB/s]  3%|▎         | 9.36M/313M [00:12<07:54, 670kB/s]  3%|▎         | 9.46M/313M [00:12<08:08, 651kB/s]  3%|▎         | 9.57M/313M [00:12<07:49, 676kB/s]  3%|▎         | 9.66M/313M [00:13<07:51, 673kB/s]  3%|▎         | 9.77M/313M [00:13<07:40, 690kB/s]  3%|▎         | 9.88M/313M [00:13<07:28, 708kB/s]  3%|▎         | 9.97M/313M [00:13<06:58, 759kB/s]  3%|▎         | 10.1M/313M [00:13<06:45, 782kB/s]  3%|▎         | 10.2M/313M [00:13<07:47, 679kB/s]  3%|▎         | 10.3M/313M [00:14<07:48, 676kB/s]  3%|▎         | 10.5M/313M [00:14<07:44, 682kB/s]  3%|▎         | 10.6M/313M [00:14<07:30, 702kB/s]  3%|▎         | 10.7M/313M [00:14<07:03, 747kB/s]  3%|▎         | 10.9M/313M [00:14<06:25, 821kB/s]  4%|▎         | 11.0M/313M [00:14<05:59, 879kB/s]  4%|▎         | 11.2M/313M [00:15<05:18, 992kB/s]  4%|▎         | 11.4M/313M [00:15<04:49, 1.09MB/s]  4%|▎         | 11.6M/313M [00:15<04:13, 1.25MB/s]  4%|▍         | 11.8M/313M [00:15<03:52, 1.36MB/s]  4%|▍         | 12.0M/313M [00:15<03:37, 1.45MB/s]  4%|▍         | 12.2M/313M [00:15<03:19, 1.58MB/s]  4%|▍         | 12.4M/313M [00:15<03:21, 1.56MB/s]  4%|▍         | 12.6M/313M [00:15<03:18, 1.58MB/s]  4%|▍         | 12.8M/313M [00:16<03:06, 1.68MB/s]  4%|▍         | 13.0M/313M [00:16<02:50, 1.85MB/s]  4%|▍         | 13.3M/313M [00:16<02:37, 1.99MB/s]  4%|▍         | 13.7M/313M [00:16<02:10, 2.40MB/s]  5%|▍         | 14.1M/313M [00:16<01:53, 2.75MB/s]  5%|▍         | 14.4M/313M [00:16<01:56, 2.68MB/s]  5%|▍         | 14.7M/313M [00:16<01:52, 2.78MB/s]  5%|▍         | 14.9M/313M [00:16<01:55, 2.70MB/s]  5%|▍         | 15.4M/313M [00:17<01:58, 2.64MB/s]  5%|▌         | 15.6M/313M [00:17<01:56, 2.66MB/s]  5%|▌         | 15.9M/313M [00:17<01:57, 2.64MB/s]  5%|▌         | 16.2M/313M [00:17<01:56, 2.67MB/s]  5%|▌         | 16.5M/313M [00:17<02:08, 2.42MB/s]  5%|▌         | 16.7M/313M [00:17<02:16, 2.27MB/s]  5%|▌         | 17.0M/313M [00:17<02:16, 2.27MB/s]  6%|▌         | 17.2M/313M [00:17<02:34, 2.00MB/s]  6%|▌         | 17.5M/313M [00:18<02:30, 2.05MB/s]  6%|▌         | 17.7M/313M [00:18<02:40, 1.92MB/s]  6%|▌         | 17.9M/313M [00:18<02:38, 1.95MB/s]  6%|▌         | 18.1M/313M [00:18<02:59, 1.72MB/s]  6%|▌         | 18.3M/313M [00:18<03:02, 1.69MB/s]  6%|▌         | 18.5M/313M [00:18<02:50, 1.81MB/s]  6%|▌         | 18.8M/313M [00:18<02:37, 1.96MB/s]  6%|▌         | 19.0M/313M [00:18<02:38, 1.94MB/s]  6%|▌         | 19.3M/313M [00:19<02:32, 2.02MB/s]  6%|▋         | 19.6M/313M [00:19<02:25, 2.11MB/s]  6%|▋         | 19.9M/313M [00:19<02:21, 2.17MB/s]  6%|▋         | 20.2M/313M [00:19<02:14, 2.28MB/s]  7%|▋         | 20.4M/313M [00:19<02:52, 1.78MB/s]  7%|▋         | 20.7M/313M [00:19<02:47, 1.83MB/s]  7%|▋         | 20.9M/313M [00:19<03:07, 1.63MB/s]  7%|▋         | 21.1M/313M [00:20<03:12, 1.59MB/s]  7%|▋         | 21.3M/313M [00:20<03:36, 1.41MB/s]  7%|▋         | 21.5M/313M [00:20<03:39, 1.39MB/s]  7%|▋         | 21.7M/313M [00:20<03:47, 1.34MB/s]  7%|▋         | 21.9M/313M [00:20<03:34, 1.42MB/s]  7%|▋         | 22.0M/313M [00:20<03:54, 1.30MB/s]  7%|▋         | 22.2M/313M [00:21<03:49, 1.33MB/s]  7%|▋         | 22.4M/313M [00:21<03:40, 1.38MB/s]  7%|▋         | 22.6M/313M [00:21<03:33, 1.43MB/s]  7%|▋         | 22.9M/313M [00:21<03:42, 1.36MB/s]  7%|▋         | 23.1M/313M [00:21<03:39, 1.38MB/s]  7%|▋         | 23.3M/313M [00:21<03:25, 1.47MB/s]  8%|▊         | 23.6M/313M [00:22<03:28, 1.46MB/s]  8%|▊         | 23.8M/313M [00:22<03:35, 1.41MB/s]  8%|▊         | 24.0M/313M [00:22<03:26, 1.46MB/s]  8%|▊         | 24.3M/313M [00:22<03:31, 1.43MB/s]  8%|▊         | 24.5M/313M [00:22<03:27, 1.46MB/s]  8%|▊         | 24.8M/313M [00:22<03:26, 1.46MB/s]  8%|▊         | 25.0M/313M [00:23<03:16, 1.54MB/s]  8%|▊         | 25.2M/313M [00:23<03:05, 1.62MB/s]  8%|▊         | 25.4M/313M [00:23<03:20, 1.50MB/s]  8%|▊         | 25.5M/313M [00:23<03:30, 1.43MB/s]  8%|▊         | 25.8M/313M [00:23<03:19, 1.51MB/s]  8%|▊         | 26.0M/313M [00:23<03:08, 1.60MB/s]  8%|▊         | 26.2M/313M [00:23<02:59, 1.67MB/s]  8%|▊         | 26.5M/313M [00:24<03:04, 1.63MB/s]  9%|▊         | 26.7M/313M [00:24<03:06, 1.61MB/s]  9%|▊         | 27.0M/313M [00:24<03:04, 1.62MB/s]  9%|▊         | 27.2M/313M [00:24<03:06, 1.60MB/s]  9%|▉         | 27.5M/313M [00:24<03:09, 1.58MB/s]  9%|▉         | 27.7M/313M [00:24<03:11, 1.56MB/s]  9%|▉         | 27.9M/313M [00:25<03:19, 1.50MB/s]  9%|▉         | 28.2M/313M [00:25<03:19, 1.49MB/s]  9%|▉         | 28.4M/313M [00:25<03:07, 1.59MB/s]  9%|▉         | 28.6M/313M [00:25<02:53, 1.72MB/s]  9%|▉         | 28.8M/313M [00:25<03:13, 1.54MB/s]  9%|▉         | 29.0M/313M [00:25<03:34, 1.38MB/s]  9%|▉         | 29.2M/313M [00:25<03:18, 1.50MB/s]  9%|▉         | 29.4M/313M [00:25<03:00, 1.64MB/s] 10%|▉         | 29.7M/313M [00:26<02:33, 1.93MB/s] 10%|▉         | 30.1M/313M [00:26<02:03, 2.39MB/s] 10%|▉         | 30.4M/313M [00:26<02:01, 2.44MB/s] 10%|▉         | 30.7M/313M [00:26<01:55, 2.57MB/s] 10%|▉         | 31.0M/313M [00:26<01:49, 2.70MB/s] 10%|█         | 31.4M/313M [00:26<01:41, 2.91MB/s] 10%|█         | 31.9M/313M [00:26<01:26, 3.41MB/s] 10%|█         | 32.4M/313M [00:26<01:23, 3.54MB/s] 11%|█         | 32.8M/313M [00:27<01:19, 3.68MB/s] 11%|█         | 33.2M/313M [00:27<01:39, 2.95MB/s] 11%|█         | 33.5M/313M [00:27<01:50, 2.65MB/s] 11%|█         | 33.7M/313M [00:27<01:58, 2.47MB/s] 11%|█         | 34.0M/313M [00:27<02:02, 2.38MB/s] 11%|█         | 34.2M/313M [00:27<02:12, 2.20MB/s] 11%|█         | 34.5M/313M [00:27<02:17, 2.12MB/s] 11%|█         | 34.8M/313M [00:28<02:30, 1.93MB/s] 11%|█         | 35.0M/313M [00:28<02:37, 1.85MB/s] 11%|█▏        | 35.3M/313M [00:28<02:47, 1.74MB/s] 11%|█▏        | 35.6M/313M [00:28<02:45, 1.75MB/s] 11%|█▏        | 35.9M/313M [00:28<02:42, 1.79MB/s] 12%|█▏        | 36.2M/313M [00:28<02:33, 1.89MB/s] 12%|█▏        | 36.5M/313M [00:29<02:31, 1.91MB/s] 12%|█▏        | 36.8M/313M [00:29<02:35, 1.86MB/s] 12%|█▏        | 37.2M/313M [00:29<02:36, 1.84MB/s] 12%|█▏        | 37.5M/313M [00:29<02:29, 1.93MB/s] 12%|█▏        | 37.8M/313M [00:29<02:23, 2.00MB/s] 12%|█▏        | 38.2M/313M [00:30<02:18, 2.07MB/s] 12%|█▏        | 38.5M/313M [00:30<02:22, 2.02MB/s] 12%|█▏        | 38.9M/313M [00:30<02:21, 2.03MB/s] 13%|█▎        | 39.3M/313M [00:30<02:16, 2.10MB/s] 13%|█▎        | 39.7M/313M [00:30<02:06, 2.26MB/s] 13%|█▎        | 40.1M/313M [00:30<01:58, 2.42MB/s] 13%|█▎        | 40.5M/313M [00:31<01:48, 2.62MB/s] 13%|█▎        | 40.9M/313M [00:31<01:45, 2.70MB/s] 13%|█▎        | 41.4M/313M [00:31<01:52, 2.52MB/s] 13%|█▎        | 41.8M/313M [00:31<01:53, 2.50MB/s] 14%|█▎        | 42.3M/313M [00:31<01:51, 2.55MB/s] 14%|█▎        | 42.8M/313M [00:31<01:42, 2.75MB/s] 14%|█▍        | 43.3M/313M [00:32<01:27, 3.24MB/s] 14%|█▍        | 43.8M/313M [00:32<01:16, 3.69MB/s] 14%|█▍        | 44.2M/313M [00:32<01:19, 3.55MB/s] 14%|█▍        | 44.5M/313M [00:32<01:32, 3.03MB/s] 14%|█▍        | 44.9M/313M [00:32<01:42, 2.74MB/s] 15%|█▍        | 45.5M/313M [00:32<01:37, 2.88MB/s] 15%|█▍        | 46.1M/313M [00:32<01:27, 3.19MB/s] 15%|█▍        | 46.7M/313M [00:33<01:16, 3.64MB/s] 15%|█▌        | 47.4M/313M [00:33<01:07, 4.12MB/s] 15%|█▌        | 48.0M/313M [00:33<01:05, 4.20MB/s] 16%|█▌        | 48.7M/313M [00:33<00:57, 4.82MB/s] 16%|█▌        | 49.2M/313M [00:33<00:58, 4.68MB/s] 16%|█▌        | 49.6M/313M [00:33<01:10, 3.93MB/s] 16%|█▌        | 50.1M/313M [00:33<01:12, 3.79MB/s] 16%|█▋        | 50.9M/313M [00:34<01:06, 4.15MB/s] 16%|█▋        | 51.5M/313M [00:34<01:12, 3.78MB/s] 17%|█▋        | 52.4M/313M [00:34<01:01, 4.41MB/s] 17%|█▋        | 52.9M/313M [00:34<01:03, 4.28MB/s] 17%|█▋        | 53.5M/313M [00:34<01:07, 4.04MB/s] 17%|█▋        | 54.1M/313M [00:34<01:05, 4.13MB/s] 18%|█▊        | 54.7M/313M [00:34<01:00, 4.48MB/s] 18%|█▊        | 55.3M/313M [00:35<00:56, 4.77MB/s] 18%|█▊        | 56.0M/313M [00:35<00:55, 4.87MB/s] 18%|█▊        | 56.6M/313M [00:35<00:56, 4.78MB/s] 18%|█▊        | 57.3M/313M [00:35<00:58, 4.58MB/s] 19%|█▊        | 57.8M/313M [00:35<01:03, 4.22MB/s] 19%|█▊        | 58.5M/313M [00:35<00:58, 4.55MB/s] 19%|█▉        | 59.0M/313M [00:35<00:58, 4.58MB/s] 19%|█▉        | 59.5M/313M [00:36<01:01, 4.29MB/s] 19%|█▉        | 59.9M/313M [00:36<01:05, 4.06MB/s] 19%|█▉        | 60.3M/313M [00:36<01:10, 3.74MB/s] 19%|█▉        | 60.7M/313M [00:36<01:27, 3.02MB/s] 20%|█▉        | 61.0M/313M [00:36<01:43, 2.56MB/s] 20%|█▉        | 61.4M/313M [00:36<01:41, 2.61MB/s] 20%|█▉        | 62.0M/313M [00:37<01:28, 2.96MB/s] 20%|█▉        | 62.5M/313M [00:37<01:16, 3.41MB/s] 20%|██        | 63.0M/313M [00:37<01:07, 3.85MB/s] 20%|██        | 63.4M/313M [00:37<01:13, 3.55MB/s] 20%|██        | 63.9M/313M [00:37<01:16, 3.39MB/s] 21%|██        | 64.3M/313M [00:37<01:25, 3.04MB/s] 21%|██        | 64.6M/313M [00:37<01:32, 2.82MB/s] 21%|██        | 65.0M/313M [00:38<01:25, 3.05MB/s] 21%|██        | 65.4M/313M [00:38<01:41, 2.55MB/s] 21%|██        | 65.8M/313M [00:38<01:37, 2.66MB/s] 21%|██        | 66.0M/313M [00:38<02:21, 1.83MB/s] 21%|██▏       | 66.5M/313M [00:38<02:10, 1.97MB/s] 21%|██▏       | 66.7M/313M [00:39<02:16, 1.89MB/s] 21%|██▏       | 67.0M/313M [00:39<02:11, 1.96MB/s] 22%|██▏       | 67.3M/313M [00:39<02:16, 1.88MB/s] 22%|██▏       | 67.6M/313M [00:39<02:24, 1.78MB/s] 22%|██▏       | 67.9M/313M [00:39<02:13, 1.92MB/s] 22%|██▏       | 68.1M/313M [00:40<03:05, 1.38MB/s] 22%|██▏       | 68.5M/313M [00:40<02:39, 1.61MB/s] 22%|██▏       | 68.7M/313M [00:40<02:49, 1.51MB/s] 22%|██▏       | 68.9M/313M [00:40<02:56, 1.44MB/s] 22%|██▏       | 69.1M/313M [00:40<03:04, 1.39MB/s] 22%|██▏       | 69.4M/313M [00:40<03:01, 1.41MB/s] 22%|██▏       | 69.6M/313M [00:41<03:01, 1.40MB/s] 22%|██▏       | 69.8M/313M [00:41<02:38, 1.60MB/s] 22%|██▏       | 70.0M/313M [00:41<02:37, 1.62MB/s] 22%|██▏       | 70.2M/313M [00:41<03:13, 1.31MB/s] 23%|██▎       | 70.3M/313M [00:41<03:37, 1.17MB/s] 23%|██▎       | 70.6M/313M [00:41<03:18, 1.28MB/s] 23%|██▎       | 70.8M/313M [00:41<02:55, 1.45MB/s] 23%|██▎       | 70.9M/313M [00:42<03:29, 1.21MB/s] 23%|██▎       | 71.1M/313M [00:42<03:54, 1.08MB/s] 23%|██▎       | 71.3M/313M [00:42<03:08, 1.35MB/s] 23%|██▎       | 71.5M/313M [00:42<03:21, 1.25MB/s] 23%|██▎       | 71.6M/313M [00:42<03:20, 1.26MB/s] 23%|██▎       | 71.9M/313M [00:42<03:03, 1.38MB/s] 23%|██▎       | 72.1M/313M [00:43<02:46, 1.51MB/s] 23%|██▎       | 72.4M/313M [00:43<02:35, 1.62MB/s] 23%|██▎       | 72.7M/313M [00:43<02:45, 1.52MB/s] 23%|██▎       | 72.9M/313M [00:43<02:43, 1.53MB/s] 23%|██▎       | 73.2M/313M [00:43<02:41, 1.55MB/s] 24%|██▎       | 73.5M/313M [00:43<02:35, 1.61MB/s] 24%|██▎       | 73.7M/313M [00:44<02:30, 1.67MB/s] 24%|██▎       | 73.9M/313M [00:44<02:49, 1.47MB/s] 24%|██▎       | 74.2M/313M [00:44<02:28, 1.68MB/s] 24%|██▍       | 74.4M/313M [00:44<02:48, 1.49MB/s] 24%|██▍       | 74.6M/313M [00:44<03:09, 1.31MB/s] 24%|██▍       | 74.7M/313M [00:44<03:40, 1.13MB/s] 24%|██▍       | 74.9M/313M [00:45<03:29, 1.19MB/s] 24%|██▍       | 75.0M/313M [00:45<03:40, 1.13MB/s] 24%|██▍       | 75.2M/313M [00:45<03:34, 1.16MB/s] 24%|██▍       | 75.3M/313M [00:45<03:53, 1.07MB/s] 24%|██▍       | 75.4M/313M [00:45<04:02, 1.02MB/s] 24%|██▍       | 75.5M/313M [00:45<04:45, 871kB/s]  24%|██▍       | 75.6M/313M [00:45<04:49, 858kB/s] 24%|██▍       | 75.8M/313M [00:46<04:19, 956kB/s] 24%|██▍       | 75.8M/313M [00:46<05:01, 824kB/s] 24%|██▍       | 76.0M/313M [00:46<06:04, 680kB/s] 24%|██▍       | 76.1M/313M [00:46<05:45, 718kB/s] 24%|██▍       | 76.3M/313M [00:46<05:56, 696kB/s] 24%|██▍       | 76.4M/313M [00:47<05:07, 804kB/s] 24%|██▍       | 76.6M/313M [00:47<04:36, 894kB/s] 25%|██▍       | 76.7M/313M [00:47<05:22, 768kB/s] 25%|██▍       | 76.8M/313M [00:47<04:55, 838kB/s] 25%|██▍       | 77.0M/313M [00:47<04:40, 879kB/s] 25%|██▍       | 77.1M/313M [00:47<04:13, 972kB/s] 25%|██▍       | 77.3M/313M [00:48<04:12, 975kB/s] 25%|██▍       | 77.5M/313M [00:48<04:44, 866kB/s] 25%|██▍       | 77.6M/313M [00:48<04:55, 834kB/s] 25%|██▍       | 77.8M/313M [00:48<04:24, 929kB/s] 25%|██▍       | 77.9M/313M [00:48<04:41, 875kB/s] 25%|██▍       | 78.0M/313M [00:48<05:09, 795kB/s] 25%|██▌       | 78.1M/313M [00:49<04:22, 937kB/s] 25%|██▌       | 78.2M/313M [00:49<04:18, 950kB/s] 25%|██▌       | 78.3M/313M [00:49<04:47, 854kB/s] 25%|██▌       | 78.5M/313M [00:49<04:39, 877kB/s] 25%|██▌       | 78.7M/313M [00:49<04:36, 888kB/s] 25%|██▌       | 78.8M/313M [00:49<04:24, 925kB/s] 25%|██▌       | 79.0M/313M [00:50<04:35, 887kB/s] 25%|██▌       | 79.2M/313M [00:50<03:59, 1.02MB/s] 25%|██▌       | 79.3M/313M [00:50<04:05, 995kB/s]  25%|██▌       | 79.5M/313M [00:50<04:14, 958kB/s] 25%|██▌       | 79.6M/313M [00:50<04:13, 962kB/s] 26%|██▌       | 79.7M/313M [00:50<04:52, 835kB/s] 26%|██▌       | 79.8M/313M [00:51<06:06, 666kB/s] 26%|██▌       | 80.0M/313M [00:51<05:18, 766kB/s] 26%|██▌       | 80.1M/313M [00:51<05:02, 807kB/s] 26%|██▌       | 80.2M/313M [00:51<05:15, 773kB/s] 26%|██▌       | 80.2M/313M [00:51<06:06, 665kB/s] 26%|██▌       | 80.3M/313M [00:51<05:52, 692kB/s] 26%|██▌       | 80.4M/313M [00:52<06:10, 656kB/s] 26%|██▌       | 80.5M/313M [00:52<06:05, 666kB/s] 26%|██▌       | 80.6M/313M [00:52<06:22, 636kB/s] 26%|██▌       | 80.6M/313M [00:52<06:48, 595kB/s] 26%|██▌       | 80.8M/313M [00:52<06:32, 620kB/s] 26%|██▌       | 81.0M/313M [00:52<06:23, 634kB/s] 26%|██▌       | 81.1M/313M [00:53<05:43, 705kB/s] 26%|██▌       | 81.3M/313M [00:53<05:46, 700kB/s] 26%|██▌       | 81.4M/313M [00:53<05:10, 781kB/s] 26%|██▌       | 81.5M/313M [00:53<05:15, 769kB/s] 26%|██▌       | 81.6M/313M [00:53<05:12, 775kB/s] 26%|██▌       | 81.8M/313M [00:53<04:41, 859kB/s] 26%|██▌       | 81.9M/313M [00:54<04:49, 834kB/s] 26%|██▌       | 82.0M/313M [00:54<04:46, 844kB/s] 26%|██▋       | 82.1M/313M [00:54<04:29, 896kB/s] 26%|██▋       | 82.2M/313M [00:54<04:46, 843kB/s] 26%|██▋       | 82.3M/313M [00:54<04:27, 903kB/s] 26%|██▋       | 82.5M/313M [00:54<03:57, 1.02MB/s] 26%|██▋       | 82.7M/313M [00:54<03:26, 1.17MB/s] 27%|██▋       | 82.8M/313M [00:54<02:59, 1.34MB/s] 27%|██▋       | 83.0M/313M [00:55<02:44, 1.46MB/s] 27%|██▋       | 83.2M/313M [00:55<02:36, 1.54MB/s] 27%|██▋       | 83.4M/313M [00:55<02:34, 1.56MB/s] 27%|██▋       | 83.5M/313M [00:55<02:36, 1.53MB/s] 27%|██▋       | 83.7M/313M [00:55<02:31, 1.59MB/s] 27%|██▋       | 83.9M/313M [00:55<02:33, 1.56MB/s] 27%|██▋       | 84.0M/313M [00:55<02:35, 1.54MB/s] 27%|██▋       | 84.2M/313M [00:55<02:16, 1.75MB/s] 27%|██▋       | 84.5M/313M [00:55<02:20, 1.71MB/s] 27%|██▋       | 84.7M/313M [00:56<02:22, 1.67MB/s] 27%|██▋       | 84.9M/313M [00:56<02:13, 1.79MB/s] 27%|██▋       | 85.0M/313M [00:56<02:13, 1.79MB/s] 27%|██▋       | 85.2M/313M [00:56<02:19, 1.71MB/s] 27%|██▋       | 85.4M/313M [00:56<02:30, 1.58MB/s] 27%|██▋       | 85.5M/313M [00:56<02:48, 1.41MB/s] 27%|██▋       | 85.7M/313M [00:56<02:51, 1.38MB/s] 27%|██▋       | 85.9M/313M [00:56<02:43, 1.45MB/s] 28%|██▊       | 86.1M/313M [00:57<02:38, 1.49MB/s] 28%|██▊       | 86.2M/313M [00:57<02:35, 1.53MB/s] 28%|██▊       | 86.4M/313M [00:57<02:36, 1.51MB/s] 28%|██▊       | 86.6M/313M [00:57<02:42, 1.46MB/s] 28%|██▊       | 86.8M/313M [00:57<02:39, 1.48MB/s] 28%|██▊       | 87.0M/313M [00:57<02:41, 1.46MB/s] 28%|██▊       | 87.1M/313M [00:57<02:39, 1.48MB/s] 28%|██▊       | 87.3M/313M [00:57<02:31, 1.56MB/s] 28%|██▊       | 87.5M/313M [00:58<02:22, 1.65MB/s] 28%|██▊       | 87.8M/313M [00:58<01:58, 1.99MB/s] 28%|██▊       | 88.0M/313M [00:58<02:05, 1.87MB/s] 28%|██▊       | 88.2M/313M [00:58<02:39, 1.47MB/s] 28%|██▊       | 88.4M/313M [00:58<02:35, 1.51MB/s] 28%|██▊       | 88.6M/313M [00:58<02:17, 1.70MB/s] 28%|██▊       | 88.8M/313M [00:58<02:40, 1.46MB/s] 28%|██▊       | 88.9M/313M [00:58<02:42, 1.44MB/s] 29%|██▊       | 89.1M/313M [00:59<02:48, 1.39MB/s] 29%|██▊       | 89.2M/313M [00:59<02:50, 1.37MB/s] 29%|██▊       | 89.3M/313M [00:59<02:48, 1.39MB/s] 29%|██▊       | 89.7M/313M [00:59<02:26, 1.59MB/s] 29%|██▉       | 90.0M/313M [00:59<02:07, 1.82MB/s] 29%|██▉       | 90.1M/313M [00:59<02:08, 1.82MB/s] 29%|██▉       | 90.3M/313M [00:59<02:10, 1.79MB/s] 29%|██▉       | 90.5M/313M [00:59<02:15, 1.71MB/s] 29%|██▉       | 90.6M/313M [01:00<02:26, 1.59MB/s] 29%|██▉       | 90.8M/313M [01:00<02:42, 1.43MB/s] 29%|██▉       | 91.0M/313M [01:00<02:59, 1.29MB/s] 29%|██▉       | 91.1M/313M [01:00<02:58, 1.30MB/s] 29%|██▉       | 91.3M/313M [01:00<02:52, 1.34MB/s] 29%|██▉       | 91.5M/313M [01:00<02:43, 1.42MB/s] 29%|██▉       | 91.8M/313M [01:00<02:03, 1.88MB/s] 29%|██▉       | 92.0M/313M [01:01<02:17, 1.68MB/s] 29%|██▉       | 92.2M/313M [01:01<02:18, 1.67MB/s] 30%|██▉       | 92.4M/313M [01:01<02:19, 1.65MB/s] 30%|██▉       | 92.5M/313M [01:01<02:21, 1.62MB/s] 30%|██▉       | 92.7M/313M [01:01<02:19, 1.65MB/s] 30%|██▉       | 92.9M/313M [01:01<02:17, 1.67MB/s] 30%|██▉       | 93.1M/313M [01:01<02:27, 1.56MB/s] 30%|██▉       | 93.3M/313M [01:01<02:26, 1.57MB/s] 30%|██▉       | 93.5M/313M [01:01<02:21, 1.62MB/s] 30%|██▉       | 93.6M/313M [01:02<02:33, 1.50MB/s] 30%|███       | 93.8M/313M [01:02<02:40, 1.43MB/s] 30%|███       | 94.0M/313M [01:02<02:47, 1.37MB/s] 30%|███       | 94.2M/313M [01:02<02:50, 1.34MB/s] 30%|███       | 94.4M/313M [01:02<02:56, 1.30MB/s] 30%|███       | 94.6M/313M [01:02<03:02, 1.25MB/s] 30%|███       | 94.8M/313M [01:03<03:10, 1.20MB/s] 30%|███       | 94.9M/313M [01:03<03:06, 1.22MB/s] 30%|███       | 95.1M/313M [01:03<03:02, 1.25MB/s] 30%|███       | 95.3M/313M [01:03<02:55, 1.30MB/s] 31%|███       | 95.5M/313M [01:03<02:47, 1.36MB/s] 31%|███       | 95.7M/313M [01:03<02:48, 1.35MB/s] 31%|███       | 95.8M/313M [01:03<02:41, 1.41MB/s] 31%|███       | 96.0M/313M [01:04<02:38, 1.43MB/s] 31%|███       | 96.2M/313M [01:04<02:39, 1.43MB/s] 31%|███       | 96.4M/313M [01:04<02:42, 1.40MB/s] 31%|███       | 96.6M/313M [01:04<02:36, 1.44MB/s] 31%|███       | 96.8M/313M [01:04<02:36, 1.44MB/s] 31%|███       | 97.0M/313M [01:04<02:48, 1.34MB/s] 31%|███       | 97.1M/313M [01:04<02:54, 1.29MB/s] 31%|███       | 97.3M/313M [01:05<02:58, 1.26MB/s] 31%|███       | 97.5M/313M [01:05<03:03, 1.23MB/s] 31%|███▏      | 97.7M/313M [01:05<03:04, 1.22MB/s] 31%|███▏      | 97.9M/313M [01:05<03:03, 1.23MB/s] 31%|███▏      | 98.1M/313M [01:05<03:04, 1.22MB/s] 31%|███▏      | 98.3M/313M [01:05<03:12, 1.17MB/s] 32%|███▏      | 98.5M/313M [01:06<02:55, 1.28MB/s] 32%|███▏      | 98.7M/313M [01:06<03:00, 1.24MB/s] 32%|███▏      | 98.9M/313M [01:06<02:51, 1.31MB/s] 32%|███▏      | 99.1M/313M [01:06<02:36, 1.43MB/s] 32%|███▏      | 99.3M/313M [01:06<02:45, 1.35MB/s] 32%|███▏      | 99.5M/313M [01:06<02:47, 1.33MB/s] 32%|███▏      | 99.7M/313M [01:06<02:50, 1.31MB/s] 32%|███▏      | 99.9M/313M [01:07<02:45, 1.34MB/s] 32%|███▏      | 100M/313M [01:07<02:51, 1.29MB/s]  32%|███▏      | 100M/313M [01:07<02:59, 1.24MB/s] 32%|███▏      | 101M/313M [01:07<02:59, 1.24MB/s] 32%|███▏      | 101M/313M [01:07<02:57, 1.25MB/s] 32%|███▏      | 101M/313M [01:08<02:40, 1.38MB/s] 32%|███▏      | 101M/313M [01:08<02:33, 1.44MB/s] 32%|███▏      | 102M/313M [01:08<02:28, 1.49MB/s] 33%|███▎      | 102M/313M [01:08<02:38, 1.40MB/s] 33%|███▎      | 102M/313M [01:08<02:37, 1.40MB/s] 33%|███▎      | 102M/313M [01:09<02:32, 1.45MB/s] 33%|███▎      | 103M/313M [01:09<02:18, 1.59MB/s] 33%|███▎      | 103M/313M [01:09<02:14, 1.63MB/s] 33%|███▎      | 103M/313M [01:09<02:16, 1.60MB/s] 33%|███▎      | 104M/313M [01:09<02:11, 1.66MB/s] 33%|███▎      | 104M/313M [01:09<02:04, 1.76MB/s] 33%|███▎      | 104M/313M [01:10<01:50, 1.97MB/s] 33%|███▎      | 105M/313M [01:10<01:44, 2.08MB/s] 34%|███▎      | 105M/313M [01:10<01:48, 2.01MB/s] 34%|███▎      | 105M/313M [01:10<01:54, 1.90MB/s] 34%|███▍      | 106M/313M [01:10<01:55, 1.88MB/s] 34%|███▍      | 106M/313M [01:11<01:50, 1.96MB/s] 34%|███▍      | 107M/313M [01:11<01:43, 2.09MB/s] 34%|███▍      | 107M/313M [01:11<01:41, 2.12MB/s] 34%|███▍      | 108M/313M [01:11<01:36, 2.23MB/s] 35%|███▍      | 108M/313M [01:11<01:19, 2.69MB/s] 35%|███▍      | 108M/313M [01:11<01:17, 2.76MB/s] 35%|███▍      | 109M/313M [01:12<01:21, 2.61MB/s] 35%|███▍      | 109M/313M [01:12<01:10, 3.03MB/s] 35%|███▌      | 110M/313M [01:12<01:07, 3.14MB/s] 35%|███▌      | 110M/313M [01:12<01:07, 3.13MB/s] 36%|███▌      | 111M/313M [01:12<01:03, 3.31MB/s] 36%|███▌      | 112M/313M [01:12<00:59, 3.55MB/s] 36%|███▌      | 112M/313M [01:13<00:54, 3.82MB/s] 36%|███▌      | 113M/313M [01:13<00:51, 4.06MB/s] 36%|███▋      | 114M/313M [01:13<00:52, 3.96MB/s] 37%|███▋      | 115M/313M [01:13<00:52, 3.92MB/s] 37%|███▋      | 115M/313M [01:13<00:51, 3.98MB/s] 37%|███▋      | 116M/313M [01:14<00:42, 4.81MB/s] 37%|███▋      | 117M/313M [01:14<00:46, 4.44MB/s] 38%|███▊      | 117M/313M [01:14<00:47, 4.31MB/s] 38%|███▊      | 118M/313M [01:14<00:38, 5.26MB/s] 38%|███▊      | 119M/313M [01:14<00:47, 4.32MB/s] 38%|███▊      | 119M/313M [01:14<00:50, 4.00MB/s] 38%|███▊      | 120M/313M [01:14<00:43, 4.66MB/s] 39%|███▊      | 121M/313M [01:15<00:45, 4.40MB/s] 39%|███▉      | 122M/313M [01:15<00:33, 5.94MB/s] 39%|███▉      | 123M/313M [01:15<00:34, 5.76MB/s] 40%|███▉      | 124M/313M [01:15<00:37, 5.33MB/s] 40%|███▉      | 124M/313M [01:15<00:37, 5.25MB/s] 40%|███▉      | 125M/313M [01:15<00:43, 4.52MB/s] 40%|████      | 126M/313M [01:15<00:33, 5.86MB/s] 41%|████      | 127M/313M [01:16<00:32, 5.98MB/s] 41%|████      | 127M/313M [01:16<00:34, 5.67MB/s] 41%|████      | 128M/313M [01:16<00:38, 4.97MB/s] 41%|████      | 128M/313M [01:16<00:45, 4.28MB/s] 41%|████      | 129M/313M [01:16<00:48, 3.97MB/s] 41%|████▏     | 129M/313M [01:16<00:48, 3.97MB/s] 42%|████▏     | 130M/313M [01:17<00:48, 3.97MB/s] 42%|████▏     | 131M/313M [01:17<00:48, 3.93MB/s] 42%|████▏     | 131M/313M [01:17<00:51, 3.68MB/s] 42%|████▏     | 132M/313M [01:17<00:55, 3.43MB/s] 42%|████▏     | 132M/313M [01:17<00:49, 3.83MB/s] 42%|████▏     | 133M/313M [01:17<00:52, 3.57MB/s] 43%|████▎     | 133M/313M [01:17<00:53, 3.49MB/s] 43%|████▎     | 134M/313M [01:18<00:48, 3.84MB/s] 43%|████▎     | 134M/313M [01:18<00:47, 3.90MB/s] 43%|████▎     | 135M/313M [01:18<00:49, 3.73MB/s] 43%|████▎     | 135M/313M [01:18<00:48, 3.86MB/s] 43%|████▎     | 136M/313M [01:18<00:51, 3.58MB/s] 44%|████▎     | 136M/313M [01:18<00:54, 3.41MB/s] 44%|████▍     | 137M/313M [01:19<00:46, 3.98MB/s] 44%|████▍     | 137M/313M [01:19<00:53, 3.42MB/s] 44%|████▍     | 138M/313M [01:19<00:52, 3.46MB/s] 44%|████▍     | 138M/313M [01:19<00:50, 3.61MB/s] 44%|████▍     | 139M/313M [01:19<00:53, 3.42MB/s] 45%|████▍     | 139M/313M [01:19<00:51, 3.52MB/s] 45%|████▍     | 140M/313M [01:19<00:47, 3.80MB/s] 45%|████▍     | 141M/313M [01:20<00:43, 4.19MB/s] 45%|████▌     | 141M/313M [01:20<00:40, 4.46MB/s] 45%|████▌     | 142M/313M [01:20<00:40, 4.38MB/s] 46%|████▌     | 143M/313M [01:20<00:44, 4.01MB/s] 46%|████▌     | 143M/313M [01:20<00:38, 4.59MB/s] 46%|████▌     | 144M/313M [01:20<00:44, 3.95MB/s] 46%|████▌     | 144M/313M [01:20<00:50, 3.53MB/s] 46%|████▋     | 145M/313M [01:21<00:45, 3.87MB/s] 46%|████▋     | 145M/313M [01:21<00:56, 3.12MB/s] 47%|████▋     | 147M/313M [01:21<00:43, 3.99MB/s] 47%|████▋     | 147M/313M [01:21<00:51, 3.34MB/s] 47%|████▋     | 148M/313M [01:22<00:51, 3.38MB/s] 47%|████▋     | 148M/313M [01:22<00:50, 3.44MB/s] 48%|████▊     | 149M/313M [01:22<00:49, 3.45MB/s] 48%|████▊     | 149M/313M [01:22<00:47, 3.59MB/s] 48%|████▊     | 149M/313M [01:22<00:47, 3.60MB/s] 48%|████▊     | 150M/313M [01:22<00:58, 2.89MB/s] 48%|████▊     | 150M/313M [01:22<00:55, 3.09MB/s] 48%|████▊     | 150M/313M [01:22<00:55, 3.09MB/s] 48%|████▊     | 151M/313M [01:23<01:02, 2.70MB/s] 48%|████▊     | 151M/313M [01:23<00:58, 2.91MB/s] 49%|████▊     | 152M/313M [01:23<00:47, 3.56MB/s] 49%|████▊     | 152M/313M [01:23<00:51, 3.28MB/s] 49%|████▉     | 153M/313M [01:23<01:00, 2.79MB/s] 49%|████▉     | 153M/313M [01:23<00:53, 3.12MB/s] 49%|████▉     | 154M/313M [01:24<00:43, 3.78MB/s] 49%|████▉     | 154M/313M [01:24<00:37, 4.40MB/s] 50%|████▉     | 155M/313M [01:24<00:27, 5.94MB/s] 50%|████▉     | 156M/313M [01:24<00:33, 4.96MB/s] 50%|█████     | 157M/313M [01:24<00:33, 4.86MB/s] 50%|█████     | 157M/313M [01:24<00:35, 4.58MB/s] 50%|█████     | 158M/313M [01:24<00:36, 4.41MB/s] 51%|█████     | 158M/313M [01:24<00:39, 4.13MB/s] 51%|█████     | 158M/313M [01:25<00:44, 3.62MB/s] 51%|█████     | 159M/313M [01:25<00:45, 3.52MB/s] 51%|█████     | 159M/313M [01:25<00:51, 3.15MB/s] 51%|█████     | 160M/313M [01:25<00:53, 3.02MB/s] 51%|█████▏    | 160M/313M [01:25<00:54, 2.95MB/s] 51%|█████▏    | 161M/313M [01:25<00:52, 3.06MB/s] 52%|█████▏    | 161M/313M [01:26<00:50, 3.14MB/s] 52%|█████▏    | 162M/313M [01:26<00:48, 3.29MB/s] 52%|█████▏    | 162M/313M [01:26<00:47, 3.29MB/s] 52%|█████▏    | 163M/313M [01:26<00:48, 3.22MB/s] 52%|█████▏    | 163M/313M [01:26<00:48, 3.22MB/s] 52%|█████▏    | 164M/313M [01:26<00:45, 3.45MB/s] 53%|█████▎    | 164M/313M [01:27<00:53, 2.93MB/s] 53%|█████▎    | 165M/313M [01:27<00:35, 4.41MB/s] 53%|█████▎    | 166M/313M [01:27<00:35, 4.30MB/s] 53%|█████▎    | 166M/313M [01:27<00:38, 4.03MB/s] 53%|█████▎    | 167M/313M [01:27<00:43, 3.53MB/s] 53%|█████▎    | 167M/313M [01:27<00:46, 3.26MB/s] 54%|█████▎    | 167M/313M [01:27<00:47, 3.24MB/s] 54%|█████▎    | 168M/313M [01:27<00:46, 3.28MB/s] 54%|█████▍    | 168M/313M [01:28<00:44, 3.43MB/s] 54%|█████▍    | 168M/313M [01:28<00:41, 3.62MB/s] 54%|█████▍    | 169M/313M [01:28<00:43, 3.44MB/s] 54%|█████▍    | 169M/313M [01:28<00:49, 3.01MB/s] 54%|█████▍    | 170M/313M [01:28<00:54, 2.74MB/s] 54%|█████▍    | 170M/313M [01:28<00:57, 2.58MB/s] 55%|█████▍    | 171M/313M [01:29<00:57, 2.57MB/s] 55%|█████▍    | 171M/313M [01:29<00:58, 2.54MB/s] 55%|█████▍    | 171M/313M [01:29<00:59, 2.49MB/s] 55%|█████▍    | 172M/313M [01:29<00:59, 2.46MB/s] 55%|█████▌    | 172M/313M [01:29<00:57, 2.57MB/s] 55%|█████▌    | 173M/313M [01:29<00:54, 2.70MB/s] 55%|█████▌    | 173M/313M [01:30<00:52, 2.78MB/s] 56%|█████▌    | 174M/313M [01:30<00:48, 3.00MB/s] 56%|█████▌    | 174M/313M [01:30<00:47, 3.06MB/s] 56%|█████▌    | 174M/313M [01:30<00:49, 2.93MB/s] 56%|█████▌    | 175M/313M [01:30<00:44, 3.21MB/s] 56%|█████▌    | 175M/313M [01:30<00:41, 3.45MB/s] 56%|█████▌    | 176M/313M [01:30<00:42, 3.36MB/s] 56%|█████▋    | 176M/313M [01:30<00:43, 3.28MB/s] 56%|█████▋    | 176M/313M [01:31<00:45, 3.15MB/s] 57%|█████▋    | 177M/313M [01:31<00:47, 3.02MB/s] 57%|█████▋    | 177M/313M [01:31<00:47, 2.98MB/s] 57%|█████▋    | 178M/313M [01:31<00:49, 2.88MB/s] 57%|█████▋    | 178M/313M [01:31<00:43, 3.27MB/s] 57%|█████▋    | 178M/313M [01:31<00:49, 2.82MB/s] 57%|█████▋    | 179M/313M [01:32<00:45, 3.10MB/s] 57%|█████▋    | 179M/313M [01:32<00:40, 3.43MB/s] 58%|█████▊    | 180M/313M [01:32<00:37, 3.68MB/s] 58%|█████▊    | 180M/313M [01:32<00:44, 3.09MB/s] 58%|█████▊    | 181M/313M [01:32<00:45, 3.01MB/s] 58%|█████▊    | 181M/313M [01:32<00:55, 2.50MB/s] 58%|█████▊    | 181M/313M [01:32<00:57, 2.41MB/s] 58%|█████▊    | 182M/313M [01:33<00:56, 2.41MB/s] 58%|█████▊    | 182M/313M [01:33<00:52, 2.62MB/s] 58%|█████▊    | 182M/313M [01:33<00:54, 2.52MB/s] 58%|█████▊    | 183M/313M [01:33<00:55, 2.44MB/s] 59%|█████▊    | 183M/313M [01:33<00:58, 2.32MB/s] 59%|█████▊    | 183M/313M [01:33<00:57, 2.37MB/s] 59%|█████▉    | 184M/313M [01:34<00:59, 2.26MB/s] 59%|█████▉    | 184M/313M [01:34<00:57, 2.32MB/s] 59%|█████▉    | 185M/313M [01:34<00:51, 2.62MB/s] 59%|█████▉    | 185M/313M [01:34<00:50, 2.64MB/s] 59%|█████▉    | 185M/313M [01:34<00:56, 2.36MB/s] 59%|█████▉    | 185M/313M [01:34<01:00, 2.20MB/s] 59%|█████▉    | 186M/313M [01:34<00:54, 2.42MB/s] 60%|█████▉    | 186M/313M [01:35<01:06, 2.00MB/s] 60%|█████▉    | 187M/313M [01:35<00:41, 3.20MB/s] 60%|█████▉    | 187M/313M [01:35<00:39, 3.32MB/s] 60%|██████    | 188M/313M [01:35<00:40, 3.24MB/s] 60%|██████    | 188M/313M [01:35<00:55, 2.37MB/s] 60%|██████    | 188M/313M [01:35<00:56, 2.32MB/s] 60%|██████    | 189M/313M [01:36<00:54, 2.39MB/s] 60%|██████    | 189M/313M [01:36<00:51, 2.52MB/s] 61%|██████    | 189M/313M [01:36<00:49, 2.60MB/s] 61%|██████    | 190M/313M [01:36<00:48, 2.65MB/s] 61%|██████    | 190M/313M [01:36<00:48, 2.66MB/s] 61%|██████    | 190M/313M [01:36<00:47, 2.69MB/s] 61%|██████    | 191M/313M [01:36<00:48, 2.65MB/s] 61%|██████    | 191M/313M [01:36<00:47, 2.66MB/s] 61%|██████    | 191M/313M [01:37<00:47, 2.68MB/s] 61%|██████▏   | 192M/313M [01:37<00:49, 2.54MB/s] 61%|██████▏   | 192M/313M [01:37<00:47, 2.68MB/s] 62%|██████▏   | 192M/313M [01:37<00:44, 2.82MB/s] 62%|██████▏   | 193M/313M [01:37<00:42, 2.94MB/s] 62%|██████▏   | 193M/313M [01:37<00:40, 3.07MB/s] 62%|██████▏   | 193M/313M [01:37<00:39, 3.16MB/s] 62%|██████▏   | 194M/313M [01:37<00:39, 3.18MB/s] 62%|██████▏   | 194M/313M [01:37<00:38, 3.23MB/s] 62%|██████▏   | 194M/313M [01:38<00:37, 3.30MB/s] 62%|██████▏   | 195M/313M [01:38<00:36, 3.35MB/s] 62%|██████▏   | 195M/313M [01:38<00:36, 3.40MB/s] 62%|██████▏   | 195M/313M [01:38<00:35, 3.42MB/s] 63%|██████▎   | 196M/313M [01:38<00:35, 3.43MB/s] 63%|██████▎   | 196M/313M [01:38<00:46, 2.62MB/s] 63%|██████▎   | 197M/313M [01:38<00:38, 3.13MB/s] 63%|██████▎   | 197M/313M [01:38<00:44, 2.76MB/s] 63%|██████▎   | 197M/313M [01:39<00:44, 2.74MB/s] 63%|██████▎   | 197M/313M [01:39<00:44, 2.70MB/s] 63%|██████▎   | 198M/313M [01:39<00:47, 2.54MB/s] 63%|██████▎   | 198M/313M [01:39<00:49, 2.45MB/s] 63%|██████▎   | 198M/313M [01:39<00:50, 2.35MB/s] 63%|██████▎   | 198M/313M [01:39<00:48, 2.45MB/s] 64%|██████▎   | 199M/313M [01:39<00:46, 2.56MB/s] 64%|██████▎   | 199M/313M [01:39<00:46, 2.58MB/s] 64%|██████▍   | 199M/313M [01:39<00:45, 2.59MB/s] 64%|██████▍   | 200M/313M [01:40<00:47, 2.49MB/s] 64%|██████▍   | 200M/313M [01:40<00:47, 2.50MB/s] 64%|██████▍   | 200M/313M [01:40<00:45, 2.60MB/s] 64%|██████▍   | 201M/313M [01:40<00:45, 2.58MB/s] 64%|██████▍   | 201M/313M [01:40<00:44, 2.64MB/s] 64%|██████▍   | 201M/313M [01:40<00:41, 2.79MB/s] 64%|██████▍   | 201M/313M [01:40<00:43, 2.70MB/s] 65%|██████▍   | 202M/313M [01:40<00:40, 2.90MB/s] 65%|██████▍   | 202M/313M [01:41<00:42, 2.74MB/s] 65%|██████▍   | 202M/313M [01:41<00:46, 2.48MB/s] 65%|██████▍   | 203M/313M [01:41<00:52, 2.20MB/s] 65%|██████▍   | 203M/313M [01:41<00:58, 1.97MB/s] 65%|██████▍   | 203M/313M [01:41<01:05, 1.75MB/s] 65%|██████▌   | 203M/313M [01:41<01:11, 1.61MB/s] 65%|██████▌   | 203M/313M [01:41<01:04, 1.78MB/s] 65%|██████▌   | 204M/313M [01:42<01:03, 1.79MB/s] 65%|██████▌   | 204M/313M [01:42<01:06, 1.71MB/s] 65%|██████▌   | 204M/313M [01:42<01:04, 1.75MB/s] 65%|██████▌   | 204M/313M [01:42<01:04, 1.77MB/s] 65%|██████▌   | 205M/313M [01:42<00:57, 1.95MB/s] 66%|██████▌   | 205M/313M [01:42<01:06, 1.71MB/s] 66%|██████▌   | 205M/313M [01:42<01:04, 1.76MB/s] 66%|██████▌   | 205M/313M [01:43<00:59, 1.87MB/s] 66%|██████▌   | 206M/313M [01:43<00:56, 1.99MB/s] 66%|██████▌   | 206M/313M [01:43<00:55, 2.03MB/s] 66%|██████▌   | 206M/313M [01:43<00:53, 2.07MB/s] 66%|██████▌   | 207M/313M [01:43<00:52, 2.11MB/s] 66%|██████▌   | 207M/313M [01:43<00:51, 2.17MB/s] 66%|██████▋   | 207M/313M [01:43<00:52, 2.11MB/s] 66%|██████▋   | 207M/313M [01:44<00:53, 2.07MB/s] 66%|██████▋   | 208M/313M [01:44<00:51, 2.15MB/s] 67%|██████▋   | 208M/313M [01:44<00:54, 2.03MB/s] 67%|██████▋   | 208M/313M [01:44<00:54, 1.99MB/s] 67%|██████▋   | 208M/313M [01:44<00:55, 1.97MB/s] 67%|██████▋   | 209M/313M [01:44<00:54, 1.98MB/s] 67%|██████▋   | 209M/313M [01:44<00:57, 1.90MB/s] 67%|██████▋   | 209M/313M [01:45<00:53, 2.01MB/s] 67%|██████▋   | 210M/313M [01:45<00:56, 1.92MB/s] 67%|██████▋   | 210M/313M [01:45<00:54, 1.97MB/s] 67%|██████▋   | 210M/313M [01:45<00:59, 1.81MB/s] 67%|██████▋   | 210M/313M [01:45<00:57, 1.86MB/s] 67%|██████▋   | 211M/313M [01:45<01:01, 1.74MB/s] 67%|██████▋   | 211M/313M [01:46<01:02, 1.69MB/s] 68%|██████▊   | 211M/313M [01:46<01:05, 1.63MB/s] 68%|██████▊   | 211M/313M [01:46<01:03, 1.66MB/s] 68%|██████▊   | 212M/313M [01:46<01:01, 1.71MB/s] 68%|██████▊   | 212M/313M [01:46<00:59, 1.78MB/s] 68%|██████▊   | 212M/313M [01:46<00:56, 1.85MB/s] 68%|██████▊   | 213M/313M [01:47<00:58, 1.81MB/s] 68%|██████▊   | 213M/313M [01:47<00:58, 1.78MB/s] 68%|██████▊   | 213M/313M [01:47<00:58, 1.80MB/s] 68%|██████▊   | 213M/313M [01:47<00:58, 1.78MB/s] 68%|██████▊   | 214M/313M [01:47<00:57, 1.80MB/s] 68%|██████▊   | 214M/313M [01:47<00:58, 1.77MB/s] 69%|██████▊   | 214M/313M [01:48<01:00, 1.70MB/s] 69%|██████▊   | 215M/313M [01:48<00:58, 1.77MB/s] 69%|██████▊   | 215M/313M [01:48<00:55, 1.83MB/s] 69%|██████▉   | 215M/313M [01:48<00:52, 1.94MB/s] 69%|██████▉   | 215M/313M [01:48<00:50, 2.03MB/s] 69%|██████▉   | 216M/313M [01:48<00:48, 2.10MB/s] 69%|██████▉   | 216M/313M [01:48<00:45, 2.21MB/s] 69%|██████▉   | 216M/313M [01:48<00:42, 2.35MB/s] 69%|██████▉   | 217M/313M [01:49<00:40, 2.49MB/s] 69%|██████▉   | 217M/313M [01:49<00:40, 2.47MB/s] 69%|██████▉   | 217M/313M [01:49<00:40, 2.46MB/s] 70%|██████▉   | 217M/313M [01:49<00:39, 2.51MB/s] 70%|██████▉   | 218M/313M [01:49<00:38, 2.60MB/s] 70%|██████▉   | 218M/313M [01:49<00:37, 2.61MB/s] 70%|██████▉   | 218M/313M [01:49<00:37, 2.63MB/s] 70%|██████▉   | 219M/313M [01:49<00:37, 2.64MB/s] 70%|███████   | 219M/313M [01:50<00:37, 2.61MB/s] 70%|███████   | 219M/313M [01:50<00:38, 2.56MB/s] 70%|███████   | 220M/313M [01:50<00:38, 2.53MB/s] 70%|███████   | 220M/313M [01:50<00:37, 2.58MB/s] 71%|███████   | 220M/313M [01:50<00:36, 2.66MB/s] 71%|███████   | 221M/313M [01:50<00:33, 2.90MB/s] 71%|███████   | 221M/313M [01:50<00:30, 3.10MB/s] 71%|███████   | 222M/313M [01:50<00:26, 3.59MB/s] 71%|███████   | 222M/313M [01:51<00:33, 2.80MB/s] 71%|███████   | 222M/313M [01:51<00:32, 2.96MB/s] 71%|███████   | 223M/313M [01:51<00:42, 2.24MB/s] 71%|███████▏  | 223M/313M [01:51<00:38, 2.47MB/s] 71%|███████▏  | 223M/313M [01:51<00:45, 2.07MB/s] 71%|███████▏  | 223M/313M [01:51<00:44, 2.12MB/s] 72%|███████▏  | 224M/313M [01:52<00:44, 2.11MB/s] 72%|███████▏  | 224M/313M [01:52<00:44, 2.08MB/s] 72%|███████▏  | 224M/313M [01:52<00:46, 2.02MB/s] 72%|███████▏  | 224M/313M [01:52<00:51, 1.80MB/s] 72%|███████▏  | 225M/313M [01:52<00:44, 2.08MB/s] 72%|███████▏  | 225M/313M [01:52<00:46, 1.97MB/s] 72%|███████▏  | 225M/313M [01:52<00:46, 1.98MB/s] 72%|███████▏  | 225M/313M [01:52<00:45, 1.99MB/s] 72%|███████▏  | 226M/313M [01:53<00:47, 1.93MB/s] 72%|███████▏  | 226M/313M [01:53<00:50, 1.82MB/s] 72%|███████▏  | 226M/313M [01:53<00:47, 1.90MB/s] 72%|███████▏  | 226M/313M [01:53<00:46, 1.94MB/s] 72%|███████▏  | 226M/313M [01:53<00:44, 2.02MB/s] 73%|███████▎  | 227M/313M [01:53<00:43, 2.09MB/s] 73%|███████▎  | 227M/313M [01:53<00:38, 2.34MB/s] 73%|███████▎  | 228M/313M [01:53<00:32, 2.75MB/s] 73%|███████▎  | 228M/313M [01:54<00:23, 3.71MB/s] 73%|███████▎  | 229M/313M [01:54<00:25, 3.41MB/s] 73%|███████▎  | 229M/313M [01:54<00:31, 2.79MB/s] 73%|███████▎  | 229M/313M [01:54<00:27, 3.15MB/s] 73%|███████▎  | 230M/313M [01:54<00:30, 2.88MB/s] 74%|███████▎  | 230M/313M [01:54<00:30, 2.86MB/s] 74%|███████▎  | 230M/313M [01:54<00:30, 2.84MB/s] 74%|███████▍  | 231M/313M [01:55<00:29, 2.87MB/s] 74%|███████▍  | 231M/313M [01:55<00:29, 2.86MB/s] 74%|███████▍  | 231M/313M [01:55<00:30, 2.83MB/s] 74%|███████▍  | 232M/313M [01:55<00:31, 2.71MB/s] 74%|███████▍  | 232M/313M [01:55<00:40, 2.09MB/s] 74%|███████▍  | 232M/313M [01:55<00:35, 2.38MB/s] 74%|███████▍  | 232M/313M [01:55<00:37, 2.23MB/s] 75%|███████▍  | 233M/313M [01:55<00:32, 2.55MB/s] 75%|███████▍  | 233M/313M [01:56<00:27, 3.00MB/s] 75%|███████▍  | 234M/313M [01:56<00:24, 3.35MB/s] 75%|███████▍  | 234M/313M [01:56<00:24, 3.35MB/s] 75%|███████▌  | 234M/313M [01:56<00:27, 2.99MB/s] 75%|███████▌  | 235M/313M [01:56<00:28, 2.87MB/s] 75%|███████▌  | 235M/313M [01:56<00:32, 2.53MB/s] 75%|███████▌  | 235M/313M [01:56<00:31, 2.56MB/s] 75%|███████▌  | 236M/313M [01:57<00:33, 2.40MB/s] 75%|███████▌  | 236M/313M [01:57<00:33, 2.37MB/s] 76%|███████▌  | 236M/313M [01:57<00:42, 1.88MB/s] 76%|███████▌  | 236M/313M [01:57<00:42, 1.89MB/s] 76%|███████▌  | 237M/313M [01:57<00:42, 1.89MB/s] 76%|███████▌  | 237M/313M [01:57<00:41, 1.90MB/s] 76%|███████▌  | 237M/313M [01:57<00:43, 1.80MB/s] 76%|███████▌  | 237M/313M [01:57<00:46, 1.70MB/s] 76%|███████▌  | 237M/313M [01:57<00:45, 1.73MB/s] 76%|███████▌  | 238M/313M [01:58<00:41, 1.91MB/s] 76%|███████▌  | 238M/313M [01:58<00:37, 2.09MB/s] 76%|███████▌  | 238M/313M [01:58<00:34, 2.23MB/s] 76%|███████▋  | 238M/313M [01:58<00:34, 2.23MB/s] 76%|███████▋  | 239M/313M [01:58<00:35, 2.20MB/s] 76%|███████▋  | 239M/313M [01:58<00:35, 2.16MB/s] 77%|███████▋  | 239M/313M [01:58<00:36, 2.13MB/s] 77%|███████▋  | 239M/313M [01:58<00:37, 2.04MB/s] 77%|███████▋  | 240M/313M [01:59<00:39, 1.92MB/s] 77%|███████▋  | 240M/313M [01:59<00:40, 1.90MB/s] 77%|███████▋  | 240M/313M [01:59<00:38, 1.96MB/s] 77%|███████▋  | 240M/313M [01:59<00:33, 2.26MB/s] 77%|███████▋  | 240M/313M [01:59<00:33, 2.23MB/s] 77%|███████▋  | 241M/313M [01:59<00:31, 2.40MB/s] 77%|███████▋  | 241M/313M [01:59<00:30, 2.46MB/s] 77%|███████▋  | 241M/313M [01:59<00:28, 2.62MB/s] 77%|███████▋  | 242M/313M [01:59<00:32, 2.26MB/s] 77%|███████▋  | 242M/313M [02:00<00:35, 2.10MB/s] 77%|███████▋  | 242M/313M [02:00<00:37, 1.98MB/s] 78%|███████▊  | 242M/313M [02:00<00:39, 1.88MB/s] 78%|███████▊  | 242M/313M [02:00<00:41, 1.75MB/s] 78%|███████▊  | 243M/313M [02:00<00:42, 1.72MB/s] 78%|███████▊  | 243M/313M [02:00<00:43, 1.67MB/s] 78%|███████▊  | 243M/313M [02:00<00:41, 1.75MB/s] 78%|███████▊  | 243M/313M [02:01<00:40, 1.78MB/s] 78%|███████▊  | 244M/313M [02:01<00:39, 1.84MB/s] 78%|███████▊  | 244M/313M [02:01<00:36, 1.95MB/s] 78%|███████▊  | 244M/313M [02:01<00:34, 2.08MB/s] 78%|███████▊  | 244M/313M [02:01<00:30, 2.31MB/s] 78%|███████▊  | 245M/313M [02:01<00:30, 2.35MB/s] 78%|███████▊  | 245M/313M [02:01<00:34, 2.09MB/s] 78%|███████▊  | 245M/313M [02:01<00:33, 2.12MB/s] 78%|███████▊  | 245M/313M [02:01<00:33, 2.10MB/s] 79%|███████▊  | 245M/313M [02:02<00:36, 1.94MB/s] 79%|███████▊  | 246M/313M [02:02<00:39, 1.76MB/s] 79%|███████▊  | 246M/313M [02:02<00:32, 2.13MB/s] 79%|███████▉  | 246M/313M [02:02<00:37, 1.85MB/s] 79%|███████▉  | 246M/313M [02:02<00:49, 1.40MB/s] 79%|███████▉  | 247M/313M [02:03<00:59, 1.16MB/s] 79%|███████▉  | 247M/313M [02:03<01:08, 1.01MB/s] 79%|███████▉  | 247M/313M [02:03<01:14, 924kB/s]  79%|███████▉  | 247M/313M [02:03<01:24, 817kB/s] 79%|███████▉  | 247M/313M [02:03<01:32, 739kB/s] 79%|███████▉  | 247M/313M [02:03<01:34, 728kB/s] 79%|███████▉  | 247M/313M [02:04<01:36, 712kB/s] 79%|███████▉  | 247M/313M [02:04<01:34, 723kB/s] 79%|███████▉  | 247M/313M [02:04<01:31, 744kB/s] 79%|███████▉  | 247M/313M [02:04<01:30, 751kB/s] 79%|███████▉  | 248M/313M [02:04<01:37, 696kB/s] 79%|███████▉  | 248M/313M [02:04<01:33, 729kB/s] 79%|███████▉  | 248M/313M [02:04<01:33, 728kB/s] 79%|███████▉  | 248M/313M [02:04<01:43, 656kB/s] 79%|███████▉  | 248M/313M [02:05<01:49, 617kB/s] 79%|███████▉  | 248M/313M [02:05<01:41, 666kB/s] 79%|███████▉  | 248M/313M [02:05<01:29, 753kB/s] 79%|███████▉  | 248M/313M [02:05<01:30, 743kB/s] 79%|███████▉  | 248M/313M [02:05<01:27, 772kB/s] 79%|███████▉  | 248M/313M [02:05<01:27, 768kB/s] 80%|███████▉  | 248M/313M [02:05<01:25, 785kB/s] 80%|███████▉  | 249M/313M [02:05<01:25, 783kB/s] 80%|███████▉  | 249M/313M [02:06<01:20, 827kB/s] 80%|███████▉  | 249M/313M [02:06<01:16, 873kB/s] 80%|███████▉  | 249M/313M [02:06<01:14, 896kB/s] 80%|███████▉  | 249M/313M [02:06<01:06, 1.00MB/s] 80%|███████▉  | 249M/313M [02:06<00:55, 1.20MB/s] 80%|███████▉  | 249M/313M [02:06<00:51, 1.30MB/s] 80%|███████▉  | 250M/313M [02:06<00:51, 1.28MB/s] 80%|███████▉  | 250M/313M [02:06<00:53, 1.23MB/s] 80%|███████▉  | 250M/313M [02:07<00:57, 1.14MB/s] 80%|███████▉  | 250M/313M [02:07<01:02, 1.05MB/s] 80%|███████▉  | 250M/313M [02:07<01:04, 1.02MB/s] 80%|████████  | 250M/313M [02:07<01:04, 1.01MB/s] 80%|████████  | 250M/313M [02:07<01:04, 1.02MB/s] 80%|████████  | 250M/313M [02:07<01:09, 941kB/s]  80%|████████  | 250M/313M [02:07<01:07, 969kB/s] 80%|████████  | 251M/313M [02:07<01:14, 868kB/s] 80%|████████  | 251M/313M [02:08<01:14, 865kB/s] 80%|████████  | 251M/313M [02:08<01:19, 815kB/s] 80%|████████  | 251M/313M [02:08<01:22, 780kB/s] 80%|████████  | 251M/313M [02:08<01:27, 739kB/s] 80%|████████  | 251M/313M [02:08<01:21, 790kB/s] 80%|████████  | 251M/313M [02:08<01:25, 752kB/s] 80%|████████  | 251M/313M [02:08<01:34, 680kB/s] 80%|████████  | 251M/313M [02:09<01:33, 689kB/s] 80%|████████  | 251M/313M [02:09<01:32, 697kB/s] 80%|████████  | 252M/313M [02:09<01:28, 726kB/s] 81%|████████  | 252M/313M [02:09<01:15, 850kB/s] 81%|████████  | 252M/313M [02:09<01:16, 831kB/s] 81%|████████  | 252M/313M [02:09<01:17, 824kB/s] 81%|████████  | 252M/313M [02:09<01:14, 849kB/s] 81%|████████  | 252M/313M [02:09<01:07, 935kB/s] 81%|████████  | 252M/313M [02:10<01:02, 1.01MB/s] 81%|████████  | 252M/313M [02:10<00:59, 1.05MB/s] 81%|████████  | 253M/313M [02:10<00:59, 1.06MB/s] 81%|████████  | 253M/313M [02:10<00:56, 1.12MB/s] 81%|████████  | 253M/313M [02:10<00:56, 1.10MB/s] 81%|████████  | 253M/313M [02:10<00:55, 1.12MB/s] 81%|████████  | 253M/313M [02:11<00:53, 1.16MB/s] 81%|████████  | 253M/313M [02:11<00:53, 1.15MB/s] 81%|████████  | 254M/313M [02:11<00:51, 1.20MB/s] 81%|████████  | 254M/313M [02:11<00:45, 1.37MB/s] 81%|████████  | 254M/313M [02:11<00:47, 1.30MB/s] 81%|████████▏ | 254M/313M [02:11<00:49, 1.23MB/s] 81%|████████▏ | 254M/313M [02:11<00:46, 1.32MB/s] 81%|████████▏ | 254M/313M [02:11<00:43, 1.42MB/s] 81%|████████▏ | 255M/313M [02:12<00:42, 1.44MB/s] 82%|████████▏ | 255M/313M [02:12<00:41, 1.44MB/s] 82%|████████▏ | 255M/313M [02:12<00:38, 1.58MB/s] 82%|████████▏ | 255M/313M [02:12<00:39, 1.54MB/s] 82%|████████▏ | 255M/313M [02:12<00:39, 1.52MB/s] 82%|████████▏ | 256M/313M [02:12<00:34, 1.74MB/s] 82%|████████▏ | 256M/313M [02:12<00:36, 1.64MB/s] 82%|████████▏ | 256M/313M [02:12<00:36, 1.64MB/s] 82%|████████▏ | 256M/313M [02:13<00:30, 1.95MB/s] 82%|████████▏ | 257M/313M [02:13<00:26, 2.22MB/s] 82%|████████▏ | 257M/313M [02:13<00:22, 2.56MB/s] 82%|████████▏ | 257M/313M [02:13<00:18, 3.06MB/s] 83%|████████▎ | 258M/313M [02:13<00:14, 3.84MB/s] 83%|████████▎ | 258M/313M [02:13<00:14, 3.85MB/s] 83%|████████▎ | 259M/313M [02:13<00:13, 4.11MB/s] 83%|████████▎ | 259M/313M [02:13<00:14, 3.98MB/s] 83%|████████▎ | 260M/313M [02:14<00:15, 3.57MB/s] 83%|████████▎ | 261M/313M [02:14<00:11, 4.56MB/s] 84%|████████▎ | 261M/313M [02:14<00:14, 3.84MB/s] 84%|████████▎ | 262M/313M [02:14<00:13, 3.90MB/s] 84%|████████▍ | 262M/313M [02:14<00:14, 3.70MB/s] 84%|████████▍ | 263M/313M [02:14<00:13, 3.96MB/s] 84%|████████▍ | 263M/313M [02:14<00:12, 4.03MB/s] 84%|████████▍ | 264M/313M [02:15<00:13, 3.88MB/s] 85%|████████▍ | 264M/313M [02:15<00:12, 4.14MB/s] 85%|████████▍ | 265M/313M [02:15<00:13, 3.85MB/s] 85%|████████▍ | 265M/313M [02:15<00:13, 3.71MB/s] 85%|████████▍ | 265M/313M [02:15<00:13, 3.63MB/s] 85%|████████▌ | 266M/313M [02:15<00:13, 3.56MB/s] 85%|████████▌ | 266M/313M [02:15<00:19, 2.52MB/s] 85%|████████▌ | 266M/313M [02:16<00:19, 2.42MB/s] 85%|████████▌ | 267M/313M [02:16<00:17, 2.80MB/s] 85%|████████▌ | 267M/313M [02:16<00:17, 2.67MB/s] 86%|████████▌ | 267M/313M [02:16<00:18, 2.50MB/s] 86%|████████▌ | 268M/313M [02:16<00:19, 2.36MB/s] 86%|████████▌ | 268M/313M [02:16<00:20, 2.27MB/s] 86%|████████▌ | 268M/313M [02:16<00:20, 2.29MB/s] 86%|████████▌ | 268M/313M [02:16<00:19, 2.32MB/s] 86%|████████▌ | 269M/313M [02:17<00:20, 2.28MB/s] 86%|████████▌ | 269M/313M [02:17<00:19, 2.40MB/s] 86%|████████▌ | 269M/313M [02:17<00:18, 2.50MB/s] 86%|████████▌ | 269M/313M [02:17<00:19, 2.38MB/s] 86%|████████▋ | 270M/313M [02:17<00:18, 2.46MB/s] 86%|████████▋ | 270M/313M [02:17<00:18, 2.41MB/s] 86%|████████▋ | 270M/313M [02:17<00:17, 2.58MB/s] 87%|████████▋ | 271M/313M [02:17<00:16, 2.74MB/s] 87%|████████▋ | 271M/313M [02:17<00:14, 2.98MB/s] 87%|████████▋ | 271M/313M [02:18<00:12, 3.52MB/s] 87%|████████▋ | 272M/313M [02:18<00:13, 3.27MB/s] 87%|████████▋ | 272M/313M [02:18<00:13, 3.18MB/s] 87%|████████▋ | 272M/313M [02:18<00:14, 2.98MB/s] 87%|████████▋ | 273M/313M [02:18<00:15, 2.70MB/s] 87%|████████▋ | 273M/313M [02:18<00:16, 2.47MB/s] 87%|████████▋ | 273M/313M [02:18<00:17, 2.37MB/s] 88%|████████▊ | 274M/313M [02:18<00:17, 2.32MB/s] 88%|████████▊ | 274M/313M [02:19<00:18, 2.24MB/s] 88%|████████▊ | 274M/313M [02:19<00:18, 2.22MB/s] 88%|████████▊ | 275M/313M [02:19<00:17, 2.30MB/s] 88%|████████▊ | 275M/313M [02:19<00:16, 2.47MB/s] 88%|████████▊ | 275M/313M [02:19<00:15, 2.56MB/s] 88%|████████▊ | 276M/313M [02:19<00:15, 2.54MB/s] 88%|████████▊ | 276M/313M [02:19<00:17, 2.15MB/s] 88%|████████▊ | 276M/313M [02:20<00:17, 2.15MB/s] 88%|████████▊ | 276M/313M [02:20<00:19, 1.99MB/s] 88%|████████▊ | 276M/313M [02:20<00:20, 1.86MB/s] 89%|████████▊ | 277M/313M [02:20<00:20, 1.80MB/s] 89%|████████▊ | 277M/313M [02:20<00:21, 1.72MB/s] 89%|████████▊ | 277M/313M [02:20<00:22, 1.67MB/s] 89%|████████▉ | 277M/313M [02:21<00:22, 1.61MB/s] 89%|████████▉ | 278M/313M [02:21<00:23, 1.52MB/s] 89%|████████▉ | 278M/313M [02:21<00:23, 1.54MB/s] 89%|████████▉ | 278M/313M [02:21<00:21, 1.67MB/s] 89%|████████▉ | 279M/313M [02:21<00:20, 1.71MB/s] 89%|████████▉ | 279M/313M [02:21<00:19, 1.78MB/s] 89%|████████▉ | 279M/313M [02:22<00:18, 1.86MB/s] 89%|████████▉ | 279M/313M [02:22<00:17, 1.96MB/s] 89%|████████▉ | 280M/313M [02:22<00:17, 2.02MB/s] 90%|████████▉ | 280M/313M [02:22<00:16, 2.13MB/s] 90%|████████▉ | 280M/313M [02:22<00:15, 2.12MB/s] 90%|████████▉ | 281M/313M [02:22<00:15, 2.10MB/s] 90%|████████▉ | 281M/313M [02:22<00:16, 2.00MB/s] 90%|████████▉ | 281M/313M [02:23<00:16, 1.99MB/s] 90%|█████████ | 281M/313M [02:23<00:16, 2.00MB/s] 90%|█████████ | 282M/313M [02:23<00:15, 2.10MB/s] 90%|█████████ | 282M/313M [02:23<00:14, 2.25MB/s] 90%|█████████ | 282M/313M [02:23<00:13, 2.42MB/s] 90%|█████████ | 283M/313M [02:23<00:12, 2.59MB/s] 91%|█████████ | 283M/313M [02:23<00:09, 3.32MB/s] 91%|█████████ | 284M/313M [02:23<00:08, 3.57MB/s] 91%|█████████ | 284M/313M [02:23<00:07, 4.01MB/s] 91%|█████████ | 285M/313M [02:24<00:07, 3.81MB/s] 91%|█████████ | 285M/313M [02:24<00:07, 3.69MB/s] 91%|█████████▏| 285M/313M [02:24<00:08, 3.54MB/s] 91%|█████████▏| 286M/313M [02:24<00:08, 3.44MB/s] 92%|█████████▏| 286M/313M [02:24<00:08, 3.42MB/s] 92%|█████████▏| 286M/313M [02:24<00:08, 3.37MB/s] 92%|█████████▏| 287M/313M [02:24<00:08, 3.34MB/s] 92%|█████████▏| 287M/313M [02:24<00:06, 3.91MB/s] 92%|█████████▏| 288M/313M [02:25<00:07, 3.68MB/s] 92%|█████████▏| 288M/313M [02:25<00:07, 3.45MB/s] 92%|█████████▏| 288M/313M [02:25<00:07, 3.26MB/s] 92%|█████████▏| 289M/313M [02:25<00:09, 2.60MB/s] 92%|█████████▏| 289M/313M [02:25<00:09, 2.58MB/s] 93%|█████████▎| 289M/313M [02:25<00:08, 2.96MB/s] 93%|█████████▎| 290M/313M [02:25<00:08, 2.70MB/s] 93%|█████████▎| 290M/313M [02:25<00:07, 3.24MB/s] 93%|█████████▎| 290M/313M [02:26<00:08, 2.72MB/s] 93%|█████████▎| 291M/313M [02:26<00:08, 2.69MB/s] 93%|█████████▎| 291M/313M [02:26<00:08, 2.53MB/s] 93%|█████████▎| 291M/313M [02:26<00:11, 2.01MB/s] 93%|█████████▎| 292M/313M [02:26<00:08, 2.49MB/s] 93%|█████████▎| 292M/313M [02:26<00:08, 2.47MB/s] 94%|█████████▎| 292M/313M [02:26<00:09, 2.31MB/s] 94%|█████████▎| 292M/313M [02:27<00:09, 2.19MB/s] 94%|█████████▎| 293M/313M [02:27<00:09, 2.11MB/s] 94%|█████████▎| 293M/313M [02:27<00:10, 1.99MB/s] 94%|█████████▍| 293M/313M [02:27<00:11, 1.82MB/s] 94%|█████████▍| 293M/313M [02:27<00:11, 1.71MB/s] 94%|█████████▍| 293M/313M [02:27<00:12, 1.59MB/s] 94%|█████████▍| 294M/313M [02:27<00:13, 1.52MB/s] 94%|█████████▍| 294M/313M [02:27<00:11, 1.66MB/s] 94%|█████████▍| 294M/313M [02:28<00:11, 1.72MB/s] 94%|█████████▍| 294M/313M [02:28<00:11, 1.73MB/s] 94%|█████████▍| 294M/313M [02:28<00:11, 1.69MB/s] 94%|█████████▍| 295M/313M [02:28<00:10, 1.83MB/s] 94%|█████████▍| 295M/313M [02:28<00:10, 1.82MB/s] 94%|█████████▍| 295M/313M [02:28<00:09, 1.97MB/s] 95%|█████████▍| 296M/313M [02:28<00:07, 2.54MB/s] 95%|█████████▍| 296M/313M [02:28<00:06, 2.51MB/s] 95%|█████████▍| 296M/313M [02:29<00:07, 2.34MB/s] 95%|█████████▍| 296M/313M [02:29<00:07, 2.35MB/s] 95%|█████████▍| 297M/313M [02:29<00:05, 2.87MB/s] 95%|█████████▌| 297M/313M [02:29<00:05, 2.81MB/s] 95%|█████████▌| 297M/313M [02:29<00:05, 3.06MB/s] 95%|█████████▌| 298M/313M [02:29<00:05, 2.95MB/s] 95%|█████████▌| 298M/313M [02:29<00:06, 2.45MB/s] 95%|█████████▌| 298M/313M [02:29<00:06, 2.20MB/s] 96%|█████████▌| 298M/313M [02:30<00:07, 2.04MB/s] 96%|█████████▌| 299M/313M [02:30<00:07, 1.86MB/s] 96%|█████████▌| 299M/313M [02:30<00:07, 1.90MB/s] 96%|█████████▌| 299M/313M [02:30<00:07, 1.93MB/s] 96%|█████████▌| 299M/313M [02:30<00:07, 1.95MB/s] 96%|█████████▌| 300M/313M [02:30<00:07, 1.76MB/s] 96%|█████████▌| 300M/313M [02:30<00:07, 1.78MB/s] 96%|█████████▌| 300M/313M [02:30<00:06, 1.91MB/s] 96%|█████████▌| 300M/313M [02:31<00:06, 2.04MB/s] 96%|█████████▌| 300M/313M [02:31<00:05, 2.13MB/s] 96%|█████████▌| 301M/313M [02:31<00:05, 2.10MB/s] 96%|█████████▋| 301M/313M [02:31<00:05, 2.19MB/s] 96%|█████████▋| 301M/313M [02:31<00:05, 2.08MB/s] 96%|█████████▋| 301M/313M [02:31<00:05, 2.09MB/s] 97%|█████████▋| 302M/313M [02:31<00:04, 2.33MB/s] 97%|█████████▋| 302M/313M [02:31<00:04, 2.49MB/s] 97%|█████████▋| 302M/313M [02:31<00:03, 3.14MB/s] 97%|█████████▋| 303M/313M [02:32<00:02, 3.50MB/s] 97%|█████████▋| 303M/313M [02:32<00:02, 3.41MB/s] 97%|█████████▋| 304M/313M [02:32<00:02, 3.30MB/s] 97%|█████████▋| 304M/313M [02:32<00:03, 2.35MB/s] 97%|█████████▋| 304M/313M [02:32<00:03, 2.24MB/s] 97%|█████████▋| 304M/313M [02:32<00:04, 2.08MB/s] 97%|█████████▋| 305M/313M [02:32<00:04, 1.96MB/s] 98%|█████████▊| 305M/313M [02:33<00:04, 1.85MB/s] 98%|█████████▊| 305M/313M [02:33<00:04, 1.83MB/s] 98%|█████████▊| 305M/313M [02:33<00:03, 1.90MB/s] 98%|█████████▊| 306M/313M [02:33<00:03, 1.92MB/s] 98%|█████████▊| 306M/313M [02:33<00:03, 2.00MB/s] 98%|█████████▊| 306M/313M [02:33<00:03, 1.99MB/s] 98%|█████████▊| 306M/313M [02:33<00:03, 1.93MB/s] 98%|█████████▊| 307M/313M [02:33<00:03, 1.96MB/s] 98%|█████████▊| 307M/313M [02:34<00:02, 2.06MB/s] 98%|█████████▊| 307M/313M [02:34<00:02, 2.03MB/s] 98%|█████████▊| 307M/313M [02:34<00:03, 1.83MB/s] 98%|█████████▊| 307M/313M [02:34<00:03, 1.51MB/s] 98%|█████████▊| 308M/313M [02:34<00:03, 1.54MB/s] 98%|█████████▊| 308M/313M [02:34<00:03, 1.50MB/s] 98%|█████████▊| 308M/313M [02:35<00:04, 1.16MB/s] 99%|█████████▊| 308M/313M [02:35<00:03, 1.25MB/s] 99%|█████████▊| 308M/313M [02:35<00:03, 1.44MB/s] 99%|█████████▉| 309M/313M [02:35<00:02, 1.57MB/s] 99%|█████████▉| 309M/313M [02:35<00:02, 1.78MB/s] 99%|█████████▉| 309M/313M [02:35<00:01, 1.96MB/s] 99%|█████████▉| 309M/313M [02:35<00:01, 1.92MB/s] 99%|█████████▉| 310M/313M [02:36<00:01, 1.58MB/s] 99%|█████████▉| 310M/313M [02:36<00:01, 1.67MB/s] 99%|█████████▉| 310M/313M [02:36<00:01, 1.85MB/s] 99%|█████████▉| 310M/313M [02:36<00:01, 1.85MB/s] 99%|█████████▉| 311M/313M [02:36<00:01, 1.54MB/s] 99%|█████████▉| 311M/313M [02:36<00:00, 1.87MB/s]100%|█████████▉| 311M/313M [02:36<00:00, 2.02MB/s]100%|█████████▉| 312M/313M [02:36<00:00, 2.34MB/s]100%|█████████▉| 312M/313M [02:37<00:00, 2.58MB/s]100%|██████████| 313M/313M [02:37<00:00, 2.08MB/s]
[32m[2023-12-21 12:39:56,407] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:39:56,779] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:39:56.784608  3759 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:39:59,976] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieForSequenceClassification: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:39:59,976] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['classifier.weight', 'classifier.bias', 'ernie.pooler.dense.bias', 'ernie.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:39:59,989] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:39:59,989] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt and saved to /root/.paddlenlp/models/ernie-3.0-medium-zh[0m
[32m[2023-12-21 12:40:01,226] [    INFO][0m - Downloading ernie_3.0_medium_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt[0m
  0%|          | 0.00/182k [00:00<?, ?B/s] 10%|█         | 19.0k/182k [00:00<00:01, 122kB/s] 28%|██▊       | 51.0k/182k [00:00<00:00, 154kB/s] 37%|███▋      | 67.0k/182k [00:00<00:00, 127kB/s] 54%|█████▍    | 99.0k/182k [00:00<00:00, 145kB/s] 72%|███████▏  | 131k/182k [00:00<00:00, 167kB/s]  89%|████████▉ | 163k/182k [00:01<00:00, 189kB/s]100%|██████████| 182k/182k [00:01<00:00, 186kB/s]
[32m[2023-12-21 12:40:02,499] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:40:02,499] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
[32m[2023-12-21 12:40:34,360] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:40:35,195] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:40:35,196] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:40:35,196] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 10, epoch: 1, batch: 10, loss: 0.42427, accu: 0.66563, speed: 3.48 step/s
global step 20, epoch: 1, batch: 20, loss: 0.50328, accu: 0.77031, speed: 3.42 step/s
global step 30, epoch: 1, batch: 30, loss: 0.19650, accu: 0.81458, speed: 3.40 step/s
global step 40, epoch: 1, batch: 40, loss: 0.29401, accu: 0.83047, speed: 3.43 step/s
global step 50, epoch: 1, batch: 50, loss: 0.19590, accu: 0.84250, speed: 3.44 step/s
global step 60, epoch: 1, batch: 60, loss: 0.31651, accu: 0.85260, speed: 3.45 step/s
global step 70, epoch: 1, batch: 70, loss: 0.34248, accu: 0.86339, speed: 3.48 step/s
global step 80, epoch: 1, batch: 80, loss: 0.40667, accu: 0.86484, speed: 3.47 step/s
global step 90, epoch: 1, batch: 90, loss: 0.35929, accu: 0.87049, speed: 3.44 step/s
global step 100, epoch: 1, batch: 100, loss: 0.11432, accu: 0.87375, speed: 3.45 step/s
eval loss: 0.26708, accu: 0.88917
global step 110, epoch: 1, batch: 110, loss: 0.26887, accu: 0.88125, speed: 1.56 step/s
global step 120, epoch: 1, batch: 120, loss: 0.20054, accu: 0.88906, speed: 3.44 step/s
global step 130, epoch: 1, batch: 130, loss: 0.14079, accu: 0.89479, speed: 3.44 step/s
global step 140, epoch: 1, batch: 140, loss: 0.14635, accu: 0.89375, speed: 3.45 step/s
global step 150, epoch: 1, batch: 150, loss: 0.16516, accu: 0.89875, speed: 3.42 step/s
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/train.py --max_steps 150 --device=xpu  --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 --epoch=1     --batch_size=32     >/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:40:54,233] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load '/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model'.[0m
[32m[2023-12-21 12:40:54,234] [    INFO][0m - Loading configuration file /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:40:54,234] [    INFO][0m - Loading weights file /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:40:54,587] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:40:54.596236  3833 xpu_context.cc:151] Please NOTE: xpu device: 0
[32m[2023-12-21 12:40:57,711] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.
[0m
[32m[2023-12-21 12:40:57,712] [    INFO][0m - All the weights of ErnieForSequenceClassification were initialized from the model checkpoint at /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 12:40:59.099231  3833 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:41:04.337399  3901 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:41:04.340456  3901 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:41:04.532073  3901 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:41:04.533931  3901 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:41:04.552052  3901 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:41:04.570536  3901 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:41:04.587409  3901 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:41:04.595968  3901 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:41:04.603286  3901 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:41:04.615593  3901 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:41:04.617435  3901 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:41:04.620565  3901 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:41:04.630901  3901 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:41:04.651489  3901 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:41:04.652278  3901 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:41:04.652289  3901 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:41:04.653167  3901 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:41:04,653] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:41:04,653] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:41:04,679] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:41:04,679] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:41:04.720727  3901 onednn_context.cc:81] oneDNN v3.2.1
Data: 这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 	 Label: negative
Data: 怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！ 	 Label: negative
Data: 还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。 	 Label: negative
Data: 交通方便；环境很好；服务态度很好 房间较小 	 Label: positive
Data: 不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。 	 Label: positive
Data: 有了第一本书的铺垫，读第二本的时候开始进入状态。基本上第二本就围绕主角们的能力训练展开，故事的主要发生场地设置在美洲的亚马逊丛林。心里一直疑惑这和西藏有什么关系，不过大概看完全书才能知道内里的线索。其中描述了很多热带雨林中特有的神秘动植物以及一些生存技巧和常识，受益匪浅。能够想像出要写这样一部书，融合这样许多的知识，作者需要花费多少心血来搜集和整理并成文。 	 Label: positive
Data: 前台接待太差，酒店有A B楼之分，本人check－in后，前台未告诉B楼在何处，并且B楼无明显指示；房间太小，根本不像4星级设施，下次不会再选择入住此店啦。 	 Label: negative
Data: 1. 白色的，很漂亮，做工还可以； 2. 网上的软件资源非常丰富，这是我买它的最主要原因； 3. 电池不错，昨天从下午两点到晚上十点还有25分钟的剩余时间（关闭摄像头，无线和蓝牙）主要拷贝东西，看起来正常使用八小时左右没问题； 4. 散热不错，CPU核心不过40~55度，很多小本要上到80度了； 5. 变压器很小巧，很多小本的电源都用的是大本的电源，本倒是很轻，可旅行重量还是比较重。 	 Label: positive
Data: 在当当上买了很多书，都懒于评论。但这套书真的很好，3册都非常精彩。我家小一的女儿，认字多，非常喜爱，每天睡前必读。她还告诉我，学校的语文课本中也有相同的文章。我还借给我的同事的女儿，我同事一直头疼她女儿不爱看书，但这套书，她女儿非常喜欢。两周就看完了。建议买。很少写评论，但忍不住为这套书写下。也给别的读者参考下。 	 Label: positive
Data: 19天硬盘就罢工了~~~算上运来的一周都没用上15天~~~可就是不能换了~~~唉~~~~你说这算什么事呀~~~ 	 Label: negative
Data: 书一到，即被朋友借走，我是早已看过了的，买它，是因为张爱玲。看过张子静的《我的姊姊张爱玲》，可以看出，张对自己的弟弟也是不甚热情，这是她的性格。但是有一次张子静去找她，那天张爱玲却是很开心的样子。后来张子静推算，彼时张爱玲正与胡兰成相恋。很多人不耻于胡兰成的用情不专，我当然也很反感，但我也想说，胡兰成是给过张爱的欢喜的。张是怎样聪明的女子，若不是爱，她也不会那么痴。属于他们的爱情，让他们自己品尝与承担。胡的文字，确实是不错的，有一种儒雅在里面。不知道是不是那份淡淡的气氛，曾经让张心动——“岁月静好，现世安稳”。 	 Label: positive
Data: 书的印刷看起来也不好,画面也不漂亮,内容的连惯性也不好,所以让人看起来一点也不能吸引孩子的兴趣和眼球,只是实在不想睡觉看一下,起催眠作用.作为家长来说,我也不爱讲这几本书. 	 Label: negative
Data: 没有许多网友评价热度高的问题，第一次要手动安装winxp，主板的blis-硬盘重新设置 	 Label: negative
Data: 请问：有些字打错了，我怎么样才可以回去编辑一下啊？ 	 Label: negative
Data: 没有蓝牙、摄像头，连麦克风都省了 装XP有点小麻烦，MSDN原版没有SATA驱动无法安装，装雨林木风GHOST纯净版OK。 自带无线网卡驱动不能用，需到官网下载。 	 Label: negative
Data: 在网上偶尔看到别人的推荐，于是买一本看看，求的是轻松。那天正好坐飞机去北京，在飞机上我不停喷笑，努力克制，好歹也要装出“杜拉拉”型的职场白骨精形象。等下飞机的时候，刚好看完，却开始心里泛堵。总觉得像咬了口大红薯，虽然味美，但是卡在食道下不去，憋得慌。坐出租去酒店的时候，看到初春的北京还是处处冷涩，突然有种顿悟的感觉，北京的青春和南方的如此不同，彪悍之余还带着一点忧伤。 	 Label: negative
Data: 新机拿到手就有硬件问题，而且等了6天才到货，第二天就返修，到现在还没得到处理意见！ 	 Label: negative
Data: 风扇确实够响的，尤其是到晚上周围安静下来。风扇频频开启，发热量有些惊人 	 Label: negative
Data: 价廉物美的好选择，晚上到机住十分合适，服务很热情，房间没地毯很清爽，海景很好看 	 Label: positive
Data: 我在晚上6点30左右入住的,当时是一位男服务员为我登记,我问他可不可以作信用卡预授权，他说不可以但建议我可以用现金作为押金到退房时可以用信用卡消费．房间很大但很旧，电视机只能收到４个台的节目，在第三天我退房时要求用信用卡消费，当时一位女服务员说我之前是用现金作为押金是不可以再用信用卡消费的，还反问我说如果我要消费的话在入住时为什么不做预授权，当时我就把之前那位男服务员说的话跟他说一次后又反口说是因为现在机器坏了不能用，我跟她说我在３０分钟之前见到有一位客人才用信用卡消费过，为什么会这么快就坏了，她见我态度那么强硬就帮我用卡消费了，事实上证明了刷卡机并没有坏，当时因为我们赶着到机场坐飞机就没有再跟她理论了，但一直觉得心中有气． 	 Label: negative
Data: 3999的时候抢购的 运气真好 这个价格还有什么号说的 品牌和价格都很不错 	 Label: positive
Data: 这次入住少林宾馆还是比较满意的,宾馆的位置比较好找,从客运站打部计程车只要三几分钟,5块钱就可以到了,房间比较干净,算是比较宽敞的,洗澡的热水也挺舒服,最好的地方是宾馆外面有一条很旺的小吃街,晚上6点左右就开了,很多麻辣烫之类的小吃,而且有起码二三十档,选择很多,解决吃的问题很方便,而且价格也相当评议,我和我朋友一起,两个人十来块钱就吃得好饱了:) 	 Label: positive
Data: 这一套书我基本买齐了，也看了好多本了。是利用闲暇时间巩固英语，学习知识的好东东。当然，再好的东西也要适合才真的能用上，如果你觉得你目前的任务是冲刺学习，那这书不一定适合你。但是，如果，你想有一个长效的学习习惯，你就可以考虑这套英文读物。另外，MP3也很不错，可在上下班的路上听，就像听广播小说，在娱乐中提高了听力。 	 Label: positive
Data: 酒店设施尚可,房间内镜子太多.携程的套餐包括送水果,还有巧克力,比较温馨. 服务一般,入住时前台试图诱导我买早餐,两大一小每天早餐440元,两天共计880元,另外冰箱里的东西可以随便吃,被我拒绝了!第二天就不再送水果巧克力了,连免费的两瓶水也不给了...酒店离九龙又一城步行约15分钟,旁边有个公园,但是离地铁远了点,需要做小巴,有时6元每人,有时3元每人,开得飞快很吓人,小孩同样费用,就是这点不满意,还是住得离地铁近些比较方便. 	 Label: positive
Data: 这个价格这种房间环境很不错，感觉很干净^_^ 兴旺楼给我个人感觉很雅...下次来，肯定还定这里。。。 	 Label: positive
Data: 选择的事例太离奇了，夸大了心理咨询的现实意义，让人失去了信任感！如果说这样写的效果能在一开始抓住读者的眼球，但是看到案例主人公心理问题的原因解释时就逐渐失去了兴趣，反正有点拣了芝麻丢了西瓜的感觉。 	 Label: negative
Data: 简单，大方，在同类尺寸的款型的笔记本中不显厚重，轻薄感！很安静，几乎没有声音，音质不错屏幕不错，显的细腻可观。 感谢马连道提货点的工作人员（前台客服），服务态度超好，值得贵公司其他员工学习。 	 Label: positive
Data: 硬盘到手就发现一个坏块，因为是完美屏，没回京东换新，花了两天在本地换新硬盘，发票都不需要；电池衔接很松，可有1mm间隙；出厂时A、B面贴的保护膜太敷衍，太多气泡，虽然反正要撕掉，但说明厂家态度不严谨。 	 Label: negative
Data: 没发现什么优点，回来了开机什么都没有，有自己装的XP也没费什么劲，麦有些问题。对方听不清。调了好久才好了，整体还不错 	 Label: positive
Data: 外观漂亮，系统不是很难弄，刚开始看了很多评价说难弄，实际上没那么困难，将boot 选项第一选项选从DVD起动，其它就直接安装了。我的是安装盘，不是Ghost的。内存是三星DDR3-1066，不是DDR3-800.Everest测试是DDR3-800.我用超级兔子测试是DDR3-1066.友达LED屏，独显，满意，到手是完美屏，不错 	 Label: positive
Data: 9号下午下的单子,11中午拿到货,真的很赞.机子开开后发现送个内胆包,一个擦拭布,试用了2个多小时,发现声音很小,下在速度很很不错，开网页能同时6-7不卡. 	 Label: positive
Data: 赠送的系统慢就改装XP算了，内装的备份是用GHOST的就好了，应该再配一根USB电脑连接线就更理想了。 	 Label: positive
Data: 原本在网上订了两个套房，入住后，携程还给我打电话问是否只入住了一间，要扣我信用卡里的钱。真不知道酒店与携程是如何衔接的？ 宾馆反馈 2007年12月7日 ： 我们非常感谢您的留言，非常抱歉由于一些小误会给您带来的困惑。根据我们的担保订房要求，对于确认担保的房间未入住的会收取当天房费。当然，我们也会进一步加强与携程的沟通与协调，让您今后的入住感觉更加舒适与愉快。 	 Label: negative
Data: 这本书实在太烂了,什么朗读手册,一点朗读的内容都没有.看了几页就不想看下去了. 	 Label: negative
Data: 该酒店实际是兰州铁路局的内部招待所，位于火车站出站口处，如果不怕火车吵，那么晚上可以睡得着。 价格不算便宜，只能是一般。 房间很旧，房内设备一般，卫生排气扇坏了，抽水马桶也坏了，维修人员脾气很暴，要很有礼貌的对待。 前台服务人员素质比较差，早上退房时前台竟然找不到服务员，等了近半个小时，耽误宝贵的时间。 入住酒店停车竟然要收费。 建议不要将该酒店做为入住选择，除非你对火车有特殊的感情。 	 Label: negative
Data: 当时是同事极力推荐这本书。我看到网上的介绍和那么多的“名人”位置鼓噪就觉得忽悠大于内容。同事不甘心硬是买了来看。。果然，看后大呼上当！ 	 Label: negative
Data: 从携程订房无数，唯独这家实在不感恭维。从走廊就开始一股酶味，房间也是。而且窗户是封死的。最可恶的是明明订的房间含两份早餐，前台只给一份的餐券，还说要另一份的话要就餐者拿身份证来登记。我靠，中国什么时候兴吃饭也要身份证了。我花了两份的钱，凭什么只给我一份。我的另一份，我把餐券撕了扔了，关别人什么事。此宾馆实在霸道，以后离之远点。 	 Label: negative
Data: 钱也付了，书干等不来，好不容易找到客服电话，原来书没有了。钱突然退回来，单位查起来还以为受贿，连个证明都没有。伤心。。当当做了那么多年，你们的总裁也天天媒体。服务还是不够精细，要不然我去你们那里管客服得了，肯定比现在好书又买了一次，总算买到了。。下次到卓越试试，看看是不是服务一样的烂 	 Label: negative
Data: 容易产生指纹。不习惯分区。由于出货量大了，我觉得在配货的时候更快一点就好了，我昨天到中通迅递，看到的好多都是京东的物品啊。 	 Label: negative
Data: 六心电池装在后面突出一大块影响美观，速度慢机器有点卡，放歌一停顿一停顿，重量比一般的上网本重，触摸板鼠标反应不灵敏，滑的区域太小， 	 Label: negative
Data: 这套书是买给儿子的，小家伙两岁半不到，正是这套书适合的年龄阶段。之前看了许多评价，这套书好评如潮，令我十分期待。因为一本好书可以带给人知识，给人以愉悦，哪怕他只是一个小孩子。拿到书的瞬间我就肯定我的孩子会非常喜欢这套书。果真如此，小家伙非常喜欢，与其说是看，不如说是玩起这套书来不亦乐乎。再加上大人在一旁的指点与协助，真得是一个非常好的亲子游戏呢。第一次写评价，希望好书共享。 	 Label: positive
Data: 本人于清明假期一家三口高高兴兴通过携程订房入住，本想来个豪华的自驾游，哪知给我的房间小得可怜，对比我住过的国内外四星级以上酒店的环境，环境算得上是“恶劣”！ 在前台办理入住手续时，工作人员告诉我1.2米以下小孩早餐免费，于是补上一个成年人的早餐，第二天用早餐时，餐厅经理却跟我说小孩需付半价，而且是算对外收费全价！ 最后经过一番交涉，酒店大堂经理给折了个8折价，但已经让我们一家游趣全无！ 酒店的房间环境根本不值四星水平！装修旧，房间小，但酒店的地理位置好，所以就能如此定高价么？不解！ 补充点评 2008年4月23日 ： 对了，还有上网居然是收费的，这还是在国内第一碰到！不可思议！ 	 Label: negative
Data: 这样的男人，温柔的过了头，找不到一丝男人的气概。只会说：这是好的。。。这也是好的。。。无法欣赏这样的文笔优美在何处。爱情也是奇怪，张爱玲爱上了这样的一个人，隐隐地，大概是一辈子的影子。 	 Label: negative
Data: 第一次装xp到一半蓝屏，不过进bios，在advanced一栏里把硬盘模式从achi改为ide即可。驱动网上都有，下载g430驱动就行了 	 Label: negative
Data: 光驱确实不怎么好，装系统的时候就怕它挂掉，还好顺利装上系统了，装好后立马做了个镜像，估计以后也不用光驱了 	 Label: negative
Data: 在我刚开始从事设计的时候，阿尔瓦 阿尔托是第一个与我有缘，超越时代，地域对我指点的前辈。在他的世界里 没有概念的约束，这是任何渴望为了自由设计而用心的人最需要的精神内在与动力。美丽需要的两个前提是 干净 和 幽默。美丽的人。而这位建筑师 设计家可能具备的更多。宜家家居至今还在销售他在上个世纪30年代设计的曲线板读书椅。这本书物美价廉，适合热爱自由设计的每个人，我想是这样的。一九七九五月韩 	 Label: positive
Data: 外包装写是内存2G，可是内清单标的是1G，不知道是怎么回事，希望有了解的朋友帮忙解决一下，谢谢。 	 Label: negative
Data: 这本书，让我觉得很不值得买。关于毕淑敏的犀利洒脱细腻，在这本小说中无法得以体现。似乎迎合了目前一部分低层次读者的需要，有些媚俗之嫌。很想退掉。因为，我直接没有看完。 	 Label: negative
Data: 10月16日入住该酒店.感觉不错哦,离汽车站和火车站的距离都不是非常远.房间很干净,空间也非常大.周边的交通环境也好.可能住的楼层比较高,也没有任何的噪音.晚上有发生热水管道出问题的小插曲,但服务态度良好,解决问题也很及时,丝毫没有影响到对酒店的好感.主要是价格非常合理.可以说对该酒店非常满意.小缺点就是硬件 	 Label: positive
Data: 总体说来,应该是二星级宾馆.因为定的时间比较晚,所以只有这家酒店了.入住时,问前台一些黄山城市的信息,她们说不知道.一个入住也要办理二十几分钟,而且就我们几个人.房间的设施很陈旧,住的也不舒服. 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:41:20.231243  3968 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:41:20.234295  3968 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:41:20.437371  3968 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:41:20.439237  3968 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:41:20.457389  3968 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:41:20.475772  3968 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:41:20.492573  3968 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:41:20.501060  3968 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:41:20.508432  3968 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:41:20.521021  3968 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:41:20.522881  3968 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:41:20.525920  3968 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:41:20.536273  3968 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:41:20.557061  3968 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:41:20.557865  3968 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:41:20.557876  3968 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:41:20.558748  3968 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:41:20,559] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:41:20,559] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:41:20,585] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:41:20,585] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:41:20.626617  3968 onednn_context.cc:81] oneDNN v3.2.1
Data: 这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 	 Label: negative
Data: 怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！ 	 Label: negative
Data: 还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。 	 Label: negative
Data: 交通方便；环境很好；服务态度很好 房间较小 	 Label: positive
Data: 不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。 	 Label: positive
Data: 有了第一本书的铺垫，读第二本的时候开始进入状态。基本上第二本就围绕主角们的能力训练展开，故事的主要发生场地设置在美洲的亚马逊丛林。心里一直疑惑这和西藏有什么关系，不过大概看完全书才能知道内里的线索。其中描述了很多热带雨林中特有的神秘动植物以及一些生存技巧和常识，受益匪浅。能够想像出要写这样一部书，融合这样许多的知识，作者需要花费多少心血来搜集和整理并成文。 	 Label: positive
Data: 前台接待太差，酒店有A B楼之分，本人check－in后，前台未告诉B楼在何处，并且B楼无明显指示；房间太小，根本不像4星级设施，下次不会再选择入住此店啦。 	 Label: negative
Data: 1. 白色的，很漂亮，做工还可以； 2. 网上的软件资源非常丰富，这是我买它的最主要原因； 3. 电池不错，昨天从下午两点到晚上十点还有25分钟的剩余时间（关闭摄像头，无线和蓝牙）主要拷贝东西，看起来正常使用八小时左右没问题； 4. 散热不错，CPU核心不过40~55度，很多小本要上到80度了； 5. 变压器很小巧，很多小本的电源都用的是大本的电源，本倒是很轻，可旅行重量还是比较重。 	 Label: positive
Data: 在当当上买了很多书，都懒于评论。但这套书真的很好，3册都非常精彩。我家小一的女儿，认字多，非常喜爱，每天睡前必读。她还告诉我，学校的语文课本中也有相同的文章。我还借给我的同事的女儿，我同事一直头疼她女儿不爱看书，但这套书，她女儿非常喜欢。两周就看完了。建议买。很少写评论，但忍不住为这套书写下。也给别的读者参考下。 	 Label: positive
Data: 19天硬盘就罢工了~~~算上运来的一周都没用上15天~~~可就是不能换了~~~唉~~~~你说这算什么事呀~~~ 	 Label: negative
Data: 书一到，即被朋友借走，我是早已看过了的，买它，是因为张爱玲。看过张子静的《我的姊姊张爱玲》，可以看出，张对自己的弟弟也是不甚热情，这是她的性格。但是有一次张子静去找她，那天张爱玲却是很开心的样子。后来张子静推算，彼时张爱玲正与胡兰成相恋。很多人不耻于胡兰成的用情不专，我当然也很反感，但我也想说，胡兰成是给过张爱的欢喜的。张是怎样聪明的女子，若不是爱，她也不会那么痴。属于他们的爱情，让他们自己品尝与承担。胡的文字，确实是不错的，有一种儒雅在里面。不知道是不是那份淡淡的气氛，曾经让张心动——“岁月静好，现世安稳”。 	 Label: positive
Data: 书的印刷看起来也不好,画面也不漂亮,内容的连惯性也不好,所以让人看起来一点也不能吸引孩子的兴趣和眼球,只是实在不想睡觉看一下,起催眠作用.作为家长来说,我也不爱讲这几本书. 	 Label: negative
Data: 没有许多网友评价热度高的问题，第一次要手动安装winxp，主板的blis-硬盘重新设置 	 Label: negative
Data: 请问：有些字打错了，我怎么样才可以回去编辑一下啊？ 	 Label: negative
Data: 没有蓝牙、摄像头，连麦克风都省了 装XP有点小麻烦，MSDN原版没有SATA驱动无法安装，装雨林木风GHOST纯净版OK。 自带无线网卡驱动不能用，需到官网下载。 	 Label: negative
Data: 在网上偶尔看到别人的推荐，于是买一本看看，求的是轻松。那天正好坐飞机去北京，在飞机上我不停喷笑，努力克制，好歹也要装出“杜拉拉”型的职场白骨精形象。等下飞机的时候，刚好看完，却开始心里泛堵。总觉得像咬了口大红薯，虽然味美，但是卡在食道下不去，憋得慌。坐出租去酒店的时候，看到初春的北京还是处处冷涩，突然有种顿悟的感觉，北京的青春和南方的如此不同，彪悍之余还带着一点忧伤。 	 Label: negative
Data: 新机拿到手就有硬件问题，而且等了6天才到货，第二天就返修，到现在还没得到处理意见！ 	 Label: negative
Data: 风扇确实够响的，尤其是到晚上周围安静下来。风扇频频开启，发热量有些惊人 	 Label: negative
Data: 价廉物美的好选择，晚上到机住十分合适，服务很热情，房间没地毯很清爽，海景很好看 	 Label: positive
Data: 我在晚上6点30左右入住的,当时是一位男服务员为我登记,我问他可不可以作信用卡预授权，他说不可以但建议我可以用现金作为押金到退房时可以用信用卡消费．房间很大但很旧，电视机只能收到４个台的节目，在第三天我退房时要求用信用卡消费，当时一位女服务员说我之前是用现金作为押金是不可以再用信用卡消费的，还反问我说如果我要消费的话在入住时为什么不做预授权，当时我就把之前那位男服务员说的话跟他说一次后又反口说是因为现在机器坏了不能用，我跟她说我在３０分钟之前见到有一位客人才用信用卡消费过，为什么会这么快就坏了，她见我态度那么强硬就帮我用卡消费了，事实上证明了刷卡机并没有坏，当时因为我们赶着到机场坐飞机就没有再跟她理论了，但一直觉得心中有气． 	 Label: negative
Data: 3999的时候抢购的 运气真好 这个价格还有什么号说的 品牌和价格都很不错 	 Label: positive
Data: 这次入住少林宾馆还是比较满意的,宾馆的位置比较好找,从客运站打部计程车只要三几分钟,5块钱就可以到了,房间比较干净,算是比较宽敞的,洗澡的热水也挺舒服,最好的地方是宾馆外面有一条很旺的小吃街,晚上6点左右就开了,很多麻辣烫之类的小吃,而且有起码二三十档,选择很多,解决吃的问题很方便,而且价格也相当评议,我和我朋友一起,两个人十来块钱就吃得好饱了:) 	 Label: positive
Data: 这一套书我基本买齐了，也看了好多本了。是利用闲暇时间巩固英语，学习知识的好东东。当然，再好的东西也要适合才真的能用上，如果你觉得你目前的任务是冲刺学习，那这书不一定适合你。但是，如果，你想有一个长效的学习习惯，你就可以考虑这套英文读物。另外，MP3也很不错，可在上下班的路上听，就像听广播小说，在娱乐中提高了听力。 	 Label: positive
Data: 酒店设施尚可,房间内镜子太多.携程的套餐包括送水果,还有巧克力,比较温馨. 服务一般,入住时前台试图诱导我买早餐,两大一小每天早餐440元,两天共计880元,另外冰箱里的东西可以随便吃,被我拒绝了!第二天就不再送水果巧克力了,连免费的两瓶水也不给了...酒店离九龙又一城步行约15分钟,旁边有个公园,但是离地铁远了点,需要做小巴,有时6元每人,有时3元每人,开得飞快很吓人,小孩同样费用,就是这点不满意,还是住得离地铁近些比较方便. 	 Label: positive
Data: 这个价格这种房间环境很不错，感觉很干净^_^ 兴旺楼给我个人感觉很雅...下次来，肯定还定这里。。。 	 Label: positive
Data: 选择的事例太离奇了，夸大了心理咨询的现实意义，让人失去了信任感！如果说这样写的效果能在一开始抓住读者的眼球，但是看到案例主人公心理问题的原因解释时就逐渐失去了兴趣，反正有点拣了芝麻丢了西瓜的感觉。 	 Label: negative
Data: 简单，大方，在同类尺寸的款型的笔记本中不显厚重，轻薄感！很安静，几乎没有声音，音质不错屏幕不错，显的细腻可观。 感谢马连道提货点的工作人员（前台客服），服务态度超好，值得贵公司其他员工学习。 	 Label: positive
Data: 硬盘到手就发现一个坏块，因为是完美屏，没回京东换新，花了两天在本地换新硬盘，发票都不需要；电池衔接很松，可有1mm间隙；出厂时A、B面贴的保护膜太敷衍，太多气泡，虽然反正要撕掉，但说明厂家态度不严谨。 	 Label: negative
Data: 没发现什么优点，回来了开机什么都没有，有自己装的XP也没费什么劲，麦有些问题。对方听不清。调了好久才好了，整体还不错 	 Label: positive
Data: 外观漂亮，系统不是很难弄，刚开始看了很多评价说难弄，实际上没那么困难，将boot 选项第一选项选从DVD起动，其它就直接安装了。我的是安装盘，不是Ghost的。内存是三星DDR3-1066，不是DDR3-800.Everest测试是DDR3-800.我用超级兔子测试是DDR3-1066.友达LED屏，独显，满意，到手是完美屏，不错 	 Label: positive
Data: 9号下午下的单子,11中午拿到货,真的很赞.机子开开后发现送个内胆包,一个擦拭布,试用了2个多小时,发现声音很小,下在速度很很不错，开网页能同时6-7不卡. 	 Label: positive
Data: 赠送的系统慢就改装XP算了，内装的备份是用GHOST的就好了，应该再配一根USB电脑连接线就更理想了。 	 Label: positive
Data: 原本在网上订了两个套房，入住后，携程还给我打电话问是否只入住了一间，要扣我信用卡里的钱。真不知道酒店与携程是如何衔接的？ 宾馆反馈 2007年12月7日 ： 我们非常感谢您的留言，非常抱歉由于一些小误会给您带来的困惑。根据我们的担保订房要求，对于确认担保的房间未入住的会收取当天房费。当然，我们也会进一步加强与携程的沟通与协调，让您今后的入住感觉更加舒适与愉快。 	 Label: negative
Data: 这本书实在太烂了,什么朗读手册,一点朗读的内容都没有.看了几页就不想看下去了. 	 Label: negative
Data: 该酒店实际是兰州铁路局的内部招待所，位于火车站出站口处，如果不怕火车吵，那么晚上可以睡得着。 价格不算便宜，只能是一般。 房间很旧，房内设备一般，卫生排气扇坏了，抽水马桶也坏了，维修人员脾气很暴，要很有礼貌的对待。 前台服务人员素质比较差，早上退房时前台竟然找不到服务员，等了近半个小时，耽误宝贵的时间。 入住酒店停车竟然要收费。 建议不要将该酒店做为入住选择，除非你对火车有特殊的感情。 	 Label: negative
Data: 当时是同事极力推荐这本书。我看到网上的介绍和那么多的“名人”位置鼓噪就觉得忽悠大于内容。同事不甘心硬是买了来看。。果然，看后大呼上当！ 	 Label: negative
Data: 从携程订房无数，唯独这家实在不感恭维。从走廊就开始一股酶味，房间也是。而且窗户是封死的。最可恶的是明明订的房间含两份早餐，前台只给一份的餐券，还说要另一份的话要就餐者拿身份证来登记。我靠，中国什么时候兴吃饭也要身份证了。我花了两份的钱，凭什么只给我一份。我的另一份，我把餐券撕了扔了，关别人什么事。此宾馆实在霸道，以后离之远点。 	 Label: negative
Data: 钱也付了，书干等不来，好不容易找到客服电话，原来书没有了。钱突然退回来，单位查起来还以为受贿，连个证明都没有。伤心。。当当做了那么多年，你们的总裁也天天媒体。服务还是不够精细，要不然我去你们那里管客服得了，肯定比现在好书又买了一次，总算买到了。。下次到卓越试试，看看是不是服务一样的烂 	 Label: negative
Data: 容易产生指纹。不习惯分区。由于出货量大了，我觉得在配货的时候更快一点就好了，我昨天到中通迅递，看到的好多都是京东的物品啊。 	 Label: negative
Data: 六心电池装在后面突出一大块影响美观，速度慢机器有点卡，放歌一停顿一停顿，重量比一般的上网本重，触摸板鼠标反应不灵敏，滑的区域太小， 	 Label: negative
Data: 这套书是买给儿子的，小家伙两岁半不到，正是这套书适合的年龄阶段。之前看了许多评价，这套书好评如潮，令我十分期待。因为一本好书可以带给人知识，给人以愉悦，哪怕他只是一个小孩子。拿到书的瞬间我就肯定我的孩子会非常喜欢这套书。果真如此，小家伙非常喜欢，与其说是看，不如说是玩起这套书来不亦乐乎。再加上大人在一旁的指点与协助，真得是一个非常好的亲子游戏呢。第一次写评价，希望好书共享。 	 Label: positive
Data: 本人于清明假期一家三口高高兴兴通过携程订房入住，本想来个豪华的自驾游，哪知给我的房间小得可怜，对比我住过的国内外四星级以上酒店的环境，环境算得上是“恶劣”！ 在前台办理入住手续时，工作人员告诉我1.2米以下小孩早餐免费，于是补上一个成年人的早餐，第二天用早餐时，餐厅经理却跟我说小孩需付半价，而且是算对外收费全价！ 最后经过一番交涉，酒店大堂经理给折了个8折价，但已经让我们一家游趣全无！ 酒店的房间环境根本不值四星水平！装修旧，房间小，但酒店的地理位置好，所以就能如此定高价么？不解！ 补充点评 2008年4月23日 ： 对了，还有上网居然是收费的，这还是在国内第一碰到！不可思议！ 	 Label: negative
Data: 这样的男人，温柔的过了头，找不到一丝男人的气概。只会说：这是好的。。。这也是好的。。。无法欣赏这样的文笔优美在何处。爱情也是奇怪，张爱玲爱上了这样的一个人，隐隐地，大概是一辈子的影子。 	 Label: negative
Data: 第一次装xp到一半蓝屏，不过进bios，在advanced一栏里把硬盘模式从achi改为ide即可。驱动网上都有，下载g430驱动就行了 	 Label: negative
Data: 光驱确实不怎么好，装系统的时候就怕它挂掉，还好顺利装上系统了，装好后立马做了个镜像，估计以后也不用光驱了 	 Label: negative
Data: 在我刚开始从事设计的时候，阿尔瓦 阿尔托是第一个与我有缘，超越时代，地域对我指点的前辈。在他的世界里 没有概念的约束，这是任何渴望为了自由设计而用心的人最需要的精神内在与动力。美丽需要的两个前提是 干净 和 幽默。美丽的人。而这位建筑师 设计家可能具备的更多。宜家家居至今还在销售他在上个世纪30年代设计的曲线板读书椅。这本书物美价廉，适合热爱自由设计的每个人，我想是这样的。一九七九五月韩 	 Label: positive
Data: 外包装写是内存2G，可是内清单标的是1G，不知道是怎么回事，希望有了解的朋友帮忙解决一下，谢谢。 	 Label: negative
Data: 这本书，让我觉得很不值得买。关于毕淑敏的犀利洒脱细腻，在这本小说中无法得以体现。似乎迎合了目前一部分低层次读者的需要，有些媚俗之嫌。很想退掉。因为，我直接没有看完。 	 Label: negative
Data: 10月16日入住该酒店.感觉不错哦,离汽车站和火车站的距离都不是非常远.房间很干净,空间也非常大.周边的交通环境也好.可能住的楼层比较高,也没有任何的噪音.晚上有发生热水管道出问题的小插曲,但服务态度良好,解决问题也很及时,丝毫没有影响到对酒店的好感.主要是价格非常合理.可以说对该酒店非常满意.小缺点就是硬件 	 Label: positive
Data: 总体说来,应该是二星级宾馆.因为定的时间比较晚,所以只有这家酒店了.入住时,问前台一些黄山城市的信息,她们说不知道.一个入住也要办理二十几分钟,而且就我们几个人.房间的设施很陈旧,住的也不舒服. 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
Does not support hardware other than CPU and GPU Currently!
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:41:24,925 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 12:41:24,925 auto_parallel_config: None
LAUNCH INFO 2023-12-21 12:41:24,926 auto_tuner_json: None
LAUNCH INFO 2023-12-21 12:41:24,926 devices: 0,1
LAUNCH INFO 2023-12-21 12:41:24,926 elastic_level: -1
LAUNCH INFO 2023-12-21 12:41:24,926 elastic_timeout: 30
LAUNCH INFO 2023-12-21 12:41:24,926 enable_gpu_log: True
LAUNCH INFO 2023-12-21 12:41:24,926 gloo_port: 6767
LAUNCH INFO 2023-12-21 12:41:24,926 host: None
LAUNCH INFO 2023-12-21 12:41:24,926 ips: None
LAUNCH INFO 2023-12-21 12:41:24,926 job_id: default
LAUNCH INFO 2023-12-21 12:41:24,926 legacy: False
LAUNCH INFO 2023-12-21 12:41:24,926 log_dir: log
LAUNCH INFO 2023-12-21 12:41:24,926 log_level: INFO
LAUNCH INFO 2023-12-21 12:41:24,926 log_overwrite: False
LAUNCH INFO 2023-12-21 12:41:24,926 master: None
LAUNCH INFO 2023-12-21 12:41:24,926 max_restart: 3
LAUNCH INFO 2023-12-21 12:41:24,926 nnodes: 1
LAUNCH INFO 2023-12-21 12:41:24,926 nproc_per_node: None
LAUNCH INFO 2023-12-21 12:41:24,926 rank: -1
LAUNCH INFO 2023-12-21 12:41:24,926 run_mode: collective
LAUNCH INFO 2023-12-21 12:41:24,926 server_num: None
LAUNCH INFO 2023-12-21 12:41:24,926 servers: 
LAUNCH INFO 2023-12-21 12:41:24,926 sort_ip: False
LAUNCH INFO 2023-12-21 12:41:24,926 start_port: 6070
LAUNCH INFO 2023-12-21 12:41:24,926 trainer_num: None
LAUNCH INFO 2023-12-21 12:41:24,926 trainers: 
LAUNCH INFO 2023-12-21 12:41:24,926 training_script: ./test_tipc/ernie_text_cls/train.py
LAUNCH INFO 2023-12-21 12:41:24,926 training_script_args: ['--max_steps', '150', '--device=xpu', '--save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1', '--epoch=1', '--batch_size=32']
LAUNCH INFO 2023-12-21 12:41:24,926 with_gloo: 1
LAUNCH INFO 2023-12-21 12:41:24,926 --------------------------------------------------
LAUNCH INFO 2023-12-21 12:41:24,927 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 12:41:24,928 Run Pod: gentob, replicas 2, status ready
LAUNCH INFO 2023-12-21 12:41:24,945 Watching Pod: gentob, replicas 2, status running
[32m[2023-12-21 12:35:48,088] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:35:49,163] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:35:49,164] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:35:49,164] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 4, batch: 9, loss: 0.01433, speed: 1.77 step/s
global step 120, epoch: 4, batch: 19, loss: 0.00785, speed: 2.61 step/s
global step 130, epoch: 5, batch: 4, loss: 0.00875, speed: 2.62 step/s
global step 140, epoch: 5, batch: 14, loss: 0.02638, speed: 2.59 step/s
global step 150, epoch: 5, batch: 24, loss: 0.00780, speed: 2.80 step/s
I1221 12:36:10.369403  3361 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
[32m[2023-12-21 12:40:32,044] [    INFO][0m - global step 60 / 2668000, loss: 2.493156, avg_reader_cost: 0.00039 sec, avg_batch_cost: 37.93461 sec, avg_samples: 512.00000, ips: 13.49691 words/sec,  [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 12:41:28.279942  4091 tcp_utils.cc:181] The server starts to listen on IP_ANY:57985
I1221 12:41:28.280287  4091 tcp_utils.cc:130] Successfully connected to 127.0.0.1:57985
[32m[2023-12-21 12:41:28,591] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:41:28,591] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:41:28,591] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:41:28,881] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:41:28.886257  4091 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:41:31,839] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieForSequenceClassification: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:41:31,839] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['classifier.weight', 'ernie.pooler.dense.weight', 'classifier.bias', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:41:31,843] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:41:31,844] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:41:31,868] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:41:31,868] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
global step 10, epoch: 1, batch: 10, loss: 0.46738, accu: 0.64062, speed: 1.88 step/s
global step 20, epoch: 1, batch: 20, loss: 0.19723, accu: 0.76250, speed: 1.80 step/s
global step 30, epoch: 1, batch: 30, loss: 0.19607, accu: 0.80625, speed: 1.81 step/s
global step 40, epoch: 1, batch: 40, loss: 0.69362, accu: 0.82188, speed: 1.82 step/s
global step 50, epoch: 1, batch: 50, loss: 0.42909, accu: 0.83062, speed: 1.81 step/s
global step 60, epoch: 1, batch: 60, loss: 0.26616, accu: 0.84271, speed: 1.81 step/s
global step 70, epoch: 1, batch: 70, loss: 0.27535, accu: 0.84911, speed: 1.81 step/s
global step 80, epoch: 1, batch: 80, loss: 0.22878, accu: 0.85352, speed: 1.81 step/s
global step 90, epoch: 1, batch: 90, loss: 0.37708, accu: 0.85694, speed: 1.82 step/s
global step 100, epoch: 1, batch: 100, loss: 0.49107, accu: 0.85844, speed: 1.83 step/s
eval loss: 0.24852, accu: 0.90667
[32m[2023-12-21 12:42:31,464] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:42:32,136] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:42:32,137] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:42:32,137] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 1, batch: 110, loss: 0.27770, accu: 0.90625, speed: 1.16 step/s
LAUNCH INFO 2023-12-21 12:43:00,050 Pod completed
LAUNCH INFO 2023-12-21 12:43:00,050 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 12:41:28.279942  4091 tcp_utils.cc:181] The server starts to listen on IP_ANY:57985
I1221 12:41:28.280287  4091 tcp_utils.cc:130] Successfully connected to 127.0.0.1:57985
[32m[2023-12-21 12:41:28,591] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:41:28,591] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:41:28,591] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:41:28,881] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:41:28.886257  4091 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:41:31,839] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieForSequenceClassification: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:41:31,839] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['classifier.weight', 'ernie.pooler.dense.weight', 'classifier.bias', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:41:31,843] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:41:31,844] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:41:31,868] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:41:31,868] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
global step 10, epoch: 1, batch: 10, loss: 0.46738, accu: 0.64062, speed: 1.88 step/s
global step 20, epoch: 1, batch: 20, loss: 0.19723, accu: 0.76250, speed: 1.80 step/s
global step 30, epoch: 1, batch: 30, loss: 0.19607, accu: 0.80625, speed: 1.81 step/s
global step 40, epoch: 1, batch: 40, loss: 0.69362, accu: 0.82188, speed: 1.82 step/s
global step 50, epoch: 1, batch: 50, loss: 0.42909, accu: 0.83062, speed: 1.81 step/s
global step 60, epoch: 1, batch: 60, loss: 0.26616, accu: 0.84271, speed: 1.81 step/s
global step 70, epoch: 1, batch: 70, loss: 0.27535, accu: 0.84911, speed: 1.81 step/s
global step 80, epoch: 1, batch: 80, loss: 0.22878, accu: 0.85352, speed: 1.81 step/s
global step 90, epoch: 1, batch: 90, loss: 0.37708, accu: 0.85694, speed: 1.82 step/s
global step 100, epoch: 1, batch: 100, loss: 0.49107, accu: 0.85844, speed: 1.83 step/s
eval loss: 0.24852, accu: 0.90667
[32m[2023-12-21 12:42:31,464] [    INFO][0m - Configuration saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:42:32,136] [    INFO][0m - Model weights saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:42:32,137] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:42:32,137] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 1, batch: 110, loss: 0.27770, accu: 0.90625, speed: 1.16 step/s
global step 120, epoch: 1, batch: 120, loss: 0.13740, accu: 0.90938, speed: 1.82 step/s
global step 130, epoch: 1, batch: 130, loss: 0.28602, accu: 0.91354, speed: 1.82 step/s
global step 140, epoch: 1, batch: 140, loss: 0.34615, accu: 0.91328, speed: 1.84 step/s
global step 150, epoch: 1, batch: 150, loss: 0.21612, accu: 0.91500, speed: 1.90 step/s
I1221 12:42:59.695907  4125 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python -m paddle.distributed.launch --gpus=0,1 ./test_tipc/ernie_text_cls/train.py --max_steps 150 --device=xpu --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 --epoch=1     --batch_size=32     - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:43:03,881] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load '/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model'.[0m
[32m[2023-12-21 12:43:03,882] [    INFO][0m - Loading configuration file /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/config.json[0m
[32m[2023-12-21 12:43:03,882] [    INFO][0m - Loading weights file /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams[0m
[32m[2023-12-21 12:43:04,258] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:43:04.262944  4155 xpu_context.cc:151] Please NOTE: xpu device: 0
[32m[2023-12-21 12:43:07,515] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.
[0m
[32m[2023-12-21 12:43:07,515] [    INFO][0m - All the weights of ErnieForSequenceClassification were initialized from the model checkpoint at /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 12:43:09.001647  4155 program_interpreter.cc:214] New Executor is Running.
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:43:14.140400  4223 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:43:14.143568  4223 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:43:14.356308  4223 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:43:14.358147  4223 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:43:14.376127  4223 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:43:14.394187  4223 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:43:14.411100  4223 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:43:14.419653  4223 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:43:14.426982  4223 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:43:14.439502  4223 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:43:14.441354  4223 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:43:14.444401  4223 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:43:14.454836  4223 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:43:14.475670  4223 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:43:14.476460  4223 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:43:14.476469  4223 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:43:14.477334  4223 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:43:14,477] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:43:14,477] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:43:14,504] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:43:14,504] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:43:14.545308  4223 onednn_context.cc:81] oneDNN v3.2.1
Data: 这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 	 Label: negative
Data: 怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！ 	 Label: negative
Data: 还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。 	 Label: negative
Data: 交通方便；环境很好；服务态度很好 房间较小 	 Label: positive
Data: 不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。 	 Label: positive
Data: 有了第一本书的铺垫，读第二本的时候开始进入状态。基本上第二本就围绕主角们的能力训练展开，故事的主要发生场地设置在美洲的亚马逊丛林。心里一直疑惑这和西藏有什么关系，不过大概看完全书才能知道内里的线索。其中描述了很多热带雨林中特有的神秘动植物以及一些生存技巧和常识，受益匪浅。能够想像出要写这样一部书，融合这样许多的知识，作者需要花费多少心血来搜集和整理并成文。 	 Label: positive
Data: 前台接待太差，酒店有A B楼之分，本人check－in后，前台未告诉B楼在何处，并且B楼无明显指示；房间太小，根本不像4星级设施，下次不会再选择入住此店啦。 	 Label: negative
Data: 1. 白色的，很漂亮，做工还可以； 2. 网上的软件资源非常丰富，这是我买它的最主要原因； 3. 电池不错，昨天从下午两点到晚上十点还有25分钟的剩余时间（关闭摄像头，无线和蓝牙）主要拷贝东西，看起来正常使用八小时左右没问题； 4. 散热不错，CPU核心不过40~55度，很多小本要上到80度了； 5. 变压器很小巧，很多小本的电源都用的是大本的电源，本倒是很轻，可旅行重量还是比较重。 	 Label: positive
Data: 在当当上买了很多书，都懒于评论。但这套书真的很好，3册都非常精彩。我家小一的女儿，认字多，非常喜爱，每天睡前必读。她还告诉我，学校的语文课本中也有相同的文章。我还借给我的同事的女儿，我同事一直头疼她女儿不爱看书，但这套书，她女儿非常喜欢。两周就看完了。建议买。很少写评论，但忍不住为这套书写下。也给别的读者参考下。 	 Label: positive
Data: 19天硬盘就罢工了~~~算上运来的一周都没用上15天~~~可就是不能换了~~~唉~~~~你说这算什么事呀~~~ 	 Label: negative
Data: 书一到，即被朋友借走，我是早已看过了的，买它，是因为张爱玲。看过张子静的《我的姊姊张爱玲》，可以看出，张对自己的弟弟也是不甚热情，这是她的性格。但是有一次张子静去找她，那天张爱玲却是很开心的样子。后来张子静推算，彼时张爱玲正与胡兰成相恋。很多人不耻于胡兰成的用情不专，我当然也很反感，但我也想说，胡兰成是给过张爱的欢喜的。张是怎样聪明的女子，若不是爱，她也不会那么痴。属于他们的爱情，让他们自己品尝与承担。胡的文字，确实是不错的，有一种儒雅在里面。不知道是不是那份淡淡的气氛，曾经让张心动——“岁月静好，现世安稳”。 	 Label: positive
Data: 书的印刷看起来也不好,画面也不漂亮,内容的连惯性也不好,所以让人看起来一点也不能吸引孩子的兴趣和眼球,只是实在不想睡觉看一下,起催眠作用.作为家长来说,我也不爱讲这几本书. 	 Label: negative
Data: 没有许多网友评价热度高的问题，第一次要手动安装winxp，主板的blis-硬盘重新设置 	 Label: positive
Data: 请问：有些字打错了，我怎么样才可以回去编辑一下啊？ 	 Label: negative
Data: 没有蓝牙、摄像头，连麦克风都省了 装XP有点小麻烦，MSDN原版没有SATA驱动无法安装，装雨林木风GHOST纯净版OK。 自带无线网卡驱动不能用，需到官网下载。 	 Label: negative
Data: 在网上偶尔看到别人的推荐，于是买一本看看，求的是轻松。那天正好坐飞机去北京，在飞机上我不停喷笑，努力克制，好歹也要装出“杜拉拉”型的职场白骨精形象。等下飞机的时候，刚好看完，却开始心里泛堵。总觉得像咬了口大红薯，虽然味美，但是卡在食道下不去，憋得慌。坐出租去酒店的时候，看到初春的北京还是处处冷涩，突然有种顿悟的感觉，北京的青春和南方的如此不同，彪悍之余还带着一点忧伤。 	 Label: negative
Data: 新机拿到手就有硬件问题，而且等了6天才到货，第二天就返修，到现在还没得到处理意见！ 	 Label: negative
Data: 风扇确实够响的，尤其是到晚上周围安静下来。风扇频频开启，发热量有些惊人 	 Label: negative
Data: 价廉物美的好选择，晚上到机住十分合适，服务很热情，房间没地毯很清爽，海景很好看 	 Label: positive
Data: 我在晚上6点30左右入住的,当时是一位男服务员为我登记,我问他可不可以作信用卡预授权，他说不可以但建议我可以用现金作为押金到退房时可以用信用卡消费．房间很大但很旧，电视机只能收到４个台的节目，在第三天我退房时要求用信用卡消费，当时一位女服务员说我之前是用现金作为押金是不可以再用信用卡消费的，还反问我说如果我要消费的话在入住时为什么不做预授权，当时我就把之前那位男服务员说的话跟他说一次后又反口说是因为现在机器坏了不能用，我跟她说我在３０分钟之前见到有一位客人才用信用卡消费过，为什么会这么快就坏了，她见我态度那么强硬就帮我用卡消费了，事实上证明了刷卡机并没有坏，当时因为我们赶着到机场坐飞机就没有再跟她理论了，但一直觉得心中有气． 	 Label: negative
Data: 3999的时候抢购的 运气真好 这个价格还有什么号说的 品牌和价格都很不错 	 Label: positive
Data: 这次入住少林宾馆还是比较满意的,宾馆的位置比较好找,从客运站打部计程车只要三几分钟,5块钱就可以到了,房间比较干净,算是比较宽敞的,洗澡的热水也挺舒服,最好的地方是宾馆外面有一条很旺的小吃街,晚上6点左右就开了,很多麻辣烫之类的小吃,而且有起码二三十档,选择很多,解决吃的问题很方便,而且价格也相当评议,我和我朋友一起,两个人十来块钱就吃得好饱了:) 	 Label: positive
Data: 这一套书我基本买齐了，也看了好多本了。是利用闲暇时间巩固英语，学习知识的好东东。当然，再好的东西也要适合才真的能用上，如果你觉得你目前的任务是冲刺学习，那这书不一定适合你。但是，如果，你想有一个长效的学习习惯，你就可以考虑这套英文读物。另外，MP3也很不错，可在上下班的路上听，就像听广播小说，在娱乐中提高了听力。 	 Label: positive
Data: 酒店设施尚可,房间内镜子太多.携程的套餐包括送水果,还有巧克力,比较温馨. 服务一般,入住时前台试图诱导我买早餐,两大一小每天早餐440元,两天共计880元,另外冰箱里的东西可以随便吃,被我拒绝了!第二天就不再送水果巧克力了,连免费的两瓶水也不给了...酒店离九龙又一城步行约15分钟,旁边有个公园,但是离地铁远了点,需要做小巴,有时6元每人,有时3元每人,开得飞快很吓人,小孩同样费用,就是这点不满意,还是住得离地铁近些比较方便. 	 Label: positive
Data: 这个价格这种房间环境很不错，感觉很干净^_^ 兴旺楼给我个人感觉很雅...下次来，肯定还定这里。。。 	 Label: positive
Data: 选择的事例太离奇了，夸大了心理咨询的现实意义，让人失去了信任感！如果说这样写的效果能在一开始抓住读者的眼球，但是看到案例主人公心理问题的原因解释时就逐渐失去了兴趣，反正有点拣了芝麻丢了西瓜的感觉。 	 Label: negative
Data: 简单，大方，在同类尺寸的款型的笔记本中不显厚重，轻薄感！很安静，几乎没有声音，音质不错屏幕不错，显的细腻可观。 感谢马连道提货点的工作人员（前台客服），服务态度超好，值得贵公司其他员工学习。 	 Label: positive
Data: 硬盘到手就发现一个坏块，因为是完美屏，没回京东换新，花了两天在本地换新硬盘，发票都不需要；电池衔接很松，可有1mm间隙；出厂时A、B面贴的保护膜太敷衍，太多气泡，虽然反正要撕掉，但说明厂家态度不严谨。 	 Label: negative
Data: 没发现什么优点，回来了开机什么都没有，有自己装的XP也没费什么劲，麦有些问题。对方听不清。调了好久才好了，整体还不错 	 Label: positive
Data: 外观漂亮，系统不是很难弄，刚开始看了很多评价说难弄，实际上没那么困难，将boot 选项第一选项选从DVD起动，其它就直接安装了。我的是安装盘，不是Ghost的。内存是三星DDR3-1066，不是DDR3-800.Everest测试是DDR3-800.我用超级兔子测试是DDR3-1066.友达LED屏，独显，满意，到手是完美屏，不错 	 Label: positive
Data: 9号下午下的单子,11中午拿到货,真的很赞.机子开开后发现送个内胆包,一个擦拭布,试用了2个多小时,发现声音很小,下在速度很很不错，开网页能同时6-7不卡. 	 Label: positive
Data: 赠送的系统慢就改装XP算了，内装的备份是用GHOST的就好了，应该再配一根USB电脑连接线就更理想了。 	 Label: positive
Data: 原本在网上订了两个套房，入住后，携程还给我打电话问是否只入住了一间，要扣我信用卡里的钱。真不知道酒店与携程是如何衔接的？ 宾馆反馈 2007年12月7日 ： 我们非常感谢您的留言，非常抱歉由于一些小误会给您带来的困惑。根据我们的担保订房要求，对于确认担保的房间未入住的会收取当天房费。当然，我们也会进一步加强与携程的沟通与协调，让您今后的入住感觉更加舒适与愉快。 	 Label: negative
Data: 这本书实在太烂了,什么朗读手册,一点朗读的内容都没有.看了几页就不想看下去了. 	 Label: negative
Data: 该酒店实际是兰州铁路局的内部招待所，位于火车站出站口处，如果不怕火车吵，那么晚上可以睡得着。 价格不算便宜，只能是一般。 房间很旧，房内设备一般，卫生排气扇坏了，抽水马桶也坏了，维修人员脾气很暴，要很有礼貌的对待。 前台服务人员素质比较差，早上退房时前台竟然找不到服务员，等了近半个小时，耽误宝贵的时间。 入住酒店停车竟然要收费。 建议不要将该酒店做为入住选择，除非你对火车有特殊的感情。 	 Label: negative
Data: 当时是同事极力推荐这本书。我看到网上的介绍和那么多的“名人”位置鼓噪就觉得忽悠大于内容。同事不甘心硬是买了来看。。果然，看后大呼上当！ 	 Label: negative
Data: 从携程订房无数，唯独这家实在不感恭维。从走廊就开始一股酶味，房间也是。而且窗户是封死的。最可恶的是明明订的房间含两份早餐，前台只给一份的餐券，还说要另一份的话要就餐者拿身份证来登记。我靠，中国什么时候兴吃饭也要身份证了。我花了两份的钱，凭什么只给我一份。我的另一份，我把餐券撕了扔了，关别人什么事。此宾馆实在霸道，以后离之远点。 	 Label: negative
Data: 钱也付了，书干等不来，好不容易找到客服电话，原来书没有了。钱突然退回来，单位查起来还以为受贿，连个证明都没有。伤心。。当当做了那么多年，你们的总裁也天天媒体。服务还是不够精细，要不然我去你们那里管客服得了，肯定比现在好书又买了一次，总算买到了。。下次到卓越试试，看看是不是服务一样的烂 	 Label: negative
Data: 容易产生指纹。不习惯分区。由于出货量大了，我觉得在配货的时候更快一点就好了，我昨天到中通迅递，看到的好多都是京东的物品啊。 	 Label: negative
Data: 六心电池装在后面突出一大块影响美观，速度慢机器有点卡，放歌一停顿一停顿，重量比一般的上网本重，触摸板鼠标反应不灵敏，滑的区域太小， 	 Label: negative
Data: 这套书是买给儿子的，小家伙两岁半不到，正是这套书适合的年龄阶段。之前看了许多评价，这套书好评如潮，令我十分期待。因为一本好书可以带给人知识，给人以愉悦，哪怕他只是一个小孩子。拿到书的瞬间我就肯定我的孩子会非常喜欢这套书。果真如此，小家伙非常喜欢，与其说是看，不如说是玩起这套书来不亦乐乎。再加上大人在一旁的指点与协助，真得是一个非常好的亲子游戏呢。第一次写评价，希望好书共享。 	 Label: positive
Data: 本人于清明假期一家三口高高兴兴通过携程订房入住，本想来个豪华的自驾游，哪知给我的房间小得可怜，对比我住过的国内外四星级以上酒店的环境，环境算得上是“恶劣”！ 在前台办理入住手续时，工作人员告诉我1.2米以下小孩早餐免费，于是补上一个成年人的早餐，第二天用早餐时，餐厅经理却跟我说小孩需付半价，而且是算对外收费全价！ 最后经过一番交涉，酒店大堂经理给折了个8折价，但已经让我们一家游趣全无！ 酒店的房间环境根本不值四星水平！装修旧，房间小，但酒店的地理位置好，所以就能如此定高价么？不解！ 补充点评 2008年4月23日 ： 对了，还有上网居然是收费的，这还是在国内第一碰到！不可思议！ 	 Label: negative
Data: 这样的男人，温柔的过了头，找不到一丝男人的气概。只会说：这是好的。。。这也是好的。。。无法欣赏这样的文笔优美在何处。爱情也是奇怪，张爱玲爱上了这样的一个人，隐隐地，大概是一辈子的影子。 	 Label: negative
Data: 第一次装xp到一半蓝屏，不过进bios，在advanced一栏里把硬盘模式从achi改为ide即可。驱动网上都有，下载g430驱动就行了 	 Label: negative
Data: 光驱确实不怎么好，装系统的时候就怕它挂掉，还好顺利装上系统了，装好后立马做了个镜像，估计以后也不用光驱了 	 Label: negative
Data: 在我刚开始从事设计的时候，阿尔瓦 阿尔托是第一个与我有缘，超越时代，地域对我指点的前辈。在他的世界里 没有概念的约束，这是任何渴望为了自由设计而用心的人最需要的精神内在与动力。美丽需要的两个前提是 干净 和 幽默。美丽的人。而这位建筑师 设计家可能具备的更多。宜家家居至今还在销售他在上个世纪30年代设计的曲线板读书椅。这本书物美价廉，适合热爱自由设计的每个人，我想是这样的。一九七九五月韩 	 Label: positive
Data: 外包装写是内存2G，可是内清单标的是1G，不知道是怎么回事，希望有了解的朋友帮忙解决一下，谢谢。 	 Label: negative
Data: 这本书，让我觉得很不值得买。关于毕淑敏的犀利洒脱细腻，在这本小说中无法得以体现。似乎迎合了目前一部分低层次读者的需要，有些媚俗之嫌。很想退掉。因为，我直接没有看完。 	 Label: negative
Data: 10月16日入住该酒店.感觉不错哦,离汽车站和火车站的距离都不是非常远.房间很干净,空间也非常大.周边的交通环境也好.可能住的楼层比较高,也没有任何的噪音.晚上有发生热水管道出问题的小插曲,但服务态度良好,解决问题也很及时,丝毫没有影响到对酒店的好感.主要是价格非常合理.可以说对该酒店非常满意.小缺点就是硬件 	 Label: positive
Data: 总体说来,应该是二星级宾馆.因为定的时间比较晚,所以只有这家酒店了.入住时,问前台一些黄山城市的信息,她们说不知道.一个入住也要办理二十几分钟,而且就我们几个人.房间的设施很陈旧,住的也不舒服. 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:43:29.875900  4290 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:43:29.878844  4290 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:43:30.058462  4290 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:43:30.060256  4290 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:43:30.077493  4290 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:43:30.094521  4290 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:43:30.110422  4290 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:43:30.118587  4290 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:43:30.125514  4290 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:43:30.137316  4290 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:43:30.139096  4290 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:43:30.141989  4290 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
I1221 12:43:30.151859  4290 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with scale[0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:43:30.171797  4290 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:43:30.172578  4290 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:43:30.172590  4290 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:43:30.173554  4290 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:43:30,173] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:43:30,174] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:43:30,198] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:43:30,199] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:43:30.237764  4290 onednn_context.cc:81] oneDNN v3.2.1
Data: 这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般 	 Label: negative
Data: 怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！ 	 Label: negative
Data: 还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。 	 Label: negative
Data: 交通方便；环境很好；服务态度很好 房间较小 	 Label: positive
Data: 不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。 	 Label: positive
Data: 有了第一本书的铺垫，读第二本的时候开始进入状态。基本上第二本就围绕主角们的能力训练展开，故事的主要发生场地设置在美洲的亚马逊丛林。心里一直疑惑这和西藏有什么关系，不过大概看完全书才能知道内里的线索。其中描述了很多热带雨林中特有的神秘动植物以及一些生存技巧和常识，受益匪浅。能够想像出要写这样一部书，融合这样许多的知识，作者需要花费多少心血来搜集和整理并成文。 	 Label: positive
Data: 前台接待太差，酒店有A B楼之分，本人check－in后，前台未告诉B楼在何处，并且B楼无明显指示；房间太小，根本不像4星级设施，下次不会再选择入住此店啦。 	 Label: negative
Data: 1. 白色的，很漂亮，做工还可以； 2. 网上的软件资源非常丰富，这是我买它的最主要原因； 3. 电池不错，昨天从下午两点到晚上十点还有25分钟的剩余时间（关闭摄像头，无线和蓝牙）主要拷贝东西，看起来正常使用八小时左右没问题； 4. 散热不错，CPU核心不过40~55度，很多小本要上到80度了； 5. 变压器很小巧，很多小本的电源都用的是大本的电源，本倒是很轻，可旅行重量还是比较重。 	 Label: positive
Data: 在当当上买了很多书，都懒于评论。但这套书真的很好，3册都非常精彩。我家小一的女儿，认字多，非常喜爱，每天睡前必读。她还告诉我，学校的语文课本中也有相同的文章。我还借给我的同事的女儿，我同事一直头疼她女儿不爱看书，但这套书，她女儿非常喜欢。两周就看完了。建议买。很少写评论，但忍不住为这套书写下。也给别的读者参考下。 	 Label: positive
Data: 19天硬盘就罢工了~~~算上运来的一周都没用上15天~~~可就是不能换了~~~唉~~~~你说这算什么事呀~~~ 	 Label: negative
Data: 书一到，即被朋友借走，我是早已看过了的，买它，是因为张爱玲。看过张子静的《我的姊姊张爱玲》，可以看出，张对自己的弟弟也是不甚热情，这是她的性格。但是有一次张子静去找她，那天张爱玲却是很开心的样子。后来张子静推算，彼时张爱玲正与胡兰成相恋。很多人不耻于胡兰成的用情不专，我当然也很反感，但我也想说，胡兰成是给过张爱的欢喜的。张是怎样聪明的女子，若不是爱，她也不会那么痴。属于他们的爱情，让他们自己品尝与承担。胡的文字，确实是不错的，有一种儒雅在里面。不知道是不是那份淡淡的气氛，曾经让张心动——“岁月静好，现世安稳”。 	 Label: positive
Data: 书的印刷看起来也不好,画面也不漂亮,内容的连惯性也不好,所以让人看起来一点也不能吸引孩子的兴趣和眼球,只是实在不想睡觉看一下,起催眠作用.作为家长来说,我也不爱讲这几本书. 	 Label: negative
Data: 没有许多网友评价热度高的问题，第一次要手动安装winxp，主板的blis-硬盘重新设置 	 Label: positive
Data: 请问：有些字打错了，我怎么样才可以回去编辑一下啊？ 	 Label: negative
Data: 没有蓝牙、摄像头，连麦克风都省了 装XP有点小麻烦，MSDN原版没有SATA驱动无法安装，装雨林木风GHOST纯净版OK。 自带无线网卡驱动不能用，需到官网下载。 	 Label: negative
Data: 在网上偶尔看到别人的推荐，于是买一本看看，求的是轻松。那天正好坐飞机去北京，在飞机上我不停喷笑，努力克制，好歹也要装出“杜拉拉”型的职场白骨精形象。等下飞机的时候，刚好看完，却开始心里泛堵。总觉得像咬了口大红薯，虽然味美，但是卡在食道下不去，憋得慌。坐出租去酒店的时候，看到初春的北京还是处处冷涩，突然有种顿悟的感觉，北京的青春和南方的如此不同，彪悍之余还带着一点忧伤。 	 Label: negative
Data: 新机拿到手就有硬件问题，而且等了6天才到货，第二天就返修，到现在还没得到处理意见！ 	 Label: negative
Data: 风扇确实够响的，尤其是到晚上周围安静下来。风扇频频开启，发热量有些惊人 	 Label: negative
Data: 价廉物美的好选择，晚上到机住十分合适，服务很热情，房间没地毯很清爽，海景很好看 	 Label: positive
Data: 我在晚上6点30左右入住的,当时是一位男服务员为我登记,我问他可不可以作信用卡预授权，他说不可以但建议我可以用现金作为押金到退房时可以用信用卡消费．房间很大但很旧，电视机只能收到４个台的节目，在第三天我退房时要求用信用卡消费，当时一位女服务员说我之前是用现金作为押金是不可以再用信用卡消费的，还反问我说如果我要消费的话在入住时为什么不做预授权，当时我就把之前那位男服务员说的话跟他说一次后又反口说是因为现在机器坏了不能用，我跟她说我在３０分钟之前见到有一位客人才用信用卡消费过，为什么会这么快就坏了，她见我态度那么强硬就帮我用卡消费了，事实上证明了刷卡机并没有坏，当时因为我们赶着到机场坐飞机就没有再跟她理论了，但一直觉得心中有气． 	 Label: negative
Data: 3999的时候抢购的 运气真好 这个价格还有什么号说的 品牌和价格都很不错 	 Label: positive
Data: 这次入住少林宾馆还是比较满意的,宾馆的位置比较好找,从客运站打部计程车只要三几分钟,5块钱就可以到了,房间比较干净,算是比较宽敞的,洗澡的热水也挺舒服,最好的地方是宾馆外面有一条很旺的小吃街,晚上6点左右就开了,很多麻辣烫之类的小吃,而且有起码二三十档,选择很多,解决吃的问题很方便,而且价格也相当评议,我和我朋友一起,两个人十来块钱就吃得好饱了:) 	 Label: positive
Data: 这一套书我基本买齐了，也看了好多本了。是利用闲暇时间巩固英语，学习知识的好东东。当然，再好的东西也要适合才真的能用上，如果你觉得你目前的任务是冲刺学习，那这书不一定适合你。但是，如果，你想有一个长效的学习习惯，你就可以考虑这套英文读物。另外，MP3也很不错，可在上下班的路上听，就像听广播小说，在娱乐中提高了听力。 	 Label: positive
Data: 酒店设施尚可,房间内镜子太多.携程的套餐包括送水果,还有巧克力,比较温馨. 服务一般,入住时前台试图诱导我买早餐,两大一小每天早餐440元,两天共计880元,另外冰箱里的东西可以随便吃,被我拒绝了!第二天就不再送水果巧克力了,连免费的两瓶水也不给了...酒店离九龙又一城步行约15分钟,旁边有个公园,但是离地铁远了点,需要做小巴,有时6元每人,有时3元每人,开得飞快很吓人,小孩同样费用,就是这点不满意,还是住得离地铁近些比较方便. 	 Label: positive
Data: 这个价格这种房间环境很不错，感觉很干净^_^ 兴旺楼给我个人感觉很雅...下次来，肯定还定这里。。。 	 Label: positive
Data: 选择的事例太离奇了，夸大了心理咨询的现实意义，让人失去了信任感！如果说这样写的效果能在一开始抓住读者的眼球，但是看到案例主人公心理问题的原因解释时就逐渐失去了兴趣，反正有点拣了芝麻丢了西瓜的感觉。 	 Label: negative
Data: 简单，大方，在同类尺寸的款型的笔记本中不显厚重，轻薄感！很安静，几乎没有声音，音质不错屏幕不错，显的细腻可观。 感谢马连道提货点的工作人员（前台客服），服务态度超好，值得贵公司其他员工学习。 	 Label: positive
Data: 硬盘到手就发现一个坏块，因为是完美屏，没回京东换新，花了两天在本地换新硬盘，发票都不需要；电池衔接很松，可有1mm间隙；出厂时A、B面贴的保护膜太敷衍，太多气泡，虽然反正要撕掉，但说明厂家态度不严谨。 	 Label: negative
Data: 没发现什么优点，回来了开机什么都没有，有自己装的XP也没费什么劲，麦有些问题。对方听不清。调了好久才好了，整体还不错 	 Label: positive
Data: 外观漂亮，系统不是很难弄，刚开始看了很多评价说难弄，实际上没那么困难，将boot 选项第一选项选从DVD起动，其它就直接安装了。我的是安装盘，不是Ghost的。内存是三星DDR3-1066，不是DDR3-800.Everest测试是DDR3-800.我用超级兔子测试是DDR3-1066.友达LED屏，独显，满意，到手是完美屏，不错 	 Label: positive
Data: 9号下午下的单子,11中午拿到货,真的很赞.机子开开后发现送个内胆包,一个擦拭布,试用了2个多小时,发现声音很小,下在速度很很不错，开网页能同时6-7不卡. 	 Label: positive
Data: 赠送的系统慢就改装XP算了，内装的备份是用GHOST的就好了，应该再配一根USB电脑连接线就更理想了。 	 Label: positive
Data: 原本在网上订了两个套房，入住后，携程还给我打电话问是否只入住了一间，要扣我信用卡里的钱。真不知道酒店与携程是如何衔接的？ 宾馆反馈 2007年12月7日 ： 我们非常感谢您的留言，非常抱歉由于一些小误会给您带来的困惑。根据我们的担保订房要求，对于确认担保的房间未入住的会收取当天房费。当然，我们也会进一步加强与携程的沟通与协调，让您今后的入住感觉更加舒适与愉快。 	 Label: negative
Data: 这本书实在太烂了,什么朗读手册,一点朗读的内容都没有.看了几页就不想看下去了. 	 Label: negative
Data: 该酒店实际是兰州铁路局的内部招待所，位于火车站出站口处，如果不怕火车吵，那么晚上可以睡得着。 价格不算便宜，只能是一般。 房间很旧，房内设备一般，卫生排气扇坏了，抽水马桶也坏了，维修人员脾气很暴，要很有礼貌的对待。 前台服务人员素质比较差，早上退房时前台竟然找不到服务员，等了近半个小时，耽误宝贵的时间。 入住酒店停车竟然要收费。 建议不要将该酒店做为入住选择，除非你对火车有特殊的感情。 	 Label: negative
Data: 当时是同事极力推荐这本书。我看到网上的介绍和那么多的“名人”位置鼓噪就觉得忽悠大于内容。同事不甘心硬是买了来看。。果然，看后大呼上当！ 	 Label: negative
Data: 从携程订房无数，唯独这家实在不感恭维。从走廊就开始一股酶味，房间也是。而且窗户是封死的。最可恶的是明明订的房间含两份早餐，前台只给一份的餐券，还说要另一份的话要就餐者拿身份证来登记。我靠，中国什么时候兴吃饭也要身份证了。我花了两份的钱，凭什么只给我一份。我的另一份，我把餐券撕了扔了，关别人什么事。此宾馆实在霸道，以后离之远点。 	 Label: negative
Data: 钱也付了，书干等不来，好不容易找到客服电话，原来书没有了。钱突然退回来，单位查起来还以为受贿，连个证明都没有。伤心。。当当做了那么多年，你们的总裁也天天媒体。服务还是不够精细，要不然我去你们那里管客服得了，肯定比现在好书又买了一次，总算买到了。。下次到卓越试试，看看是不是服务一样的烂 	 Label: negative
Data: 容易产生指纹。不习惯分区。由于出货量大了，我觉得在配货的时候更快一点就好了，我昨天到中通迅递，看到的好多都是京东的物品啊。 	 Label: negative
Data: 六心电池装在后面突出一大块影响美观，速度慢机器有点卡，放歌一停顿一停顿，重量比一般的上网本重，触摸板鼠标反应不灵敏，滑的区域太小， 	 Label: negative
Data: 这套书是买给儿子的，小家伙两岁半不到，正是这套书适合的年龄阶段。之前看了许多评价，这套书好评如潮，令我十分期待。因为一本好书可以带给人知识，给人以愉悦，哪怕他只是一个小孩子。拿到书的瞬间我就肯定我的孩子会非常喜欢这套书。果真如此，小家伙非常喜欢，与其说是看，不如说是玩起这套书来不亦乐乎。再加上大人在一旁的指点与协助，真得是一个非常好的亲子游戏呢。第一次写评价，希望好书共享。 	 Label: positive
Data: 本人于清明假期一家三口高高兴兴通过携程订房入住，本想来个豪华的自驾游，哪知给我的房间小得可怜，对比我住过的国内外四星级以上酒店的环境，环境算得上是“恶劣”！ 在前台办理入住手续时，工作人员告诉我1.2米以下小孩早餐免费，于是补上一个成年人的早餐，第二天用早餐时，餐厅经理却跟我说小孩需付半价，而且是算对外收费全价！ 最后经过一番交涉，酒店大堂经理给折了个8折价，但已经让我们一家游趣全无！ 酒店的房间环境根本不值四星水平！装修旧，房间小，但酒店的地理位置好，所以就能如此定高价么？不解！ 补充点评 2008年4月23日 ： 对了，还有上网居然是收费的，这还是在国内第一碰到！不可思议！ 	 Label: negative
Data: 这样的男人，温柔的过了头，找不到一丝男人的气概。只会说：这是好的。。。这也是好的。。。无法欣赏这样的文笔优美在何处。爱情也是奇怪，张爱玲爱上了这样的一个人，隐隐地，大概是一辈子的影子。 	 Label: negative
Data: 第一次装xp到一半蓝屏，不过进bios，在advanced一栏里把硬盘模式从achi改为ide即可。驱动网上都有，下载g430驱动就行了 	 Label: negative
Data: 光驱确实不怎么好，装系统的时候就怕它挂掉，还好顺利装上系统了，装好后立马做了个镜像，估计以后也不用光驱了 	 Label: negative
Data: 在我刚开始从事设计的时候，阿尔瓦 阿尔托是第一个与我有缘，超越时代，地域对我指点的前辈。在他的世界里 没有概念的约束，这是任何渴望为了自由设计而用心的人最需要的精神内在与动力。美丽需要的两个前提是 干净 和 幽默。美丽的人。而这位建筑师 设计家可能具备的更多。宜家家居至今还在销售他在上个世纪30年代设计的曲线板读书椅。这本书物美价廉，适合热爱自由设计的每个人，我想是这样的。一九七九五月韩 	 Label: positive
Data: 外包装写是内存2G，可是内清单标的是1G，不知道是怎么回事，希望有了解的朋友帮忙解决一下，谢谢。 	 Label: negative
Data: 这本书，让我觉得很不值得买。关于毕淑敏的犀利洒脱细腻，在这本小说中无法得以体现。似乎迎合了目前一部分低层次读者的需要，有些媚俗之嫌。很想退掉。因为，我直接没有看完。 	 Label: negative
Data: 10月16日入住该酒店.感觉不错哦,离汽车站和火车站的距离都不是非常远.房间很干净,空间也非常大.周边的交通环境也好.可能住的楼层比较高,也没有任何的噪音.晚上有发生热水管道出问题的小插曲,但服务态度良好,解决问题也很及时,丝毫没有影响到对酒店的好感.主要是价格非常合理.可以说对该酒店非常满意.小缺点就是硬件 	 Label: positive
Data: 总体说来,应该是二星级宾馆.因为定的时间比较晚,所以只有这家酒店了.入住时,问前台一些黄山城市的信息,她们说不知道.一个入住也要办理二十几分钟,而且就我们几个人.房间的设施很陈旧,住的也不舒服. 	 Label: negative
No XPU Memory Leak
[33m Run successfully with command - ernie_text_cls - python ./test_tipc/ernie_text_cls/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
Does not support hardware other than CPU and GPU Currently!
+ watchcat=2822
+ kill -9 3648
+ sleep 10
run.sh: line 334:  3648 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/ernie_text_cls/train_infer_python.txt
==END==test_tipc/configs/ernie_text_cls/train_infer_python.txt
++ date +%s
+ end=1703133823
++ echo 1703133413 1703133823
++ awk '{print $2-$1-2}'
+ time=408
test_tipc/configs/ernie_text_cls/train_infer_python.txt spend time seconds 408
+ echo 'test_tipc/configs/ernie_text_cls/train_infer_python.txt spend time seconds 408'
+ read config_file
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703133823
+ echo ==START==test_tipc/configs/ernie_text_matching/train_infer_python.txt
==START==test_tipc/configs/ernie_text_matching/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/ernie_text_matching/train_infer_python.txt
+ dataline='===========================train_params=========================== 
model_name:ernie_text_matching
python:python
gpu_list:0|0,1
--device:gpu|gpu
null:null
--epoch:lite_train_lite_infer=1|lite_train_whole_infer=1|whole_train_whole_infer=3
--save_dir:null
--batch_size:lite_train_lite_infer=32|lite_train_whole_infer=32|whole_train_whole_infer=32
null:null
null:model
null:null
null:null
##
trainer:norm
norm_train:./test_tipc/ernie_text_matching/train.py --max_steps 150
pact_train:null
fpgm_train:null
distill_train:null
null:null
null:null
##
===========================eval_params=========================== 
eval:null
null:null
##
===========================infer_params===========================
--output_path:null
--params_path:null
norm_export:./test_tipc/ernie_text_matching/export_model.py
quant_export:null
fpgm_export:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:ernie_text_matching
++ strs=model_name:ernie_text_matching
++ IFS=:
++ array=(${strs})
++ tmp=ernie_text_matching
++ echo ernie_text_matching
+ model_name=ernie_text_matching
+ sleep 10
+ run run_model test_tipc/configs/ernie_text_matching/train_infer_python.txt lite_train_lite_infer 3600 ernie_text_matching
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ waitfor=7200
+ command='run_model
test_tipc/configs/ernie_text_matching/train_infer_python.txt
lite_train_lite_infer
3600
ernie_text_matching'
+ commandpid=4374
+ run_model test_tipc/configs/ernie_text_matching/train_infer_python.txt lite_train_lite_infer 3600 ernie_text_matching
+ config_file=test_tipc/configs/ernie_text_matching/train_infer_python.txt
+ mode=lite_train_lite_infer
+ watchdog=4375
+ bash test_tipc/prepare.sh test_tipc/configs/ernie_text_matching/train_infer_python.txt lite_train_lite_infer
+ wait 4374
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/ernie_text_matching/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/ernie_text_matching/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
  0%|          | 0/6827 [00:00<?, ?it/s]  0%|          | 3/6827 [00:00<04:04, 27.92it/s]  1%|          | 35/6827 [00:00<00:43, 154.71it/s]  1%|          | 51/6827 [00:00<00:45, 148.65it/s]  1%|          | 67/6827 [00:00<00:53, 127.33it/s]  1%|▏         | 99/6827 [00:00<00:42, 157.61it/s]  2%|▏         | 131/6827 [00:00<00:40, 165.61it/s]  2%|▏         | 148/6827 [00:00<00:41, 161.31it/s]  2%|▏         | 164/6827 [00:01<00:42, 156.35it/s]  3%|▎         | 180/6827 [00:01<00:44, 150.07it/s]  3%|▎         | 211/6827 [00:01<00:38, 169.77it/s]  3%|▎         | 228/6827 [00:01<00:40, 162.80it/s]  4%|▍         | 259/6827 [00:01<00:34, 189.20it/s]  4%|▍         | 278/6827 [00:01<00:35, 184.56it/s]  4%|▍         | 307/6827 [00:01<00:36, 178.81it/s]  5%|▌         | 355/6827 [00:02<00:31, 203.45it/s]  6%|▌         | 387/6827 [00:02<00:30, 208.48it/s]  6%|▌         | 408/6827 [00:02<00:31, 203.79it/s]  6%|▋         | 435/6827 [00:02<00:32, 194.46it/s]  7%|▋         | 467/6827 [00:02<00:34, 184.18it/s]  7%|▋         | 486/6827 [00:02<00:34, 185.01it/s]  8%|▊         | 515/6827 [00:02<00:34, 182.24it/s]  8%|▊         | 547/6827 [00:03<00:33, 189.97it/s]  8%|▊         | 579/6827 [00:03<00:33, 184.35it/s]  9%|▉         | 598/6827 [00:03<00:34, 180.83it/s]  9%|▉         | 627/6827 [00:03<00:36, 171.27it/s] 10%|▉         | 659/6827 [00:03<00:36, 169.15it/s] 10%|█         | 691/6827 [00:03<00:36, 166.00it/s] 11%|█         | 723/6827 [00:04<00:35, 169.80it/s] 11%|█         | 755/6827 [00:04<00:34, 175.50it/s] 12%|█▏        | 787/6827 [00:04<00:31, 192.61it/s] 12%|█▏        | 819/6827 [00:04<00:27, 216.50it/s] 12%|█▏        | 851/6827 [00:04<00:26, 225.73it/s] 13%|█▎        | 883/6827 [00:04<00:24, 244.69it/s] 13%|█▎        | 915/6827 [00:04<00:24, 242.52it/s] 14%|█▍        | 963/6827 [00:05<00:21, 273.71it/s] 15%|█▍        | 995/6827 [00:05<00:22, 257.55it/s] 15%|█▌        | 1026/6827 [00:05<00:23, 247.55it/s] 16%|█▌        | 1074/6827 [00:05<00:20, 274.09it/s] 16%|█▋        | 1122/6827 [00:05<00:19, 292.81it/s] 17%|█▋        | 1154/6827 [00:05<00:19, 294.22it/s] 18%|█▊        | 1202/6827 [00:05<00:18, 301.86it/s] 18%|█▊        | 1250/6827 [00:06<00:16, 329.87it/s] 19%|█▉        | 1298/6827 [00:06<00:15, 356.54it/s] 20%|█▉        | 1335/6827 [00:06<00:15, 358.40it/s] 20%|██        | 1378/6827 [00:06<00:14, 376.71it/s] 21%|██        | 1442/6827 [00:06<00:13, 406.36it/s] 22%|██▏       | 1522/6827 [00:06<00:11, 467.33it/s] 23%|██▎       | 1586/6827 [00:06<00:10, 505.82it/s] 24%|██▍       | 1666/6827 [00:06<00:09, 568.85it/s] 26%|██▌       | 1746/6827 [00:06<00:08, 578.11it/s] 27%|██▋       | 1842/6827 [00:07<00:08, 617.76it/s] 29%|██▊       | 1954/6827 [00:07<00:07, 625.66it/s] 30%|███       | 2066/6827 [00:07<00:07, 631.45it/s] 31%|███▏      | 2141/6827 [00:07<00:07, 657.66it/s] 32%|███▏      | 2210/6827 [00:07<00:07, 616.99it/s] 34%|███▍      | 2306/6827 [00:07<00:06, 660.16it/s] 35%|███▍      | 2373/6827 [00:07<00:07, 633.05it/s] 36%|███▋      | 2482/6827 [00:08<00:06, 696.78it/s] 38%|███▊      | 2610/6827 [00:08<00:05, 777.74it/s] 39%|███▉      | 2688/6827 [00:08<00:05, 744.71it/s] 41%|████      | 2802/6827 [00:08<00:05, 773.86it/s] 43%|████▎     | 2962/6827 [00:08<00:04, 901.84it/s] 45%|████▍     | 3052/6827 [00:08<00:04, 839.07it/s] 47%|████▋     | 3186/6827 [00:08<00:04, 907.16it/s] 49%|████▉     | 3362/6827 [00:08<00:03, 1038.83it/s] 51%|█████     | 3466/6827 [00:09<00:03, 1012.77it/s] 53%|█████▎    | 3634/6827 [00:09<00:02, 1157.13it/s] 56%|█████▋    | 3842/6827 [00:09<00:02, 1292.09it/s] 59%|█████▉    | 4018/6827 [00:09<00:01, 1412.08it/s] 61%|██████    | 4162/6827 [00:09<00:01, 1378.81it/s] 64%|██████▍   | 4386/6827 [00:09<00:01, 1471.91it/s] 68%|██████▊   | 4674/6827 [00:09<00:01, 1634.61it/s] 71%|███████   | 4836/6827 [00:09<00:01, 1591.99it/s] 74%|███████▍  | 5042/6827 [00:10<00:01, 1654.59it/s] 78%|███████▊  | 5330/6827 [00:10<00:00, 1869.75it/s] 83%|████████▎ | 5634/6827 [00:10<00:00, 2169.22it/s] 86%|████████▌ | 5855/6827 [00:10<00:00, 2087.73it/s] 90%|█████████ | 6146/6827 [00:10<00:00, 2269.46it/s] 95%|█████████▍| 6482/6827 [00:10<00:00, 2567.54it/s]100%|██████████| 6827/6827 [00:10<00:00, 643.34it/s] 
[32m[2023-12-21 12:44:08,927] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:44:08,928] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:44:08,928] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:44:09,314] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:44:09.323647  4486 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:44:12,525] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieModel: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:44:12,525] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.weight', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:44:12,539] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:44:12,539] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:44:12,564] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:44:12,564] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddle/distributed/parallel.py:410: UserWarning: The program will return to single-card operation. Please check 1, whether you use spawn or fleetrun to start the program. 2, Whether it is a multi-card program. 3, Is the current environment multi-card.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
[32m[2023-12-21 12:44:41,340] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:44:41,340] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 10, epoch: 1, batch: 10, loss: 0.42038, accu: 0.68437, speed: 4.90 step/s
global step 20, epoch: 1, batch: 20, loss: 0.43424, accu: 0.76562, speed: 4.88 step/s
global step 30, epoch: 1, batch: 30, loss: 0.53105, accu: 0.78958, speed: 4.97 step/s
global step 40, epoch: 1, batch: 40, loss: 0.40462, accu: 0.78984, speed: 4.85 step/s
global step 50, epoch: 1, batch: 50, loss: 0.39286, accu: 0.79812, speed: 4.85 step/s
global step 60, epoch: 1, batch: 60, loss: 0.55107, accu: 0.80312, speed: 4.86 step/s
global step 70, epoch: 1, batch: 70, loss: 0.46567, accu: 0.80536, speed: 4.71 step/s
global step 80, epoch: 1, batch: 80, loss: 0.31357, accu: 0.81211, speed: 5.08 step/s
global step 90, epoch: 1, batch: 90, loss: 0.70521, accu: 0.80521, speed: 5.07 step/s
global step 100, epoch: 1, batch: 100, loss: 0.49783, accu: 0.80812, speed: 4.91 step/s
eval dev loss: 0.53563, accu: 0.77062
global step 110, epoch: 1, batch: 110, loss: 0.42955, accu: 0.81875, speed: 0.88 step/s
global step 120, epoch: 1, batch: 120, loss: 0.39829, accu: 0.85000, speed: 4.57 step/s
global step 130, epoch: 1, batch: 130, loss: 0.39596, accu: 0.84896, speed: 4.44 step/s
global step 140, epoch: 1, batch: 140, loss: 0.46460, accu: 0.85547, speed: 4.63 step/s
global step 150, epoch: 1, batch: 150, loss: 0.42624, accu: 0.85813, speed: 4.61 step/s
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/train.py --max_steps 150 --device=xpu  --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 --epoch=1     --batch_size=32     >/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:44:57,616] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:44:57,616] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:44:57,616] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:44:57,894] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:44:57.903314  4557 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:45:00,940] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieModel: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:45:00,940] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.weight', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 12:45:02.510263  4557 program_interpreter.cc:214] New Executor is Running.
Loaded parameters from /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model/model_state.pdparams
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:45:07.578450  4625 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:45:07.581595  4625 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:45:07.798411  4625 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:45:07.800262  4625 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:45:07.818506  4625 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:45:07.836908  4625 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:45:07.854051  4625 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:45:07.862854  4625 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:45:07.870281  4625 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:45:07.882949  4625 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:45:07.884816  4625 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:45:07.887866  4625 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:45:07.920152  4625 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:45:07.920936  4625 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:45:07.920951  4625 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:45:07.921864  4625 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:45:07,922] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:45:07,922] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:45:07,948] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:45:07,949] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:45:08.093171  4625 onednn_context.cc:81] oneDNN v3.2.1
Data: {'query': '谁有狂三这张高清的', 'title': '这张高清图，谁有'} 	 Label: similar
Data: {'query': '英雄联盟什么英雄最好', 'title': '英雄联盟最好英雄是什么'} 	 Label: similar
Data: {'query': '这是什么意思，被蹭网吗', 'title': '我也是醉了，这是什么意思'} 	 Label: dissimilar
Data: {'query': '现在有什么动画片好看呢？', 'title': '现在有什么好看的动画片吗？'} 	 Label: similar
Data: {'query': '请问晶达电子厂现在的工资待遇怎么样要求有哪些', 'title': '三星电子厂工资待遇怎么样啊'} 	 Label: dissimilar
Data: {'query': '文章真的爱姚笛吗', 'title': '姚笛真的被文章干了吗'} 	 Label: similar
Data: {'query': '送自己做的闺蜜什么生日礼物好', 'title': '送闺蜜什么生日礼物好'} 	 Label: similar
Data: {'query': '近期上映的电影', 'title': '近期上映的电影有哪些'} 	 Label: similar
Data: {'query': '求英雄联盟大神带？', 'title': '英雄联盟，求大神带~'} 	 Label: similar
Data: {'query': '如加上什么部首', 'title': '给东加上部首是什么字？'} 	 Label: dissimilar
Data: {'query': '杭州哪里好玩', 'title': '杭州哪里好玩点'} 	 Label: similar
Data: {'query': '这是什么乌龟值钱吗', 'title': '这是什么乌龟！值钱嘛？'} 	 Label: similar
Data: {'query': '心各有所属是什么意思？', 'title': '心有所属是什么意思?'} 	 Label: similar
Data: {'query': '什么东西越热爬得越高', 'title': '什么东西越热爬得很高'} 	 Label: similar
Data: {'query': '世界杯哪位球员进球最多', 'title': '世界杯单界进球最多是哪位球员'} 	 Label: similar
Data: {'query': '韭菜多吃什么好处', 'title': '多吃韭菜有什么好处'} 	 Label: similar
Data: {'query': '云赚钱怎么样', 'title': '怎么才能赚钱'} 	 Label: dissimilar
Data: {'query': '何炅结婚了嘛', 'title': '何炅结婚了么'} 	 Label: similar
Data: {'query': '长的清新是什么意思', 'title': '小清新的意思是什么'} 	 Label: similar
Data: {'query': '我们可以结婚了吗？', 'title': '在熙结婚了吗？'} 	 Label: dissimilar
Data: {'query': '想买男人酒补肾壮阳酒哪里有啊', 'title': '哪里有男人酒补肾壮阳酒'} 	 Label: similar
Data: {'query': '淘宝上怎么用信用卡分期付款', 'title': '淘宝怎么分期付款，没有信用卡'} 	 Label: similar
Data: {'query': '最近有没有什么好看的韩剧', 'title': '最近有什么好看的韩剧'} 	 Label: similar
Data: {'query': '《校花的贴身高手》中的林逸', 'title': '校花贴身高手'} 	 Label: dissimilar
Data: {'query': '叔叔是什么人', 'title': '我是叔叔的什么人'} 	 Label: similar
Data: {'query': '这姑娘漂亮不', 'title': '我姑娘漂亮吧'} 	 Label: similar
Data: {'query': '在淘宝网买手机可靠吗？', 'title': '在淘宝网上买手机可靠吗？'} 	 Label: similar
Data: {'query': '山楂干怎么吃好吃？', 'title': '山楂怎么做好吃'} 	 Label: similar
Data: {'query': '时间都去哪怕了歌谱', 'title': '时间煮雨歌谱'} 	 Label: dissimilar
Data: {'query': '苏州哪里能买到这个衣服', 'title': '苏州哪里有买大号衣服的？'} 	 Label: dissimilar
Data: {'query': '最好玩的手机网游', 'title': '好玩的手机网游'} 	 Label: similar
Data: {'query': '石榴是什么时候成熟的？', 'title': '成熟的石榴像什么？'} 	 Label: similar
Data: {'query': '刘诗诗杨幂谁漂亮', 'title': '刘诗诗和杨幂谁漂亮'} 	 Label: similar
Data: {'query': '微信号怎么二次修改', 'title': '怎么再二次修改微信号'} 	 Label: similar
Data: {'query': '什么牌子的精油皂好', 'title': '什么牌子的精油好？'} 	 Label: dissimilar
Data: {'query': '刚出生的小野鸡怎么养', 'title': '刚抓来的野鸡怎么养殖'} 	 Label: similar
Data: {'query': '如何入侵他人手机', 'title': '如何入侵别人的手机'} 	 Label: similar
Data: {'query': '红米刷什么系统好', 'title': '红米可以刷什么系统'} 	 Label: similar
Data: {'query': '这叫什么高跟鞋', 'title': '这种高跟鞋叫什么呀'} 	 Label: similar
Data: {'query': '汇理财怎么样', 'title': '怎么样去理财？'} 	 Label: dissimilar
Data: {'query': '什么是刷屏', 'title': '什么叫刷屏？'} 	 Label: similar
Data: {'query': '各国货币符号是什么?', 'title': '如何用电脑打出各国货币符号呀'} 	 Label: dissimilar
Data: {'query': '上嘴唇有痣代表什么', 'title': '咬嘴唇代表什么'} 	 Label: dissimilar
Data: {'query': '哪种减肥药最快最有效', 'title': '哪种减肥药最有效，减肥效果最好'} 	 Label: similar
Data: {'query': '邓紫棋唱功怎么样', 'title': '邓紫棋唱功怎么样？'} 	 Label: similar
Data: {'query': '怎么做视频', 'title': '贴吧怎么贴视频'} 	 Label: similar
Data: {'query': '现在女生流行什么发型？', 'title': '女生现在流行什么发型？'} 	 Label: similar
Data: {'query': '为什么坐车玩手机会晕车', 'title': '为什么我坐车玩手机不晕车'} 	 Label: similar
Data: {'query': '为什么老婆不喜欢和我做爱', 'title': '我老婆为什么不喜欢做爱'} 	 Label: similar
Data: {'query': '怎么测试我爱的他爱不爱我。', 'title': '怎么测试老公爱不爱我啊'} 	 Label: similar
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:45:15.816350  4692 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:45:15.819591  4692 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:45:15.997681  4692 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:45:15.999478  4692 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:45:16.016628  4692 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:45:16.034143  4692 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:45:16.050160  4692 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:45:16.058388  4692 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:45:16.065304  4692 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:45:16.077114  4692 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:45:16.078917  4692 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:45:16.081872  4692 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:45:16.111840  4692 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:45:16.112620  4692 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:45:16.112633  4692 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:45:16.113624  4692 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:45:16,113] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:45:16,114] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:45:16,139] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:45:16,139] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:45:16.290901  4692 onednn_context.cc:81] oneDNN v3.2.1
Data: {'query': '谁有狂三这张高清的', 'title': '这张高清图，谁有'} 	 Label: similar
Data: {'query': '英雄联盟什么英雄最好', 'title': '英雄联盟最好英雄是什么'} 	 Label: similar
Data: {'query': '这是什么意思，被蹭网吗', 'title': '我也是醉了，这是什么意思'} 	 Label: dissimilar
Data: {'query': '现在有什么动画片好看呢？', 'title': '现在有什么好看的动画片吗？'} 	 Label: similar
Data: {'query': '请问晶达电子厂现在的工资待遇怎么样要求有哪些', 'title': '三星电子厂工资待遇怎么样啊'} 	 Label: dissimilar
Data: {'query': '文章真的爱姚笛吗', 'title': '姚笛真的被文章干了吗'} 	 Label: similar
Data: {'query': '送自己做的闺蜜什么生日礼物好', 'title': '送闺蜜什么生日礼物好'} 	 Label: similar
Data: {'query': '近期上映的电影', 'title': '近期上映的电影有哪些'} 	 Label: similar
Data: {'query': '求英雄联盟大神带？', 'title': '英雄联盟，求大神带~'} 	 Label: similar
Data: {'query': '如加上什么部首', 'title': '给东加上部首是什么字？'} 	 Label: dissimilar
Data: {'query': '杭州哪里好玩', 'title': '杭州哪里好玩点'} 	 Label: similar
Data: {'query': '这是什么乌龟值钱吗', 'title': '这是什么乌龟！值钱嘛？'} 	 Label: similar
Data: {'query': '心各有所属是什么意思？', 'title': '心有所属是什么意思?'} 	 Label: similar
Data: {'query': '什么东西越热爬得越高', 'title': '什么东西越热爬得很高'} 	 Label: similar
Data: {'query': '世界杯哪位球员进球最多', 'title': '世界杯单界进球最多是哪位球员'} 	 Label: similar
Data: {'query': '韭菜多吃什么好处', 'title': '多吃韭菜有什么好处'} 	 Label: similar
Data: {'query': '云赚钱怎么样', 'title': '怎么才能赚钱'} 	 Label: dissimilar
Data: {'query': '何炅结婚了嘛', 'title': '何炅结婚了么'} 	 Label: similar
Data: {'query': '长的清新是什么意思', 'title': '小清新的意思是什么'} 	 Label: similar
Data: {'query': '我们可以结婚了吗？', 'title': '在熙结婚了吗？'} 	 Label: dissimilar
Data: {'query': '想买男人酒补肾壮阳酒哪里有啊', 'title': '哪里有男人酒补肾壮阳酒'} 	 Label: similar
Data: {'query': '淘宝上怎么用信用卡分期付款', 'title': '淘宝怎么分期付款，没有信用卡'} 	 Label: similar
Data: {'query': '最近有没有什么好看的韩剧', 'title': '最近有什么好看的韩剧'} 	 Label: similar
Data: {'query': '《校花的贴身高手》中的林逸', 'title': '校花贴身高手'} 	 Label: dissimilar
Data: {'query': '叔叔是什么人', 'title': '我是叔叔的什么人'} 	 Label: similar
Data: {'query': '这姑娘漂亮不', 'title': '我姑娘漂亮吧'} 	 Label: similar
Data: {'query': '在淘宝网买手机可靠吗？', 'title': '在淘宝网上买手机可靠吗？'} 	 Label: similar
Data: {'query': '山楂干怎么吃好吃？', 'title': '山楂怎么做好吃'} 	 Label: similar
Data: {'query': '时间都去哪怕了歌谱', 'title': '时间煮雨歌谱'} 	 Label: dissimilar
Data: {'query': '苏州哪里能买到这个衣服', 'title': '苏州哪里有买大号衣服的？'} 	 Label: dissimilar
Data: {'query': '最好玩的手机网游', 'title': '好玩的手机网游'} 	 Label: similar
Data: {'query': '石榴是什么时候成熟的？', 'title': '成熟的石榴像什么？'} 	 Label: similar
Data: {'query': '刘诗诗杨幂谁漂亮', 'title': '刘诗诗和杨幂谁漂亮'} 	 Label: similar
Data: {'query': '微信号怎么二次修改', 'title': '怎么再二次修改微信号'} 	 Label: similar
Data: {'query': '什么牌子的精油皂好', 'title': '什么牌子的精油好？'} 	 Label: dissimilar
Data: {'query': '刚出生的小野鸡怎么养', 'title': '刚抓来的野鸡怎么养殖'} 	 Label: similar
Data: {'query': '如何入侵他人手机', 'title': '如何入侵别人的手机'} 	 Label: similar
Data: {'query': '红米刷什么系统好', 'title': '红米可以刷什么系统'} 	 Label: similar
Data: {'query': '这叫什么高跟鞋', 'title': '这种高跟鞋叫什么呀'} 	 Label: similar
Data: {'query': '汇理财怎么样', 'title': '怎么样去理财？'} 	 Label: dissimilar
Data: {'query': '什么是刷屏', 'title': '什么叫刷屏？'} 	 Label: similar
Data: {'query': '各国货币符号是什么?', 'title': '如何用电脑打出各国货币符号呀'} 	 Label: dissimilar
Data: {'query': '上嘴唇有痣代表什么', 'title': '咬嘴唇代表什么'} 	 Label: dissimilar
Data: {'query': '哪种减肥药最快最有效', 'title': '哪种减肥药最有效，减肥效果最好'} 	 Label: similar
Data: {'query': '邓紫棋唱功怎么样', 'title': '邓紫棋唱功怎么样？'} 	 Label: similar
Data: {'query': '怎么做视频', 'title': '贴吧怎么贴视频'} 	 Label: similar
Data: {'query': '现在女生流行什么发型？', 'title': '女生现在流行什么发型？'} 	 Label: similar
Data: {'query': '为什么坐车玩手机会晕车', 'title': '为什么我坐车玩手机不晕车'} 	 Label: similar
Data: {'query': '为什么老婆不喜欢和我做爱', 'title': '我老婆为什么不喜欢做爱'} 	 Label: similar
Data: {'query': '怎么测试我爱的他爱不爱我。', 'title': '怎么测试老公爱不爱我啊'} 	 Label: similar
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
Does not support hardware other than CPU and GPU Currently!
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:45:19,046 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 12:45:19,046 auto_parallel_config: None
LAUNCH INFO 2023-12-21 12:45:19,046 auto_tuner_json: None
LAUNCH INFO 2023-12-21 12:45:19,046 devices: 0,1
LAUNCH INFO 2023-12-21 12:45:19,046 elastic_level: -1
LAUNCH INFO 2023-12-21 12:45:19,046 elastic_timeout: 30
LAUNCH INFO 2023-12-21 12:45:19,046 enable_gpu_log: True
LAUNCH INFO 2023-12-21 12:45:19,046 gloo_port: 6767
LAUNCH INFO 2023-12-21 12:45:19,046 host: None
LAUNCH INFO 2023-12-21 12:45:19,046 ips: None
LAUNCH INFO 2023-12-21 12:45:19,046 job_id: default
LAUNCH INFO 2023-12-21 12:45:19,046 legacy: False
LAUNCH INFO 2023-12-21 12:45:19,046 log_dir: log
LAUNCH INFO 2023-12-21 12:45:19,046 log_level: INFO
LAUNCH INFO 2023-12-21 12:45:19,046 log_overwrite: False
LAUNCH INFO 2023-12-21 12:45:19,047 master: None
LAUNCH INFO 2023-12-21 12:45:19,047 max_restart: 3
LAUNCH INFO 2023-12-21 12:45:19,047 nnodes: 1
LAUNCH INFO 2023-12-21 12:45:19,047 nproc_per_node: None
LAUNCH INFO 2023-12-21 12:45:19,047 rank: -1
LAUNCH INFO 2023-12-21 12:45:19,047 run_mode: collective
LAUNCH INFO 2023-12-21 12:45:19,047 server_num: None
LAUNCH INFO 2023-12-21 12:45:19,047 servers: 
LAUNCH INFO 2023-12-21 12:45:19,047 sort_ip: False
LAUNCH INFO 2023-12-21 12:45:19,047 start_port: 6070
LAUNCH INFO 2023-12-21 12:45:19,047 trainer_num: None
LAUNCH INFO 2023-12-21 12:45:19,047 trainers: 
LAUNCH INFO 2023-12-21 12:45:19,047 training_script: ./test_tipc/ernie_text_matching/train.py
LAUNCH INFO 2023-12-21 12:45:19,047 training_script_args: ['--max_steps', '150', '--device=xpu', '--save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1', '--epoch=1', '--batch_size=32']
LAUNCH INFO 2023-12-21 12:45:19,047 with_gloo: 1
LAUNCH INFO 2023-12-21 12:45:19,047 --------------------------------------------------
LAUNCH INFO 2023-12-21 12:45:19,047 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 12:45:19,048 Run Pod: rceitv, replicas 2, status ready
LAUNCH INFO 2023-12-21 12:45:19,065 Watching Pod: rceitv, replicas 2, status running
LAUNCH INFO 2023-12-21 12:46:50,162 Pod completed
LAUNCH INFO 2023-12-21 12:46:50,162 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 12:45:22.415892  4815 tcp_utils.cc:181] The server starts to listen on IP_ANY:41884
I1221 12:45:22.417420  4815 tcp_utils.cc:130] Successfully connected to 127.0.0.1:41884
[32m[2023-12-21 12:45:23,111] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:45:23,111] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:45:23,112] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:45:23,384] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:45:23.388751  4815 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:45:26,253] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieModel: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:45:26,253] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.weight', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:45:26,257] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:45:26,258] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:45:26,281] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:45:26,281] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
global step 10, epoch: 1, batch: 10, loss: 0.48282, accu: 0.66563, speed: 2.32 step/s
global step 20, epoch: 1, batch: 20, loss: 0.41112, accu: 0.75313, speed: 2.10 step/s
global step 30, epoch: 1, batch: 30, loss: 0.39826, accu: 0.78125, speed: 2.10 step/s
global step 40, epoch: 1, batch: 40, loss: 0.48339, accu: 0.80156, speed: 2.13 step/s
global step 50, epoch: 1, batch: 50, loss: 0.40343, accu: 0.81750, speed: 2.10 step/s
global step 60, epoch: 1, batch: 60, loss: 0.46257, accu: 0.82656, speed: 2.10 step/s
global step 70, epoch: 1, batch: 70, loss: 0.34538, accu: 0.83661, speed: 2.09 step/s
global step 80, epoch: 1, batch: 80, loss: 0.41523, accu: 0.84453, speed: 2.10 step/s
global step 90, epoch: 1, batch: 90, loss: 0.38378, accu: 0.84722, speed: 2.13 step/s
global step 100, epoch: 1, batch: 100, loss: 0.40280, accu: 0.85156, speed: 2.13 step/s
eval dev loss: 0.51494, accu: 0.79084
[32m[2023-12-21 12:46:23,575] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:46:23,575] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 1, batch: 110, loss: 0.44757, accu: 0.84062, speed: 0.75 step/s
global step 120, epoch: 1, batch: 120, loss: 0.55862, accu: 0.83906, speed: 2.14 step/s
global step 130, epoch: 1, batch: 130, loss: 0.45515, accu: 0.84479, speed: 2.10 step/s
global step 140, epoch: 1, batch: 140, loss: 0.46599, accu: 0.84375, speed: 2.12 step/s
global step 150, epoch: 1, batch: 150, loss: 0.44931, accu: 0.84562, speed: 2.17 step/s
I1221 12:46:48.564839  4849 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python -m paddle.distributed.launch --gpus=0,1 ./test_tipc/ernie_text_matching/train.py --max_steps 150 --device=xpu --save_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 --epoch=1     --batch_size=32     - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:46:53,911] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:46:53,912] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:46:53,912] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:46:54,189] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:46:54.198503  4879 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:46:57,402] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieModel: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:46:57,402] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.weight', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
/opt/py39/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py:709: UserWarning: full_graph=False don't support input_spec arguments. It will not produce any effect.
You can set full_graph=True, then you can assign input spec.

  warnings.warn(
I1221 12:46:59.097661  4879 program_interpreter.cc:214] New Executor is Running.
Loaded parameters from /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/model_state.pdparams
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/export_model.py --params_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model --output_path=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1 >/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:47:04.209132  4947 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:47:04.212198  4947 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:47:04.418004  4947 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:47:04.419842  4947 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:47:04.438006  4947 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:47:04.456189  4947 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:47:04.473250  4947 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:47:04.481793  4947 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:47:04.489141  4947 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:47:04.501570  4947 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:47:04.503423  4947 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:47:04.506471  4947 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:47:04.537902  4947 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:47:04.538664  4947 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:47:04.538676  4947 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:47:04.539559  4947 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:47:04,539] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:47:04,540] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:47:04,565] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:47:04,566] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:47:04.702276  4947 onednn_context.cc:81] oneDNN v3.2.1
Data: {'query': '谁有狂三这张高清的', 'title': '这张高清图，谁有'} 	 Label: dissimilar
Data: {'query': '英雄联盟什么英雄最好', 'title': '英雄联盟最好英雄是什么'} 	 Label: similar
Data: {'query': '这是什么意思，被蹭网吗', 'title': '我也是醉了，这是什么意思'} 	 Label: dissimilar
Data: {'query': '现在有什么动画片好看呢？', 'title': '现在有什么好看的动画片吗？'} 	 Label: similar
Data: {'query': '请问晶达电子厂现在的工资待遇怎么样要求有哪些', 'title': '三星电子厂工资待遇怎么样啊'} 	 Label: dissimilar
Data: {'query': '文章真的爱姚笛吗', 'title': '姚笛真的被文章干了吗'} 	 Label: dissimilar
Data: {'query': '送自己做的闺蜜什么生日礼物好', 'title': '送闺蜜什么生日礼物好'} 	 Label: similar
Data: {'query': '近期上映的电影', 'title': '近期上映的电影有哪些'} 	 Label: similar
Data: {'query': '求英雄联盟大神带？', 'title': '英雄联盟，求大神带~'} 	 Label: similar
Data: {'query': '如加上什么部首', 'title': '给东加上部首是什么字？'} 	 Label: dissimilar
Data: {'query': '杭州哪里好玩', 'title': '杭州哪里好玩点'} 	 Label: similar
Data: {'query': '这是什么乌龟值钱吗', 'title': '这是什么乌龟！值钱嘛？'} 	 Label: similar
Data: {'query': '心各有所属是什么意思？', 'title': '心有所属是什么意思?'} 	 Label: similar
Data: {'query': '什么东西越热爬得越高', 'title': '什么东西越热爬得很高'} 	 Label: similar
Data: {'query': '世界杯哪位球员进球最多', 'title': '世界杯单界进球最多是哪位球员'} 	 Label: similar
Data: {'query': '韭菜多吃什么好处', 'title': '多吃韭菜有什么好处'} 	 Label: similar
Data: {'query': '云赚钱怎么样', 'title': '怎么才能赚钱'} 	 Label: dissimilar
Data: {'query': '何炅结婚了嘛', 'title': '何炅结婚了么'} 	 Label: similar
Data: {'query': '长的清新是什么意思', 'title': '小清新的意思是什么'} 	 Label: dissimilar
Data: {'query': '我们可以结婚了吗？', 'title': '在熙结婚了吗？'} 	 Label: dissimilar
Data: {'query': '想买男人酒补肾壮阳酒哪里有啊', 'title': '哪里有男人酒补肾壮阳酒'} 	 Label: similar
Data: {'query': '淘宝上怎么用信用卡分期付款', 'title': '淘宝怎么分期付款，没有信用卡'} 	 Label: similar
Data: {'query': '最近有没有什么好看的韩剧', 'title': '最近有什么好看的韩剧'} 	 Label: similar
Data: {'query': '《校花的贴身高手》中的林逸', 'title': '校花贴身高手'} 	 Label: dissimilar
Data: {'query': '叔叔是什么人', 'title': '我是叔叔的什么人'} 	 Label: similar
Data: {'query': '这姑娘漂亮不', 'title': '我姑娘漂亮吧'} 	 Label: dissimilar
Data: {'query': '在淘宝网买手机可靠吗？', 'title': '在淘宝网上买手机可靠吗？'} 	 Label: similar
Data: {'query': '山楂干怎么吃好吃？', 'title': '山楂怎么做好吃'} 	 Label: dissimilar
Data: {'query': '时间都去哪怕了歌谱', 'title': '时间煮雨歌谱'} 	 Label: dissimilar
Data: {'query': '苏州哪里能买到这个衣服', 'title': '苏州哪里有买大号衣服的？'} 	 Label: dissimilar
Data: {'query': '最好玩的手机网游', 'title': '好玩的手机网游'} 	 Label: similar
Data: {'query': '石榴是什么时候成熟的？', 'title': '成熟的石榴像什么？'} 	 Label: dissimilar
Data: {'query': '刘诗诗杨幂谁漂亮', 'title': '刘诗诗和杨幂谁漂亮'} 	 Label: similar
Data: {'query': '微信号怎么二次修改', 'title': '怎么再二次修改微信号'} 	 Label: similar
Data: {'query': '什么牌子的精油皂好', 'title': '什么牌子的精油好？'} 	 Label: similar
Data: {'query': '刚出生的小野鸡怎么养', 'title': '刚抓来的野鸡怎么养殖'} 	 Label: dissimilar
Data: {'query': '如何入侵他人手机', 'title': '如何入侵别人的手机'} 	 Label: similar
Data: {'query': '红米刷什么系统好', 'title': '红米可以刷什么系统'} 	 Label: similar
Data: {'query': '这叫什么高跟鞋', 'title': '这种高跟鞋叫什么呀'} 	 Label: similar
Data: {'query': '汇理财怎么样', 'title': '怎么样去理财？'} 	 Label: dissimilar
Data: {'query': '什么是刷屏', 'title': '什么叫刷屏？'} 	 Label: similar
Data: {'query': '各国货币符号是什么?', 'title': '如何用电脑打出各国货币符号呀'} 	 Label: similar
Data: {'query': '上嘴唇有痣代表什么', 'title': '咬嘴唇代表什么'} 	 Label: dissimilar
Data: {'query': '哪种减肥药最快最有效', 'title': '哪种减肥药最有效，减肥效果最好'} 	 Label: similar
Data: {'query': '邓紫棋唱功怎么样', 'title': '邓紫棋唱功怎么样？'} 	 Label: similar
Data: {'query': '怎么做视频', 'title': '贴吧怎么贴视频'} 	 Label: dissimilar
Data: {'query': '现在女生流行什么发型？', 'title': '女生现在流行什么发型？'} 	 Label: similar
Data: {'query': '为什么坐车玩手机会晕车', 'title': '为什么我坐车玩手机不晕车'} 	 Label: similar
Data: {'query': '为什么老婆不喜欢和我做爱', 'title': '我老婆为什么不喜欢做爱'} 	 Label: similar
Data: {'query': '怎么测试我爱的他爱不爱我。', 'title': '怎么测试老公爱不爱我啊'} 	 Label: dissimilar
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=1 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
I1221 12:47:12.084180  5014 analysis_predictor.cc:1636] MKLDNN is enabled
[1m[35m--- Running analysis [ir_graph_build_pass][0m
I1221 12:47:12.087630  5014 executor.cc:187] Old Executor is Running.
[1m[35m--- Running analysis [ir_analysis_pass][0m
[32m--- Running IR pass [mkldnn_placement_pass][0m
[32m--- Running IR pass [simplify_with_basic_ops_pass][0m
[32m--- Running IR pass [layer_norm_fuse_pass][0m
[32m--- Running IR pass [attention_lstm_fuse_pass][0m
[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
[32m--- Running IR pass [mul_lstm_fuse_pass][0m
[32m--- Running IR pass [fc_gru_fuse_pass][0m
[32m--- Running IR pass [mul_gru_fuse_pass][0m
[32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
[32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
I1221 12:47:12.291965  5014 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
I1221 12:47:12.293880  5014 fuse_pass_base.cc:59] ---  detected 12 subgraphs
[32m--- Running IR pass [matmul_scale_fuse_pass][0m
[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
[32m--- Running IR pass [fc_fuse_pass][0m
I1221 12:47:12.312139  5014 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
[32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [is_test_pass][0m
[32m--- Running IR pass [constant_folding_pass][0m
I1221 12:47:12.330749  5014 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass][0m
[37m--- fused 0 squeeze2 with transpose2[0m
[32m--- Running IR pass [depthwise_conv_mkldnn_pass][0m
[32m--- Running IR pass [conv_bn_fuse_pass][0m
[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass][0m
[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [scale_matmul_fuse_pass][0m
I1221 12:47:12.347687  5014 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 scale with matmul[0m
[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass][0m
I1221 12:47:12.356254  5014 fuse_pass_base.cc:59] ---  detected 18 subgraphs
[37m---    fused 18 reshape + transpose + matmul with reshape's xshape[0m
[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass][0m
[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass][0m
I1221 12:47:12.363610  5014 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fused_matmul (as x) with elementwise_add[0m
[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass][0m
[32m--- Running IR pass [fc_mkldnn_pass][0m
I1221 12:47:12.376096  5014 fuse_pass_base.cc:59] ---  detected 38 subgraphs
[37m---    enabled FC MKL-DNN for 38 fc ops [0m
[32m--- Running IR pass [fc_act_mkldnn_fuse_pass][0m
I1221 12:47:12.377949  5014 fuse_pass_base.cc:59] ---  detected 6 subgraphs
[37m---    fused 6 fc with gelu activation[0m
I1221 12:47:12.381009  5014 fuse_pass_base.cc:59] ---  detected 1 subgraphs
[37m---    fused 1 fc with tanh activation[0m
[32m--- Running IR pass [self_attention_fuse_pass][0m
[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse[0m
[32m--- Running IR pass [batch_norm_act_fuse_pass][0m
[32m--- Running IR pass [softplus_activation_onednn_fuse_pass][0m
[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass][0m
[32m--- Running IR pass [elementwise_act_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_scale_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass][0m
[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass][0m
[1m[35m--- Running analysis [save_optimized_model_pass][0m
[1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
[1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
[1m[35m--- Running analysis [inference_op_replace_pass][0m
[1m[35m--- Running analysis [ir_graph_to_program_pass][0m
I1221 12:47:12.413553  5014 analysis_predictor.cc:1851] ======= optimize end =======
I1221 12:47:12.414530  5014 naive_executor.cc:200] ---  skip [feed], feed -> token_type_ids
I1221 12:47:12.414541  5014 naive_executor.cc:200] ---  skip [feed], feed -> input_ids
I1221 12:47:12.415620  5014 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
[32m[2023-12-21 12:47:12,415] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:47:12,416] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:47:12,442] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:47:12,443] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
I1221 12:47:12.626545  5014 onednn_context.cc:81] oneDNN v3.2.1
Data: {'query': '谁有狂三这张高清的', 'title': '这张高清图，谁有'} 	 Label: dissimilar
Data: {'query': '英雄联盟什么英雄最好', 'title': '英雄联盟最好英雄是什么'} 	 Label: similar
Data: {'query': '这是什么意思，被蹭网吗', 'title': '我也是醉了，这是什么意思'} 	 Label: dissimilar
Data: {'query': '现在有什么动画片好看呢？', 'title': '现在有什么好看的动画片吗？'} 	 Label: similar
Data: {'query': '请问晶达电子厂现在的工资待遇怎么样要求有哪些', 'title': '三星电子厂工资待遇怎么样啊'} 	 Label: dissimilar
Data: {'query': '文章真的爱姚笛吗', 'title': '姚笛真的被文章干了吗'} 	 Label: dissimilar
Data: {'query': '送自己做的闺蜜什么生日礼物好', 'title': '送闺蜜什么生日礼物好'} 	 Label: similar
Data: {'query': '近期上映的电影', 'title': '近期上映的电影有哪些'} 	 Label: similar
Data: {'query': '求英雄联盟大神带？', 'title': '英雄联盟，求大神带~'} 	 Label: similar
Data: {'query': '如加上什么部首', 'title': '给东加上部首是什么字？'} 	 Label: dissimilar
Data: {'query': '杭州哪里好玩', 'title': '杭州哪里好玩点'} 	 Label: similar
Data: {'query': '这是什么乌龟值钱吗', 'title': '这是什么乌龟！值钱嘛？'} 	 Label: similar
Data: {'query': '心各有所属是什么意思？', 'title': '心有所属是什么意思?'} 	 Label: similar
Data: {'query': '什么东西越热爬得越高', 'title': '什么东西越热爬得很高'} 	 Label: similar
Data: {'query': '世界杯哪位球员进球最多', 'title': '世界杯单界进球最多是哪位球员'} 	 Label: similar
Data: {'query': '韭菜多吃什么好处', 'title': '多吃韭菜有什么好处'} 	 Label: similar
Data: {'query': '云赚钱怎么样', 'title': '怎么才能赚钱'} 	 Label: dissimilar
Data: {'query': '何炅结婚了嘛', 'title': '何炅结婚了么'} 	 Label: similar
Data: {'query': '长的清新是什么意思', 'title': '小清新的意思是什么'} 	 Label: dissimilar
Data: {'query': '我们可以结婚了吗？', 'title': '在熙结婚了吗？'} 	 Label: dissimilar
Data: {'query': '想买男人酒补肾壮阳酒哪里有啊', 'title': '哪里有男人酒补肾壮阳酒'} 	 Label: similar
Data: {'query': '淘宝上怎么用信用卡分期付款', 'title': '淘宝怎么分期付款，没有信用卡'} 	 Label: similar
Data: {'query': '最近有没有什么好看的韩剧', 'title': '最近有什么好看的韩剧'} 	 Label: similar
Data: {'query': '《校花的贴身高手》中的林逸', 'title': '校花贴身高手'} 	 Label: dissimilar
Data: {'query': '叔叔是什么人', 'title': '我是叔叔的什么人'} 	 Label: similar
Data: {'query': '这姑娘漂亮不', 'title': '我姑娘漂亮吧'} 	 Label: dissimilar
Data: {'query': '在淘宝网买手机可靠吗？', 'title': '在淘宝网上买手机可靠吗？'} 	 Label: similar
Data: {'query': '山楂干怎么吃好吃？', 'title': '山楂怎么做好吃'} 	 Label: dissimilar
Data: {'query': '时间都去哪怕了歌谱', 'title': '时间煮雨歌谱'} 	 Label: dissimilar
Data: {'query': '苏州哪里能买到这个衣服', 'title': '苏州哪里有买大号衣服的？'} 	 Label: dissimilar
Data: {'query': '最好玩的手机网游', 'title': '好玩的手机网游'} 	 Label: similar
Data: {'query': '石榴是什么时候成熟的？', 'title': '成熟的石榴像什么？'} 	 Label: dissimilar
Data: {'query': '刘诗诗杨幂谁漂亮', 'title': '刘诗诗和杨幂谁漂亮'} 	 Label: similar
Data: {'query': '微信号怎么二次修改', 'title': '怎么再二次修改微信号'} 	 Label: similar
Data: {'query': '什么牌子的精油皂好', 'title': '什么牌子的精油好？'} 	 Label: similar
Data: {'query': '刚出生的小野鸡怎么养', 'title': '刚抓来的野鸡怎么养殖'} 	 Label: dissimilar
Data: {'query': '如何入侵他人手机', 'title': '如何入侵别人的手机'} 	 Label: similar
Data: {'query': '红米刷什么系统好', 'title': '红米可以刷什么系统'} 	 Label: similar
Data: {'query': '这叫什么高跟鞋', 'title': '这种高跟鞋叫什么呀'} 	 Label: similar
Data: {'query': '汇理财怎么样', 'title': '怎么样去理财？'} 	 Label: dissimilar
Data: {'query': '什么是刷屏', 'title': '什么叫刷屏？'} 	 Label: similar
Data: {'query': '各国货币符号是什么?', 'title': '如何用电脑打出各国货币符号呀'} 	 Label: similar
Data: {'query': '上嘴唇有痣代表什么', 'title': '咬嘴唇代表什么'} 	 Label: dissimilar
Data: {'query': '哪种减肥药最快最有效', 'title': '哪种减肥药最有效，减肥效果最好'} 	 Label: similar
Data: {'query': '邓紫棋唱功怎么样', 'title': '邓紫棋唱功怎么样？'} 	 Label: similar
Data: {'query': '怎么做视频', 'title': '贴吧怎么贴视频'} 	 Label: dissimilar
Data: {'query': '现在女生流行什么发型？', 'title': '女生现在流行什么发型？'} 	 Label: similar
Data: {'query': '为什么坐车玩手机会晕车', 'title': '为什么我坐车玩手机不晕车'} 	 Label: similar
Data: {'query': '为什么老婆不喜欢和我做爱', 'title': '我老婆为什么不喜欢做爱'} 	 Label: similar
Data: {'query': '怎么测试我爱的他爱不爱我。', 'title': '怎么测试老公爱不爱我啊'} 	 Label: dissimilar
No XPU Memory Leak
[33m Run successfully with command - ernie_text_matching - python ./test_tipc/ernie_text_matching/predict.py --max_steps 50 --device=cpu --enable_mkldnn=False --cpu_threads=6 --model_dir=/workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/ --batch_size=32     --benchmark=False --precision=fp32   > /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log 2>&1  - /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log [0m
Does not support hardware other than CPU and GPU Currently!
+ watchcat=3648
+ kill -9 4375
+ sleep 10
run.sh: line 334:  4375 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/ernie_text_matching/train_infer_python.txt
==END==test_tipc/configs/ernie_text_matching/train_infer_python.txt
++ date +%s
+ end=1703134044
++ echo 1703133823 1703134044
++ awk '{print $2-$1-2}'
+ time=219
+ echo 'test_tipc/configs/ernie_text_matching/train_infer_python.txt spend time seconds 219'
+ read config_file
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
test_tipc/configs/ernie_text_matching/train_infer_python.txt spend time seconds 219
++ date +%s
+ start=1703134044
+ echo ==START==test_tipc/configs/ernie_tiny/train_infer_python.txt
==START==test_tipc/configs/ernie_tiny/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/ernie_tiny/train_infer_python.txt
+ dataline='===========================train_params===========================
model_name:ernie_tiny
python:python3.7
gpu_list:0|0,1
--device:gpu|gpu
null:null
--max_steps:null
--save_model:./test_tipc/output/
--batch_size:null
null:null
null:null
null:null
null:null
##
trainer:norm_train
norm_train:test_tipc/train.py --amp_level O2 --model ernie_tiny --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path ernie-tiny --pad_to_max_seq_len --max_seq_len 128 --logging_steps 1 --task_name tnews --max_steps 150
null:null
null:null
null:null
null:null
null:null
##
===========================eval_params=========================== 
null:null
null:null
##
===========================infer_params===========================
null:null
null:null
null:null
null:null
null:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:ernie_tiny
++ strs=model_name:ernie_tiny
++ IFS=:
++ array=(${strs})
++ tmp=ernie_tiny
++ echo ernie_tiny
+ model_name=ernie_tiny
+ sleep 10
+ run run_model test_tipc/configs/ernie_tiny/train_infer_python.txt lite_train_lite_infer 3600 ernie_tiny
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ waitfor=7200
+ command='run_model
test_tipc/configs/ernie_tiny/train_infer_python.txt
lite_train_lite_infer
3600
ernie_tiny'
+ commandpid=5098
+ run_model test_tipc/configs/ernie_tiny/train_infer_python.txt lite_train_lite_infer 3600 ernie_tiny
+ config_file=test_tipc/configs/ernie_tiny/train_infer_python.txt
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/ernie_tiny/train_infer_python.txt lite_train_lite_infer
+ watchdog=5099
+ wait 5098
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/ernie_tiny/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/ernie_tiny/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:47:37,716] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_tiny/vocab.txt and saved to /root/.paddlenlp/models/ernie-tiny[0m
[32m[2023-12-21 12:47:39,912] [    INFO][0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_tiny/vocab.txt[0m
Namespace(device='xpu', model='ernie_tiny', logging_steps=1, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1', batch_size=1, max_seq_len=128, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='ernie-tiny', task_name='tnews', max_seq_length=128, warmup_steps=0, warmup_proportion=0.1)
  0%|          | 0.00/459k [00:00<?, ?B/s]  1%|          | 3.00k/459k [00:00<00:30, 15.4kB/s]  4%|▍         | 19.0k/459k [00:00<00:08, 51.8kB/s]  8%|▊         | 35.0k/459k [00:00<00:05, 73.8kB/s] 11%|█         | 51.0k/459k [00:00<00:04, 88.1kB/s] 15%|█▍        | 67.0k/459k [00:00<00:03, 101kB/s]  18%|█▊        | 83.0k/459k [00:00<00:03, 110kB/s] 22%|██▏       | 99.0k/459k [00:01<00:03, 121kB/s] 29%|██▊       | 131k/459k [00:01<00:02, 146kB/s]  36%|███▌      | 163k/459k [00:01<00:01, 161kB/s] 42%|████▏     | 195k/459k [00:01<00:01, 169kB/s] 53%|█████▎    | 243k/459k [00:01<00:01, 209kB/s] 60%|█████▉    | 275k/459k [00:01<00:00, 210kB/s] 70%|███████   | 323k/459k [00:02<00:00, 232kB/s] 81%|████████  | 371k/459k [00:02<00:00, 255kB/s] 91%|█████████▏| 419k/459k [00:02<00:00, 269kB/s]100%|██████████| 459k/459k [00:02<00:00, 188kB/s]
[32m[2023-12-21 12:47:43,207] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-tiny/tokenizer_config.json[0m
[32m[2023-12-21 12:47:43,208] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-tiny/special_tokens_map.json[0m
[32m[2023-12-21 12:47:43,667] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/ernie-tiny/config.json[0m
[32m[2023-12-21 12:47:44,149] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_tiny/ernie_tiny.pdparams[0m
[32m[2023-12-21 12:47:44,150] [    INFO][0m - Downloading ernie_tiny.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_tiny/ernie_tiny.pdparams[0m
  0%|          | 0.00/346M [00:00<?, ?B/s]  0%|          | 3.00k/346M [00:00<6:19:40, 15.9kB/s]  0%|          | 19.0k/346M [00:00<1:21:16, 74.4kB/s]  0%|          | 35.0k/346M [00:00<57:12, 106kB/s]     0%|          | 67.0k/346M [00:00<43:29, 139kB/s]  0%|          | 83.0k/346M [00:00<55:01, 110kB/s]  0%|          | 99.0k/346M [00:00<50:20, 120kB/s]  0%|          | 115k/346M [00:01<47:04, 128kB/s]   0%|          | 147k/346M [00:01<40:40, 149kB/s]  0%|          | 195k/346M [00:01<30:26, 198kB/s]  0%|          | 215k/346M [00:01<31:08, 194kB/s]  0%|          | 234k/346M [00:01<31:46, 190kB/s]  0%|          | 259k/346M [00:01<30:11, 200kB/s]  0%|          | 279k/346M [00:01<30:57, 195kB/s]  0%|          | 299k/346M [00:01<30:58, 195kB/s]  0%|          | 339k/346M [00:02<28:28, 212kB/s]  0%|          | 387k/346M [00:02<25:27, 237kB/s]  0%|          | 435k/346M [00:02<23:26, 258kB/s]  0%|          | 483k/346M [00:02<22:09, 272kB/s]  0%|          | 547k/346M [00:02<19:51, 304kB/s]  0%|          | 595k/346M [00:02<20:49, 290kB/s]  0%|          | 659k/346M [00:03<19:27, 310kB/s]  0%|          | 691k/346M [00:03<19:30, 309kB/s]  0%|          | 755k/346M [00:03<19:00, 317kB/s]  0%|          | 803k/346M [00:03<17:10, 351kB/s]  0%|          | 839k/346M [00:03<17:03, 354kB/s]  0%|          | 899k/346M [00:03<18:19, 329kB/s]  0%|          | 945k/346M [00:03<16:46, 360kB/s]  0%|          | 983k/346M [00:04<16:49, 358kB/s]  0%|          | 1.00M/346M [00:04<17:11, 351kB/s]  0%|          | 1.03M/346M [00:04<17:59, 335kB/s]  0%|          | 1.06M/346M [00:04<19:33, 308kB/s]  0%|          | 1.10M/346M [00:04<20:47, 290kB/s]  0%|          | 1.14M/346M [00:04<18:32, 325kB/s]  0%|          | 1.19M/346M [00:04<16:35, 363kB/s]  0%|          | 1.23M/346M [00:04<16:32, 364kB/s]  0%|          | 1.27M/346M [00:05<16:14, 371kB/s]  0%|          | 1.33M/346M [00:05<14:11, 425kB/s]  0%|          | 1.38M/346M [00:05<13:52, 434kB/s]  0%|          | 1.42M/346M [00:05<13:57, 431kB/s]  0%|          | 1.49M/346M [00:05<13:33, 444kB/s]  0%|          | 1.53M/346M [00:05<13:17, 453kB/s]  0%|          | 1.60M/346M [00:05<12:52, 467kB/s]  0%|          | 1.67M/346M [00:05<11:38, 517kB/s]  1%|          | 1.74M/346M [00:06<11:26, 526kB/s]  1%|          | 1.86M/346M [00:06<09:04, 662kB/s]  1%|          | 1.97M/346M [00:06<08:00, 751kB/s]  1%|          | 2.10M/346M [00:06<07:18, 822kB/s]  1%|          | 2.24M/346M [00:06<06:33, 916kB/s]  1%|          | 2.38M/346M [00:06<05:43, 1.05MB/s]  1%|          | 2.53M/346M [00:06<05:19, 1.13MB/s]  1%|          | 2.71M/346M [00:06<05:14, 1.15MB/s]  1%|          | 2.88M/346M [00:07<05:30, 1.09MB/s]  1%|          | 3.08M/346M [00:07<05:12, 1.15MB/s]  1%|          | 3.19M/346M [00:07<05:12, 1.15MB/s]  1%|          | 3.30M/346M [00:07<05:26, 1.10MB/s]  1%|          | 3.41M/346M [00:07<05:42, 1.05MB/s]  1%|          | 3.53M/346M [00:07<05:48, 1.03MB/s]  1%|          | 3.66M/346M [00:07<05:30, 1.09MB/s]  1%|          | 3.80M/346M [00:07<05:07, 1.17MB/s]  1%|          | 3.92M/346M [00:08<05:09, 1.16MB/s]  1%|          | 4.08M/346M [00:08<04:55, 1.21MB/s]  1%|          | 4.24M/346M [00:08<04:51, 1.23MB/s]  1%|▏         | 4.39M/346M [00:08<04:48, 1.24MB/s]  1%|▏         | 4.58M/346M [00:08<04:30, 1.32MB/s]  1%|▏         | 4.75M/346M [00:08<04:16, 1.40MB/s]  1%|▏         | 5.14M/346M [00:08<02:55, 2.04MB/s]  2%|▏         | 5.36M/346M [00:08<03:07, 1.91MB/s]  2%|▏         | 5.71M/346M [00:09<02:33, 2.33MB/s]  2%|▏         | 6.02M/346M [00:09<02:41, 2.21MB/s]  2%|▏         | 6.50M/346M [00:09<02:24, 2.46MB/s]  2%|▏         | 6.75M/346M [00:09<02:25, 2.44MB/s]  2%|▏         | 7.02M/346M [00:09<02:24, 2.46MB/s]  2%|▏         | 7.30M/346M [00:09<02:20, 2.53MB/s]  2%|▏         | 7.61M/346M [00:09<02:10, 2.72MB/s]  2%|▏         | 8.19M/346M [00:10<02:04, 2.84MB/s]  2%|▏         | 8.52M/346M [00:10<02:02, 2.89MB/s]  3%|▎         | 8.85M/346M [00:10<02:01, 2.91MB/s]  3%|▎         | 9.19M/346M [00:10<01:58, 2.98MB/s]  3%|▎         | 9.55M/346M [00:10<01:54, 3.09MB/s]  3%|▎         | 9.92M/346M [00:10<01:47, 3.27MB/s]  3%|▎         | 10.7M/346M [00:10<01:28, 3.96MB/s]  3%|▎         | 11.4M/346M [00:10<01:24, 4.15MB/s]  3%|▎         | 11.9M/346M [00:10<01:16, 4.59MB/s]  4%|▎         | 12.4M/346M [00:11<01:23, 4.20MB/s]  4%|▎         | 12.9M/346M [00:11<01:31, 3.84MB/s]  4%|▍         | 13.2M/346M [00:11<01:33, 3.75MB/s]  4%|▍         | 13.6M/346M [00:11<01:35, 3.66MB/s]  4%|▍         | 13.9M/346M [00:11<01:37, 3.56MB/s]  4%|▍         | 14.3M/346M [00:11<01:38, 3.53MB/s]  4%|▍         | 14.6M/346M [00:11<01:39, 3.48MB/s]  4%|▍         | 15.0M/346M [00:11<01:43, 3.34MB/s]  4%|▍         | 15.3M/346M [00:12<01:45, 3.30MB/s]  5%|▍         | 15.6M/346M [00:12<01:41, 3.42MB/s]  5%|▍         | 16.0M/346M [00:12<01:42, 3.38MB/s]  5%|▍         | 16.3M/346M [00:12<02:23, 2.41MB/s]  5%|▍         | 16.8M/346M [00:12<01:58, 2.90MB/s]  5%|▍         | 17.1M/346M [00:12<02:11, 2.63MB/s]  5%|▌         | 17.4M/346M [00:12<02:20, 2.45MB/s]  5%|▌         | 17.7M/346M [00:13<02:23, 2.40MB/s]  5%|▌         | 18.1M/346M [00:13<02:09, 2.66MB/s]  5%|▌         | 18.4M/346M [00:13<02:08, 2.68MB/s]  5%|▌         | 18.7M/346M [00:13<02:05, 2.74MB/s]  5%|▌         | 19.0M/346M [00:13<02:07, 2.70MB/s]  6%|▌         | 19.2M/346M [00:13<02:08, 2.66MB/s]  6%|▌         | 19.5M/346M [00:13<02:11, 2.60MB/s]  6%|▌         | 19.8M/346M [00:13<02:07, 2.68MB/s]  6%|▌         | 20.1M/346M [00:13<02:06, 2.71MB/s]  6%|▌         | 20.4M/346M [00:14<02:04, 2.75MB/s]  6%|▌         | 20.7M/346M [00:14<02:05, 2.72MB/s]  6%|▌         | 21.0M/346M [00:14<02:08, 2.66MB/s]  6%|▌         | 21.3M/346M [00:14<02:08, 2.65MB/s]  6%|▌         | 21.5M/346M [00:14<02:09, 2.63MB/s]  6%|▋         | 21.8M/346M [00:14<02:07, 2.67MB/s]  6%|▋         | 22.1M/346M [00:14<02:10, 2.61MB/s]  6%|▋         | 22.3M/346M [00:14<02:13, 2.54MB/s]  7%|▋         | 22.6M/346M [00:14<02:17, 2.47MB/s]  7%|▋         | 22.9M/346M [00:15<02:11, 2.58MB/s]  7%|▋         | 23.2M/346M [00:15<02:04, 2.72MB/s]  7%|▋         | 23.5M/346M [00:15<02:01, 2.79MB/s]  7%|▋         | 23.8M/346M [00:15<02:01, 2.78MB/s]  7%|▋         | 24.1M/346M [00:15<02:10, 2.59MB/s]  7%|▋         | 24.3M/346M [00:15<02:20, 2.40MB/s]  7%|▋         | 24.6M/346M [00:15<02:19, 2.42MB/s]  7%|▋         | 24.9M/346M [00:15<02:18, 2.42MB/s]  7%|▋         | 25.2M/346M [00:16<02:11, 2.55MB/s]  7%|▋         | 25.5M/346M [00:16<02:12, 2.54MB/s]  7%|▋         | 25.8M/346M [00:16<02:14, 2.50MB/s]  8%|▊         | 26.1M/346M [00:16<02:16, 2.46MB/s]  8%|▊         | 26.4M/346M [00:16<02:21, 2.37MB/s]  8%|▊         | 26.7M/346M [00:16<02:21, 2.36MB/s]  8%|▊         | 27.0M/346M [00:16<02:25, 2.30MB/s]  8%|▊         | 27.3M/346M [00:16<02:25, 2.29MB/s]  8%|▊         | 27.6M/346M [00:17<02:25, 2.29MB/s]  8%|▊         | 27.9M/346M [00:17<02:26, 2.28MB/s]  8%|▊         | 28.3M/346M [00:17<02:26, 2.28MB/s]  8%|▊         | 28.5M/346M [00:17<02:28, 2.24MB/s]  8%|▊         | 28.9M/346M [00:17<02:23, 2.32MB/s]  8%|▊         | 29.1M/346M [00:17<02:17, 2.41MB/s]  8%|▊         | 29.4M/346M [00:17<02:18, 2.40MB/s]  9%|▊         | 29.6M/346M [00:18<02:27, 2.25MB/s]  9%|▊         | 29.8M/346M [00:18<02:50, 1.94MB/s]  9%|▊         | 30.0M/346M [00:18<02:50, 1.94MB/s]  9%|▉         | 30.3M/346M [00:18<02:41, 2.06MB/s]  9%|▉         | 30.5M/346M [00:18<02:45, 2.00MB/s]  9%|▉         | 30.7M/346M [00:18<02:57, 1.87MB/s]  9%|▉         | 31.0M/346M [00:18<02:39, 2.07MB/s]  9%|▉         | 31.3M/346M [00:18<02:26, 2.26MB/s]  9%|▉         | 31.6M/346M [00:19<02:24, 2.29MB/s]  9%|▉         | 31.9M/346M [00:19<02:15, 2.43MB/s]  9%|▉         | 32.2M/346M [00:19<02:08, 2.56MB/s]  9%|▉         | 32.5M/346M [00:19<01:59, 2.75MB/s] 10%|▉         | 32.9M/346M [00:19<01:51, 2.96MB/s] 10%|▉         | 33.5M/346M [00:19<01:25, 3.83MB/s] 10%|▉         | 34.1M/346M [00:19<01:22, 3.98MB/s] 10%|█         | 34.8M/346M [00:19<01:04, 5.09MB/s] 10%|█         | 35.3M/346M [00:19<01:14, 4.37MB/s] 10%|█         | 35.8M/346M [00:20<01:27, 3.73MB/s] 10%|█         | 36.2M/346M [00:20<01:32, 3.51MB/s] 11%|█         | 36.5M/346M [00:20<01:47, 3.02MB/s] 11%|█         | 36.9M/346M [00:20<01:46, 3.05MB/s] 11%|█         | 37.3M/346M [00:20<01:44, 3.09MB/s] 11%|█         | 37.6M/346M [00:20<01:39, 3.24MB/s] 11%|█         | 38.0M/346M [00:20<01:36, 3.34MB/s] 11%|█         | 38.3M/346M [00:21<01:51, 2.88MB/s] 11%|█         | 38.8M/346M [00:21<01:41, 3.16MB/s] 11%|█▏        | 39.2M/346M [00:21<01:41, 3.18MB/s] 11%|█▏        | 39.7M/346M [00:21<01:26, 3.72MB/s] 12%|█▏        | 40.0M/346M [00:21<01:43, 3.10MB/s] 12%|█▏        | 40.4M/346M [00:21<01:42, 3.11MB/s] 12%|█▏        | 40.7M/346M [00:21<01:42, 3.13MB/s] 12%|█▏        | 41.0M/346M [00:21<01:41, 3.15MB/s] 12%|█▏        | 41.3M/346M [00:22<01:44, 3.06MB/s] 12%|█▏        | 41.6M/346M [00:22<01:49, 2.91MB/s] 12%|█▏        | 41.9M/346M [00:22<01:53, 2.82MB/s] 12%|█▏        | 42.3M/346M [00:22<01:38, 3.24MB/s] 12%|█▏        | 42.6M/346M [00:22<01:47, 2.95MB/s] 12%|█▏        | 43.1M/346M [00:22<01:38, 3.22MB/s] 13%|█▎        | 43.6M/346M [00:22<01:30, 3.51MB/s] 13%|█▎        | 44.1M/346M [00:22<01:32, 3.43MB/s] 13%|█▎        | 44.5M/346M [00:23<01:39, 3.18MB/s] 13%|█▎        | 44.8M/346M [00:23<01:40, 3.13MB/s] 13%|█▎        | 45.1M/346M [00:23<01:43, 3.04MB/s] 13%|█▎        | 45.4M/346M [00:23<01:43, 3.04MB/s] 13%|█▎        | 45.7M/346M [00:23<02:15, 2.33MB/s] 13%|█▎        | 45.9M/346M [00:23<02:16, 2.30MB/s] 13%|█▎        | 46.2M/346M [00:23<02:16, 2.29MB/s] 13%|█▎        | 46.6M/346M [00:24<02:08, 2.45MB/s] 14%|█▎        | 46.8M/346M [00:24<02:06, 2.47MB/s] 14%|█▎        | 47.1M/346M [00:24<02:09, 2.41MB/s] 14%|█▎        | 47.3M/346M [00:24<02:58, 1.75MB/s] 14%|█▍        | 47.7M/346M [00:24<02:24, 2.16MB/s] 14%|█▍        | 48.0M/346M [00:24<02:40, 1.95MB/s] 14%|█▍        | 48.2M/346M [00:24<02:57, 1.76MB/s] 14%|█▍        | 48.3M/346M [00:25<03:06, 1.68MB/s] 14%|█▍        | 48.5M/346M [00:25<03:15, 1.60MB/s] 14%|█▍        | 48.7M/346M [00:25<03:12, 1.62MB/s] 14%|█▍        | 49.0M/346M [00:25<02:37, 1.98MB/s] 14%|█▍        | 49.2M/346M [00:25<02:39, 1.95MB/s] 14%|█▍        | 49.4M/346M [00:25<02:40, 1.93MB/s] 14%|█▍        | 49.6M/346M [00:25<02:48, 1.85MB/s] 14%|█▍        | 49.8M/346M [00:25<02:51, 1.81MB/s] 14%|█▍        | 50.0M/346M [00:25<02:45, 1.88MB/s] 15%|█▍        | 50.3M/346M [00:26<02:33, 2.01MB/s] 15%|█▍        | 50.6M/346M [00:26<02:09, 2.39MB/s] 15%|█▍        | 50.9M/346M [00:26<02:12, 2.33MB/s] 15%|█▍        | 51.2M/346M [00:26<02:01, 2.55MB/s] 15%|█▍        | 51.5M/346M [00:26<02:03, 2.51MB/s] 15%|█▍        | 51.8M/346M [00:26<02:03, 2.50MB/s] 15%|█▌        | 52.0M/346M [00:26<02:09, 2.38MB/s] 15%|█▌        | 52.3M/346M [00:26<02:22, 2.17MB/s] 15%|█▌        | 52.5M/346M [00:27<02:26, 2.10MB/s] 15%|█▌        | 52.7M/346M [00:27<02:34, 1.99MB/s] 15%|█▌        | 52.9M/346M [00:27<02:40, 1.91MB/s] 15%|█▌        | 53.1M/346M [00:27<02:26, 2.10MB/s] 15%|█▌        | 53.4M/346M [00:27<02:28, 2.06MB/s] 16%|█▌        | 53.7M/346M [00:27<02:02, 2.49MB/s] 16%|█▌        | 54.0M/346M [00:27<02:03, 2.48MB/s] 16%|█▌        | 54.2M/346M [00:28<03:10, 1.61MB/s] 16%|█▌        | 54.5M/346M [00:28<02:44, 1.86MB/s] 16%|█▌        | 54.7M/346M [00:28<02:38, 1.92MB/s] 16%|█▌        | 55.0M/346M [00:28<03:00, 1.69MB/s] 16%|█▌        | 55.3M/346M [00:28<02:52, 1.77MB/s] 16%|█▌        | 55.6M/346M [00:28<02:42, 1.87MB/s] 16%|█▌        | 55.8M/346M [00:28<02:42, 1.88MB/s] 16%|█▌        | 56.0M/346M [00:29<02:49, 1.79MB/s] 16%|█▌        | 56.2M/346M [00:29<03:01, 1.68MB/s] 16%|█▋        | 56.3M/346M [00:29<03:16, 1.54MB/s] 16%|█▋        | 56.5M/346M [00:29<03:09, 1.60MB/s] 16%|█▋        | 56.7M/346M [00:29<03:02, 1.66MB/s] 16%|█▋        | 56.9M/346M [00:29<02:58, 1.70MB/s] 17%|█▋        | 57.1M/346M [00:29<03:00, 1.67MB/s] 17%|█▋        | 57.3M/346M [00:29<03:04, 1.64MB/s] 17%|█▋        | 57.4M/346M [00:30<04:24, 1.14MB/s] 17%|█▋        | 57.8M/346M [00:30<03:12, 1.57MB/s] 17%|█▋        | 58.0M/346M [00:30<03:31, 1.42MB/s] 17%|█▋        | 58.1M/346M [00:30<03:52, 1.30MB/s] 17%|█▋        | 58.2M/346M [00:30<03:56, 1.27MB/s] 17%|█▋        | 58.4M/346M [00:30<04:13, 1.19MB/s] 17%|█▋        | 58.5M/346M [00:31<04:21, 1.15MB/s] 17%|█▋        | 58.7M/346M [00:31<04:05, 1.22MB/s] 17%|█▋        | 58.8M/346M [00:31<03:49, 1.31MB/s] 17%|█▋        | 59.0M/346M [00:31<03:30, 1.43MB/s] 17%|█▋        | 59.3M/346M [00:31<03:29, 1.43MB/s] 17%|█▋        | 59.4M/346M [00:31<03:38, 1.37MB/s] 17%|█▋        | 59.6M/346M [00:31<03:49, 1.31MB/s] 17%|█▋        | 59.7M/346M [00:31<03:47, 1.32MB/s] 17%|█▋        | 59.9M/346M [00:32<03:46, 1.33MB/s] 17%|█▋        | 60.0M/346M [00:32<03:54, 1.28MB/s] 17%|█▋        | 60.2M/346M [00:32<03:49, 1.31MB/s] 17%|█▋        | 60.4M/346M [00:32<03:43, 1.34MB/s] 18%|█▊        | 60.5M/346M [00:32<03:32, 1.41MB/s] 18%|█▊        | 60.9M/346M [00:32<03:08, 1.58MB/s] 18%|█▊        | 61.0M/346M [00:32<03:14, 1.53MB/s] 18%|█▊        | 61.2M/346M [00:32<03:23, 1.47MB/s] 18%|█▊        | 61.4M/346M [00:33<03:34, 1.39MB/s] 18%|█▊        | 61.7M/346M [00:33<03:13, 1.54MB/s] 18%|█▊        | 62.0M/346M [00:33<03:07, 1.58MB/s] 18%|█▊        | 62.4M/346M [00:33<03:02, 1.63MB/s] 18%|█▊        | 62.5M/346M [00:33<03:05, 1.60MB/s] 18%|█▊        | 62.7M/346M [00:33<03:11, 1.55MB/s] 18%|█▊        | 62.9M/346M [00:34<03:11, 1.55MB/s] 18%|█▊        | 63.0M/346M [00:34<03:14, 1.53MB/s] 18%|█▊        | 63.2M/346M [00:34<03:12, 1.54MB/s] 18%|█▊        | 63.4M/346M [00:34<03:09, 1.57MB/s] 18%|█▊        | 63.5M/346M [00:34<03:11, 1.55MB/s] 18%|█▊        | 63.7M/346M [00:34<03:15, 1.51MB/s] 18%|█▊        | 63.9M/346M [00:34<03:15, 1.51MB/s] 19%|█▊        | 64.0M/346M [00:34<03:26, 1.43MB/s] 19%|█▊        | 64.1M/346M [00:35<03:47, 1.30MB/s] 19%|█▊        | 64.3M/346M [00:35<04:06, 1.20MB/s] 19%|█▊        | 64.4M/346M [00:35<04:36, 1.07MB/s] 19%|█▊        | 64.5M/346M [00:35<04:55, 997kB/s]  19%|█▊        | 64.6M/346M [00:35<04:44, 1.04MB/s] 19%|█▊        | 64.8M/346M [00:35<04:28, 1.10MB/s] 19%|█▉        | 64.9M/346M [00:35<04:26, 1.11MB/s] 19%|█▉        | 65.1M/346M [00:35<04:28, 1.10MB/s] 19%|█▉        | 65.2M/346M [00:36<04:47, 1.02MB/s] 19%|█▉        | 65.3M/346M [00:36<05:51, 837kB/s]  19%|█▉        | 65.5M/346M [00:36<05:37, 872kB/s] 19%|█▉        | 65.6M/346M [00:36<06:09, 794kB/s] 19%|█▉        | 65.7M/346M [00:36<06:48, 718kB/s] 19%|█▉        | 65.8M/346M [00:37<07:19, 668kB/s] 19%|█▉        | 65.9M/346M [00:37<07:49, 625kB/s] 19%|█▉        | 66.0M/346M [00:37<08:08, 600kB/s] 19%|█▉        | 66.1M/346M [00:37<07:45, 631kB/s] 19%|█▉        | 66.3M/346M [00:37<07:50, 623kB/s] 19%|█▉        | 66.4M/346M [00:38<07:41, 635kB/s] 19%|█▉        | 66.5M/346M [00:38<07:40, 636kB/s] 19%|█▉        | 66.6M/346M [00:38<07:40, 636kB/s] 19%|█▉        | 66.8M/346M [00:38<07:39, 637kB/s] 19%|█▉        | 66.9M/346M [00:38<07:27, 654kB/s] 19%|█▉        | 67.0M/346M [00:39<07:40, 634kB/s] 19%|█▉        | 67.1M/346M [00:39<07:40, 634kB/s] 19%|█▉        | 67.3M/346M [00:39<06:54, 705kB/s] 19%|█▉        | 67.4M/346M [00:39<06:49, 712kB/s] 20%|█▉        | 67.5M/346M [00:39<06:44, 722kB/s] 20%|█▉        | 67.7M/346M [00:40<06:20, 767kB/s] 20%|█▉        | 67.8M/346M [00:40<05:55, 821kB/s] 20%|█▉        | 67.9M/346M [00:40<05:19, 913kB/s] 20%|█▉        | 68.1M/346M [00:40<04:42, 1.03MB/s] 20%|█▉        | 68.3M/346M [00:40<04:00, 1.21MB/s] 20%|█▉        | 68.5M/346M [00:40<03:32, 1.37MB/s] 20%|█▉        | 68.7M/346M [00:40<03:24, 1.42MB/s] 20%|█▉        | 69.0M/346M [00:40<02:56, 1.65MB/s] 20%|██        | 69.2M/346M [00:41<02:40, 1.81MB/s] 20%|██        | 69.4M/346M [00:41<02:41, 1.80MB/s] 20%|██        | 69.6M/346M [00:41<02:50, 1.70MB/s] 20%|██        | 69.7M/346M [00:41<03:17, 1.46MB/s] 20%|██        | 69.9M/346M [00:41<03:16, 1.47MB/s] 20%|██        | 70.0M/346M [00:41<03:30, 1.37MB/s] 20%|██        | 70.2M/346M [00:41<03:40, 1.31MB/s] 20%|██        | 70.3M/346M [00:41<03:44, 1.29MB/s] 20%|██        | 70.4M/346M [00:42<04:09, 1.16MB/s] 20%|██        | 70.6M/346M [00:42<03:46, 1.27MB/s] 20%|██        | 70.8M/346M [00:42<03:44, 1.28MB/s] 20%|██        | 70.9M/346M [00:42<03:46, 1.27MB/s] 21%|██        | 71.0M/346M [00:42<03:50, 1.25MB/s] 21%|██        | 71.1M/346M [00:42<03:53, 1.23MB/s] 21%|██        | 71.3M/346M [00:42<04:07, 1.16MB/s] 21%|██        | 71.4M/346M [00:42<04:22, 1.10MB/s] 21%|██        | 71.5M/346M [00:43<04:36, 1.04MB/s] 21%|██        | 71.6M/346M [00:43<04:52, 982kB/s]  21%|██        | 71.7M/346M [00:43<04:45, 1.01MB/s] 21%|██        | 71.8M/346M [00:43<04:36, 1.04MB/s] 21%|██        | 72.0M/346M [00:43<04:25, 1.08MB/s] 21%|██        | 72.1M/346M [00:43<04:21, 1.10MB/s] 21%|██        | 72.2M/346M [00:43<04:16, 1.12MB/s] 21%|██        | 72.4M/346M [00:43<04:26, 1.08MB/s] 21%|██        | 72.5M/346M [00:44<04:31, 1.06MB/s] 21%|██        | 72.6M/346M [00:44<04:20, 1.10MB/s] 21%|██        | 72.8M/346M [00:44<04:03, 1.17MB/s] 21%|██        | 72.9M/346M [00:44<03:33, 1.34MB/s] 21%|██        | 73.1M/346M [00:44<03:52, 1.23MB/s] 21%|██        | 73.3M/346M [00:44<03:53, 1.23MB/s] 21%|██        | 73.4M/346M [00:44<03:57, 1.20MB/s] 21%|██▏       | 73.5M/346M [00:44<04:05, 1.17MB/s] 21%|██▏       | 73.7M/346M [00:45<04:26, 1.07MB/s] 21%|██▏       | 73.8M/346M [00:45<04:24, 1.08MB/s] 21%|██▏       | 73.9M/346M [00:45<04:35, 1.03MB/s] 21%|██▏       | 74.0M/346M [00:45<04:36, 1.03MB/s] 21%|██▏       | 74.2M/346M [00:45<04:37, 1.03MB/s] 21%|██▏       | 74.3M/346M [00:45<04:23, 1.08MB/s] 22%|██▏       | 74.4M/346M [00:45<04:24, 1.08MB/s] 22%|██▏       | 74.6M/346M [00:45<04:24, 1.08MB/s] 22%|██▏       | 74.7M/346M [00:46<04:06, 1.15MB/s] 22%|██▏       | 74.8M/346M [00:46<03:59, 1.18MB/s] 22%|██▏       | 75.0M/346M [00:46<03:54, 1.21MB/s] 22%|██▏       | 75.1M/346M [00:46<04:00, 1.18MB/s] 22%|██▏       | 75.2M/346M [00:46<04:13, 1.12MB/s] 22%|██▏       | 75.3M/346M [00:46<04:25, 1.07MB/s] 22%|██▏       | 75.5M/346M [00:46<04:23, 1.07MB/s] 22%|██▏       | 75.6M/346M [00:46<04:12, 1.12MB/s] 22%|██▏       | 75.8M/346M [00:47<04:14, 1.11MB/s] 22%|██▏       | 75.9M/346M [00:47<04:25, 1.07MB/s] 22%|██▏       | 76.0M/346M [00:47<04:39, 1.01MB/s] 22%|██▏       | 76.1M/346M [00:47<04:34, 1.03MB/s] 22%|██▏       | 76.3M/346M [00:47<04:40, 1.01MB/s] 22%|██▏       | 76.4M/346M [00:47<04:57, 950kB/s]  22%|██▏       | 76.5M/346M [00:47<04:53, 963kB/s] 22%|██▏       | 76.7M/346M [00:48<04:44, 992kB/s] 22%|██▏       | 76.8M/346M [00:48<04:26, 1.06MB/s] 22%|██▏       | 77.0M/346M [00:48<04:10, 1.13MB/s] 22%|██▏       | 77.1M/346M [00:48<03:56, 1.19MB/s] 22%|██▏       | 77.3M/346M [00:48<03:35, 1.31MB/s] 22%|██▏       | 77.4M/346M [00:48<03:41, 1.27MB/s] 22%|██▏       | 77.5M/346M [00:48<03:51, 1.21MB/s] 22%|██▏       | 77.7M/346M [00:48<03:55, 1.19MB/s] 22%|██▏       | 77.8M/346M [00:48<04:18, 1.09MB/s] 23%|██▎       | 77.9M/346M [00:49<04:32, 1.03MB/s] 23%|██▎       | 78.1M/346M [00:49<04:30, 1.04MB/s] 23%|██▎       | 78.3M/346M [00:49<04:12, 1.11MB/s] 23%|██▎       | 78.4M/346M [00:49<04:08, 1.13MB/s] 23%|██▎       | 78.6M/346M [00:49<03:51, 1.21MB/s] 23%|██▎       | 78.8M/346M [00:49<03:40, 1.27MB/s] 23%|██▎       | 79.0M/346M [00:49<03:01, 1.54MB/s] 23%|██▎       | 79.2M/346M [00:50<02:46, 1.67MB/s] 23%|██▎       | 79.5M/346M [00:50<02:50, 1.64MB/s] 23%|██▎       | 79.7M/346M [00:50<02:50, 1.63MB/s] 23%|██▎       | 79.8M/346M [00:50<02:51, 1.62MB/s] 23%|██▎       | 80.0M/346M [00:50<02:54, 1.59MB/s] 23%|██▎       | 80.3M/346M [00:50<02:55, 1.59MB/s] 23%|██▎       | 80.5M/346M [00:50<02:50, 1.63MB/s] 23%|██▎       | 80.7M/346M [00:51<02:50, 1.63MB/s] 23%|██▎       | 80.9M/346M [00:51<02:47, 1.66MB/s] 23%|██▎       | 81.1M/346M [00:51<02:48, 1.64MB/s] 24%|██▎       | 81.4M/346M [00:51<02:45, 1.68MB/s] 24%|██▎       | 81.6M/346M [00:51<02:45, 1.68MB/s] 24%|██▎       | 81.9M/346M [00:51<02:43, 1.69MB/s] 24%|██▍       | 82.1M/346M [00:51<02:44, 1.68MB/s] 24%|██▍       | 82.4M/346M [00:52<02:35, 1.77MB/s] 24%|██▍       | 82.7M/346M [00:52<02:19, 1.97MB/s] 24%|██▍       | 83.0M/346M [00:52<02:08, 2.14MB/s] 24%|██▍       | 83.3M/346M [00:52<02:00, 2.28MB/s] 24%|██▍       | 83.5M/346M [00:52<02:28, 1.85MB/s] 24%|██▍       | 84.0M/346M [00:52<01:50, 2.48MB/s] 24%|██▍       | 84.3M/346M [00:52<02:07, 2.15MB/s] 24%|██▍       | 84.5M/346M [00:53<02:10, 2.09MB/s] 24%|██▍       | 84.7M/346M [00:53<02:13, 2.06MB/s] 25%|██▍       | 85.0M/346M [00:53<02:07, 2.14MB/s] 25%|██▍       | 85.2M/346M [00:53<02:04, 2.19MB/s] 25%|██▍       | 85.5M/346M [00:53<02:01, 2.25MB/s] 25%|██▍       | 85.7M/346M [00:53<01:58, 2.31MB/s] 25%|██▍       | 86.0M/346M [00:53<02:03, 2.21MB/s] 25%|██▍       | 86.3M/346M [00:53<01:55, 2.35MB/s] 25%|██▌       | 86.6M/346M [00:53<02:03, 2.19MB/s] 25%|██▌       | 86.8M/346M [00:54<02:06, 2.15MB/s] 25%|██▌       | 87.0M/346M [00:54<02:11, 2.07MB/s] 25%|██▌       | 87.2M/346M [00:54<02:20, 1.93MB/s] 25%|██▌       | 87.3M/346M [00:54<02:31, 1.79MB/s] 25%|██▌       | 87.5M/346M [00:54<02:34, 1.75MB/s] 25%|██▌       | 87.7M/346M [00:54<02:29, 1.81MB/s] 25%|██▌       | 87.9M/346M [00:54<02:26, 1.85MB/s] 25%|██▌       | 88.1M/346M [00:54<02:20, 1.92MB/s] 26%|██▌       | 88.5M/346M [00:55<01:59, 2.27MB/s] 26%|██▌       | 88.7M/346M [00:55<02:18, 1.94MB/s] 26%|██▌       | 89.0M/346M [00:55<02:10, 2.06MB/s] 26%|██▌       | 89.2M/346M [00:55<02:20, 1.92MB/s] 26%|██▌       | 89.4M/346M [00:55<02:28, 1.81MB/s] 26%|██▌       | 89.6M/346M [00:55<02:31, 1.77MB/s] 26%|██▌       | 89.8M/346M [00:55<02:32, 1.76MB/s] 26%|██▌       | 90.0M/346M [00:55<02:49, 1.58MB/s] 26%|██▌       | 90.1M/346M [00:56<02:53, 1.55MB/s] 26%|██▌       | 90.3M/346M [00:56<02:51, 1.56MB/s] 26%|██▌       | 90.5M/346M [00:56<02:58, 1.50MB/s] 26%|██▌       | 90.6M/346M [00:56<02:54, 1.53MB/s] 26%|██▌       | 90.8M/346M [00:56<02:59, 1.49MB/s] 26%|██▋       | 91.0M/346M [00:56<03:10, 1.40MB/s] 26%|██▋       | 91.1M/346M [00:56<03:22, 1.32MB/s] 26%|██▋       | 91.3M/346M [00:57<03:31, 1.26MB/s] 26%|██▋       | 91.5M/346M [00:57<03:24, 1.30MB/s] 27%|██▋       | 91.7M/346M [00:57<03:24, 1.30MB/s] 27%|██▋       | 91.8M/346M [00:57<03:23, 1.31MB/s] 27%|██▋       | 92.0M/346M [00:57<03:29, 1.27MB/s] 27%|██▋       | 92.2M/346M [00:57<03:34, 1.24MB/s] 27%|██▋       | 92.4M/346M [00:57<03:46, 1.17MB/s] 27%|██▋       | 92.5M/346M [00:58<03:30, 1.26MB/s] 27%|██▋       | 92.7M/346M [00:58<03:35, 1.23MB/s] 27%|██▋       | 92.8M/346M [00:58<04:00, 1.11MB/s] 27%|██▋       | 92.9M/346M [00:58<03:57, 1.12MB/s] 27%|██▋       | 93.1M/346M [00:58<03:52, 1.14MB/s] 27%|██▋       | 93.3M/346M [00:58<03:43, 1.19MB/s] 27%|██▋       | 93.5M/346M [00:58<03:32, 1.25MB/s] 27%|██▋       | 93.6M/346M [00:58<03:33, 1.24MB/s] 27%|██▋       | 93.7M/346M [00:59<04:02, 1.09MB/s] 27%|██▋       | 93.8M/346M [00:59<04:18, 1.02MB/s] 27%|██▋       | 94.0M/346M [00:59<04:20, 1.01MB/s] 27%|██▋       | 94.2M/346M [00:59<04:08, 1.06MB/s] 27%|██▋       | 94.3M/346M [00:59<04:13, 1.04MB/s] 27%|██▋       | 94.4M/346M [00:59<04:21, 1.01MB/s] 27%|██▋       | 94.5M/346M [00:59<04:23, 999kB/s]  27%|██▋       | 94.6M/346M [01:00<04:43, 929kB/s] 27%|██▋       | 94.7M/346M [01:00<04:19, 1.01MB/s] 27%|██▋       | 94.9M/346M [01:00<04:11, 1.05MB/s] 27%|██▋       | 95.1M/346M [01:00<04:21, 1.01MB/s] 28%|██▊       | 95.3M/346M [01:00<04:18, 1.02MB/s] 28%|██▊       | 95.5M/346M [01:00<03:59, 1.10MB/s] 28%|██▊       | 95.6M/346M [01:01<03:33, 1.23MB/s] 28%|██▊       | 95.8M/346M [01:01<03:04, 1.42MB/s] 28%|██▊       | 96.0M/346M [01:01<02:55, 1.50MB/s] 28%|██▊       | 96.4M/346M [01:01<02:26, 1.78MB/s] 28%|██▊       | 96.6M/346M [01:01<02:19, 1.87MB/s] 28%|██▊       | 96.9M/346M [01:01<02:00, 2.17MB/s] 28%|██▊       | 97.1M/346M [01:01<02:01, 2.15MB/s] 28%|██▊       | 97.4M/346M [01:01<01:53, 2.29MB/s] 28%|██▊       | 97.7M/346M [01:01<01:46, 2.43MB/s] 28%|██▊       | 98.0M/346M [01:02<01:42, 2.54MB/s] 28%|██▊       | 98.2M/346M [01:02<01:57, 2.21MB/s] 29%|██▊       | 98.7M/346M [01:02<01:34, 2.75MB/s] 29%|██▊       | 99.0M/346M [01:02<01:35, 2.72MB/s] 29%|██▊       | 99.3M/346M [01:02<01:39, 2.61MB/s] 29%|██▉       | 99.5M/346M [01:02<01:43, 2.50MB/s] 29%|██▉       | 99.8M/346M [01:02<01:37, 2.65MB/s] 29%|██▉       | 100M/346M [01:02<01:33, 2.75MB/s]  29%|██▉       | 100M/346M [01:03<01:30, 2.84MB/s] 29%|██▉       | 101M/346M [01:03<01:33, 2.75MB/s] 29%|██▉       | 101M/346M [01:03<01:42, 2.51MB/s] 29%|██▉       | 101M/346M [01:03<01:42, 2.51MB/s] 29%|██▉       | 102M/346M [01:03<01:39, 2.57MB/s] 29%|██▉       | 102M/346M [01:03<01:41, 2.52MB/s] 30%|██▉       | 102M/346M [01:03<01:42, 2.49MB/s] 30%|██▉       | 102M/346M [01:03<01:40, 2.55MB/s] 30%|██▉       | 103M/346M [01:03<01:39, 2.56MB/s] 30%|██▉       | 103M/346M [01:04<01:40, 2.54MB/s] 30%|██▉       | 103M/346M [01:04<01:34, 2.69MB/s] 30%|██▉       | 103M/346M [01:04<01:36, 2.63MB/s] 30%|██▉       | 104M/346M [01:04<01:34, 2.69MB/s] 30%|███       | 104M/346M [01:04<01:20, 3.15MB/s] 30%|███       | 104M/346M [01:04<01:23, 3.02MB/s] 30%|███       | 105M/346M [01:04<01:17, 3.27MB/s] 30%|███       | 105M/346M [01:04<01:06, 3.77MB/s] 31%|███       | 106M/346M [01:04<01:04, 3.92MB/s] 31%|███       | 106M/346M [01:05<01:08, 3.64MB/s] 31%|███       | 107M/346M [01:05<01:10, 3.58MB/s] 31%|███       | 107M/346M [01:05<01:13, 3.42MB/s] 31%|███       | 108M/346M [01:05<01:14, 3.37MB/s] 31%|███       | 108M/346M [01:05<01:16, 3.26MB/s] 31%|███▏      | 108M/346M [01:05<01:13, 3.40MB/s] 31%|███▏      | 109M/346M [01:05<01:21, 3.04MB/s] 31%|███▏      | 109M/346M [01:05<01:31, 2.72MB/s] 32%|███▏      | 109M/346M [01:06<01:36, 2.58MB/s] 32%|███▏      | 109M/346M [01:06<01:36, 2.58MB/s] 32%|███▏      | 110M/346M [01:06<01:32, 2.66MB/s] 32%|███▏      | 110M/346M [01:06<01:29, 2.76MB/s] 32%|███▏      | 111M/346M [01:06<01:26, 2.86MB/s] 32%|███▏      | 111M/346M [01:06<01:19, 3.11MB/s] 32%|███▏      | 111M/346M [01:06<01:15, 3.26MB/s] 32%|███▏      | 112M/346M [01:06<01:04, 3.79MB/s] 33%|███▎      | 113M/346M [01:07<00:52, 4.63MB/s] 33%|███▎      | 113M/346M [01:07<00:46, 5.21MB/s] 33%|███▎      | 114M/346M [01:07<00:46, 5.25MB/s] 33%|███▎      | 115M/346M [01:07<00:41, 5.78MB/s] 33%|███▎      | 115M/346M [01:07<00:41, 5.76MB/s] 34%|███▎      | 116M/346M [01:07<00:39, 6.09MB/s] 34%|███▎      | 117M/346M [01:07<00:38, 6.23MB/s] 34%|███▍      | 117M/346M [01:07<00:39, 6.09MB/s] 34%|███▍      | 118M/346M [01:07<00:40, 5.84MB/s] 34%|███▍      | 118M/346M [01:08<00:43, 5.52MB/s] 34%|███▍      | 119M/346M [01:08<00:42, 5.63MB/s] 35%|███▍      | 120M/346M [01:08<00:39, 5.97MB/s] 35%|███▍      | 120M/346M [01:08<00:41, 5.74MB/s] 35%|███▍      | 121M/346M [01:08<00:39, 5.92MB/s] 35%|███▌      | 121M/346M [01:08<00:42, 5.52MB/s] 35%|███▌      | 122M/346M [01:08<00:42, 5.54MB/s] 35%|███▌      | 122M/346M [01:08<00:47, 4.94MB/s] 36%|███▌      | 123M/346M [01:09<01:01, 3.83MB/s] 36%|███▌      | 123M/346M [01:09<01:10, 3.31MB/s] 36%|███▌      | 124M/346M [01:09<01:21, 2.85MB/s] 36%|███▌      | 124M/346M [01:09<01:29, 2.59MB/s] 36%|███▌      | 124M/346M [01:09<01:39, 2.34MB/s] 36%|███▌      | 125M/346M [01:09<01:44, 2.22MB/s] 36%|███▌      | 125M/346M [01:10<01:45, 2.20MB/s] 36%|███▌      | 125M/346M [01:10<01:45, 2.20MB/s] 36%|███▌      | 125M/346M [01:10<01:42, 2.25MB/s] 36%|███▋      | 125M/346M [01:10<01:42, 2.26MB/s] 36%|███▋      | 126M/346M [01:10<01:36, 2.40MB/s] 36%|███▋      | 126M/346M [01:10<01:35, 2.41MB/s] 37%|███▋      | 126M/346M [01:10<01:36, 2.38MB/s] 37%|███▋      | 126M/346M [01:10<01:36, 2.39MB/s] 37%|███▋      | 127M/346M [01:10<01:40, 2.29MB/s] 37%|███▋      | 127M/346M [01:11<01:40, 2.28MB/s] 37%|███▋      | 127M/346M [01:11<01:34, 2.42MB/s] 37%|███▋      | 128M/346M [01:11<01:31, 2.49MB/s] 37%|███▋      | 128M/346M [01:11<01:30, 2.52MB/s] 37%|███▋      | 128M/346M [01:11<01:25, 2.67MB/s] 37%|███▋      | 129M/346M [01:11<01:13, 3.08MB/s] 37%|███▋      | 129M/346M [01:11<01:12, 3.15MB/s] 37%|███▋      | 129M/346M [01:11<01:08, 3.29MB/s] 37%|███▋      | 130M/346M [01:11<01:12, 3.14MB/s] 38%|███▊      | 130M/346M [01:12<01:11, 3.17MB/s] 38%|███▊      | 130M/346M [01:12<01:13, 3.07MB/s] 38%|███▊      | 131M/346M [01:12<01:13, 3.06MB/s] 38%|███▊      | 131M/346M [01:12<01:20, 2.79MB/s] 38%|███▊      | 131M/346M [01:12<01:21, 2.77MB/s] 38%|███▊      | 131M/346M [01:12<01:19, 2.83MB/s] 38%|███▊      | 132M/346M [01:12<01:20, 2.81MB/s] 38%|███▊      | 132M/346M [01:12<01:20, 2.80MB/s] 38%|███▊      | 132M/346M [01:12<01:19, 2.83MB/s] 38%|███▊      | 133M/346M [01:13<01:27, 2.54MB/s] 38%|███▊      | 133M/346M [01:13<01:33, 2.40MB/s] 38%|███▊      | 133M/346M [01:13<01:37, 2.28MB/s] 39%|███▊      | 133M/346M [01:13<01:43, 2.16MB/s] 39%|███▊      | 133M/346M [01:13<01:47, 2.07MB/s] 39%|███▊      | 134M/346M [01:13<01:50, 2.02MB/s] 39%|███▊      | 134M/346M [01:13<01:41, 2.18MB/s] 39%|███▉      | 134M/346M [01:13<01:22, 2.70MB/s] 39%|███▉      | 135M/346M [01:13<01:11, 3.09MB/s] 39%|███▉      | 135M/346M [01:14<01:05, 3.39MB/s] 39%|███▉      | 136M/346M [01:14<01:00, 3.63MB/s] 39%|███▉      | 136M/346M [01:14<01:02, 3.53MB/s] 39%|███▉      | 137M/346M [01:14<01:02, 3.51MB/s] 40%|███▉      | 137M/346M [01:14<01:04, 3.42MB/s] 40%|███▉      | 137M/346M [01:14<01:04, 3.40MB/s] 40%|███▉      | 138M/346M [01:14<01:19, 2.76MB/s] 40%|███▉      | 138M/346M [01:15<01:17, 2.80MB/s] 40%|████      | 138M/346M [01:15<01:16, 2.83MB/s] 40%|████      | 139M/346M [01:15<01:16, 2.82MB/s] 40%|████      | 139M/346M [01:15<01:14, 2.91MB/s] 40%|████      | 139M/346M [01:15<01:21, 2.67MB/s] 40%|████      | 140M/346M [01:15<01:21, 2.65MB/s] 40%|████      | 140M/346M [01:15<01:31, 2.37MB/s] 40%|████      | 140M/346M [01:15<01:33, 2.32MB/s] 41%|████      | 140M/346M [01:16<01:32, 2.33MB/s] 41%|████      | 141M/346M [01:16<01:41, 2.13MB/s] 41%|████      | 141M/346M [01:16<01:29, 2.41MB/s] 41%|████      | 141M/346M [01:16<01:36, 2.22MB/s] 41%|████      | 141M/346M [01:16<01:36, 2.23MB/s] 41%|████      | 142M/346M [01:16<01:27, 2.44MB/s] 41%|████      | 142M/346M [01:16<01:35, 2.25MB/s] 41%|████      | 142M/346M [01:16<01:35, 2.25MB/s] 41%|████      | 142M/346M [01:16<01:21, 2.61MB/s] 41%|████▏     | 143M/346M [01:17<01:24, 2.53MB/s] 41%|████▏     | 143M/346M [01:17<01:12, 2.94MB/s] 41%|████▏     | 143M/346M [01:17<01:15, 2.81MB/s] 42%|████▏     | 144M/346M [01:17<01:15, 2.83MB/s] 42%|████▏     | 144M/346M [01:17<01:19, 2.65MB/s] 42%|████▏     | 144M/346M [01:17<01:35, 2.22MB/s] 42%|████▏     | 144M/346M [01:17<01:39, 2.13MB/s] 42%|████▏     | 145M/346M [01:17<01:41, 2.09MB/s] 42%|████▏     | 145M/346M [01:18<01:39, 2.13MB/s] 42%|████▏     | 145M/346M [01:18<01:39, 2.13MB/s] 42%|████▏     | 145M/346M [01:18<01:40, 2.09MB/s] 42%|████▏     | 146M/346M [01:18<01:43, 2.04MB/s] 42%|████▏     | 146M/346M [01:18<01:46, 1.97MB/s] 42%|████▏     | 146M/346M [01:18<01:50, 1.89MB/s] 42%|████▏     | 146M/346M [01:18<01:58, 1.76MB/s] 42%|████▏     | 146M/346M [01:18<01:53, 1.84MB/s] 42%|████▏     | 147M/346M [01:19<01:56, 1.79MB/s] 42%|████▏     | 147M/346M [01:19<01:55, 1.80MB/s] 43%|████▎     | 147M/346M [01:19<01:51, 1.87MB/s] 43%|████▎     | 147M/346M [01:19<01:54, 1.82MB/s] 43%|████▎     | 147M/346M [01:19<01:54, 1.81MB/s] 43%|████▎     | 148M/346M [01:19<01:59, 1.74MB/s] 43%|████▎     | 148M/346M [01:19<01:54, 1.82MB/s] 43%|████▎     | 148M/346M [01:19<01:32, 2.24MB/s] 43%|████▎     | 148M/346M [01:19<01:36, 2.14MB/s] 43%|████▎     | 149M/346M [01:20<01:41, 2.04MB/s] 43%|████▎     | 149M/346M [01:20<01:49, 1.89MB/s] 43%|████▎     | 149M/346M [01:20<01:56, 1.77MB/s] 43%|████▎     | 149M/346M [01:20<02:01, 1.69MB/s] 43%|████▎     | 149M/346M [01:20<02:00, 1.70MB/s] 43%|████▎     | 150M/346M [01:20<01:59, 1.73MB/s] 43%|████▎     | 150M/346M [01:20<02:00, 1.70MB/s] 43%|████▎     | 150M/346M [01:21<02:06, 1.62MB/s] 43%|████▎     | 150M/346M [01:21<02:01, 1.69MB/s] 43%|████▎     | 150M/346M [01:21<01:57, 1.74MB/s] 44%|████▎     | 151M/346M [01:21<01:49, 1.87MB/s] 44%|████▎     | 151M/346M [01:21<01:47, 1.91MB/s] 44%|████▎     | 151M/346M [01:21<01:44, 1.96MB/s] 44%|████▎     | 151M/346M [01:21<01:42, 1.98MB/s] 44%|████▍     | 152M/346M [01:21<01:40, 2.03MB/s] 44%|████▍     | 152M/346M [01:21<01:48, 1.88MB/s] 44%|████▍     | 152M/346M [01:22<01:50, 1.84MB/s] 44%|████▍     | 152M/346M [01:22<01:57, 1.72MB/s] 44%|████▍     | 152M/346M [01:22<01:59, 1.70MB/s] 44%|████▍     | 153M/346M [01:22<01:58, 1.71MB/s] 44%|████▍     | 153M/346M [01:22<01:54, 1.77MB/s] 44%|████▍     | 153M/346M [01:22<01:52, 1.80MB/s] 44%|████▍     | 153M/346M [01:22<01:48, 1.86MB/s] 44%|████▍     | 154M/346M [01:23<01:46, 1.90MB/s] 44%|████▍     | 154M/346M [01:23<01:47, 1.87MB/s] 45%|████▍     | 154M/346M [01:23<01:53, 1.77MB/s] 45%|████▍     | 154M/346M [01:23<01:56, 1.72MB/s] 45%|████▍     | 155M/346M [01:23<01:58, 1.69MB/s] 45%|████▍     | 155M/346M [01:23<01:58, 1.69MB/s] 45%|████▍     | 155M/346M [01:23<01:50, 1.80MB/s] 45%|████▍     | 155M/346M [01:24<01:40, 2.00MB/s] 45%|████▍     | 156M/346M [01:24<01:44, 1.90MB/s] 45%|████▌     | 156M/346M [01:24<01:12, 2.74MB/s] 45%|████▌     | 157M/346M [01:24<01:13, 2.70MB/s] 45%|████▌     | 157M/346M [01:24<01:06, 2.96MB/s] 45%|████▌     | 157M/346M [01:24<01:13, 2.68MB/s] 46%|████▌     | 158M/346M [01:24<01:17, 2.54MB/s] 46%|████▌     | 158M/346M [01:25<01:16, 2.58MB/s] 46%|████▌     | 158M/346M [01:25<01:08, 2.87MB/s] 46%|████▌     | 159M/346M [01:25<01:01, 3.21MB/s] 46%|████▌     | 159M/346M [01:25<01:00, 3.22MB/s] 46%|████▌     | 160M/346M [01:25<00:55, 3.52MB/s] 46%|████▋     | 160M/346M [01:25<00:55, 3.53MB/s] 46%|████▋     | 160M/346M [01:25<00:59, 3.26MB/s] 46%|████▋     | 161M/346M [01:25<00:58, 3.29MB/s] 47%|████▋     | 161M/346M [01:26<00:57, 3.39MB/s] 47%|████▋     | 162M/346M [01:26<01:00, 3.18MB/s] 47%|████▋     | 162M/346M [01:26<00:54, 3.52MB/s] 47%|████▋     | 162M/346M [01:26<00:55, 3.48MB/s] 47%|████▋     | 163M/346M [01:26<00:59, 3.20MB/s] 47%|████▋     | 163M/346M [01:26<01:00, 3.14MB/s] 47%|████▋     | 164M/346M [01:26<01:00, 3.15MB/s] 47%|████▋     | 164M/346M [01:26<01:05, 2.94MB/s] 47%|████▋     | 164M/346M [01:27<01:14, 2.56MB/s] 48%|████▊     | 164M/346M [01:27<01:16, 2.49MB/s] 48%|████▊     | 165M/346M [01:27<01:14, 2.56MB/s] 48%|████▊     | 165M/346M [01:27<01:22, 2.29MB/s] 48%|████▊     | 165M/346M [01:27<01:25, 2.21MB/s] 48%|████▊     | 166M/346M [01:27<01:23, 2.27MB/s] 48%|████▊     | 166M/346M [01:27<01:15, 2.49MB/s] 48%|████▊     | 166M/346M [01:28<01:15, 2.49MB/s] 48%|████▊     | 167M/346M [01:28<01:12, 2.59MB/s] 48%|████▊     | 167M/346M [01:28<01:06, 2.80MB/s] 48%|████▊     | 168M/346M [01:28<01:05, 2.85MB/s] 49%|████▊     | 168M/346M [01:28<01:03, 2.92MB/s] 49%|████▊     | 168M/346M [01:28<00:57, 3.23MB/s] 49%|████▉     | 169M/346M [01:28<01:04, 2.88MB/s] 49%|████▉     | 169M/346M [01:29<01:11, 2.59MB/s] 49%|████▉     | 169M/346M [01:29<01:05, 2.84MB/s] 49%|████▉     | 170M/346M [01:29<01:08, 2.68MB/s] 49%|████▉     | 170M/346M [01:29<01:04, 2.86MB/s] 49%|████▉     | 170M/346M [01:29<01:01, 2.99MB/s] 49%|████▉     | 171M/346M [01:29<00:59, 3.11MB/s] 50%|████▉     | 171M/346M [01:29<00:57, 3.17MB/s] 50%|████▉     | 172M/346M [01:29<00:58, 3.11MB/s] 50%|████▉     | 172M/346M [01:29<00:56, 3.20MB/s] 50%|████▉     | 172M/346M [01:30<00:58, 3.11MB/s] 50%|████▉     | 173M/346M [01:30<00:59, 3.06MB/s] 50%|█████     | 173M/346M [01:30<01:03, 2.83MB/s] 50%|█████     | 174M/346M [01:30<01:04, 2.78MB/s] 50%|█████     | 174M/346M [01:30<01:05, 2.74MB/s] 50%|█████     | 174M/346M [01:30<01:06, 2.71MB/s] 50%|█████     | 174M/346M [01:30<01:14, 2.40MB/s] 50%|█████     | 175M/346M [01:31<01:16, 2.35MB/s] 51%|█████     | 175M/346M [01:31<01:06, 2.70MB/s] 51%|█████     | 175M/346M [01:31<01:06, 2.67MB/s] 51%|█████     | 176M/346M [01:31<00:59, 2.99MB/s] 51%|█████     | 176M/346M [01:31<01:16, 2.32MB/s] 51%|█████     | 176M/346M [01:31<01:12, 2.45MB/s] 51%|█████     | 176M/346M [01:31<01:13, 2.42MB/s] 51%|█████     | 177M/346M [01:32<01:28, 2.01MB/s] 51%|█████     | 177M/346M [01:32<01:26, 2.04MB/s] 51%|█████     | 177M/346M [01:32<01:24, 2.10MB/s] 51%|█████▏    | 177M/346M [01:32<01:24, 2.08MB/s] 51%|█████▏    | 178M/346M [01:32<01:14, 2.38MB/s] 52%|█████▏    | 178M/346M [01:32<01:11, 2.47MB/s] 52%|█████▏    | 178M/346M [01:32<01:05, 2.68MB/s] 52%|█████▏    | 179M/346M [01:32<01:09, 2.54MB/s] 52%|█████▏    | 179M/346M [01:32<01:06, 2.63MB/s] 52%|█████▏    | 179M/346M [01:33<01:00, 2.89MB/s] 52%|█████▏    | 180M/346M [01:33<00:55, 3.11MB/s] 52%|█████▏    | 180M/346M [01:33<00:51, 3.36MB/s] 52%|█████▏    | 181M/346M [01:33<00:50, 3.45MB/s] 52%|█████▏    | 181M/346M [01:33<00:51, 3.34MB/s] 52%|█████▏    | 181M/346M [01:33<00:55, 3.11MB/s] 53%|█████▎    | 182M/346M [01:33<00:58, 2.92MB/s] 53%|█████▎    | 182M/346M [01:34<01:01, 2.77MB/s] 53%|█████▎    | 183M/346M [01:34<01:09, 2.46MB/s] 53%|█████▎    | 183M/346M [01:34<01:09, 2.46MB/s] 53%|█████▎    | 183M/346M [01:34<01:03, 2.70MB/s] 53%|█████▎    | 184M/346M [01:34<01:05, 2.59MB/s] 53%|█████▎    | 184M/346M [01:34<01:12, 2.35MB/s] 53%|█████▎    | 184M/346M [01:34<01:19, 2.15MB/s] 53%|█████▎    | 184M/346M [01:35<01:25, 1.99MB/s] 53%|█████▎    | 184M/346M [01:35<01:37, 1.74MB/s] 53%|█████▎    | 185M/346M [01:35<01:38, 1.72MB/s] 53%|█████▎    | 185M/346M [01:35<01:41, 1.66MB/s] 53%|█████▎    | 185M/346M [01:35<02:03, 1.37MB/s] 54%|█████▎    | 185M/346M [01:35<01:56, 1.45MB/s] 54%|█████▎    | 185M/346M [01:35<01:46, 1.58MB/s] 54%|█████▎    | 186M/346M [01:36<01:36, 1.73MB/s] 54%|█████▍    | 186M/346M [01:36<01:27, 1.91MB/s] 54%|█████▍    | 186M/346M [01:36<01:17, 2.15MB/s] 54%|█████▍    | 187M/346M [01:36<01:09, 2.39MB/s] 54%|█████▍    | 187M/346M [01:36<01:07, 2.46MB/s] 54%|█████▍    | 187M/346M [01:36<01:09, 2.40MB/s] 54%|█████▍    | 187M/346M [01:36<01:10, 2.37MB/s] 54%|█████▍    | 188M/346M [01:36<01:13, 2.25MB/s] 54%|█████▍    | 188M/346M [01:37<01:30, 1.84MB/s] 54%|█████▍    | 188M/346M [01:37<01:28, 1.87MB/s] 54%|█████▍    | 188M/346M [01:37<01:37, 1.69MB/s] 54%|█████▍    | 188M/346M [01:37<01:30, 1.82MB/s] 55%|█████▍    | 189M/346M [01:37<01:36, 1.71MB/s] 55%|█████▍    | 189M/346M [01:37<01:31, 1.79MB/s] 55%|█████▍    | 189M/346M [01:37<01:40, 1.63MB/s] 55%|█████▍    | 189M/346M [01:37<01:30, 1.81MB/s] 55%|█████▍    | 189M/346M [01:37<01:32, 1.77MB/s] 55%|█████▍    | 190M/346M [01:38<01:23, 1.95MB/s] 55%|█████▍    | 190M/346M [01:38<01:24, 1.93MB/s] 55%|█████▍    | 190M/346M [01:38<01:18, 2.09MB/s] 55%|█████▌    | 190M/346M [01:38<01:23, 1.96MB/s] 55%|█████▌    | 191M/346M [01:38<01:22, 1.96MB/s] 55%|█████▌    | 191M/346M [01:38<01:24, 1.92MB/s] 55%|█████▌    | 191M/346M [01:38<01:19, 2.04MB/s] 55%|█████▌    | 191M/346M [01:38<01:32, 1.75MB/s] 55%|█████▌    | 191M/346M [01:39<01:33, 1.73MB/s] 55%|█████▌    | 192M/346M [01:39<01:24, 1.91MB/s] 55%|█████▌    | 192M/346M [01:39<01:38, 1.64MB/s] 56%|█████▌    | 192M/346M [01:39<01:37, 1.66MB/s] 56%|█████▌    | 192M/346M [01:39<01:32, 1.75MB/s] 56%|█████▌    | 192M/346M [01:39<01:31, 1.75MB/s] 56%|█████▌    | 193M/346M [01:39<01:34, 1.70MB/s] 56%|█████▌    | 193M/346M [01:39<01:36, 1.66MB/s] 56%|█████▌    | 193M/346M [01:39<01:39, 1.61MB/s] 56%|█████▌    | 193M/346M [01:40<01:42, 1.56MB/s] 56%|█████▌    | 193M/346M [01:40<01:37, 1.64MB/s] 56%|█████▌    | 193M/346M [01:40<01:39, 1.61MB/s] 56%|█████▌    | 193M/346M [01:40<01:45, 1.51MB/s] 56%|█████▌    | 194M/346M [01:40<01:46, 1.49MB/s] 56%|█████▌    | 194M/346M [01:40<01:40, 1.59MB/s] 56%|█████▌    | 194M/346M [01:40<01:37, 1.63MB/s] 56%|█████▌    | 194M/346M [01:40<01:37, 1.63MB/s] 56%|█████▌    | 194M/346M [01:40<01:36, 1.65MB/s] 56%|█████▌    | 195M/346M [01:41<01:36, 1.64MB/s] 56%|█████▋    | 195M/346M [01:41<01:35, 1.65MB/s] 56%|█████▋    | 195M/346M [01:41<01:36, 1.64MB/s] 56%|█████▋    | 195M/346M [01:41<01:34, 1.67MB/s] 56%|█████▋    | 195M/346M [01:41<01:35, 1.65MB/s] 56%|█████▋    | 195M/346M [01:41<01:39, 1.59MB/s] 57%|█████▋    | 196M/346M [01:41<01:45, 1.49MB/s] 57%|█████▋    | 196M/346M [01:41<01:45, 1.50MB/s] 57%|█████▋    | 196M/346M [01:41<01:42, 1.54MB/s] 57%|█████▋    | 196M/346M [01:42<01:38, 1.60MB/s] 57%|█████▋    | 196M/346M [01:42<01:38, 1.59MB/s] 57%|█████▋    | 196M/346M [01:42<01:38, 1.60MB/s] 57%|█████▋    | 197M/346M [01:42<01:36, 1.62MB/s] 57%|█████▋    | 197M/346M [01:42<01:35, 1.64MB/s] 57%|█████▋    | 197M/346M [01:42<01:35, 1.63MB/s] 57%|█████▋    | 197M/346M [01:42<01:38, 1.58MB/s] 57%|█████▋    | 197M/346M [01:42<01:39, 1.56MB/s] 57%|█████▋    | 198M/346M [01:43<01:40, 1.55MB/s] 57%|█████▋    | 198M/346M [01:43<02:05, 1.24MB/s] 57%|█████▋    | 198M/346M [01:43<01:36, 1.61MB/s] 57%|█████▋    | 198M/346M [01:43<01:41, 1.53MB/s] 57%|█████▋    | 198M/346M [01:43<01:50, 1.40MB/s] 57%|█████▋    | 198M/346M [01:43<01:59, 1.29MB/s] 57%|█████▋    | 199M/346M [01:43<02:03, 1.25MB/s] 57%|█████▋    | 199M/346M [01:44<02:04, 1.24MB/s] 58%|█████▊    | 199M/346M [01:44<01:55, 1.33MB/s] 58%|█████▊    | 199M/346M [01:44<02:02, 1.26MB/s] 58%|█████▊    | 199M/346M [01:44<01:59, 1.29MB/s] 58%|█████▊    | 199M/346M [01:44<01:49, 1.40MB/s] 58%|█████▊    | 200M/346M [01:44<01:54, 1.34MB/s] 58%|█████▊    | 200M/346M [01:44<01:55, 1.32MB/s] 58%|█████▊    | 200M/346M [01:44<01:52, 1.36MB/s] 58%|█████▊    | 200M/346M [01:44<01:49, 1.40MB/s] 58%|█████▊    | 200M/346M [01:45<01:45, 1.45MB/s] 58%|█████▊    | 200M/346M [01:45<01:48, 1.40MB/s] 58%|█████▊    | 201M/346M [01:45<01:39, 1.52MB/s] 58%|█████▊    | 201M/346M [01:45<01:47, 1.42MB/s] 58%|█████▊    | 201M/346M [01:45<01:51, 1.36MB/s] 58%|█████▊    | 201M/346M [01:45<01:49, 1.38MB/s] 58%|█████▊    | 201M/346M [01:45<01:38, 1.55MB/s] 58%|█████▊    | 201M/346M [01:45<01:27, 1.73MB/s] 58%|█████▊    | 202M/346M [01:46<01:05, 2.29MB/s] 58%|█████▊    | 202M/346M [01:46<00:51, 2.91MB/s] 59%|█████▊    | 203M/346M [01:46<00:48, 3.08MB/s] 59%|█████▊    | 203M/346M [01:46<00:51, 2.93MB/s] 59%|█████▊    | 203M/346M [01:46<00:52, 2.85MB/s] 59%|█████▉    | 203M/346M [01:46<00:52, 2.87MB/s] 59%|█████▉    | 204M/346M [01:46<00:52, 2.82MB/s] 59%|█████▉    | 204M/346M [01:46<00:54, 2.71MB/s] 59%|█████▉    | 204M/346M [01:46<00:54, 2.72MB/s] 59%|█████▉    | 205M/346M [01:47<00:45, 3.26MB/s] 59%|█████▉    | 205M/346M [01:47<00:39, 3.71MB/s] 59%|█████▉    | 206M/346M [01:47<00:35, 4.13MB/s] 60%|█████▉    | 206M/346M [01:47<00:32, 4.51MB/s] 60%|█████▉    | 207M/346M [01:47<00:30, 4.71MB/s] 60%|█████▉    | 207M/346M [01:47<00:29, 4.88MB/s] 60%|██████    | 208M/346M [01:47<00:29, 4.84MB/s] 60%|██████    | 208M/346M [01:47<00:34, 4.19MB/s] 60%|██████    | 209M/346M [01:47<00:37, 3.82MB/s] 60%|██████    | 209M/346M [01:48<00:44, 3.25MB/s] 61%|██████    | 209M/346M [01:48<00:46, 3.09MB/s] 61%|██████    | 210M/346M [01:48<00:50, 2.80MB/s] 61%|██████    | 210M/346M [01:48<00:54, 2.63MB/s] 61%|██████    | 210M/346M [01:48<00:57, 2.47MB/s] 61%|██████    | 210M/346M [01:48<01:02, 2.27MB/s] 61%|██████    | 211M/346M [01:48<01:06, 2.13MB/s] 61%|██████    | 211M/346M [01:49<01:07, 2.10MB/s] 61%|██████    | 211M/346M [01:49<01:12, 1.96MB/s] 61%|██████    | 211M/346M [01:49<01:15, 1.88MB/s] 61%|██████    | 211M/346M [01:49<01:20, 1.74MB/s] 61%|██████    | 212M/346M [01:49<01:25, 1.64MB/s] 61%|██████    | 212M/346M [01:49<01:29, 1.57MB/s] 61%|██████▏   | 212M/346M [01:49<01:32, 1.52MB/s] 61%|██████▏   | 212M/346M [01:49<01:33, 1.50MB/s] 61%|██████▏   | 212M/346M [01:50<01:35, 1.47MB/s] 61%|██████▏   | 212M/346M [01:50<01:32, 1.52MB/s] 61%|██████▏   | 213M/346M [01:50<01:32, 1.52MB/s] 62%|██████▏   | 213M/346M [01:50<01:27, 1.59MB/s] 62%|██████▏   | 213M/346M [01:50<01:24, 1.64MB/s] 62%|██████▏   | 213M/346M [01:50<01:23, 1.66MB/s] 62%|██████▏   | 213M/346M [01:50<01:20, 1.72MB/s] 62%|██████▏   | 214M/346M [01:50<01:17, 1.78MB/s] 62%|██████▏   | 214M/346M [01:50<01:12, 1.92MB/s] 62%|██████▏   | 214M/346M [01:51<01:07, 2.05MB/s] 62%|██████▏   | 215M/346M [01:51<01:01, 2.24MB/s] 62%|██████▏   | 215M/346M [01:51<00:53, 2.56MB/s] 62%|██████▏   | 215M/346M [01:51<01:02, 2.20MB/s] 62%|██████▏   | 215M/346M [01:51<01:02, 2.20MB/s] 62%|██████▏   | 216M/346M [01:51<00:59, 2.29MB/s] 62%|██████▏   | 216M/346M [01:51<01:00, 2.26MB/s] 62%|██████▏   | 216M/346M [01:51<01:01, 2.20MB/s] 63%|██████▎   | 216M/346M [01:52<01:03, 2.15MB/s] 63%|██████▎   | 217M/346M [01:52<01:03, 2.12MB/s] 63%|██████▎   | 217M/346M [01:52<01:04, 2.10MB/s] 63%|██████▎   | 217M/346M [01:52<01:02, 2.15MB/s] 63%|██████▎   | 217M/346M [01:52<01:07, 2.00MB/s] 63%|██████▎   | 217M/346M [01:52<01:06, 2.04MB/s] 63%|██████▎   | 218M/346M [01:52<00:52, 2.55MB/s] 63%|██████▎   | 218M/346M [01:52<01:00, 2.21MB/s] 63%|██████▎   | 218M/346M [01:52<01:00, 2.21MB/s] 63%|██████▎   | 219M/346M [01:53<00:57, 2.34MB/s] 63%|██████▎   | 219M/346M [01:53<00:54, 2.44MB/s] 63%|██████▎   | 219M/346M [01:53<00:58, 2.26MB/s] 63%|██████▎   | 219M/346M [01:53<00:51, 2.56MB/s] 64%|██████▎   | 220M/346M [01:53<00:46, 2.82MB/s] 64%|██████▎   | 220M/346M [01:53<00:44, 2.95MB/s] 64%|██████▍   | 221M/346M [01:53<00:45, 2.90MB/s] 64%|██████▍   | 221M/346M [01:53<00:43, 2.98MB/s] 64%|██████▍   | 221M/346M [01:54<00:46, 2.78MB/s] 64%|██████▍   | 222M/346M [01:54<00:47, 2.73MB/s] 64%|██████▍   | 222M/346M [01:54<00:54, 2.41MB/s] 64%|██████▍   | 222M/346M [01:54<00:54, 2.39MB/s] 64%|██████▍   | 222M/346M [01:54<00:54, 2.38MB/s] 64%|██████▍   | 223M/346M [01:54<00:46, 2.78MB/s] 64%|██████▍   | 223M/346M [01:54<00:54, 2.38MB/s] 65%|██████▍   | 223M/346M [01:54<00:53, 2.42MB/s] 65%|██████▍   | 223M/346M [01:55<00:53, 2.42MB/s] 65%|██████▍   | 224M/346M [01:55<00:46, 2.73MB/s] 65%|██████▍   | 224M/346M [01:55<00:47, 2.68MB/s] 65%|██████▍   | 224M/346M [01:55<00:52, 2.43MB/s] 65%|██████▍   | 225M/346M [01:55<01:08, 1.86MB/s] 65%|██████▌   | 225M/346M [01:55<00:50, 2.50MB/s] 65%|██████▌   | 225M/346M [01:55<00:50, 2.49MB/s] 65%|██████▌   | 226M/346M [01:56<00:50, 2.51MB/s] 65%|██████▌   | 226M/346M [01:56<01:03, 1.99MB/s] 65%|██████▌   | 226M/346M [01:56<01:03, 1.97MB/s] 65%|██████▌   | 226M/346M [01:56<01:06, 1.89MB/s] 66%|██████▌   | 227M/346M [01:56<01:08, 1.82MB/s] 66%|██████▌   | 227M/346M [01:56<01:05, 1.89MB/s] 66%|██████▌   | 227M/346M [01:56<01:03, 1.96MB/s] 66%|██████▌   | 227M/346M [01:57<01:05, 1.91MB/s] 66%|██████▌   | 228M/346M [01:57<01:02, 1.97MB/s] 66%|██████▌   | 228M/346M [01:57<00:54, 2.29MB/s] 66%|██████▌   | 228M/346M [01:57<00:46, 2.64MB/s] 66%|██████▌   | 228M/346M [01:57<00:48, 2.52MB/s] 66%|██████▌   | 229M/346M [01:57<00:40, 3.04MB/s] 66%|██████▋   | 229M/346M [01:57<00:36, 3.35MB/s] 66%|██████▋   | 230M/346M [01:57<00:34, 3.55MB/s] 67%|██████▋   | 230M/346M [01:57<00:31, 3.86MB/s] 67%|██████▋   | 231M/346M [01:58<00:28, 4.16MB/s] 67%|██████▋   | 231M/346M [01:58<00:28, 4.22MB/s] 67%|██████▋   | 232M/346M [01:58<00:31, 3.85MB/s] 67%|██████▋   | 232M/346M [01:58<00:28, 4.11MB/s] 67%|██████▋   | 233M/346M [01:58<00:29, 3.96MB/s] 67%|██████▋   | 233M/346M [01:58<00:33, 3.49MB/s] 68%|██████▊   | 233M/346M [01:58<00:34, 3.40MB/s] 68%|██████▊   | 234M/346M [01:58<00:36, 3.18MB/s] 68%|██████▊   | 234M/346M [01:59<00:36, 3.19MB/s] 68%|██████▊   | 234M/346M [01:59<00:36, 3.19MB/s] 68%|██████▊   | 235M/346M [01:59<00:37, 3.13MB/s] 68%|██████▊   | 235M/346M [01:59<00:45, 2.52MB/s] 68%|██████▊   | 236M/346M [01:59<00:38, 3.03MB/s] 68%|██████▊   | 236M/346M [01:59<00:44, 2.60MB/s] 68%|██████▊   | 236M/346M [01:59<00:46, 2.46MB/s] 68%|██████▊   | 237M/346M [02:00<00:55, 2.05MB/s] 68%|██████▊   | 237M/346M [02:00<00:55, 2.04MB/s] 69%|██████▊   | 237M/346M [02:00<01:06, 1.72MB/s] 69%|██████▊   | 237M/346M [02:00<00:58, 1.95MB/s] 69%|██████▊   | 237M/346M [02:00<01:03, 1.78MB/s] 69%|██████▊   | 238M/346M [02:00<00:55, 2.05MB/s] 69%|██████▉   | 238M/346M [02:00<01:01, 1.83MB/s] 69%|██████▉   | 238M/346M [02:01<01:02, 1.82MB/s] 69%|██████▉   | 238M/346M [02:01<01:01, 1.84MB/s] 69%|██████▉   | 238M/346M [02:01<01:19, 1.42MB/s] 69%|██████▉   | 239M/346M [02:01<01:17, 1.44MB/s] 69%|██████▉   | 239M/346M [02:01<01:16, 1.47MB/s] 69%|██████▉   | 239M/346M [02:01<01:15, 1.49MB/s] 69%|██████▉   | 239M/346M [02:01<01:16, 1.46MB/s] 69%|██████▉   | 239M/346M [02:01<01:25, 1.30MB/s] 69%|██████▉   | 239M/346M [02:02<01:27, 1.28MB/s] 69%|██████▉   | 240M/346M [02:02<01:25, 1.31MB/s] 69%|██████▉   | 240M/346M [02:02<01:24, 1.33MB/s] 69%|██████▉   | 240M/346M [02:02<01:22, 1.35MB/s] 69%|██████▉   | 240M/346M [02:02<01:18, 1.41MB/s] 69%|██████▉   | 240M/346M [02:02<01:19, 1.39MB/s] 69%|██████▉   | 240M/346M [02:02<01:21, 1.36MB/s] 70%|██████▉   | 240M/346M [02:02<01:26, 1.27MB/s] 70%|██████▉   | 241M/346M [02:03<01:26, 1.28MB/s] 70%|██████▉   | 241M/346M [02:03<01:27, 1.25MB/s] 70%|██████▉   | 241M/346M [02:03<01:25, 1.29MB/s] 70%|██████▉   | 241M/346M [02:03<01:23, 1.32MB/s] 70%|██████▉   | 241M/346M [02:03<01:23, 1.32MB/s] 70%|██████▉   | 241M/346M [02:03<01:20, 1.37MB/s] 70%|██████▉   | 241M/346M [02:03<01:24, 1.29MB/s] 70%|██████▉   | 242M/346M [02:03<01:30, 1.21MB/s] 70%|██████▉   | 242M/346M [02:03<01:34, 1.16MB/s] 70%|██████▉   | 242M/346M [02:04<01:38, 1.11MB/s] 70%|██████▉   | 242M/346M [02:04<01:41, 1.07MB/s] 70%|███████   | 242M/346M [02:04<01:45, 1.03MB/s] 70%|███████   | 242M/346M [02:04<01:49, 989kB/s]  70%|███████   | 242M/346M [02:04<01:57, 923kB/s] 70%|███████   | 243M/346M [02:04<02:01, 894kB/s] 70%|███████   | 243M/346M [02:05<02:02, 885kB/s] 70%|███████   | 243M/346M [02:05<01:58, 913kB/s] 70%|███████   | 243M/346M [02:05<02:00, 892kB/s] 70%|███████   | 243M/346M [02:05<02:01, 889kB/s] 70%|███████   | 243M/346M [02:05<02:05, 860kB/s] 70%|███████   | 243M/346M [02:05<02:09, 832kB/s] 70%|███████   | 244M/346M [02:06<02:04, 860kB/s] 70%|███████   | 244M/346M [02:06<02:01, 883kB/s] 71%|███████   | 244M/346M [02:06<02:04, 862kB/s] 71%|███████   | 244M/346M [02:06<01:59, 892kB/s] 71%|███████   | 244M/346M [02:06<02:00, 882kB/s] 71%|███████   | 244M/346M [02:07<02:04, 855kB/s] 71%|███████   | 244M/346M [02:07<02:03, 858kB/s] 71%|███████   | 245M/346M [02:07<02:02, 866kB/s] 71%|███████   | 245M/346M [02:07<02:02, 867kB/s] 71%|███████   | 245M/346M [02:07<01:51, 946kB/s] 71%|███████   | 245M/346M [02:07<01:35, 1.10MB/s] 71%|███████   | 245M/346M [02:07<01:14, 1.41MB/s] 71%|███████   | 246M/346M [02:08<01:10, 1.50MB/s] 71%|███████   | 246M/346M [02:08<00:56, 1.84MB/s] 71%|███████   | 246M/346M [02:08<00:51, 2.03MB/s] 71%|███████▏  | 247M/346M [02:08<00:47, 2.18MB/s] 71%|███████▏  | 247M/346M [02:08<00:55, 1.87MB/s] 71%|███████▏  | 247M/346M [02:08<00:45, 2.26MB/s] 72%|███████▏  | 248M/346M [02:08<00:44, 2.33MB/s] 72%|███████▏  | 248M/346M [02:09<00:42, 2.40MB/s] 72%|███████▏  | 248M/346M [02:09<00:44, 2.31MB/s] 72%|███████▏  | 248M/346M [02:09<00:44, 2.32MB/s] 72%|███████▏  | 249M/346M [02:09<00:45, 2.26MB/s] 72%|███████▏  | 249M/346M [02:09<00:45, 2.25MB/s] 72%|███████▏  | 249M/346M [02:09<00:46, 2.19MB/s] 72%|███████▏  | 249M/346M [02:09<00:46, 2.16MB/s] 72%|███████▏  | 250M/346M [02:09<00:45, 2.23MB/s] 72%|███████▏  | 250M/346M [02:10<00:47, 2.13MB/s] 72%|███████▏  | 250M/346M [02:10<00:44, 2.24MB/s] 72%|███████▏  | 251M/346M [02:10<00:42, 2.32MB/s] 73%|███████▎  | 251M/346M [02:10<00:46, 2.12MB/s] 73%|███████▎  | 251M/346M [02:10<00:47, 2.08MB/s] 73%|███████▎  | 251M/346M [02:10<00:47, 2.08MB/s] 73%|███████▎  | 252M/346M [02:10<00:47, 2.09MB/s] 73%|███████▎  | 252M/346M [02:11<00:53, 1.83MB/s] 73%|███████▎  | 252M/346M [02:11<00:50, 1.93MB/s] 73%|███████▎  | 253M/346M [02:11<00:47, 2.07MB/s] 73%|███████▎  | 253M/346M [02:11<00:44, 2.19MB/s] 73%|███████▎  | 253M/346M [02:11<00:43, 2.21MB/s] 73%|███████▎  | 254M/346M [02:11<00:42, 2.27MB/s] 73%|███████▎  | 254M/346M [02:11<00:37, 2.54MB/s] 74%|███████▎  | 254M/346M [02:12<00:40, 2.34MB/s] 74%|███████▎  | 255M/346M [02:12<00:42, 2.26MB/s] 74%|███████▎  | 255M/346M [02:12<00:40, 2.34MB/s] 74%|███████▍  | 255M/346M [02:12<00:39, 2.39MB/s] 74%|███████▍  | 256M/346M [02:12<00:39, 2.41MB/s] 74%|███████▍  | 256M/346M [02:12<00:38, 2.45MB/s] 74%|███████▍  | 256M/346M [02:12<00:37, 2.49MB/s] 74%|███████▍  | 257M/346M [02:13<00:39, 2.35MB/s] 74%|███████▍  | 257M/346M [02:13<00:40, 2.30MB/s] 74%|███████▍  | 257M/346M [02:13<00:42, 2.21MB/s] 74%|███████▍  | 257M/346M [02:13<00:42, 2.18MB/s] 74%|███████▍  | 257M/346M [02:13<00:44, 2.07MB/s] 74%|███████▍  | 258M/346M [02:13<00:53, 1.74MB/s] 75%|███████▍  | 258M/346M [02:13<00:53, 1.72MB/s] 75%|███████▍  | 258M/346M [02:14<00:50, 1.83MB/s] 75%|███████▍  | 258M/346M [02:14<00:48, 1.91MB/s] 75%|███████▍  | 258M/346M [02:14<00:49, 1.85MB/s] 75%|███████▍  | 259M/346M [02:14<00:50, 1.80MB/s] 75%|███████▍  | 259M/346M [02:14<00:53, 1.70MB/s] 75%|███████▍  | 259M/346M [02:14<00:53, 1.71MB/s] 75%|███████▍  | 259M/346M [02:14<00:47, 1.89MB/s] 75%|███████▌  | 259M/346M [02:14<00:44, 2.04MB/s] 75%|███████▌  | 260M/346M [02:14<00:38, 2.33MB/s] 75%|███████▌  | 260M/346M [02:15<00:37, 2.40MB/s] 75%|███████▌  | 260M/346M [02:15<00:34, 2.62MB/s] 75%|███████▌  | 261M/346M [02:15<00:40, 2.21MB/s] 75%|███████▌  | 261M/346M [02:15<00:38, 2.29MB/s] 76%|███████▌  | 261M/346M [02:15<00:41, 2.15MB/s] 76%|███████▌  | 262M/346M [02:15<00:39, 2.21MB/s] 76%|███████▌  | 262M/346M [02:15<00:36, 2.43MB/s] 76%|███████▌  | 262M/346M [02:16<00:38, 2.26MB/s] 76%|███████▌  | 263M/346M [02:16<00:35, 2.43MB/s] 76%|███████▌  | 263M/346M [02:16<00:38, 2.28MB/s] 76%|███████▌  | 263M/346M [02:16<00:36, 2.36MB/s] 76%|███████▌  | 264M/346M [02:16<00:33, 2.61MB/s] 76%|███████▋  | 264M/346M [02:16<00:34, 2.48MB/s] 76%|███████▋  | 264M/346M [02:16<00:36, 2.32MB/s] 76%|███████▋  | 264M/346M [02:16<00:37, 2.29MB/s] 76%|███████▋  | 265M/346M [02:17<00:40, 2.12MB/s] 77%|███████▋  | 265M/346M [02:17<00:43, 1.96MB/s] 77%|███████▋  | 265M/346M [02:17<00:45, 1.87MB/s] 77%|███████▋  | 265M/346M [02:17<00:44, 1.89MB/s] 77%|███████▋  | 265M/346M [02:17<00:42, 1.97MB/s] 77%|███████▋  | 266M/346M [02:17<00:40, 2.08MB/s] 77%|███████▋  | 266M/346M [02:17<00:40, 2.06MB/s] 77%|███████▋  | 266M/346M [02:17<00:40, 2.06MB/s] 77%|███████▋  | 266M/346M [02:17<00:39, 2.13MB/s] 77%|███████▋  | 266M/346M [02:18<00:39, 2.10MB/s] 77%|███████▋  | 267M/346M [02:18<00:40, 2.07MB/s] 77%|███████▋  | 267M/346M [02:18<00:50, 1.65MB/s] 77%|███████▋  | 267M/346M [02:18<00:48, 1.70MB/s] 77%|███████▋  | 267M/346M [02:18<00:49, 1.65MB/s] 77%|███████▋  | 267M/346M [02:18<00:52, 1.58MB/s] 77%|███████▋  | 268M/346M [02:18<00:54, 1.51MB/s] 77%|███████▋  | 268M/346M [02:19<00:56, 1.45MB/s] 77%|███████▋  | 268M/346M [02:19<01:18, 1.04MB/s] 78%|███████▊  | 268M/346M [02:19<00:56, 1.44MB/s] 78%|███████▊  | 268M/346M [02:19<00:59, 1.36MB/s] 78%|███████▊  | 269M/346M [02:19<01:01, 1.32MB/s] 78%|███████▊  | 269M/346M [02:19<01:02, 1.29MB/s] 78%|███████▊  | 269M/346M [02:19<01:05, 1.22MB/s] 78%|███████▊  | 269M/346M [02:20<01:05, 1.22MB/s] 78%|███████▊  | 269M/346M [02:20<01:01, 1.32MB/s] 78%|███████▊  | 269M/346M [02:20<00:53, 1.50MB/s] 78%|███████▊  | 270M/346M [02:20<00:52, 1.52MB/s] 78%|███████▊  | 270M/346M [02:20<00:52, 1.53MB/s] 78%|███████▊  | 270M/346M [02:20<00:45, 1.76MB/s] 78%|███████▊  | 270M/346M [02:20<00:46, 1.69MB/s] 78%|███████▊  | 270M/346M [02:20<00:41, 1.92MB/s] 78%|███████▊  | 271M/346M [02:20<00:36, 2.17MB/s] 78%|███████▊  | 271M/346M [02:21<00:42, 1.86MB/s] 78%|███████▊  | 271M/346M [02:21<00:40, 1.92MB/s] 78%|███████▊  | 271M/346M [02:21<00:39, 1.96MB/s] 79%|███████▊  | 272M/346M [02:21<00:46, 1.69MB/s] 79%|███████▊  | 272M/346M [02:21<00:40, 1.94MB/s] 79%|███████▊  | 272M/346M [02:21<00:45, 1.72MB/s] 79%|███████▊  | 272M/346M [02:21<00:45, 1.69MB/s] 79%|███████▊  | 272M/346M [02:21<00:45, 1.70MB/s] 79%|███████▉  | 273M/346M [02:22<00:40, 1.91MB/s] 79%|███████▉  | 273M/346M [02:22<00:40, 1.91MB/s] 79%|███████▉  | 273M/346M [02:22<00:48, 1.57MB/s] 79%|███████▉  | 273M/346M [02:22<00:43, 1.74MB/s] 79%|███████▉  | 273M/346M [02:22<00:45, 1.68MB/s] 79%|███████▉  | 274M/346M [02:22<00:47, 1.61MB/s] 79%|███████▉  | 274M/346M [02:22<00:49, 1.53MB/s] 79%|███████▉  | 274M/346M [02:22<00:51, 1.48MB/s] 79%|███████▉  | 274M/346M [02:23<00:52, 1.44MB/s] 79%|███████▉  | 274M/346M [02:23<00:53, 1.40MB/s] 79%|███████▉  | 274M/346M [02:23<00:53, 1.40MB/s] 79%|███████▉  | 274M/346M [02:23<00:52, 1.42MB/s] 79%|███████▉  | 275M/346M [02:23<00:54, 1.38MB/s] 79%|███████▉  | 275M/346M [02:23<00:55, 1.35MB/s] 79%|███████▉  | 275M/346M [02:23<00:53, 1.39MB/s] 80%|███████▉  | 275M/346M [02:23<00:52, 1.42MB/s] 80%|███████▉  | 275M/346M [02:24<00:49, 1.51MB/s] 80%|███████▉  | 276M/346M [02:24<00:44, 1.65MB/s] 80%|███████▉  | 276M/346M [02:24<00:45, 1.62MB/s] 80%|███████▉  | 276M/346M [02:24<00:45, 1.62MB/s] 80%|███████▉  | 276M/346M [02:24<00:45, 1.59MB/s] 80%|███████▉  | 276M/346M [02:24<00:46, 1.57MB/s] 80%|███████▉  | 276M/346M [02:24<00:46, 1.56MB/s] 80%|███████▉  | 277M/346M [02:24<00:48, 1.50MB/s] 80%|████████  | 277M/346M [02:25<00:48, 1.50MB/s] 80%|████████  | 277M/346M [02:25<00:47, 1.52MB/s] 80%|████████  | 277M/346M [02:25<00:45, 1.57MB/s] 80%|████████  | 277M/346M [02:25<00:45, 1.59MB/s] 80%|████████  | 278M/346M [02:25<00:42, 1.68MB/s] 80%|████████  | 278M/346M [02:25<00:37, 1.89MB/s] 80%|████████  | 278M/346M [02:25<00:38, 1.87MB/s] 80%|████████  | 278M/346M [02:25<00:38, 1.84MB/s] 80%|████████  | 278M/346M [02:25<00:38, 1.85MB/s] 81%|████████  | 278M/346M [02:26<00:38, 1.85MB/s] 81%|████████  | 279M/346M [02:26<00:37, 1.86MB/s] 81%|████████  | 279M/346M [02:26<00:36, 1.91MB/s] 81%|████████  | 279M/346M [02:26<00:35, 1.95MB/s] 81%|████████  | 279M/346M [02:26<00:34, 2.01MB/s] 81%|████████  | 279M/346M [02:26<00:38, 1.80MB/s] 81%|████████  | 280M/346M [02:26<00:35, 1.93MB/s] 81%|████████  | 280M/346M [02:26<00:35, 1.92MB/s] 81%|████████  | 280M/346M [02:26<00:37, 1.86MB/s] 81%|████████  | 280M/346M [02:26<00:37, 1.84MB/s] 81%|████████  | 280M/346M [02:27<00:40, 1.69MB/s] 81%|████████  | 281M/346M [02:27<00:40, 1.68MB/s] 81%|████████  | 281M/346M [02:27<00:39, 1.74MB/s] 81%|████████▏ | 281M/346M [02:27<00:37, 1.79MB/s] 81%|████████▏ | 281M/346M [02:27<00:40, 1.69MB/s] 81%|████████▏ | 281M/346M [02:27<00:41, 1.63MB/s] 81%|████████▏ | 282M/346M [02:27<00:40, 1.65MB/s] 82%|████████▏ | 282M/346M [02:28<00:41, 1.63MB/s] 82%|████████▏ | 282M/346M [02:28<00:39, 1.68MB/s] 82%|████████▏ | 282M/346M [02:28<00:37, 1.79MB/s] 82%|████████▏ | 283M/346M [02:28<00:36, 1.80MB/s] 82%|████████▏ | 283M/346M [02:28<00:36, 1.84MB/s] 82%|████████▏ | 283M/346M [02:28<00:34, 1.88MB/s] 82%|████████▏ | 283M/346M [02:28<00:35, 1.85MB/s] 82%|████████▏ | 284M/346M [02:28<00:35, 1.83MB/s] 82%|████████▏ | 284M/346M [02:29<00:33, 1.93MB/s] 82%|████████▏ | 284M/346M [02:29<00:30, 2.09MB/s] 82%|████████▏ | 284M/346M [02:29<00:30, 2.09MB/s] 82%|████████▏ | 285M/346M [02:29<00:33, 1.90MB/s] 82%|████████▏ | 285M/346M [02:29<00:32, 2.00MB/s] 82%|████████▏ | 285M/346M [02:29<00:30, 2.10MB/s] 83%|████████▎ | 285M/346M [02:29<00:28, 2.23MB/s] 83%|████████▎ | 286M/346M [02:29<00:26, 2.39MB/s] 83%|████████▎ | 286M/346M [02:30<00:27, 2.32MB/s] 83%|████████▎ | 286M/346M [02:30<00:31, 1.98MB/s] 83%|████████▎ | 286M/346M [02:30<00:29, 2.13MB/s] 83%|████████▎ | 287M/346M [02:30<00:25, 2.44MB/s] 83%|████████▎ | 287M/346M [02:30<00:23, 2.64MB/s] 83%|████████▎ | 287M/346M [02:30<00:23, 2.59MB/s] 83%|████████▎ | 288M/346M [02:30<00:26, 2.34MB/s] 83%|████████▎ | 288M/346M [02:30<00:24, 2.52MB/s] 83%|████████▎ | 288M/346M [02:31<00:22, 2.71MB/s] 83%|████████▎ | 289M/346M [02:31<00:24, 2.47MB/s] 84%|████████▎ | 289M/346M [02:31<00:22, 2.60MB/s] 84%|████████▎ | 289M/346M [02:31<00:19, 3.02MB/s] 84%|████████▍ | 290M/346M [02:31<00:17, 3.35MB/s] 84%|████████▍ | 290M/346M [02:31<00:15, 3.68MB/s] 84%|████████▍ | 291M/346M [02:31<00:14, 4.10MB/s] 84%|████████▍ | 291M/346M [02:31<00:12, 4.49MB/s] 84%|████████▍ | 292M/346M [02:31<00:09, 5.67MB/s] 85%|████████▍ | 293M/346M [02:32<00:08, 6.20MB/s] 85%|████████▍ | 293M/346M [02:32<00:09, 6.08MB/s] 85%|████████▌ | 294M/346M [02:32<00:09, 5.95MB/s] 85%|████████▌ | 295M/346M [02:32<00:10, 5.09MB/s] 85%|████████▌ | 295M/346M [02:32<00:11, 4.55MB/s] 86%|████████▌ | 296M/346M [02:32<00:07, 6.48MB/s] 86%|████████▌ | 297M/346M [02:33<00:11, 4.42MB/s] 86%|████████▌ | 298M/346M [02:33<00:10, 4.73MB/s] 86%|████████▋ | 298M/346M [02:33<00:09, 5.01MB/s] 86%|████████▋ | 299M/346M [02:33<00:11, 4.42MB/s] 87%|████████▋ | 299M/346M [02:33<00:13, 3.70MB/s] 87%|████████▋ | 300M/346M [02:33<00:11, 4.26MB/s] 87%|████████▋ | 301M/346M [02:33<00:12, 3.85MB/s] 87%|████████▋ | 301M/346M [02:34<00:12, 3.88MB/s] 87%|████████▋ | 301M/346M [02:34<00:14, 3.28MB/s] 87%|████████▋ | 302M/346M [02:34<00:12, 3.55MB/s] 87%|████████▋ | 302M/346M [02:34<00:14, 3.06MB/s] 88%|████████▊ | 303M/346M [02:34<00:14, 3.03MB/s] 88%|████████▊ | 303M/346M [02:34<00:17, 2.61MB/s] 88%|████████▊ | 303M/346M [02:34<00:17, 2.60MB/s] 88%|████████▊ | 303M/346M [02:35<00:19, 2.25MB/s] 88%|████████▊ | 304M/346M [02:35<00:19, 2.25MB/s] 88%|████████▊ | 304M/346M [02:35<00:18, 2.40MB/s] 88%|████████▊ | 304M/346M [02:35<00:18, 2.33MB/s] 88%|████████▊ | 304M/346M [02:35<00:18, 2.33MB/s] 88%|████████▊ | 305M/346M [02:35<00:16, 2.54MB/s] 88%|████████▊ | 305M/346M [02:35<00:17, 2.41MB/s] 88%|████████▊ | 305M/346M [02:35<00:16, 2.61MB/s] 88%|████████▊ | 306M/346M [02:36<00:15, 2.80MB/s] 89%|████████▊ | 306M/346M [02:36<00:14, 2.87MB/s] 89%|████████▊ | 307M/346M [02:36<00:13, 3.15MB/s] 89%|████████▊ | 307M/346M [02:36<00:12, 3.15MB/s] 89%|████████▉ | 307M/346M [02:36<00:13, 3.00MB/s] 89%|████████▉ | 308M/346M [02:36<00:12, 3.28MB/s] 89%|████████▉ | 308M/346M [02:36<00:13, 2.95MB/s] 89%|████████▉ | 308M/346M [02:36<00:13, 2.91MB/s] 89%|████████▉ | 309M/346M [02:37<00:15, 2.54MB/s] 89%|████████▉ | 309M/346M [02:37<00:15, 2.43MB/s] 89%|████████▉ | 309M/346M [02:37<00:15, 2.42MB/s] 90%|████████▉ | 310M/346M [02:37<00:15, 2.51MB/s] 90%|████████▉ | 310M/346M [02:37<00:13, 2.74MB/s] 90%|████████▉ | 310M/346M [02:37<00:13, 2.74MB/s] 90%|████████▉ | 311M/346M [02:37<00:12, 2.89MB/s] 90%|████████▉ | 311M/346M [02:37<00:12, 2.85MB/s] 90%|█████████ | 311M/346M [02:38<00:13, 2.77MB/s] 90%|█████████ | 312M/346M [02:38<00:13, 2.69MB/s] 90%|█████████ | 312M/346M [02:38<00:13, 2.56MB/s] 90%|█████████ | 312M/346M [02:38<00:14, 2.48MB/s] 90%|█████████ | 313M/346M [02:38<00:15, 2.22MB/s] 90%|█████████ | 313M/346M [02:38<00:13, 2.51MB/s] 91%|█████████ | 313M/346M [02:38<00:12, 2.74MB/s] 91%|█████████ | 314M/346M [02:39<00:11, 2.91MB/s] 91%|█████████ | 314M/346M [02:39<00:11, 2.91MB/s] 91%|█████████ | 314M/346M [02:39<00:12, 2.58MB/s] 91%|█████████ | 315M/346M [02:39<00:13, 2.47MB/s] 91%|█████████ | 315M/346M [02:39<00:13, 2.42MB/s] 91%|█████████ | 315M/346M [02:39<00:13, 2.43MB/s] 91%|█████████ | 316M/346M [02:39<00:11, 2.70MB/s] 91%|█████████▏| 316M/346M [02:39<00:13, 2.27MB/s] 91%|█████████▏| 316M/346M [02:40<00:13, 2.27MB/s] 91%|█████████▏| 316M/346M [02:40<00:14, 2.19MB/s] 91%|█████████▏| 316M/346M [02:40<00:14, 2.14MB/s] 92%|█████████▏| 317M/346M [02:40<00:14, 2.16MB/s] 92%|█████████▏| 317M/346M [02:40<00:12, 2.46MB/s] 92%|█████████▏| 317M/346M [02:40<00:11, 2.51MB/s] 92%|█████████▏| 318M/346M [02:40<00:12, 2.44MB/s] 92%|█████████▏| 318M/346M [02:40<00:12, 2.43MB/s] 92%|█████████▏| 318M/346M [02:40<00:11, 2.49MB/s] 92%|█████████▏| 318M/346M [02:41<00:11, 2.44MB/s] 92%|█████████▏| 319M/346M [02:41<00:12, 2.29MB/s] 92%|█████████▏| 319M/346M [02:41<00:11, 2.39MB/s] 92%|█████████▏| 319M/346M [02:41<00:10, 2.61MB/s] 92%|█████████▏| 319M/346M [02:41<00:11, 2.45MB/s] 92%|█████████▏| 320M/346M [02:41<00:11, 2.48MB/s] 93%|█████████▎| 320M/346M [02:41<00:11, 2.34MB/s] 93%|█████████▎| 320M/346M [02:41<00:10, 2.54MB/s] 93%|█████████▎| 320M/346M [02:42<00:10, 2.49MB/s] 93%|█████████▎| 321M/346M [02:42<00:11, 2.40MB/s] 93%|█████████▎| 321M/346M [02:42<00:11, 2.36MB/s] 93%|█████████▎| 321M/346M [02:42<00:11, 2.34MB/s] 93%|█████████▎| 321M/346M [02:42<00:11, 2.24MB/s] 93%|█████████▎| 322M/346M [02:42<00:11, 2.18MB/s] 93%|█████████▎| 322M/346M [02:42<00:12, 2.08MB/s] 93%|█████████▎| 322M/346M [02:42<00:11, 2.14MB/s] 93%|█████████▎| 322M/346M [02:42<00:11, 2.22MB/s] 93%|█████████▎| 323M/346M [02:43<00:08, 2.96MB/s] 93%|█████████▎| 323M/346M [02:43<00:08, 2.90MB/s] 94%|█████████▎| 323M/346M [02:43<00:08, 2.73MB/s] 94%|█████████▎| 324M/346M [02:43<00:08, 2.70MB/s] 94%|█████████▎| 324M/346M [02:43<00:09, 2.54MB/s] 94%|█████████▍| 324M/346M [02:43<00:09, 2.29MB/s] 94%|█████████▍| 325M/346M [02:43<00:07, 3.09MB/s] 94%|█████████▍| 326M/346M [02:43<00:05, 3.73MB/s] 94%|█████████▍| 326M/346M [02:44<00:05, 3.69MB/s] 94%|█████████▍| 326M/346M [02:44<00:04, 4.13MB/s] 95%|█████████▍| 327M/346M [02:44<00:04, 4.34MB/s] 95%|█████████▍| 328M/346M [02:44<00:04, 4.58MB/s] 95%|█████████▍| 328M/346M [02:44<00:03, 4.94MB/s] 95%|█████████▌| 329M/346M [02:44<00:03, 5.21MB/s] 95%|█████████▌| 330M/346M [02:44<00:02, 5.66MB/s] 96%|█████████▌| 331M/346M [02:44<00:02, 6.11MB/s] 96%|█████████▌| 331M/346M [02:44<00:02, 6.38MB/s] 96%|█████████▌| 332M/346M [02:45<00:02, 5.63MB/s] 96%|█████████▌| 332M/346M [02:45<00:02, 5.27MB/s] 96%|█████████▋| 333M/346M [02:45<00:02, 5.29MB/s] 96%|█████████▋| 334M/346M [02:45<00:02, 5.30MB/s] 97%|█████████▋| 334M/346M [02:45<00:02, 4.83MB/s] 97%|█████████▋| 335M/346M [02:45<00:02, 4.94MB/s] 97%|█████████▋| 335M/346M [02:45<00:02, 5.15MB/s] 97%|█████████▋| 336M/346M [02:45<00:01, 5.66MB/s] 97%|█████████▋| 336M/346M [02:46<00:01, 6.04MB/s] 97%|█████████▋| 337M/346M [02:46<00:01, 6.32MB/s] 98%|█████████▊| 338M/346M [02:46<00:01, 6.54MB/s] 98%|█████████▊| 338M/346M [02:46<00:01, 6.08MB/s] 98%|█████████▊| 339M/346M [02:46<00:01, 6.06MB/s] 98%|█████████▊| 340M/346M [02:46<00:01, 6.16MB/s] 98%|█████████▊| 341M/346M [02:46<00:00, 6.79MB/s] 99%|█████████▊| 341M/346M [02:46<00:00, 6.68MB/s] 99%|█████████▉| 342M/346M [02:46<00:00, 7.20MB/s] 99%|█████████▉| 343M/346M [02:47<00:00, 6.96MB/s] 99%|█████████▉| 344M/346M [02:47<00:00, 6.55MB/s]100%|█████████▉| 344M/346M [02:47<00:00, 6.15MB/s]100%|█████████▉| 345M/346M [02:47<00:00, 5.93MB/s]100%|█████████▉| 346M/346M [02:47<00:00, 5.76MB/s]100%|██████████| 346M/346M [02:47<00:00, 2.16MB/s]
[32m[2023-12-21 12:50:33,277] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-tiny/model_state.pdparams[0m
[32m[2023-12-21 12:50:33,719] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-12-21 12:50:40,132] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.
[0m
[33m[2023-12-21 12:50:40,132] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:50:41,536] [    INFO][0m - global step 1 / 533600, loss: 2.330461, avg_reader_cost: 0.01012 sec, avg_batch_cost: 1.38952 sec, avg_samples: 1.00000, ips: 0.71967 sequences/sec,  [0m
[32m[2023-12-21 12:50:42,398] [    INFO][0m - global step 2 / 533600, loss: 2.635156, avg_reader_cost: 0.00034 sec, avg_batch_cost: 0.86139 sec, avg_samples: 1.00000, ips: 1.16091 sequences/sec,  [0m
[32m[2023-12-21 12:50:43,425] [    INFO][0m - global step 3 / 533600, loss: 2.974129, avg_reader_cost: 0.00026 sec, avg_batch_cost: 1.02670 sec, avg_samples: 1.00000, ips: 0.97399 sequences/sec,  [0m
[32m[2023-12-21 12:50:44,280] [    INFO][0m - global step 4 / 533600, loss: 2.663995, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.85400 sec, avg_samples: 1.00000, ips: 1.17096 sequences/sec,  [0m
[32m[2023-12-21 12:50:45,309] [    INFO][0m - global step 5 / 533600, loss: 3.311182, avg_reader_cost: 0.00015 sec, avg_batch_cost: 1.02879 sec, avg_samples: 1.00000, ips: 0.97201 sequences/sec,  [0m
[32m[2023-12-21 12:50:46,391] [    INFO][0m - global step 6 / 533600, loss: 2.115804, avg_reader_cost: 0.00015 sec, avg_batch_cost: 1.08130 sec, avg_samples: 1.00000, ips: 0.92482 sequences/sec,  [0m
[32m[2023-12-21 12:50:47,313] [    INFO][0m - global step 7 / 533600, loss: 3.151340, avg_reader_cost: 0.00040 sec, avg_batch_cost: 0.92148 sec, avg_samples: 1.00000, ips: 1.08521 sequences/sec,  [0m
[32m[2023-12-21 12:50:48,186] [    INFO][0m - global step 8 / 533600, loss: 2.986799, avg_reader_cost: 0.00027 sec, avg_batch_cost: 0.87220 sec, avg_samples: 1.00000, ips: 1.14653 sequences/sec,  [0m
[32m[2023-12-21 12:50:49,168] [    INFO][0m - global step 9 / 533600, loss: 2.762380, avg_reader_cost: 0.00027 sec, avg_batch_cost: 0.98208 sec, avg_samples: 1.00000, ips: 1.01825 sequences/sec,  [0m
[32m[2023-12-21 12:50:50,132] [    INFO][0m - global step 10 / 533600, loss: 3.108477, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.96366 sec, avg_samples: 1.00000, ips: 1.03771 sequences/sec,  [0m
[32m[2023-12-21 12:50:51,034] [    INFO][0m - global step 11 / 533600, loss: 2.274802, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.90080 sec, avg_samples: 1.00000, ips: 1.11013 sequences/sec,  [0m
[32m[2023-12-21 12:50:51,907] [    INFO][0m - global step 12 / 533600, loss: 2.095052, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.87282 sec, avg_samples: 1.00000, ips: 1.14571 sequences/sec,  [0m
[32m[2023-12-21 12:50:52,813] [    INFO][0m - global step 13 / 533600, loss: 2.767839, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.90531 sec, avg_samples: 1.00000, ips: 1.10460 sequences/sec,  [0m
[32m[2023-12-21 12:50:53,683] [    INFO][0m - global step 14 / 533600, loss: 3.273682, avg_reader_cost: 0.00031 sec, avg_batch_cost: 0.87028 sec, avg_samples: 1.00000, ips: 1.14906 sequences/sec,  [0m
[32m[2023-12-21 12:50:54,575] [    INFO][0m - global step 15 / 533600, loss: 2.854877, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.89160 sec, avg_samples: 1.00000, ips: 1.12158 sequences/sec,  [0m
[32m[2023-12-21 12:50:55,574] [    INFO][0m - global step 16 / 533600, loss: 1.915061, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.99872 sec, avg_samples: 1.00000, ips: 1.00129 sequences/sec,  [0m
[32m[2023-12-21 12:50:56,396] [    INFO][0m - global step 17 / 533600, loss: 2.812561, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.82089 sec, avg_samples: 1.00000, ips: 1.21819 sequences/sec,  [0m
[32m[2023-12-21 12:50:57,163] [    INFO][0m - global step 18 / 533600, loss: 2.686801, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76705 sec, avg_samples: 1.00000, ips: 1.30370 sequences/sec,  [0m
[32m[2023-12-21 12:50:58,012] [    INFO][0m - global step 19 / 533600, loss: 2.492867, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84846 sec, avg_samples: 1.00000, ips: 1.17861 sequences/sec,  [0m
[32m[2023-12-21 12:50:58,864] [    INFO][0m - global step 20 / 533600, loss: 3.038737, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.85154 sec, avg_samples: 1.00000, ips: 1.17435 sequences/sec,  [0m
[32m[2023-12-21 12:50:59,782] [    INFO][0m - global step 21 / 533600, loss: 1.762895, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.91801 sec, avg_samples: 1.00000, ips: 1.08932 sequences/sec,  [0m
[32m[2023-12-21 12:51:00,691] [    INFO][0m - global step 22 / 533600, loss: 2.826903, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.90820 sec, avg_samples: 1.00000, ips: 1.10108 sequences/sec,  [0m
[32m[2023-12-21 12:51:01,551] [    INFO][0m - global step 23 / 533600, loss: 2.956163, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.86030 sec, avg_samples: 1.00000, ips: 1.16238 sequences/sec,  [0m
[32m[2023-12-21 12:51:02,401] [    INFO][0m - global step 24 / 533600, loss: 2.547118, avg_reader_cost: 0.00026 sec, avg_batch_cost: 0.84869 sec, avg_samples: 1.00000, ips: 1.17829 sequences/sec,  [0m
[32m[2023-12-21 12:51:03,301] [    INFO][0m - global step 25 / 533600, loss: 2.305513, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.89988 sec, avg_samples: 1.00000, ips: 1.11126 sequences/sec,  [0m
[32m[2023-12-21 12:51:04,091] [    INFO][0m - global step 26 / 533600, loss: 3.165647, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.78984 sec, avg_samples: 1.00000, ips: 1.26607 sequences/sec,  [0m
[32m[2023-12-21 12:51:04,894] [    INFO][0m - global step 27 / 533600, loss: 3.366566, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.80226 sec, avg_samples: 1.00000, ips: 1.24647 sequences/sec,  [0m
[32m[2023-12-21 12:51:05,648] [    INFO][0m - global step 28 / 533600, loss: 3.212911, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.75408 sec, avg_samples: 1.00000, ips: 1.32611 sequences/sec,  [0m
[32m[2023-12-21 12:51:06,542] [    INFO][0m - global step 29 / 533600, loss: 2.027257, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.89326 sec, avg_samples: 1.00000, ips: 1.11949 sequences/sec,  [0m
[32m[2023-12-21 12:51:07,406] [    INFO][0m - global step 30 / 533600, loss: 1.999131, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.86355 sec, avg_samples: 1.00000, ips: 1.15801 sequences/sec,  [0m
[32m[2023-12-21 12:51:08,261] [    INFO][0m - global step 31 / 533600, loss: 2.832950, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85471 sec, avg_samples: 1.00000, ips: 1.16999 sequences/sec,  [0m
[32m[2023-12-21 12:51:09,142] [    INFO][0m - global step 32 / 533600, loss: 2.450181, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.88045 sec, avg_samples: 1.00000, ips: 1.13579 sequences/sec,  [0m
[32m[2023-12-21 12:51:09,971] [    INFO][0m - global step 33 / 533600, loss: 2.512994, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.82889 sec, avg_samples: 1.00000, ips: 1.20643 sequences/sec,  [0m
[32m[2023-12-21 12:51:10,808] [    INFO][0m - global step 34 / 533600, loss: 2.981515, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.83685 sec, avg_samples: 1.00000, ips: 1.19496 sequences/sec,  [0m
[32m[2023-12-21 12:51:11,713] [    INFO][0m - global step 35 / 533600, loss: 2.294657, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.90436 sec, avg_samples: 1.00000, ips: 1.10576 sequences/sec,  [0m
[32m[2023-12-21 12:51:12,563] [    INFO][0m - global step 36 / 533600, loss: 2.114816, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84972 sec, avg_samples: 1.00000, ips: 1.17686 sequences/sec,  [0m
[32m[2023-12-21 12:51:13,364] [    INFO][0m - global step 37 / 533600, loss: 2.546082, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.80061 sec, avg_samples: 1.00000, ips: 1.24905 sequences/sec,  [0m
[32m[2023-12-21 12:51:14,274] [    INFO][0m - global step 38 / 533600, loss: 2.996984, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.90957 sec, avg_samples: 1.00000, ips: 1.09942 sequences/sec,  [0m
[32m[2023-12-21 12:51:15,005] [    INFO][0m - global step 39 / 533600, loss: 2.158062, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.73104 sec, avg_samples: 1.00000, ips: 1.36791 sequences/sec,  [0m
[32m[2023-12-21 12:51:15,876] [    INFO][0m - global step 40 / 533600, loss: 3.086996, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.86994 sec, avg_samples: 1.00000, ips: 1.14950 sequences/sec,  [0m
[32m[2023-12-21 12:51:16,730] [    INFO][0m - global step 41 / 533600, loss: 3.863400, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.85434 sec, avg_samples: 1.00000, ips: 1.17049 sequences/sec,  [0m
[32m[2023-12-21 12:51:17,575] [    INFO][0m - global step 42 / 533600, loss: 2.035967, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84469 sec, avg_samples: 1.00000, ips: 1.18387 sequences/sec,  [0m
[32m[2023-12-21 12:51:18,385] [    INFO][0m - global step 43 / 533600, loss: 2.235008, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.80957 sec, avg_samples: 1.00000, ips: 1.23522 sequences/sec,  [0m
[32m[2023-12-21 12:51:19,293] [    INFO][0m - global step 44 / 533600, loss: 2.998821, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.90777 sec, avg_samples: 1.00000, ips: 1.10160 sequences/sec,  [0m
[32m[2023-12-21 12:51:20,127] [    INFO][0m - global step 45 / 533600, loss: 1.890105, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.83365 sec, avg_samples: 1.00000, ips: 1.19954 sequences/sec,  [0m
[32m[2023-12-21 12:51:20,966] [    INFO][0m - global step 46 / 533600, loss: 2.986562, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.83810 sec, avg_samples: 1.00000, ips: 1.19317 sequences/sec,  [0m
[32m[2023-12-21 12:51:21,756] [    INFO][0m - global step 47 / 533600, loss: 2.240328, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.78972 sec, avg_samples: 1.00000, ips: 1.26627 sequences/sec,  [0m
[32m[2023-12-21 12:51:22,550] [    INFO][0m - global step 48 / 533600, loss: 2.702070, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.79375 sec, avg_samples: 1.00000, ips: 1.25984 sequences/sec,  [0m
[32m[2023-12-21 12:51:23,356] [    INFO][0m - global step 49 / 533600, loss: 1.923366, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.80568 sec, avg_samples: 1.00000, ips: 1.24119 sequences/sec,  [0m
[32m[2023-12-21 12:51:24,150] [    INFO][0m - global step 50 / 533600, loss: 3.569309, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.79327 sec, avg_samples: 1.00000, ips: 1.26060 sequences/sec,  [0m
[32m[2023-12-21 12:51:24,990] [    INFO][0m - global step 51 / 533600, loss: 2.279190, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.83929 sec, avg_samples: 1.00000, ips: 1.19148 sequences/sec,  [0m
[32m[2023-12-21 12:51:25,854] [    INFO][0m - global step 52 / 533600, loss: 3.478198, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.86403 sec, avg_samples: 1.00000, ips: 1.15737 sequences/sec,  [0m
[32m[2023-12-21 12:51:26,698] [    INFO][0m - global step 53 / 533600, loss: 2.060477, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84348 sec, avg_samples: 1.00000, ips: 1.18557 sequences/sec,  [0m
[32m[2023-12-21 12:51:27,473] [    INFO][0m - global step 54 / 533600, loss: 3.206979, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77490 sec, avg_samples: 1.00000, ips: 1.29049 sequences/sec,  [0m
[32m[2023-12-21 12:51:28,314] [    INFO][0m - global step 55 / 533600, loss: 1.809712, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84078 sec, avg_samples: 1.00000, ips: 1.18937 sequences/sec,  [0m
[32m[2023-12-21 12:51:29,080] [    INFO][0m - global step 56 / 533600, loss: 2.591963, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76574 sec, avg_samples: 1.00000, ips: 1.30593 sequences/sec,  [0m
[32m[2023-12-21 12:51:29,927] [    INFO][0m - global step 57 / 533600, loss: 3.749363, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84574 sec, avg_samples: 1.00000, ips: 1.18239 sequences/sec,  [0m
[32m[2023-12-21 12:51:30,698] [    INFO][0m - global step 58 / 533600, loss: 2.196810, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77130 sec, avg_samples: 1.00000, ips: 1.29651 sequences/sec,  [0m
[32m[2023-12-21 12:51:31,535] [    INFO][0m - global step 59 / 533600, loss: 3.528814, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.83675 sec, avg_samples: 1.00000, ips: 1.19510 sequences/sec,  [0m
[32m[2023-12-21 12:51:32,380] [    INFO][0m - global step 60 / 533600, loss: 1.420219, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84479 sec, avg_samples: 1.00000, ips: 1.18372 sequences/sec,  [0m
[32m[2023-12-21 12:51:33,084] [    INFO][0m - global step 61 / 533600, loss: 3.432934, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.70356 sec, avg_samples: 1.00000, ips: 1.42134 sequences/sec,  [0m
[32m[2023-12-21 12:51:33,981] [    INFO][0m - global step 62 / 533600, loss: 2.992792, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.89628 sec, avg_samples: 1.00000, ips: 1.11573 sequences/sec,  [0m
[32m[2023-12-21 12:51:35,068] [    INFO][0m - global step 63 / 533600, loss: 2.605221, avg_reader_cost: 0.00015 sec, avg_batch_cost: 1.08670 sec, avg_samples: 1.00000, ips: 0.92021 sequences/sec,  [0m
[32m[2023-12-21 12:51:35,930] [    INFO][0m - global step 64 / 533600, loss: 2.887935, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.86145 sec, avg_samples: 1.00000, ips: 1.16084 sequences/sec,  [0m
[32m[2023-12-21 12:51:36,744] [    INFO][0m - global step 65 / 533600, loss: 2.181453, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.81394 sec, avg_samples: 1.00000, ips: 1.22860 sequences/sec,  [0m
[32m[2023-12-21 12:51:37,628] [    INFO][0m - global step 66 / 533600, loss: 2.812889, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.88378 sec, avg_samples: 1.00000, ips: 1.13151 sequences/sec,  [0m
[32m[2023-12-21 12:51:38,528] [    INFO][0m - global step 67 / 533600, loss: 3.098207, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.89896 sec, avg_samples: 1.00000, ips: 1.11240 sequences/sec,  [0m
[32m[2023-12-21 12:51:39,452] [    INFO][0m - global step 68 / 533600, loss: 2.127523, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.92360 sec, avg_samples: 1.00000, ips: 1.08272 sequences/sec,  [0m
[32m[2023-12-21 12:51:40,342] [    INFO][0m - global step 69 / 533600, loss: 2.568299, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.89038 sec, avg_samples: 1.00000, ips: 1.12311 sequences/sec,  [0m
[32m[2023-12-21 12:51:41,212] [    INFO][0m - global step 70 / 533600, loss: 2.986289, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.86875 sec, avg_samples: 1.00000, ips: 1.15107 sequences/sec,  [0m
[32m[2023-12-21 12:51:42,064] [    INFO][0m - global step 71 / 533600, loss: 2.873134, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.85219 sec, avg_samples: 1.00000, ips: 1.17344 sequences/sec,  [0m
[32m[2023-12-21 12:51:42,895] [    INFO][0m - global step 72 / 533600, loss: 2.636002, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.83019 sec, avg_samples: 1.00000, ips: 1.20454 sequences/sec,  [0m
[32m[2023-12-21 12:51:43,782] [    INFO][0m - global step 73 / 533600, loss: 1.688447, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.88723 sec, avg_samples: 1.00000, ips: 1.12711 sequences/sec,  [0m
[32m[2023-12-21 12:51:44,543] [    INFO][0m - global step 74 / 533600, loss: 2.299687, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76057 sec, avg_samples: 1.00000, ips: 1.31481 sequences/sec,  [0m
[32m[2023-12-21 12:51:45,405] [    INFO][0m - global step 75 / 533600, loss: 2.331363, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.86181 sec, avg_samples: 1.00000, ips: 1.16035 sequences/sec,  [0m
[32m[2023-12-21 12:51:46,227] [    INFO][0m - global step 76 / 533600, loss: 1.897073, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82098 sec, avg_samples: 1.00000, ips: 1.21806 sequences/sec,  [0m
[32m[2023-12-21 12:51:47,016] [    INFO][0m - global step 77 / 533600, loss: 2.362768, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.78859 sec, avg_samples: 1.00000, ips: 1.26809 sequences/sec,  [0m
[32m[2023-12-21 12:51:47,892] [    INFO][0m - global step 78 / 533600, loss: 3.484141, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.87624 sec, avg_samples: 1.00000, ips: 1.14124 sequences/sec,  [0m
[32m[2023-12-21 12:51:48,795] [    INFO][0m - global step 79 / 533600, loss: 1.322280, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.90205 sec, avg_samples: 1.00000, ips: 1.10858 sequences/sec,  [0m
[32m[2023-12-21 12:51:49,638] [    INFO][0m - global step 80 / 533600, loss: 1.764003, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.84285 sec, avg_samples: 1.00000, ips: 1.18646 sequences/sec,  [0m
[32m[2023-12-21 12:51:50,427] [    INFO][0m - global step 81 / 533600, loss: 2.738216, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.78873 sec, avg_samples: 1.00000, ips: 1.26786 sequences/sec,  [0m
[32m[2023-12-21 12:51:51,191] [    INFO][0m - global step 82 / 533600, loss: 3.000863, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76336 sec, avg_samples: 1.00000, ips: 1.30999 sequences/sec,  [0m
[32m[2023-12-21 12:51:51,934] [    INFO][0m - global step 83 / 533600, loss: 2.628510, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.74339 sec, avg_samples: 1.00000, ips: 1.34519 sequences/sec,  [0m
[32m[2023-12-21 12:51:52,780] [    INFO][0m - global step 84 / 533600, loss: 3.039678, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84555 sec, avg_samples: 1.00000, ips: 1.18267 sequences/sec,  [0m
[32m[2023-12-21 12:51:53,602] [    INFO][0m - global step 85 / 533600, loss: 1.484080, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82160 sec, avg_samples: 1.00000, ips: 1.21714 sequences/sec,  [0m
[32m[2023-12-21 12:51:54,343] [    INFO][0m - global step 86 / 533600, loss: 2.438867, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.74020 sec, avg_samples: 1.00000, ips: 1.35099 sequences/sec,  [0m
[32m[2023-12-21 12:51:55,171] [    INFO][0m - global step 87 / 533600, loss: 2.323025, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82739 sec, avg_samples: 1.00000, ips: 1.20862 sequences/sec,  [0m
[32m[2023-12-21 12:51:56,006] [    INFO][0m - global step 88 / 533600, loss: 3.698430, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.83499 sec, avg_samples: 1.00000, ips: 1.19762 sequences/sec,  [0m
[32m[2023-12-21 12:51:56,851] [    INFO][0m - global step 89 / 533600, loss: 2.851830, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.84517 sec, avg_samples: 1.00000, ips: 1.18319 sequences/sec,  [0m
[32m[2023-12-21 12:51:57,690] [    INFO][0m - global step 90 / 533600, loss: 3.078319, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.83821 sec, avg_samples: 1.00000, ips: 1.19302 sequences/sec,  [0m
[32m[2023-12-21 12:51:58,441] [    INFO][0m - global step 91 / 533600, loss: 1.816575, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.75068 sec, avg_samples: 1.00000, ips: 1.33212 sequences/sec,  [0m
[32m[2023-12-21 12:51:59,277] [    INFO][0m - global step 92 / 533600, loss: 2.529262, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.83519 sec, avg_samples: 1.00000, ips: 1.19733 sequences/sec,  [0m
[32m[2023-12-21 12:52:00,071] [    INFO][0m - global step 93 / 533600, loss: 2.897481, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.79422 sec, avg_samples: 1.00000, ips: 1.25909 sequences/sec,  [0m
[32m[2023-12-21 12:52:00,854] [    INFO][0m - global step 94 / 533600, loss: 2.646746, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.78266 sec, avg_samples: 1.00000, ips: 1.27769 sequences/sec,  [0m
[32m[2023-12-21 12:52:01,627] [    INFO][0m - global step 95 / 533600, loss: 3.218907, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77219 sec, avg_samples: 1.00000, ips: 1.29501 sequences/sec,  [0m
[32m[2023-12-21 12:52:02,470] [    INFO][0m - global step 96 / 533600, loss: 3.688817, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84331 sec, avg_samples: 1.00000, ips: 1.18581 sequences/sec,  [0m
[32m[2023-12-21 12:52:03,408] [    INFO][0m - global step 97 / 533600, loss: 3.002273, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.93738 sec, avg_samples: 1.00000, ips: 1.06681 sequences/sec,  [0m
[32m[2023-12-21 12:52:04,203] [    INFO][0m - global step 98 / 533600, loss: 2.946557, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.79451 sec, avg_samples: 1.00000, ips: 1.25864 sequences/sec,  [0m
[32m[2023-12-21 12:52:05,013] [    INFO][0m - global step 99 / 533600, loss: 3.110240, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.80979 sec, avg_samples: 1.00000, ips: 1.23489 sequences/sec,  [0m
[32m[2023-12-21 12:52:05,865] [    INFO][0m - global step 100 / 533600, loss: 2.180618, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.85148 sec, avg_samples: 1.00000, ips: 1.17443 sequences/sec,  [0m
[32m[2023-12-21 12:52:06,682] [    INFO][0m - global step 101 / 533600, loss: 2.191271, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.81626 sec, avg_samples: 1.00000, ips: 1.22511 sequences/sec,  [0m
[32m[2023-12-21 12:52:07,539] [    INFO][0m - global step 102 / 533600, loss: 3.368002, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.85681 sec, avg_samples: 1.00000, ips: 1.16712 sequences/sec,  [0m
[32m[2023-12-21 12:52:08,414] [    INFO][0m - global step 103 / 533600, loss: 1.739862, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.87441 sec, avg_samples: 1.00000, ips: 1.14362 sequences/sec,  [0m
[32m[2023-12-21 12:52:09,193] [    INFO][0m - global step 104 / 533600, loss: 3.198169, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77937 sec, avg_samples: 1.00000, ips: 1.28309 sequences/sec,  [0m
[32m[2023-12-21 12:52:10,022] [    INFO][0m - global step 105 / 533600, loss: 3.325525, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82831 sec, avg_samples: 1.00000, ips: 1.20728 sequences/sec,  [0m
[32m[2023-12-21 12:52:10,827] [    INFO][0m - global step 106 / 533600, loss: 3.129721, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.80451 sec, avg_samples: 1.00000, ips: 1.24300 sequences/sec,  [0m
[32m[2023-12-21 12:52:11,648] [    INFO][0m - global step 107 / 533600, loss: 2.635922, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82023 sec, avg_samples: 1.00000, ips: 1.21917 sequences/sec,  [0m
[32m[2023-12-21 12:52:12,516] [    INFO][0m - global step 108 / 533600, loss: 1.930311, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.86852 sec, avg_samples: 1.00000, ips: 1.15138 sequences/sec,  [0m
[32m[2023-12-21 12:52:13,371] [    INFO][0m - global step 109 / 533600, loss: 2.974957, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85399 sec, avg_samples: 1.00000, ips: 1.17097 sequences/sec,  [0m
[32m[2023-12-21 12:52:14,182] [    INFO][0m - global step 110 / 533600, loss: 2.712687, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.81082 sec, avg_samples: 1.00000, ips: 1.23332 sequences/sec,  [0m
[32m[2023-12-21 12:52:15,046] [    INFO][0m - global step 111 / 533600, loss: 2.059597, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.86327 sec, avg_samples: 1.00000, ips: 1.15838 sequences/sec,  [0m
[32m[2023-12-21 12:52:15,914] [    INFO][0m - global step 112 / 533600, loss: 3.355703, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86798 sec, avg_samples: 1.00000, ips: 1.15210 sequences/sec,  [0m
[32m[2023-12-21 12:52:16,742] [    INFO][0m - global step 113 / 533600, loss: 2.583757, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82721 sec, avg_samples: 1.00000, ips: 1.20888 sequences/sec,  [0m
[32m[2023-12-21 12:52:17,578] [    INFO][0m - global step 114 / 533600, loss: 3.050853, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.83604 sec, avg_samples: 1.00000, ips: 1.19611 sequences/sec,  [0m
[32m[2023-12-21 12:52:18,427] [    INFO][0m - global step 115 / 533600, loss: 2.833714, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84810 sec, avg_samples: 1.00000, ips: 1.17911 sequences/sec,  [0m
[32m[2023-12-21 12:52:19,203] [    INFO][0m - global step 116 / 533600, loss: 1.840653, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.77633 sec, avg_samples: 1.00000, ips: 1.28811 sequences/sec,  [0m
[32m[2023-12-21 12:52:20,064] [    INFO][0m - global step 117 / 533600, loss: 3.131403, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.86078 sec, avg_samples: 1.00000, ips: 1.16174 sequences/sec,  [0m
[32m[2023-12-21 12:52:20,902] [    INFO][0m - global step 118 / 533600, loss: 3.241344, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.83712 sec, avg_samples: 1.00000, ips: 1.19457 sequences/sec,  [0m
[32m[2023-12-21 12:52:21,645] [    INFO][0m - global step 119 / 533600, loss: 2.618846, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.74272 sec, avg_samples: 1.00000, ips: 1.34640 sequences/sec,  [0m
[32m[2023-12-21 12:52:22,449] [    INFO][0m - global step 120 / 533600, loss: 2.327652, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.80387 sec, avg_samples: 1.00000, ips: 1.24399 sequences/sec,  [0m
[32m[2023-12-21 12:52:23,290] [    INFO][0m - global step 121 / 533600, loss: 3.191842, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84009 sec, avg_samples: 1.00000, ips: 1.19034 sequences/sec,  [0m
[32m[2023-12-21 12:52:24,068] [    INFO][0m - global step 122 / 533600, loss: 2.844757, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.77804 sec, avg_samples: 1.00000, ips: 1.28528 sequences/sec,  [0m
[32m[2023-12-21 12:52:24,986] [    INFO][0m - global step 123 / 533600, loss: 1.553397, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.91787 sec, avg_samples: 1.00000, ips: 1.08948 sequences/sec,  [0m
[32m[2023-12-21 12:52:25,759] [    INFO][0m - global step 124 / 533600, loss: 2.380600, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77201 sec, avg_samples: 1.00000, ips: 1.29532 sequences/sec,  [0m
[32m[2023-12-21 12:52:26,470] [    INFO][0m - global step 125 / 533600, loss: 3.102014, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.71080 sec, avg_samples: 1.00000, ips: 1.40687 sequences/sec,  [0m
[32m[2023-12-21 12:52:27,322] [    INFO][0m - global step 126 / 533600, loss: 2.901722, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85183 sec, avg_samples: 1.00000, ips: 1.17394 sequences/sec,  [0m
[32m[2023-12-21 12:52:28,154] [    INFO][0m - global step 127 / 533600, loss: 2.750828, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.83179 sec, avg_samples: 1.00000, ips: 1.20223 sequences/sec,  [0m
[32m[2023-12-21 12:52:28,931] [    INFO][0m - global step 128 / 533600, loss: 3.336352, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77662 sec, avg_samples: 1.00000, ips: 1.28763 sequences/sec,  [0m
[32m[2023-12-21 12:52:29,758] [    INFO][0m - global step 129 / 533600, loss: 2.679221, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82686 sec, avg_samples: 1.00000, ips: 1.20940 sequences/sec,  [0m
[32m[2023-12-21 12:52:30,608] [    INFO][0m - global step 130 / 533600, loss: 2.463276, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.84930 sec, avg_samples: 1.00000, ips: 1.17744 sequences/sec,  [0m
[32m[2023-12-21 12:52:31,372] [    INFO][0m - global step 131 / 533600, loss: 2.734224, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76356 sec, avg_samples: 1.00000, ips: 1.30965 sequences/sec,  [0m
[32m[2023-12-21 12:52:32,206] [    INFO][0m - global step 132 / 533600, loss: 2.817439, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.83389 sec, avg_samples: 1.00000, ips: 1.19919 sequences/sec,  [0m
[32m[2023-12-21 12:52:32,994] [    INFO][0m - global step 133 / 533600, loss: 3.460471, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.78737 sec, avg_samples: 1.00000, ips: 1.27005 sequences/sec,  [0m
[32m[2023-12-21 12:52:33,798] [    INFO][0m - global step 134 / 533600, loss: 3.625045, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.80325 sec, avg_samples: 1.00000, ips: 1.24494 sequences/sec,  [0m
[32m[2023-12-21 12:52:34,609] [    INFO][0m - global step 135 / 533600, loss: 2.815752, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.81080 sec, avg_samples: 1.00000, ips: 1.23336 sequences/sec,  [0m
[32m[2023-12-21 12:52:35,371] [    INFO][0m - global step 136 / 533600, loss: 2.956523, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76157 sec, avg_samples: 1.00000, ips: 1.31308 sequences/sec,  [0m
[32m[2023-12-21 12:52:36,247] [    INFO][0m - global step 137 / 533600, loss: 2.996216, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.87559 sec, avg_samples: 1.00000, ips: 1.14208 sequences/sec,  [0m
[32m[2023-12-21 12:52:37,154] [    INFO][0m - global step 138 / 533600, loss: 2.835146, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.90714 sec, avg_samples: 1.00000, ips: 1.10236 sequences/sec,  [0m
[32m[2023-12-21 12:52:37,957] [    INFO][0m - global step 139 / 533600, loss: 2.778127, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.80290 sec, avg_samples: 1.00000, ips: 1.24549 sequences/sec,  [0m
[32m[2023-12-21 12:52:38,737] [    INFO][0m - global step 140 / 533600, loss: 2.493107, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.77932 sec, avg_samples: 1.00000, ips: 1.28316 sequences/sec,  [0m
[32m[2023-12-21 12:52:39,521] [    INFO][0m - global step 141 / 533600, loss: 2.190649, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.78328 sec, avg_samples: 1.00000, ips: 1.27669 sequences/sec,  [0m
[32m[2023-12-21 12:52:40,358] [    INFO][0m - global step 142 / 533600, loss: 2.483906, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.83711 sec, avg_samples: 1.00000, ips: 1.19459 sequences/sec,  [0m
[32m[2023-12-21 12:52:41,119] [    INFO][0m - global step 143 / 533600, loss: 3.471321, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.76013 sec, avg_samples: 1.00000, ips: 1.31557 sequences/sec,  [0m
[32m[2023-12-21 12:52:41,917] [    INFO][0m - global step 144 / 533600, loss: 2.728571, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.79757 sec, avg_samples: 1.00000, ips: 1.25381 sequences/sec,  [0m
[32m[2023-12-21 12:52:42,742] [    INFO][0m - global step 145 / 533600, loss: 3.035022, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.82500 sec, avg_samples: 1.00000, ips: 1.21211 sequences/sec,  [0m
[32m[2023-12-21 12:52:43,557] [    INFO][0m - global step 146 / 533600, loss: 2.848756, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.81488 sec, avg_samples: 1.00000, ips: 1.22718 sequences/sec,  [0m
[32m[2023-12-21 12:52:44,353] [    INFO][0m - global step 147 / 533600, loss: 1.808325, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.79516 sec, avg_samples: 1.00000, ips: 1.25761 sequences/sec,  [0m
[32m[2023-12-21 12:52:45,166] [    INFO][0m - global step 148 / 533600, loss: 3.158578, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.81298 sec, avg_samples: 1.00000, ips: 1.23004 sequences/sec,  [0m
[32m[2023-12-21 12:52:45,924] [    INFO][0m - global step 149 / 533600, loss: 1.882093, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.75734 sec, avg_samples: 1.00000, ips: 1.32042 sequences/sec,  [0m
No XPU Memory Leak
[33m Run successfully with command - ernie_tiny - python3.9 test_tipc/train.py --amp_level O2 --model ernie_tiny --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path ernie-tiny --pad_to_max_seq_len --max_seq_len 128 --logging_steps 1 --task_name tnews --max_steps 150 --device=xpu  --save_model=/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1             >/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:52:52,548 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 12:52:52,548 auto_parallel_config: None
LAUNCH INFO 2023-12-21 12:52:52,548 auto_tuner_json: None
LAUNCH INFO 2023-12-21 12:52:52,548 devices: 0,1
LAUNCH INFO 2023-12-21 12:52:52,548 elastic_level: -1
LAUNCH INFO 2023-12-21 12:52:52,548 elastic_timeout: 30
LAUNCH INFO 2023-12-21 12:52:52,548 enable_gpu_log: True
LAUNCH INFO 2023-12-21 12:52:52,548 gloo_port: 6767
LAUNCH INFO 2023-12-21 12:52:52,548 host: None
LAUNCH INFO 2023-12-21 12:52:52,548 ips: None
LAUNCH INFO 2023-12-21 12:52:52,548 job_id: default
LAUNCH INFO 2023-12-21 12:52:52,548 legacy: False
LAUNCH INFO 2023-12-21 12:52:52,548 log_dir: log
LAUNCH INFO 2023-12-21 12:52:52,548 log_level: INFO
LAUNCH INFO 2023-12-21 12:52:52,548 log_overwrite: False
LAUNCH INFO 2023-12-21 12:52:52,548 master: None
LAUNCH INFO 2023-12-21 12:52:52,548 max_restart: 3
LAUNCH INFO 2023-12-21 12:52:52,548 nnodes: 1
LAUNCH INFO 2023-12-21 12:52:52,549 nproc_per_node: None
LAUNCH INFO 2023-12-21 12:52:52,549 rank: -1
LAUNCH INFO 2023-12-21 12:52:52,549 run_mode: collective
LAUNCH INFO 2023-12-21 12:52:52,549 server_num: None
LAUNCH INFO 2023-12-21 12:52:52,549 servers: 
LAUNCH INFO 2023-12-21 12:52:52,549 sort_ip: False
LAUNCH INFO 2023-12-21 12:52:52,549 start_port: 6070
LAUNCH INFO 2023-12-21 12:52:52,549 trainer_num: None
LAUNCH INFO 2023-12-21 12:52:52,549 trainers: 
LAUNCH INFO 2023-12-21 12:52:52,549 training_script: test_tipc/train.py
LAUNCH INFO 2023-12-21 12:52:52,549 training_script_args: ['--amp_level', 'O2', '--model', 'ernie_tiny', '--optimizer', 'adamw', '--lr_scheduler', 'linear_decay_with_warmup', '--learning_rate', '2e-5', '--max_grad_norm', '1.0', '--model_name_or_path', 'ernie-tiny', '--pad_to_max_seq_len', '--max_seq_len', '128', '--logging_steps', '1', '--task_name', 'tnews', '--max_steps', '150', '--device=xpu', '--save_model=/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1']
LAUNCH INFO 2023-12-21 12:52:52,549 with_gloo: 1
LAUNCH INFO 2023-12-21 12:52:52,549 --------------------------------------------------
LAUNCH INFO 2023-12-21 12:52:52,549 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 12:52:52,550 Run Pod: yiugbj, replicas 2, status ready
LAUNCH INFO 2023-12-21 12:52:52,567 Watching Pod: yiugbj, replicas 2, status running
global step 120, epoch: 1, batch: 120, loss: 0.13740, accu: 0.90938, speed: 1.82 step/s
global step 130, epoch: 1, batch: 130, loss: 0.28602, accu: 0.91354, speed: 1.82 step/s
global step 140, epoch: 1, batch: 140, loss: 0.34615, accu: 0.91328, speed: 1.84 step/s
global step 150, epoch: 1, batch: 150, loss: 0.21612, accu: 0.91500, speed: 1.90 step/s
I1221 12:42:59.695907  4125 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_xpus', current_value='0', default_value='')
=======================================================================
I1221 12:45:22.415892  4815 tcp_utils.cc:181] The server starts to listen on IP_ANY:41884
I1221 12:45:22.417420  4815 tcp_utils.cc:130] Successfully connected to 127.0.0.1:41884
[32m[2023-12-21 12:45:23,111] [    INFO][0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:45:23,111] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:45:23,112] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-3.0-medium-zh/model_state.pdparams[0m
[32m[2023-12-21 12:45:23,384] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
W1221 12:45:23.388751  4815 xpu_context.cc:151] Please NOTE: xpu device: 0
[33m[2023-12-21 12:45:26,253] [ WARNING][0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieModel: ['ernie.encoder.layers.6.linear1.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight']
- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).[0m
[33m[2023-12-21 12:45:26,253] [ WARNING][0m - Some weights of ErnieModel were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.weight', 'ernie.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:45:26,257] [    INFO][0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-medium-zh'.[0m
[32m[2023-12-21 12:45:26,258] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt[0m
[32m[2023-12-21 12:45:26,281] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json[0m
[32m[2023-12-21 12:45:26,281] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json[0m
[WARN][XPURT][xpu_llmemcpy_p2p_direct:445] ioctl() fail, (807) Unknown IOCTL command
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
global step 10, epoch: 1, batch: 10, loss: 0.48282, accu: 0.66563, speed: 2.32 step/s
global step 20, epoch: 1, batch: 20, loss: 0.41112, accu: 0.75313, speed: 2.10 step/s
global step 30, epoch: 1, batch: 30, loss: 0.39826, accu: 0.78125, speed: 2.10 step/s
global step 40, epoch: 1, batch: 40, loss: 0.48339, accu: 0.80156, speed: 2.13 step/s
global step 50, epoch: 1, batch: 50, loss: 0.40343, accu: 0.81750, speed: 2.10 step/s
global step 60, epoch: 1, batch: 60, loss: 0.46257, accu: 0.82656, speed: 2.10 step/s
global step 70, epoch: 1, batch: 70, loss: 0.34538, accu: 0.83661, speed: 2.09 step/s
global step 80, epoch: 1, batch: 80, loss: 0.41523, accu: 0.84453, speed: 2.10 step/s
global step 90, epoch: 1, batch: 90, loss: 0.38378, accu: 0.84722, speed: 2.13 step/s
global step 100, epoch: 1, batch: 100, loss: 0.40280, accu: 0.85156, speed: 2.13 step/s
eval dev loss: 0.51494, accu: 0.79084
[32m[2023-12-21 12:46:23,575] [    INFO][0m - tokenizer config file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/tokenizer_config.json[0m
[32m[2023-12-21 12:46:23,575] [    INFO][0m - Special tokens file saved in /workspace/PaddleNLP/tests/test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1/model/special_tokens_map.json[0m
global step 110, epoch: 1, batch: 110, loss: 0.44757, accu: 0.84062, speed: 0.75 step/s
global step 120, epoch: 1, batch: 120, loss: 0.55862, accu: 0.83906, speed: 2.14 step/s
global step 130, epoch: 1, batch: 130, loss: 0.45515, accu: 0.84479, speed: 2.10 step/s
global step 140, epoch: 1, batch: 140, loss: 0.46599, accu: 0.84375, speed: 2.12 step/s
global step 150, epoch: 1, batch: 150, loss: 0.44931, accu: 0.84562, speed: 2.17 step/s
I1221 12:46:48.564839  4849 tcp_store.cc:289] receive shutdown event and so quit from MasterDaemon run loop
No XPU Memory Leak
[32m[2023-12-21 12:47:12,631] [    INFO][0m - global step 70 / 2668000, loss: 2.639831, avg_reader_cost: 0.00038 sec, avg_batch_cost: 40.05867 sec, avg_samples: 512.00000, ips: 12.78125 words/sec,  [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:52:54,963] [    INFO][0m - global step 80 / 2668000, loss: 2.619908, avg_reader_cost: 0.00040 sec, avg_batch_cost: 34.23317 sec, avg_samples: 512.00000, ips: 14.95625 words/sec,  [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:52:54,963] [    INFO][0m - global step 80 / 2668000, loss: 2.619908, avg_reader_cost: 0.00040 sec, avg_batch_cost: 34.23317 sec, avg_samples: 512.00000, ips: 14.95625 words/sec,  [0m
Namespace(device='xpu', model='ernie_tiny', logging_steps=1, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=1, max_seq_len=128, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='ernie-tiny', task_name='tnews', max_seq_length=128, warmup_steps=0, warmup_proportion=0.1)
[32m[2023-12-21 12:52:55,959] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-tiny/vocab.txt[0m
[32m[2023-12-21 12:52:55,995] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-tiny/tokenizer_config.json[0m
[32m[2023-12-21 12:52:55,996] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-tiny/special_tokens_map.json[0m
[32m[2023-12-21 12:52:56,451] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-tiny/model_state.pdparams[0m
[32m[2023-12-21 12:52:56,451] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-tiny/model_state.pdparams[0m
[32m[2023-12-21 12:52:56,796] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-12-21 12:53:02,905] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.
[0m
[33m[2023-12-21 12:53:02,906] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:53:04,415] [    INFO][0m - global step 1 / 266800, loss: 3.315810, avg_reader_cost: 0.01649 sec, avg_batch_cost: 1.50149 sec, avg_samples: 1.00000, ips: 0.66600 sequences/sec,  [0m
[32m[2023-12-21 12:53:05,436] [    INFO][0m - global step 2 / 266800, loss: 2.188921, avg_reader_cost: 0.00021 sec, avg_batch_cost: 1.02088 sec, avg_samples: 1.00000, ips: 0.97955 sequences/sec,  [0m
[32m[2023-12-21 12:53:06,453] [    INFO][0m - global step 3 / 266800, loss: 2.762135, avg_reader_cost: 0.00025 sec, avg_batch_cost: 1.01646 sec, avg_samples: 1.00000, ips: 0.98381 sequences/sec,  [0m
[32m[2023-12-21 12:53:07,460] [    INFO][0m - global step 4 / 266800, loss: 2.819784, avg_reader_cost: 0.00019 sec, avg_batch_cost: 1.00678 sec, avg_samples: 1.00000, ips: 0.99327 sequences/sec,  [0m
[32m[2023-12-21 12:53:08,458] [    INFO][0m - global step 5 / 266800, loss: 3.455116, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.99746 sec, avg_samples: 1.00000, ips: 1.00255 sequences/sec,  [0m
[32m[2023-12-21 12:53:09,370] [    INFO][0m - global step 6 / 266800, loss: 2.514176, avg_reader_cost: 0.00026 sec, avg_batch_cost: 0.91159 sec, avg_samples: 1.00000, ips: 1.09699 sequences/sec,  [0m
[32m[2023-12-21 12:53:10,333] [    INFO][0m - global step 7 / 266800, loss: 2.274704, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.96273 sec, avg_samples: 1.00000, ips: 1.03871 sequences/sec,  [0m
[32m[2023-12-21 12:53:11,270] [    INFO][0m - global step 8 / 266800, loss: 3.113790, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.93580 sec, avg_samples: 1.00000, ips: 1.06860 sequences/sec,  [0m
[32m[2023-12-21 12:53:12,177] [    INFO][0m - global step 9 / 266800, loss: 2.391184, avg_reader_cost: 0.00026 sec, avg_batch_cost: 0.90653 sec, avg_samples: 1.00000, ips: 1.10311 sequences/sec,  [0m
[32m[2023-12-21 12:53:13,097] [    INFO][0m - global step 10 / 266800, loss: 2.685575, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91986 sec, avg_samples: 1.00000, ips: 1.08712 sequences/sec,  [0m
[32m[2023-12-21 12:53:14,125] [    INFO][0m - global step 11 / 266800, loss: 2.258142, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.02764 sec, avg_samples: 1.00000, ips: 0.97310 sequences/sec,  [0m
[32m[2023-12-21 12:53:15,009] [    INFO][0m - global step 12 / 266800, loss: 3.525985, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.88362 sec, avg_samples: 1.00000, ips: 1.13171 sequences/sec,  [0m
[32m[2023-12-21 12:53:15,967] [    INFO][0m - global step 13 / 266800, loss: 2.028726, avg_reader_cost: 0.00024 sec, avg_batch_cost: 0.95751 sec, avg_samples: 1.00000, ips: 1.04438 sequences/sec,  [0m
[32m[2023-12-21 12:53:16,926] [    INFO][0m - global step 14 / 266800, loss: 2.831665, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.95857 sec, avg_samples: 1.00000, ips: 1.04322 sequences/sec,  [0m
[32m[2023-12-21 12:53:17,906] [    INFO][0m - global step 15 / 266800, loss: 2.001536, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.97987 sec, avg_samples: 1.00000, ips: 1.02054 sequences/sec,  [0m
[32m[2023-12-21 12:53:18,850] [    INFO][0m - global step 16 / 266800, loss: 2.720022, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.94306 sec, avg_samples: 1.00000, ips: 1.06038 sequences/sec,  [0m
[32m[2023-12-21 12:53:19,793] [    INFO][0m - global step 17 / 266800, loss: 2.666220, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.94244 sec, avg_samples: 1.00000, ips: 1.06108 sequences/sec,  [0m
[32m[2023-12-21 12:53:20,715] [    INFO][0m - global step 18 / 266800, loss: 2.893883, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.92236 sec, avg_samples: 1.00000, ips: 1.08417 sequences/sec,  [0m
[32m[2023-12-21 12:53:21,679] [    INFO][0m - global step 19 / 266800, loss: 2.498476, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.96302 sec, avg_samples: 1.00000, ips: 1.03840 sequences/sec,  [0m
[32m[2023-12-21 12:53:22,591] [    INFO][0m - global step 20 / 266800, loss: 1.676192, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91222 sec, avg_samples: 1.00000, ips: 1.09623 sequences/sec,  [0m
[32m[2023-12-21 12:53:23,522] [    INFO][0m - global step 21 / 266800, loss: 3.745217, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.93070 sec, avg_samples: 1.00000, ips: 1.07446 sequences/sec,  [0m
[32m[2023-12-21 12:53:24,432] [    INFO][0m - global step 22 / 266800, loss: 3.127437, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.90958 sec, avg_samples: 1.00000, ips: 1.09941 sequences/sec,  [0m
[32m[2023-12-21 12:53:25,352] [    INFO][0m - global step 23 / 266800, loss: 3.042346, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91918 sec, avg_samples: 1.00000, ips: 1.08793 sequences/sec,  [0m
[32m[2023-12-21 12:53:26,216] [    INFO][0m - global step 24 / 266800, loss: 2.773882, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.86357 sec, avg_samples: 1.00000, ips: 1.15798 sequences/sec,  [0m
[32m[2023-12-21 12:53:27,188] [    INFO][0m - global step 25 / 266800, loss: 2.172230, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.97165 sec, avg_samples: 1.00000, ips: 1.02917 sequences/sec,  [0m
[32m[2023-12-21 12:53:28,116] [    INFO][0m - global step 26 / 266800, loss: 2.479875, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92707 sec, avg_samples: 1.00000, ips: 1.07866 sequences/sec,  [0m
[32m[2023-12-21 12:53:29,111] [    INFO][0m - global step 27 / 266800, loss: 1.499913, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.99478 sec, avg_samples: 1.00000, ips: 1.00525 sequences/sec,  [0m
Namespace(device='xpu', model='ernie_tiny', logging_steps=1, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=1, max_seq_len=128, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='ernie-tiny', task_name='tnews', max_seq_length=128, warmup_steps=0, warmup_proportion=0.1)
[32m[2023-12-21 12:52:55,959] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-tiny/vocab.txt[0m
[32m[2023-12-21 12:52:55,995] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-tiny/tokenizer_config.json[0m
[32m[2023-12-21 12:52:55,996] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/ernie-tiny/special_tokens_map.json[0m
[32m[2023-12-21 12:52:56,451] [    INFO][0m - Already cached /root/.paddlenlp/models/ernie-tiny/model_state.pdparams[0m
[32m[2023-12-21 12:52:56,451] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/ernie-tiny/model_state.pdparams[0m
[32m[2023-12-21 12:52:56,796] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-12-21 12:53:02,905] [    INFO][0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.
[0m
[33m[2023-12-21 12:53:02,906] [ WARNING][0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[0m
[32m[2023-12-21 12:53:04,415] [    INFO][0m - global step 1 / 266800, loss: 3.315810, avg_reader_cost: 0.01649 sec, avg_batch_cost: 1.50149 sec, avg_samples: 1.00000, ips: 0.66600 sequences/sec,  [0m
[32m[2023-12-21 12:53:05,436] [    INFO][0m - global step 2 / 266800, loss: 2.188921, avg_reader_cost: 0.00021 sec, avg_batch_cost: 1.02088 sec, avg_samples: 1.00000, ips: 0.97955 sequences/sec,  [0m
[32m[2023-12-21 12:53:06,453] [    INFO][0m - global step 3 / 266800, loss: 2.762135, avg_reader_cost: 0.00025 sec, avg_batch_cost: 1.01646 sec, avg_samples: 1.00000, ips: 0.98381 sequences/sec,  [0m
[32m[2023-12-21 12:53:07,460] [    INFO][0m - global step 4 / 266800, loss: 2.819784, avg_reader_cost: 0.00019 sec, avg_batch_cost: 1.00678 sec, avg_samples: 1.00000, ips: 0.99327 sequences/sec,  [0m
[32m[2023-12-21 12:53:08,458] [    INFO][0m - global step 5 / 266800, loss: 3.455116, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.99746 sec, avg_samples: 1.00000, ips: 1.00255 sequences/sec,  [0m
[32m[2023-12-21 12:53:09,370] [    INFO][0m - global step 6 / 266800, loss: 2.514176, avg_reader_cost: 0.00026 sec, avg_batch_cost: 0.91159 sec, avg_samples: 1.00000, ips: 1.09699 sequences/sec,  [0m
[32m[2023-12-21 12:53:10,333] [    INFO][0m - global step 7 / 266800, loss: 2.274704, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.96273 sec, avg_samples: 1.00000, ips: 1.03871 sequences/sec,  [0m
[32m[2023-12-21 12:53:11,270] [    INFO][0m - global step 8 / 266800, loss: 3.113790, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.93580 sec, avg_samples: 1.00000, ips: 1.06860 sequences/sec,  [0m
[32m[2023-12-21 12:53:12,177] [    INFO][0m - global step 9 / 266800, loss: 2.391184, avg_reader_cost: 0.00026 sec, avg_batch_cost: 0.90653 sec, avg_samples: 1.00000, ips: 1.10311 sequences/sec,  [0m
[32m[2023-12-21 12:53:13,097] [    INFO][0m - global step 10 / 266800, loss: 2.685575, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91986 sec, avg_samples: 1.00000, ips: 1.08712 sequences/sec,  [0m
[32m[2023-12-21 12:53:14,125] [    INFO][0m - global step 11 / 266800, loss: 2.258142, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.02764 sec, avg_samples: 1.00000, ips: 0.97310 sequences/sec,  [0m
[32m[2023-12-21 12:53:15,009] [    INFO][0m - global step 12 / 266800, loss: 3.525985, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.88362 sec, avg_samples: 1.00000, ips: 1.13171 sequences/sec,  [0m
[32m[2023-12-21 12:53:15,967] [    INFO][0m - global step 13 / 266800, loss: 2.028726, avg_reader_cost: 0.00024 sec, avg_batch_cost: 0.95751 sec, avg_samples: 1.00000, ips: 1.04438 sequences/sec,  [0m
[32m[2023-12-21 12:53:16,926] [    INFO][0m - global step 14 / 266800, loss: 2.831665, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.95857 sec, avg_samples: 1.00000, ips: 1.04322 sequences/sec,  [0m
[32m[2023-12-21 12:53:17,906] [    INFO][0m - global step 15 / 266800, loss: 2.001536, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.97987 sec, avg_samples: 1.00000, ips: 1.02054 sequences/sec,  [0m
[32m[2023-12-21 12:53:18,850] [    INFO][0m - global step 16 / 266800, loss: 2.720022, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.94306 sec, avg_samples: 1.00000, ips: 1.06038 sequences/sec,  [0m
[32m[2023-12-21 12:53:19,793] [    INFO][0m - global step 17 / 266800, loss: 2.666220, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.94244 sec, avg_samples: 1.00000, ips: 1.06108 sequences/sec,  [0m
[32m[2023-12-21 12:53:20,715] [    INFO][0m - global step 18 / 266800, loss: 2.893883, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.92236 sec, avg_samples: 1.00000, ips: 1.08417 sequences/sec,  [0m
[32m[2023-12-21 12:53:21,679] [    INFO][0m - global step 19 / 266800, loss: 2.498476, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.96302 sec, avg_samples: 1.00000, ips: 1.03840 sequences/sec,  [0m
[32m[2023-12-21 12:53:22,591] [    INFO][0m - global step 20 / 266800, loss: 1.676192, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91222 sec, avg_samples: 1.00000, ips: 1.09623 sequences/sec,  [0m
[32m[2023-12-21 12:53:23,522] [    INFO][0m - global step 21 / 266800, loss: 3.745217, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.93070 sec, avg_samples: 1.00000, ips: 1.07446 sequences/sec,  [0m
[32m[2023-12-21 12:53:24,432] [    INFO][0m - global step 22 / 266800, loss: 3.127437, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.90958 sec, avg_samples: 1.00000, ips: 1.09941 sequences/sec,  [0m
[32m[2023-12-21 12:53:25,352] [    INFO][0m - global step 23 / 266800, loss: 3.042346, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91918 sec, avg_samples: 1.00000, ips: 1.08793 sequences/sec,  [0m
[32m[2023-12-21 12:53:26,216] [    INFO][0m - global step 24 / 266800, loss: 2.773882, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.86357 sec, avg_samples: 1.00000, ips: 1.15798 sequences/sec,  [0m
[32m[2023-12-21 12:53:27,188] [    INFO][0m - global step 25 / 266800, loss: 2.172230, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.97165 sec, avg_samples: 1.00000, ips: 1.02917 sequences/sec,  [0m
[32m[2023-12-21 12:53:28,116] [    INFO][0m - global step 26 / 266800, loss: 2.479875, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92707 sec, avg_samples: 1.00000, ips: 1.07866 sequences/sec,  [0m
[32m[2023-12-21 12:53:29,111] [    INFO][0m - global step 27 / 266800, loss: 1.499913, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.99478 sec, avg_samples: 1.00000, ips: 1.00525 sequences/sec,  [0m
[32m[2023-12-21 12:53:30,052] [    INFO][0m - global step 28 / 266800, loss: 2.237146, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94050 sec, avg_samples: 1.00000, ips: 1.06327 sequences/sec,  [0m
[32m[2023-12-21 12:53:31,001] [    INFO][0m - global step 29 / 266800, loss: 2.986718, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.94851 sec, avg_samples: 1.00000, ips: 1.05429 sequences/sec,  [0m
[32m[2023-12-21 12:53:31,911] [    INFO][0m - global step 30 / 266800, loss: 2.875736, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.90995 sec, avg_samples: 1.00000, ips: 1.09896 sequences/sec,  [0m
[32m[2023-12-21 12:53:30,052] [    INFO][0m - global step 28 / 266800, loss: 2.237146, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94050 sec, avg_samples: 1.00000, ips: 1.06327 sequences/sec,  [0m
[32m[2023-12-21 12:53:31,001] [    INFO][0m - global step 29 / 266800, loss: 2.986718, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.94851 sec, avg_samples: 1.00000, ips: 1.05429 sequences/sec,  [0m
[32m[2023-12-21 12:53:31,911] [    INFO][0m - global step 30 / 266800, loss: 2.875736, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.90995 sec, avg_samples: 1.00000, ips: 1.09896 sequences/sec,  [0m
[32m[2023-12-21 12:53:32,905] [    INFO][0m - global step 31 / 266800, loss: 3.238215, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.99304 sec, avg_samples: 1.00000, ips: 1.00701 sequences/sec,  [0m
[32m[2023-12-21 12:53:33,963] [    INFO][0m - global step 32 / 266800, loss: 3.633930, avg_reader_cost: 0.00021 sec, avg_batch_cost: 1.05800 sec, avg_samples: 1.00000, ips: 0.94518 sequences/sec,  [0m
[32m[2023-12-21 12:53:34,922] [    INFO][0m - global step 33 / 266800, loss: 1.493340, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.95877 sec, avg_samples: 1.00000, ips: 1.04300 sequences/sec,  [0m
[32m[2023-12-21 12:53:35,868] [    INFO][0m - global step 34 / 266800, loss: 2.710376, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94545 sec, avg_samples: 1.00000, ips: 1.05770 sequences/sec,  [0m
[32m[2023-12-21 12:53:36,887] [    INFO][0m - global step 35 / 266800, loss: 2.392119, avg_reader_cost: 0.00018 sec, avg_batch_cost: 1.01848 sec, avg_samples: 1.00000, ips: 0.98185 sequences/sec,  [0m
[32m[2023-12-21 12:53:37,794] [    INFO][0m - global step 36 / 266800, loss: 3.010379, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.90640 sec, avg_samples: 1.00000, ips: 1.10326 sequences/sec,  [0m
[32m[2023-12-21 12:53:38,719] [    INFO][0m - global step 37 / 266800, loss: 1.927223, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92501 sec, avg_samples: 1.00000, ips: 1.08107 sequences/sec,  [0m
[32m[2023-12-21 12:53:39,613] [    INFO][0m - global step 38 / 266800, loss: 2.371315, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.89365 sec, avg_samples: 1.00000, ips: 1.11901 sequences/sec,  [0m
[32m[2023-12-21 12:53:40,532] [    INFO][0m - global step 39 / 266800, loss: 2.774712, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.91870 sec, avg_samples: 1.00000, ips: 1.08849 sequences/sec,  [0m
[32m[2023-12-21 12:53:41,484] [    INFO][0m - global step 40 / 266800, loss: 2.191054, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.95108 sec, avg_samples: 1.00000, ips: 1.05144 sequences/sec,  [0m
[32m[2023-12-21 12:53:42,397] [    INFO][0m - global step 41 / 266800, loss: 3.161422, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.91274 sec, avg_samples: 1.00000, ips: 1.09560 sequences/sec,  [0m
[32m[2023-12-21 12:53:43,254] [    INFO][0m - global step 42 / 266800, loss: 4.032553, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85673 sec, avg_samples: 1.00000, ips: 1.16723 sequences/sec,  [0m
[32m[2023-12-21 12:53:44,168] [    INFO][0m - global step 43 / 266800, loss: 1.911625, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91338 sec, avg_samples: 1.00000, ips: 1.09483 sequences/sec,  [0m
[32m[2023-12-21 12:53:45,122] [    INFO][0m - global step 44 / 266800, loss: 1.715610, avg_reader_cost: 0.00027 sec, avg_batch_cost: 0.95391 sec, avg_samples: 1.00000, ips: 1.04831 sequences/sec,  [0m
[32m[2023-12-21 12:53:46,065] [    INFO][0m - global step 45 / 266800, loss: 3.808938, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.94289 sec, avg_samples: 1.00000, ips: 1.06056 sequences/sec,  [0m
[32m[2023-12-21 12:53:46,899] [    INFO][0m - global step 46 / 266800, loss: 1.680510, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.83313 sec, avg_samples: 1.00000, ips: 1.20029 sequences/sec,  [0m
[32m[2023-12-21 12:53:47,763] [    INFO][0m - global step 47 / 266800, loss: 2.394172, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86354 sec, avg_samples: 1.00000, ips: 1.15803 sequences/sec,  [0m
[32m[2023-12-21 12:53:48,655] [    INFO][0m - global step 48 / 266800, loss: 3.072955, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.89151 sec, avg_samples: 1.00000, ips: 1.12170 sequences/sec,  [0m
[32m[2023-12-21 12:53:49,589] [    INFO][0m - global step 49 / 266800, loss: 3.231184, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.93370 sec, avg_samples: 1.00000, ips: 1.07100 sequences/sec,  [0m
[32m[2023-12-21 12:53:50,509] [    INFO][0m - global step 50 / 266800, loss: 2.987585, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92017 sec, avg_samples: 1.00000, ips: 1.08676 sequences/sec,  [0m
[32m[2023-12-21 12:53:51,457] [    INFO][0m - global step 51 / 266800, loss: 3.444610, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94717 sec, avg_samples: 1.00000, ips: 1.05577 sequences/sec,  [0m
[32m[2023-12-21 12:53:52,401] [    INFO][0m - global step 52 / 266800, loss: 2.331609, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94346 sec, avg_samples: 1.00000, ips: 1.05993 sequences/sec,  [0m
[32m[2023-12-21 12:53:53,367] [    INFO][0m - global step 53 / 266800, loss: 3.502417, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.96561 sec, avg_samples: 1.00000, ips: 1.03562 sequences/sec,  [0m
[32m[2023-12-21 12:53:54,279] [    INFO][0m - global step 54 / 266800, loss: 3.079322, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.91224 sec, avg_samples: 1.00000, ips: 1.09620 sequences/sec,  [0m
[32m[2023-12-21 12:53:55,195] [    INFO][0m - global step 55 / 266800, loss: 2.873717, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.91524 sec, avg_samples: 1.00000, ips: 1.09261 sequences/sec,  [0m
[32m[2023-12-21 12:53:56,104] [    INFO][0m - global step 56 / 266800, loss: 1.785558, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.90881 sec, avg_samples: 1.00000, ips: 1.10034 sequences/sec,  [0m
[32m[2023-12-21 12:53:57,040] [    INFO][0m - global step 57 / 266800, loss: 2.443457, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.93574 sec, avg_samples: 1.00000, ips: 1.06867 sequences/sec,  [0m
[32m[2023-12-21 12:53:57,943] [    INFO][0m - global step 58 / 266800, loss: 2.597062, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.90240 sec, avg_samples: 1.00000, ips: 1.10815 sequences/sec,  [0m
[32m[2023-12-21 12:53:58,883] [    INFO][0m - global step 59 / 266800, loss: 3.300249, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.93903 sec, avg_samples: 1.00000, ips: 1.06493 sequences/sec,  [0m
[32m[2023-12-21 12:53:59,736] [    INFO][0m - global step 60 / 266800, loss: 3.212204, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85308 sec, avg_samples: 1.00000, ips: 1.17223 sequences/sec,  [0m
[32m[2023-12-21 12:54:00,689] [    INFO][0m - global step 61 / 266800, loss: 3.028783, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.95225 sec, avg_samples: 1.00000, ips: 1.05014 sequences/sec,  [0m
[32m[2023-12-21 12:54:01,660] [    INFO][0m - global step 62 / 266800, loss: 1.843117, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.97098 sec, avg_samples: 1.00000, ips: 1.02989 sequences/sec,  [0m
[32m[2023-12-21 12:54:02,481] [    INFO][0m - global step 63 / 266800, loss: 2.615284, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.82092 sec, avg_samples: 1.00000, ips: 1.21814 sequences/sec,  [0m
[32m[2023-12-21 12:54:03,425] [    INFO][0m - global step 64 / 266800, loss: 3.025029, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.94295 sec, avg_samples: 1.00000, ips: 1.06050 sequences/sec,  [0m
[32m[2023-12-21 12:54:04,344] [    INFO][0m - global step 65 / 266800, loss: 3.197229, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.91904 sec, avg_samples: 1.00000, ips: 1.08809 sequences/sec,  [0m
[32m[2023-12-21 12:54:05,251] [    INFO][0m - global step 66 / 266800, loss: 3.508496, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90695 sec, avg_samples: 1.00000, ips: 1.10260 sequences/sec,  [0m
[32m[2023-12-21 12:54:06,160] [    INFO][0m - global step 67 / 266800, loss: 4.071703, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90820 sec, avg_samples: 1.00000, ips: 1.10108 sequences/sec,  [0m
[32m[2023-12-21 12:53:32,905] [    INFO][0m - global step 31 / 266800, loss: 3.238215, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.99304 sec, avg_samples: 1.00000, ips: 1.00701 sequences/sec,  [0m
[32m[2023-12-21 12:53:33,963] [    INFO][0m - global step 32 / 266800, loss: 3.633930, avg_reader_cost: 0.00021 sec, avg_batch_cost: 1.05800 sec, avg_samples: 1.00000, ips: 0.94518 sequences/sec,  [0m
[32m[2023-12-21 12:53:34,922] [    INFO][0m - global step 33 / 266800, loss: 1.493340, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.95877 sec, avg_samples: 1.00000, ips: 1.04300 sequences/sec,  [0m
[32m[2023-12-21 12:53:35,868] [    INFO][0m - global step 34 / 266800, loss: 2.710376, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94545 sec, avg_samples: 1.00000, ips: 1.05770 sequences/sec,  [0m
[32m[2023-12-21 12:53:36,887] [    INFO][0m - global step 35 / 266800, loss: 2.392119, avg_reader_cost: 0.00018 sec, avg_batch_cost: 1.01848 sec, avg_samples: 1.00000, ips: 0.98185 sequences/sec,  [0m
[32m[2023-12-21 12:53:37,794] [    INFO][0m - global step 36 / 266800, loss: 3.010379, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.90640 sec, avg_samples: 1.00000, ips: 1.10326 sequences/sec,  [0m
[32m[2023-12-21 12:53:38,719] [    INFO][0m - global step 37 / 266800, loss: 1.927223, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92501 sec, avg_samples: 1.00000, ips: 1.08107 sequences/sec,  [0m
[32m[2023-12-21 12:53:39,613] [    INFO][0m - global step 38 / 266800, loss: 2.371315, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.89365 sec, avg_samples: 1.00000, ips: 1.11901 sequences/sec,  [0m
[32m[2023-12-21 12:53:40,532] [    INFO][0m - global step 39 / 266800, loss: 2.774712, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.91870 sec, avg_samples: 1.00000, ips: 1.08849 sequences/sec,  [0m
[32m[2023-12-21 12:53:41,484] [    INFO][0m - global step 40 / 266800, loss: 2.191054, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.95108 sec, avg_samples: 1.00000, ips: 1.05144 sequences/sec,  [0m
[32m[2023-12-21 12:53:42,397] [    INFO][0m - global step 41 / 266800, loss: 3.161422, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.91274 sec, avg_samples: 1.00000, ips: 1.09560 sequences/sec,  [0m
[32m[2023-12-21 12:53:43,254] [    INFO][0m - global step 42 / 266800, loss: 4.032553, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85673 sec, avg_samples: 1.00000, ips: 1.16723 sequences/sec,  [0m
[32m[2023-12-21 12:53:44,168] [    INFO][0m - global step 43 / 266800, loss: 1.911625, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91338 sec, avg_samples: 1.00000, ips: 1.09483 sequences/sec,  [0m
[32m[2023-12-21 12:53:45,122] [    INFO][0m - global step 44 / 266800, loss: 1.715610, avg_reader_cost: 0.00027 sec, avg_batch_cost: 0.95391 sec, avg_samples: 1.00000, ips: 1.04831 sequences/sec,  [0m
[32m[2023-12-21 12:53:46,065] [    INFO][0m - global step 45 / 266800, loss: 3.808938, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.94289 sec, avg_samples: 1.00000, ips: 1.06056 sequences/sec,  [0m
[32m[2023-12-21 12:53:46,899] [    INFO][0m - global step 46 / 266800, loss: 1.680510, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.83313 sec, avg_samples: 1.00000, ips: 1.20029 sequences/sec,  [0m
[32m[2023-12-21 12:53:47,763] [    INFO][0m - global step 47 / 266800, loss: 2.394172, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86354 sec, avg_samples: 1.00000, ips: 1.15803 sequences/sec,  [0m
[32m[2023-12-21 12:53:48,655] [    INFO][0m - global step 48 / 266800, loss: 3.072955, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.89151 sec, avg_samples: 1.00000, ips: 1.12170 sequences/sec,  [0m
[32m[2023-12-21 12:53:49,589] [    INFO][0m - global step 49 / 266800, loss: 3.231184, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.93370 sec, avg_samples: 1.00000, ips: 1.07100 sequences/sec,  [0m
[32m[2023-12-21 12:53:50,509] [    INFO][0m - global step 50 / 266800, loss: 2.987585, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92017 sec, avg_samples: 1.00000, ips: 1.08676 sequences/sec,  [0m
[32m[2023-12-21 12:53:51,457] [    INFO][0m - global step 51 / 266800, loss: 3.444610, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94717 sec, avg_samples: 1.00000, ips: 1.05577 sequences/sec,  [0m
[32m[2023-12-21 12:53:52,401] [    INFO][0m - global step 52 / 266800, loss: 2.331609, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.94346 sec, avg_samples: 1.00000, ips: 1.05993 sequences/sec,  [0m
[32m[2023-12-21 12:53:53,367] [    INFO][0m - global step 53 / 266800, loss: 3.502417, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.96561 sec, avg_samples: 1.00000, ips: 1.03562 sequences/sec,  [0m
[32m[2023-12-21 12:53:54,279] [    INFO][0m - global step 54 / 266800, loss: 3.079322, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.91224 sec, avg_samples: 1.00000, ips: 1.09620 sequences/sec,  [0m
[32m[2023-12-21 12:53:55,195] [    INFO][0m - global step 55 / 266800, loss: 2.873717, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.91524 sec, avg_samples: 1.00000, ips: 1.09261 sequences/sec,  [0m
[32m[2023-12-21 12:53:56,104] [    INFO][0m - global step 56 / 266800, loss: 1.785558, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.90881 sec, avg_samples: 1.00000, ips: 1.10034 sequences/sec,  [0m
[32m[2023-12-21 12:53:57,040] [    INFO][0m - global step 57 / 266800, loss: 2.443457, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.93574 sec, avg_samples: 1.00000, ips: 1.06867 sequences/sec,  [0m
[32m[2023-12-21 12:53:57,943] [    INFO][0m - global step 58 / 266800, loss: 2.597062, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.90240 sec, avg_samples: 1.00000, ips: 1.10815 sequences/sec,  [0m
[32m[2023-12-21 12:53:58,883] [    INFO][0m - global step 59 / 266800, loss: 3.300249, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.93903 sec, avg_samples: 1.00000, ips: 1.06493 sequences/sec,  [0m
[32m[2023-12-21 12:53:59,736] [    INFO][0m - global step 60 / 266800, loss: 3.212204, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85308 sec, avg_samples: 1.00000, ips: 1.17223 sequences/sec,  [0m
[32m[2023-12-21 12:54:00,689] [    INFO][0m - global step 61 / 266800, loss: 3.028783, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.95225 sec, avg_samples: 1.00000, ips: 1.05014 sequences/sec,  [0m
[32m[2023-12-21 12:54:01,660] [    INFO][0m - global step 62 / 266800, loss: 1.843117, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.97098 sec, avg_samples: 1.00000, ips: 1.02989 sequences/sec,  [0m
[32m[2023-12-21 12:54:02,481] [    INFO][0m - global step 63 / 266800, loss: 2.615284, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.82092 sec, avg_samples: 1.00000, ips: 1.21814 sequences/sec,  [0m
[32m[2023-12-21 12:54:03,425] [    INFO][0m - global step 64 / 266800, loss: 3.025029, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.94295 sec, avg_samples: 1.00000, ips: 1.06050 sequences/sec,  [0m
[32m[2023-12-21 12:54:04,344] [    INFO][0m - global step 65 / 266800, loss: 3.197229, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.91904 sec, avg_samples: 1.00000, ips: 1.08809 sequences/sec,  [0m
[32m[2023-12-21 12:54:05,251] [    INFO][0m - global step 66 / 266800, loss: 3.508496, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90695 sec, avg_samples: 1.00000, ips: 1.10260 sequences/sec,  [0m
[32m[2023-12-21 12:54:06,160] [    INFO][0m - global step 67 / 266800, loss: 4.071703, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90820 sec, avg_samples: 1.00000, ips: 1.10108 sequences/sec,  [0m
[32m[2023-12-21 12:54:07,073] [    INFO][0m - global step 68 / 266800, loss: 3.009023, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91298 sec, avg_samples: 1.00000, ips: 1.09531 sequences/sec,  [0m
[32m[2023-12-21 12:54:08,061] [    INFO][0m - global step 69 / 266800, loss: 2.045416, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.98749 sec, avg_samples: 1.00000, ips: 1.01266 sequences/sec,  [0m
[32m[2023-12-21 12:54:09,054] [    INFO][0m - global step 70 / 266800, loss: 2.436188, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.99241 sec, avg_samples: 1.00000, ips: 1.00765 sequences/sec,  [0m
[32m[2023-12-21 12:54:07,073] [    INFO][0m - global step 68 / 266800, loss: 3.009023, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91298 sec, avg_samples: 1.00000, ips: 1.09531 sequences/sec,  [0m
[32m[2023-12-21 12:54:08,061] [    INFO][0m - global step 69 / 266800, loss: 2.045416, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.98749 sec, avg_samples: 1.00000, ips: 1.01266 sequences/sec,  [0m
[32m[2023-12-21 12:54:09,054] [    INFO][0m - global step 70 / 266800, loss: 2.436188, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.99241 sec, avg_samples: 1.00000, ips: 1.00765 sequences/sec,  [0m
[32m[2023-12-21 12:54:09,906] [    INFO][0m - global step 71 / 266800, loss: 2.240128, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85124 sec, avg_samples: 1.00000, ips: 1.17476 sequences/sec,  [0m
[32m[2023-12-21 12:54:10,717] [    INFO][0m - global step 72 / 266800, loss: 3.561303, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.81120 sec, avg_samples: 1.00000, ips: 1.23275 sequences/sec,  [0m
[32m[2023-12-21 12:54:11,597] [    INFO][0m - global step 73 / 266800, loss: 3.369842, avg_reader_cost: 0.00027 sec, avg_batch_cost: 0.87897 sec, avg_samples: 1.00000, ips: 1.13769 sequences/sec,  [0m
[32m[2023-12-21 12:54:12,446] [    INFO][0m - global step 74 / 266800, loss: 2.441949, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.84915 sec, avg_samples: 1.00000, ips: 1.17765 sequences/sec,  [0m
[32m[2023-12-21 12:54:13,289] [    INFO][0m - global step 75 / 266800, loss: 2.298536, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.84234 sec, avg_samples: 1.00000, ips: 1.18717 sequences/sec,  [0m
[32m[2023-12-21 12:54:14,197] [    INFO][0m - global step 76 / 266800, loss: 2.124814, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.90762 sec, avg_samples: 1.00000, ips: 1.10179 sequences/sec,  [0m
[32m[2023-12-21 12:54:15,054] [    INFO][0m - global step 77 / 266800, loss: 3.637317, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85718 sec, avg_samples: 1.00000, ips: 1.16661 sequences/sec,  [0m
[32m[2023-12-21 12:54:15,887] [    INFO][0m - global step 78 / 266800, loss: 3.018670, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.83218 sec, avg_samples: 1.00000, ips: 1.20167 sequences/sec,  [0m
[32m[2023-12-21 12:54:16,819] [    INFO][0m - global step 79 / 266800, loss: 3.254772, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.93137 sec, avg_samples: 1.00000, ips: 1.07369 sequences/sec,  [0m
[32m[2023-12-21 12:54:17,690] [    INFO][0m - global step 80 / 266800, loss: 3.296791, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.87070 sec, avg_samples: 1.00000, ips: 1.14850 sequences/sec,  [0m
[32m[2023-12-21 12:54:18,586] [    INFO][0m - global step 81 / 266800, loss: 2.779932, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.89580 sec, avg_samples: 1.00000, ips: 1.11632 sequences/sec,  [0m
[32m[2023-12-21 12:54:19,509] [    INFO][0m - global step 82 / 266800, loss: 1.995614, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92298 sec, avg_samples: 1.00000, ips: 1.08345 sequences/sec,  [0m
[32m[2023-12-21 12:54:20,374] [    INFO][0m - global step 83 / 266800, loss: 3.224613, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.86473 sec, avg_samples: 1.00000, ips: 1.15642 sequences/sec,  [0m
[32m[2023-12-21 12:54:21,244] [    INFO][0m - global step 84 / 266800, loss: 2.564518, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86892 sec, avg_samples: 1.00000, ips: 1.15086 sequences/sec,  [0m
[32m[2023-12-21 12:54:22,082] [    INFO][0m - global step 85 / 266800, loss: 3.423881, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.83795 sec, avg_samples: 1.00000, ips: 1.19339 sequences/sec,  [0m
[32m[2023-12-21 12:54:22,928] [    INFO][0m - global step 86 / 266800, loss: 3.431463, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.84527 sec, avg_samples: 1.00000, ips: 1.18305 sequences/sec,  [0m
[32m[2023-12-21 12:54:23,794] [    INFO][0m - global step 87 / 266800, loss: 2.847965, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.86631 sec, avg_samples: 1.00000, ips: 1.15432 sequences/sec,  [0m
[32m[2023-12-21 12:54:24,678] [    INFO][0m - global step 88 / 266800, loss: 2.559221, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.88308 sec, avg_samples: 1.00000, ips: 1.13240 sequences/sec,  [0m
[32m[2023-12-21 12:54:25,533] [    INFO][0m - global step 89 / 266800, loss: 3.219306, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85519 sec, avg_samples: 1.00000, ips: 1.16933 sequences/sec,  [0m
[32m[2023-12-21 12:54:26,429] [    INFO][0m - global step 90 / 266800, loss: 2.644236, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.89534 sec, avg_samples: 1.00000, ips: 1.11690 sequences/sec,  [0m
[32m[2023-12-21 12:54:27,226] [    INFO][0m - global step 91 / 266800, loss: 2.696340, avg_reader_cost: 0.00024 sec, avg_batch_cost: 0.79586 sec, avg_samples: 1.00000, ips: 1.25650 sequences/sec,  [0m
[32m[2023-12-21 12:54:28,147] [    INFO][0m - global step 92 / 266800, loss: 2.461233, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.92091 sec, avg_samples: 1.00000, ips: 1.08588 sequences/sec,  [0m
[32m[2023-12-21 12:54:29,072] [    INFO][0m - global step 93 / 266800, loss: 1.942603, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92483 sec, avg_samples: 1.00000, ips: 1.08127 sequences/sec,  [0m
[32m[2023-12-21 12:54:29,992] [    INFO][0m - global step 94 / 266800, loss: 1.968585, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91950 sec, avg_samples: 1.00000, ips: 1.08755 sequences/sec,  [0m
[32m[2023-12-21 12:54:30,870] [    INFO][0m - global step 95 / 266800, loss: 3.603681, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.87799 sec, avg_samples: 1.00000, ips: 1.13896 sequences/sec,  [0m
[32m[2023-12-21 12:54:31,760] [    INFO][0m - global step 96 / 266800, loss: 2.356632, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.88897 sec, avg_samples: 1.00000, ips: 1.12490 sequences/sec,  [0m
[32m[2023-12-21 12:54:32,561] [    INFO][0m - global step 97 / 266800, loss: 2.749433, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.80045 sec, avg_samples: 1.00000, ips: 1.24929 sequences/sec,  [0m
[32m[2023-12-21 12:54:33,484] [    INFO][0m - global step 98 / 266800, loss: 3.028171, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.92342 sec, avg_samples: 1.00000, ips: 1.08293 sequences/sec,  [0m
[32m[2023-12-21 12:54:34,288] [    INFO][0m - global step 99 / 266800, loss: 2.734200, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.80362 sec, avg_samples: 1.00000, ips: 1.24436 sequences/sec,  [0m
[32m[2023-12-21 12:54:35,206] [    INFO][0m - global step 100 / 266800, loss: 3.132534, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91705 sec, avg_samples: 1.00000, ips: 1.09045 sequences/sec,  [0m
[32m[2023-12-21 12:54:36,065] [    INFO][0m - global step 101 / 266800, loss: 3.297266, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85871 sec, avg_samples: 1.00000, ips: 1.16453 sequences/sec,  [0m
[32m[2023-12-21 12:54:36,865] [    INFO][0m - global step 102 / 266800, loss: 2.506199, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.80024 sec, avg_samples: 1.00000, ips: 1.24963 sequences/sec,  [0m
[32m[2023-12-21 12:54:37,727] [    INFO][0m - global step 103 / 266800, loss: 2.571039, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.86122 sec, avg_samples: 1.00000, ips: 1.16114 sequences/sec,  [0m
[32m[2023-12-21 12:54:38,548] [    INFO][0m - global step 104 / 266800, loss: 2.518310, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82034 sec, avg_samples: 1.00000, ips: 1.21900 sequences/sec,  [0m
[32m[2023-12-21 12:54:39,346] [    INFO][0m - global step 105 / 266800, loss: 3.098822, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.79805 sec, avg_samples: 1.00000, ips: 1.25305 sequences/sec,  [0m
[32m[2023-12-21 12:54:40,271] [    INFO][0m - global step 106 / 266800, loss: 1.963432, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92421 sec, avg_samples: 1.00000, ips: 1.08201 sequences/sec,  [0m
[32m[2023-12-21 12:54:41,132] [    INFO][0m - global step 107 / 266800, loss: 2.352144, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.86120 sec, avg_samples: 1.00000, ips: 1.16118 sequences/sec,  [0m
[32m[2023-12-21 12:54:09,906] [    INFO][0m - global step 71 / 266800, loss: 2.240128, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85124 sec, avg_samples: 1.00000, ips: 1.17476 sequences/sec,  [0m
[32m[2023-12-21 12:54:10,717] [    INFO][0m - global step 72 / 266800, loss: 3.561303, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.81120 sec, avg_samples: 1.00000, ips: 1.23275 sequences/sec,  [0m
[32m[2023-12-21 12:54:11,597] [    INFO][0m - global step 73 / 266800, loss: 3.369842, avg_reader_cost: 0.00027 sec, avg_batch_cost: 0.87897 sec, avg_samples: 1.00000, ips: 1.13769 sequences/sec,  [0m
[32m[2023-12-21 12:54:12,446] [    INFO][0m - global step 74 / 266800, loss: 2.441949, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.84915 sec, avg_samples: 1.00000, ips: 1.17765 sequences/sec,  [0m
[32m[2023-12-21 12:54:13,289] [    INFO][0m - global step 75 / 266800, loss: 2.298536, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.84234 sec, avg_samples: 1.00000, ips: 1.18717 sequences/sec,  [0m
[32m[2023-12-21 12:54:14,197] [    INFO][0m - global step 76 / 266800, loss: 2.124814, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.90762 sec, avg_samples: 1.00000, ips: 1.10179 sequences/sec,  [0m
[32m[2023-12-21 12:54:15,054] [    INFO][0m - global step 77 / 266800, loss: 3.637317, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85718 sec, avg_samples: 1.00000, ips: 1.16661 sequences/sec,  [0m
[32m[2023-12-21 12:54:15,887] [    INFO][0m - global step 78 / 266800, loss: 3.018670, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.83218 sec, avg_samples: 1.00000, ips: 1.20167 sequences/sec,  [0m
[32m[2023-12-21 12:54:16,819] [    INFO][0m - global step 79 / 266800, loss: 3.254772, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.93137 sec, avg_samples: 1.00000, ips: 1.07369 sequences/sec,  [0m
[32m[2023-12-21 12:54:17,690] [    INFO][0m - global step 80 / 266800, loss: 3.296791, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.87070 sec, avg_samples: 1.00000, ips: 1.14850 sequences/sec,  [0m
[32m[2023-12-21 12:54:18,586] [    INFO][0m - global step 81 / 266800, loss: 2.779932, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.89580 sec, avg_samples: 1.00000, ips: 1.11632 sequences/sec,  [0m
[32m[2023-12-21 12:54:19,509] [    INFO][0m - global step 82 / 266800, loss: 1.995614, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92298 sec, avg_samples: 1.00000, ips: 1.08345 sequences/sec,  [0m
[32m[2023-12-21 12:54:20,374] [    INFO][0m - global step 83 / 266800, loss: 3.224613, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.86473 sec, avg_samples: 1.00000, ips: 1.15642 sequences/sec,  [0m
[32m[2023-12-21 12:54:21,244] [    INFO][0m - global step 84 / 266800, loss: 2.564518, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86892 sec, avg_samples: 1.00000, ips: 1.15086 sequences/sec,  [0m
[32m[2023-12-21 12:54:22,082] [    INFO][0m - global step 85 / 266800, loss: 3.423881, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.83795 sec, avg_samples: 1.00000, ips: 1.19339 sequences/sec,  [0m
[32m[2023-12-21 12:54:22,928] [    INFO][0m - global step 86 / 266800, loss: 3.431463, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.84527 sec, avg_samples: 1.00000, ips: 1.18305 sequences/sec,  [0m
[32m[2023-12-21 12:54:23,794] [    INFO][0m - global step 87 / 266800, loss: 2.847965, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.86631 sec, avg_samples: 1.00000, ips: 1.15432 sequences/sec,  [0m
[32m[2023-12-21 12:54:24,678] [    INFO][0m - global step 88 / 266800, loss: 2.559221, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.88308 sec, avg_samples: 1.00000, ips: 1.13240 sequences/sec,  [0m
[32m[2023-12-21 12:54:25,533] [    INFO][0m - global step 89 / 266800, loss: 3.219306, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85519 sec, avg_samples: 1.00000, ips: 1.16933 sequences/sec,  [0m
[32m[2023-12-21 12:54:26,429] [    INFO][0m - global step 90 / 266800, loss: 2.644236, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.89534 sec, avg_samples: 1.00000, ips: 1.11690 sequences/sec,  [0m
[32m[2023-12-21 12:54:27,226] [    INFO][0m - global step 91 / 266800, loss: 2.696340, avg_reader_cost: 0.00024 sec, avg_batch_cost: 0.79586 sec, avg_samples: 1.00000, ips: 1.25650 sequences/sec,  [0m
[32m[2023-12-21 12:54:28,147] [    INFO][0m - global step 92 / 266800, loss: 2.461233, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.92091 sec, avg_samples: 1.00000, ips: 1.08588 sequences/sec,  [0m
[32m[2023-12-21 12:54:29,072] [    INFO][0m - global step 93 / 266800, loss: 1.942603, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92483 sec, avg_samples: 1.00000, ips: 1.08127 sequences/sec,  [0m
[32m[2023-12-21 12:54:29,992] [    INFO][0m - global step 94 / 266800, loss: 1.968585, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91950 sec, avg_samples: 1.00000, ips: 1.08755 sequences/sec,  [0m
[32m[2023-12-21 12:54:30,870] [    INFO][0m - global step 95 / 266800, loss: 3.603681, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.87799 sec, avg_samples: 1.00000, ips: 1.13896 sequences/sec,  [0m
[32m[2023-12-21 12:54:31,760] [    INFO][0m - global step 96 / 266800, loss: 2.356632, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.88897 sec, avg_samples: 1.00000, ips: 1.12490 sequences/sec,  [0m
[32m[2023-12-21 12:54:32,561] [    INFO][0m - global step 97 / 266800, loss: 2.749433, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.80045 sec, avg_samples: 1.00000, ips: 1.24929 sequences/sec,  [0m
[32m[2023-12-21 12:54:33,484] [    INFO][0m - global step 98 / 266800, loss: 3.028171, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.92342 sec, avg_samples: 1.00000, ips: 1.08293 sequences/sec,  [0m
[32m[2023-12-21 12:54:34,288] [    INFO][0m - global step 99 / 266800, loss: 2.734200, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.80362 sec, avg_samples: 1.00000, ips: 1.24436 sequences/sec,  [0m
[32m[2023-12-21 12:54:35,206] [    INFO][0m - global step 100 / 266800, loss: 3.132534, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91705 sec, avg_samples: 1.00000, ips: 1.09045 sequences/sec,  [0m
[32m[2023-12-21 12:54:36,065] [    INFO][0m - global step 101 / 266800, loss: 3.297266, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85871 sec, avg_samples: 1.00000, ips: 1.16453 sequences/sec,  [0m
[32m[2023-12-21 12:54:36,865] [    INFO][0m - global step 102 / 266800, loss: 2.506199, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.80024 sec, avg_samples: 1.00000, ips: 1.24963 sequences/sec,  [0m
[32m[2023-12-21 12:54:37,727] [    INFO][0m - global step 103 / 266800, loss: 2.571039, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.86122 sec, avg_samples: 1.00000, ips: 1.16114 sequences/sec,  [0m
[32m[2023-12-21 12:54:38,548] [    INFO][0m - global step 104 / 266800, loss: 2.518310, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82034 sec, avg_samples: 1.00000, ips: 1.21900 sequences/sec,  [0m
[32m[2023-12-21 12:54:39,346] [    INFO][0m - global step 105 / 266800, loss: 3.098822, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.79805 sec, avg_samples: 1.00000, ips: 1.25305 sequences/sec,  [0m
[32m[2023-12-21 12:54:40,271] [    INFO][0m - global step 106 / 266800, loss: 1.963432, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92421 sec, avg_samples: 1.00000, ips: 1.08201 sequences/sec,  [0m
[32m[2023-12-21 12:54:41,132] [    INFO][0m - global step 107 / 266800, loss: 2.352144, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.86120 sec, avg_samples: 1.00000, ips: 1.16118 sequences/sec,  [0m
[32m[2023-12-21 12:54:41,992] [    INFO][0m - global step 108 / 266800, loss: 2.282795, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85903 sec, avg_samples: 1.00000, ips: 1.16410 sequences/sec,  [0m
[32m[2023-12-21 12:54:42,873] [    INFO][0m - global step 109 / 266800, loss: 3.455350, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.88076 sec, avg_samples: 1.00000, ips: 1.13539 sequences/sec,  [0m
[32m[2023-12-21 12:54:43,739] [    INFO][0m - global step 110 / 266800, loss: 2.760070, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.86619 sec, avg_samples: 1.00000, ips: 1.15449 sequences/sec,  [0m
[32m[2023-12-21 12:54:41,992] [    INFO][0m - global step 108 / 266800, loss: 2.282795, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85903 sec, avg_samples: 1.00000, ips: 1.16410 sequences/sec,  [0m
[32m[2023-12-21 12:54:42,873] [    INFO][0m - global step 109 / 266800, loss: 3.455350, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.88076 sec, avg_samples: 1.00000, ips: 1.13539 sequences/sec,  [0m
[32m[2023-12-21 12:54:43,739] [    INFO][0m - global step 110 / 266800, loss: 2.760070, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.86619 sec, avg_samples: 1.00000, ips: 1.15449 sequences/sec,  [0m
[32m[2023-12-21 12:54:44,550] [    INFO][0m - global step 111 / 266800, loss: 4.293671, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.81076 sec, avg_samples: 1.00000, ips: 1.23340 sequences/sec,  [0m
[32m[2023-12-21 12:54:45,410] [    INFO][0m - global step 112 / 266800, loss: 1.922006, avg_reader_cost: 0.00025 sec, avg_batch_cost: 0.85905 sec, avg_samples: 1.00000, ips: 1.16407 sequences/sec,  [0m
[32m[2023-12-21 12:54:46,237] [    INFO][0m - global step 113 / 266800, loss: 2.240932, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82684 sec, avg_samples: 1.00000, ips: 1.20942 sequences/sec,  [0m
[32m[2023-12-21 12:54:47,069] [    INFO][0m - global step 114 / 266800, loss: 2.504665, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.83161 sec, avg_samples: 1.00000, ips: 1.20248 sequences/sec,  [0m
[32m[2023-12-21 12:54:47,935] [    INFO][0m - global step 115 / 266800, loss: 2.740645, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.86582 sec, avg_samples: 1.00000, ips: 1.15498 sequences/sec,  [0m
[32m[2023-12-21 12:54:49,008] [    INFO][0m - global step 116 / 266800, loss: 2.150469, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.07192 sec, avg_samples: 1.00000, ips: 0.93291 sequences/sec,  [0m
[32m[2023-12-21 12:54:49,920] [    INFO][0m - global step 117 / 266800, loss: 2.610155, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.91136 sec, avg_samples: 1.00000, ips: 1.09726 sequences/sec,  [0m
[32m[2023-12-21 12:54:50,830] [    INFO][0m - global step 118 / 266800, loss: 1.903424, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90993 sec, avg_samples: 1.00000, ips: 1.09899 sequences/sec,  [0m
[32m[2023-12-21 12:54:51,709] [    INFO][0m - global step 119 / 266800, loss: 2.981430, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.87897 sec, avg_samples: 1.00000, ips: 1.13769 sequences/sec,  [0m
[32m[2023-12-21 12:54:52,587] [    INFO][0m - global step 120 / 266800, loss: 2.056500, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.87791 sec, avg_samples: 1.00000, ips: 1.13907 sequences/sec,  [0m
[32m[2023-12-21 12:54:53,514] [    INFO][0m - global step 121 / 266800, loss: 2.837184, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92594 sec, avg_samples: 1.00000, ips: 1.07998 sequences/sec,  [0m
[32m[2023-12-21 12:54:54,411] [    INFO][0m - global step 122 / 266800, loss: 2.466178, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.89667 sec, avg_samples: 1.00000, ips: 1.11524 sequences/sec,  [0m
[32m[2023-12-21 12:54:55,338] [    INFO][0m - global step 123 / 266800, loss: 2.578913, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92641 sec, avg_samples: 1.00000, ips: 1.07944 sequences/sec,  [0m
[32m[2023-12-21 12:54:56,255] [    INFO][0m - global step 124 / 266800, loss: 2.906804, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91736 sec, avg_samples: 1.00000, ips: 1.09009 sequences/sec,  [0m
[32m[2023-12-21 12:54:57,105] [    INFO][0m - global step 125 / 266800, loss: 2.511492, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.84951 sec, avg_samples: 1.00000, ips: 1.17715 sequences/sec,  [0m
[32m[2023-12-21 12:54:57,999] [    INFO][0m - global step 126 / 266800, loss: 2.435269, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.89298 sec, avg_samples: 1.00000, ips: 1.11984 sequences/sec,  [0m
[32m[2023-12-21 12:54:58,849] [    INFO][0m - global step 127 / 266800, loss: 2.848229, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.84971 sec, avg_samples: 1.00000, ips: 1.17687 sequences/sec,  [0m
[32m[2023-12-21 12:54:59,787] [    INFO][0m - global step 128 / 266800, loss: 3.303391, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.93823 sec, avg_samples: 1.00000, ips: 1.06584 sequences/sec,  [0m
[32m[2023-12-21 12:55:00,640] [    INFO][0m - global step 129 / 266800, loss: 2.604638, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85249 sec, avg_samples: 1.00000, ips: 1.17303 sequences/sec,  [0m
[32m[2023-12-21 12:55:01,595] [    INFO][0m - global step 130 / 266800, loss: 2.308934, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.95487 sec, avg_samples: 1.00000, ips: 1.04726 sequences/sec,  [0m
[32m[2023-12-21 12:55:02,472] [    INFO][0m - global step 131 / 266800, loss: 1.772274, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.87631 sec, avg_samples: 1.00000, ips: 1.14115 sequences/sec,  [0m
[32m[2023-12-21 12:55:03,368] [    INFO][0m - global step 132 / 266800, loss: 2.264315, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.89529 sec, avg_samples: 1.00000, ips: 1.11696 sequences/sec,  [0m
[32m[2023-12-21 12:55:04,208] [    INFO][0m - global step 133 / 266800, loss: 2.714500, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.84029 sec, avg_samples: 1.00000, ips: 1.19007 sequences/sec,  [0m
[32m[2023-12-21 12:55:05,123] [    INFO][0m - global step 134 / 266800, loss: 2.812063, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.91440 sec, avg_samples: 1.00000, ips: 1.09361 sequences/sec,  [0m
[32m[2023-12-21 12:55:05,985] [    INFO][0m - global step 135 / 266800, loss: 4.126012, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.86149 sec, avg_samples: 1.00000, ips: 1.16078 sequences/sec,  [0m
[32m[2023-12-21 12:55:06,953] [    INFO][0m - global step 136 / 266800, loss: 2.003836, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.96785 sec, avg_samples: 1.00000, ips: 1.03322 sequences/sec,  [0m
[32m[2023-12-21 12:55:07,867] [    INFO][0m - global step 137 / 266800, loss: 2.398470, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91364 sec, avg_samples: 1.00000, ips: 1.09452 sequences/sec,  [0m
[32m[2023-12-21 12:55:08,790] [    INFO][0m - global step 138 / 266800, loss: 2.572164, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92203 sec, avg_samples: 1.00000, ips: 1.08456 sequences/sec,  [0m
[32m[2023-12-21 12:55:09,657] [    INFO][0m - global step 139 / 266800, loss: 2.338915, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86691 sec, avg_samples: 1.00000, ips: 1.15352 sequences/sec,  [0m
[32m[2023-12-21 12:55:10,617] [    INFO][0m - global step 140 / 266800, loss: 1.970525, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.95983 sec, avg_samples: 1.00000, ips: 1.04186 sequences/sec,  [0m
[32m[2023-12-21 12:55:11,510] [    INFO][0m - global step 141 / 266800, loss: 1.856880, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.89205 sec, avg_samples: 1.00000, ips: 1.12101 sequences/sec,  [0m
[32m[2023-12-21 12:55:12,374] [    INFO][0m - global step 142 / 266800, loss: 2.485156, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.86345 sec, avg_samples: 1.00000, ips: 1.15814 sequences/sec,  [0m
[32m[2023-12-21 12:55:13,243] [    INFO][0m - global step 143 / 266800, loss: 2.306528, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86925 sec, avg_samples: 1.00000, ips: 1.15041 sequences/sec,  [0m
[32m[2023-12-21 12:55:14,094] [    INFO][0m - global step 144 / 266800, loss: 2.971000, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85063 sec, avg_samples: 1.00000, ips: 1.17560 sequences/sec,  [0m
[32m[2023-12-21 12:55:14,917] [    INFO][0m - global step 145 / 266800, loss: 2.285067, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82265 sec, avg_samples: 1.00000, ips: 1.21559 sequences/sec,  [0m
[32m[2023-12-21 12:55:15,804] [    INFO][0m - global step 146 / 266800, loss: 2.943288, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.88666 sec, avg_samples: 1.00000, ips: 1.12783 sequences/sec,  [0m
LAUNCH INFO 2023-12-21 12:55:42,772 Pod completed
LAUNCH INFO 2023-12-21 12:55:42,772 Exit code 0
[32m[2023-12-21 12:55:16,714] [    INFO][0m - global step 147 / 266800, loss: 3.046579, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90963 sec, avg_samples: 1.00000, ips: 1.09935 sequences/sec,  [0m
[32m[2023-12-21 12:55:17,576] [    INFO][0m - global step 148 / 266800, loss: 2.143323, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86101 sec, avg_samples: 1.00000, ips: 1.16142 sequences/sec,  [0m
[32m[2023-12-21 12:55:18,390] [    INFO][0m - global step 149 / 266800, loss: 2.377167, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.81352 sec, avg_samples: 1.00000, ips: 1.22923 sequences/sec,  [0m
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - ernie_tiny - python3.9 -m paddle.distributed.launch --gpus=0,1 test_tipc/train.py --amp_level O2 --model ernie_tiny --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path ernie-tiny --pad_to_max_seq_len --max_seq_len 128 --logging_steps 1 --task_name tnews --max_steps 150 --device=xpu --save_model=/workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1             - /workspace/PaddleNLP/tests/test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log [0m
+ watchcat=4375
+ kill -9 5099
+ sleep 10
==END==test_tipc/configs/ernie_tiny/train_infer_python.txt
run.sh: line 334:  5099 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/ernie_tiny/train_infer_python.txt
++ date +%s
+ end=1703134552
++ echo 1703134044 1703134552
++ awk '{print $2-$1-2}'
+ time=506
+ echo 'test_tipc/configs/ernie_tiny/train_infer_python.txt spend time seconds 506'
+ read config_file
test_tipc/configs/ernie_tiny/train_infer_python.txt spend time seconds 506
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703134552
==START==test_tipc/configs/seq2seq/train_infer_python.txt
+ echo ==START==test_tipc/configs/seq2seq/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/seq2seq/train_infer_python.txt
+ dataline='===========================train_params===========================
model_name:seq2seq
python:python3.7
gpu_list:0|0,1
--device:gpu|gpu
null:null
--epoch:null
--save_model:./test_tipc/output/
--batch_size:null
null:null
null:null
null:null
null:null
##
trainer:norm_train
norm_train:test_tipc/train.py --model seq2seq --optimizer adam --max_seq_len 50 --learning_rate 0.001 --max_grad_norm 5.0 --max_steps 150
null:null
null:null
null:null
null:null
null:null
##
===========================eval_params=========================== 
null:null
null:null
##
===========================infer_params===========================
null:null
null:null
null:null
null:null
null:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:seq2seq
++ strs=model_name:seq2seq
++ IFS=:
++ array=(${strs})
++ tmp=seq2seq
++ echo seq2seq
+ model_name=seq2seq
+ sleep 10
+ run run_model test_tipc/configs/seq2seq/train_infer_python.txt lite_train_lite_infer 3600 seq2seq
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ waitfor=7200
+ command='run_model
test_tipc/configs/seq2seq/train_infer_python.txt
lite_train_lite_infer
3600
seq2seq'
+ commandpid=5404
+ run_model test_tipc/configs/seq2seq/train_infer_python.txt lite_train_lite_infer 3600 seq2seq
+ config_file=test_tipc/configs/seq2seq/train_infer_python.txt
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/seq2seq/train_infer_python.txt lite_train_lite_infer
+ watchdog=5405
+ wait 5404
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/seq2seq/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/seq2seq/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='seq2seq', logging_steps=10, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1', batch_size=1, max_seq_len=50, data_dir=None, pad_to_max_seq_len=False, optimizer='adam', learning_rate=0.001, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=5.0, num_layers=2, hidden_size=512, dropout=0.2, init_scale=0.1, max_len=50)
  0%|          | 0/9967 [00:00<?, ?it/s]  0%|          | 3/9967 [00:00<06:10, 26.89it/s]  0%|          | 19/9967 [00:00<02:32, 65.36it/s]  1%|          | 51/9967 [00:00<01:19, 124.85it/s]  1%|          | 83/9967 [00:00<01:06, 149.51it/s]  1%|▏         | 131/9967 [00:00<00:45, 214.36it/s]  2%|▏         | 163/9967 [00:00<00:44, 219.19it/s]  2%|▏         | 211/9967 [00:01<00:37, 259.77it/s]  2%|▏         | 243/9967 [00:01<00:37, 258.30it/s]  3%|▎         | 291/9967 [00:01<00:32, 296.39it/s]  3%|▎         | 323/9967 [00:01<00:35, 272.52it/s]  4%|▎         | 371/9967 [00:01<00:30, 312.86it/s]  5%|▍         | 451/9967 [00:01<00:23, 409.12it/s]  5%|▌         | 531/9967 [00:01<00:19, 492.64it/s]  6%|▌         | 611/9967 [00:01<00:16, 552.68it/s]  7%|▋         | 707/9967 [00:02<00:14, 654.16it/s]  8%|▊         | 819/9967 [00:02<00:12, 761.82it/s]  9%|▉         | 915/9967 [00:02<00:12, 706.13it/s] 10%|▉         | 995/9967 [00:02<00:12, 695.45it/s] 11%|█         | 1091/9967 [00:02<00:11, 760.21it/s] 12%|█▏        | 1170/9967 [00:02<00:12, 708.01it/s] 12%|█▏        | 1243/9967 [00:02<00:13, 635.28it/s] 13%|█▎        | 1309/9967 [00:02<00:13, 640.22it/s] 14%|█▍        | 1375/9967 [00:03<00:16, 514.79it/s] 14%|█▍        | 1432/9967 [00:03<00:16, 503.46it/s] 15%|█▍        | 1486/9967 [00:03<00:17, 495.79it/s] 15%|█▌        | 1538/9967 [00:03<00:17, 475.79it/s] 16%|█▌        | 1587/9967 [00:03<00:22, 376.38it/s] 17%|█▋        | 1651/9967 [00:03<00:19, 418.96it/s] 17%|█▋        | 1699/9967 [00:03<00:19, 428.00it/s] 18%|█▊        | 1747/9967 [00:03<00:18, 438.00it/s] 18%|█▊        | 1795/9967 [00:04<00:18, 439.57it/s] 18%|█▊        | 1843/9967 [00:04<00:19, 426.54it/s] 19%|█▉        | 1891/9967 [00:04<00:19, 410.65it/s] 19%|█▉        | 1939/9967 [00:04<00:19, 406.72it/s] 21%|██        | 2051/9967 [00:04<00:15, 499.97it/s] 22%|██▏       | 2163/9967 [00:04<00:13, 593.44it/s] 23%|██▎       | 2259/9967 [00:04<00:11, 671.01it/s] 24%|██▍       | 2387/9967 [00:05<00:10, 749.79it/s] 25%|██▌       | 2499/9967 [00:05<00:10, 719.57it/s] 26%|██▌       | 2572/9967 [00:05<00:10, 682.81it/s] 26%|██▋       | 2641/9967 [00:05<00:10, 671.28it/s] 27%|██▋       | 2708/9967 [00:05<00:10, 663.37it/s] 28%|██▊       | 2803/9967 [00:05<00:11, 641.97it/s] 29%|██▉       | 2931/9967 [00:05<00:09, 707.22it/s] 30%|███       | 3011/9967 [00:05<00:09, 720.12it/s] 31%|███▏      | 3139/9967 [00:06<00:08, 799.41it/s] 33%|███▎      | 3267/9967 [00:06<00:07, 893.68it/s] 34%|███▍      | 3411/9967 [00:06<00:06, 1025.87it/s] 36%|███▌      | 3555/9967 [00:06<00:05, 1078.25it/s] 37%|███▋      | 3699/9967 [00:06<00:05, 1171.32it/s] 39%|███▊      | 3843/9967 [00:06<00:05, 1196.15it/s] 40%|████      | 3987/9967 [00:06<00:05, 1141.13it/s] 42%|████▏     | 4147/9967 [00:06<00:04, 1177.50it/s] 43%|████▎     | 4307/9967 [00:06<00:04, 1245.17it/s] 45%|████▍     | 4483/9967 [00:07<00:04, 1192.45it/s] 46%|████▌     | 4605/9967 [00:07<00:04, 1193.29it/s] 48%|████▊     | 4771/9967 [00:07<00:04, 1284.65it/s] 49%|████▉     | 4902/9967 [00:07<00:04, 1039.05it/s] 50%|█████     | 5014/9967 [00:07<00:04, 1043.60it/s] 51%|█████▏    | 5124/9967 [00:07<00:04, 1020.20it/s] 53%|█████▎    | 5251/9967 [00:07<00:04, 1039.62it/s] 54%|█████▍    | 5379/9967 [00:08<00:04, 974.58it/s]  55%|█████▌    | 5523/9967 [00:08<00:04, 1005.99it/s] 57%|█████▋    | 5683/9967 [00:08<00:04, 1033.05it/s] 59%|█████▊    | 5843/9967 [00:08<00:03, 1097.88it/s] 60%|██████    | 6003/9967 [00:08<00:03, 1181.93it/s] 62%|██████▏   | 6195/9967 [00:08<00:02, 1308.01it/s] 64%|██████▍   | 6371/9967 [00:08<00:02, 1393.03it/s] 66%|██████▌   | 6563/9967 [00:08<00:02, 1489.22it/s] 68%|██████▊   | 6771/9967 [00:09<00:02, 1577.70it/s] 72%|███████▏  | 7203/9967 [00:09<00:01, 2312.90it/s] 75%|███████▍  | 7445/9967 [00:09<00:01, 2221.10it/s] 80%|████████  | 8003/9967 [00:09<00:00, 2927.52it/s] 85%|████████▌ | 8515/9967 [00:09<00:00, 3371.14it/s] 91%|█████████ | 9027/9967 [00:09<00:00, 3815.49it/s] 96%|█████████▌| 9539/9967 [00:09<00:00, 4135.14it/s]100%|██████████| 9967/9967 [00:09<00:00, 1022.29it/s]
[32m[2023-12-21 12:56:26,602] [    INFO][0m - global step 10 / 1332070, loss: 256.881287, avg_reader_cost: 0.00239 sec, avg_batch_cost: 0.43108 sec, avg_samples: 30.00000, ips: 28.99711 words/sec,  [0m
[32m[2023-12-21 12:56:34,605] [    INFO][0m - global step 20 / 1332070, loss: 379.982544, avg_reader_cost: 0.00025 sec, avg_batch_cost: 0.80021 sec, avg_samples: 51.00000, ips: 45.98807 words/sec,  [0m
[32m[2023-12-21 12:56:38,299] [    INFO][0m - global step 30 / 1332070, loss: 166.676224, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.36923 sec, avg_samples: 23.00000, ips: 31.14573 words/sec,  [0m
[32m[2023-12-21 12:56:44,993] [    INFO][0m - global step 40 / 1332070, loss: 340.333099, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.66929 sec, avg_samples: 45.00000, ips: 45.57053 words/sec,  [0m
[32m[2023-12-21 12:56:49,311] [    INFO][0m - global step 50 / 1332070, loss: 113.426147, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.43166 sec, avg_samples: 16.00000, ips: 36.83435 words/sec,  [0m
[32m[2023-12-21 12:56:56,457] [    INFO][0m - global step 60 / 1332070, loss: 348.413239, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.71448 sec, avg_samples: 51.00000, ips: 46.46746 words/sec,  [0m
[32m[2023-12-21 12:57:00,479] [    INFO][0m - global step 70 / 1332070, loss: 165.201996, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.40207 sec, avg_samples: 23.00000, ips: 34.81975 words/sec,  [0m
[32m[2023-12-21 12:57:07,661] [    INFO][0m - global step 80 / 1332070, loss: 389.100739, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.71816 sec, avg_samples: 51.00000, ips: 42.74788 words/sec,  [0m
[32m[2023-12-21 12:57:11,533] [    INFO][0m - global step 90 / 1332070, loss: 177.058929, avg_reader_cost: 0.00024 sec, avg_batch_cost: 0.38714 sec, avg_samples: 22.00000, ips: 32.28816 words/sec,  [0m
[32m[2023-12-21 12:57:19,179] [    INFO][0m - global step 100 / 1332070, loss: 357.381836, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.76449 sec, avg_samples: 51.00000, ips: 45.78206 words/sec,  [0m
[32m[2023-12-21 12:57:23,216] [    INFO][0m - global step 110 / 1332070, loss: 118.654655, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.40358 sec, avg_samples: 18.00000, ips: 34.68912 words/sec,  [0m
[32m[2023-12-21 12:57:30,519] [    INFO][0m - global step 120 / 1332070, loss: 370.115051, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.73019 sec, avg_samples: 51.00000, ips: 45.87829 words/sec,  [0m
[32m[2023-12-21 12:57:34,192] [    INFO][0m - global step 130 / 1332070, loss: 65.916862, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.36725 sec, avg_samples: 11.00000, ips: 32.40290 words/sec,  [0m
[32m[2023-12-21 12:57:39,643] [    INFO][0m - global step 140 / 1332070, loss: 196.294785, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.54500 sec, avg_samples: 29.00000, ips: 41.46813 words/sec,  [0m
No XPU Memory Leak
[33m Run successfully with command - seq2seq - python3.9 test_tipc/train.py --model seq2seq --optimizer adam --max_seq_len 50 --learning_rate 0.001 --max_grad_norm 5.0 --max_steps 150 --device=xpu  --save_model=/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1             >/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:57:48,634 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 12:57:48,635 auto_parallel_config: None
LAUNCH INFO 2023-12-21 12:57:48,635 auto_tuner_json: None
LAUNCH INFO 2023-12-21 12:57:48,635 devices: 0,1
LAUNCH INFO 2023-12-21 12:57:48,635 elastic_level: -1
LAUNCH INFO 2023-12-21 12:57:48,635 elastic_timeout: 30
LAUNCH INFO 2023-12-21 12:57:48,635 enable_gpu_log: True
LAUNCH INFO 2023-12-21 12:57:48,635 gloo_port: 6767
LAUNCH INFO 2023-12-21 12:57:48,635 host: None
LAUNCH INFO 2023-12-21 12:57:48,635 ips: None
LAUNCH INFO 2023-12-21 12:57:48,635 job_id: default
LAUNCH INFO 2023-12-21 12:57:48,635 legacy: False
LAUNCH INFO 2023-12-21 12:57:48,635 log_dir: log
LAUNCH INFO 2023-12-21 12:57:48,635 log_level: INFO
LAUNCH INFO 2023-12-21 12:57:48,635 log_overwrite: False
LAUNCH INFO 2023-12-21 12:57:48,635 master: None
LAUNCH INFO 2023-12-21 12:57:48,635 max_restart: 3
LAUNCH INFO 2023-12-21 12:57:48,635 nnodes: 1
LAUNCH INFO 2023-12-21 12:57:48,635 nproc_per_node: None
LAUNCH INFO 2023-12-21 12:57:48,635 rank: -1
LAUNCH INFO 2023-12-21 12:57:48,635 run_mode: collective
LAUNCH INFO 2023-12-21 12:57:48,635 server_num: None
LAUNCH INFO 2023-12-21 12:57:48,635 servers: 
LAUNCH INFO 2023-12-21 12:57:48,635 sort_ip: False
LAUNCH INFO 2023-12-21 12:57:48,635 start_port: 6070
LAUNCH INFO 2023-12-21 12:57:48,635 trainer_num: None
LAUNCH INFO 2023-12-21 12:57:48,635 trainers: 
LAUNCH INFO 2023-12-21 12:57:48,635 training_script: test_tipc/train.py
LAUNCH INFO 2023-12-21 12:57:48,636 training_script_args: ['--model', 'seq2seq', '--optimizer', 'adam', '--max_seq_len', '50', '--learning_rate', '0.001', '--max_grad_norm', '5.0', '--max_steps', '150', '--device=xpu', '--save_model=/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1']
LAUNCH INFO 2023-12-21 12:57:48,636 with_gloo: 1
LAUNCH INFO 2023-12-21 12:57:48,636 --------------------------------------------------
LAUNCH INFO 2023-12-21 12:57:48,636 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 12:57:48,637 Run Pod: lmexpj, replicas 2, status ready
LAUNCH INFO 2023-12-21 12:57:48,656 Watching Pod: lmexpj, replicas 2, status running
[32m[2023-12-21 12:54:44,550] [    INFO][0m - global step 111 / 266800, loss: 4.293671, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.81076 sec, avg_samples: 1.00000, ips: 1.23340 sequences/sec,  [0m
[32m[2023-12-21 12:54:45,410] [    INFO][0m - global step 112 / 266800, loss: 1.922006, avg_reader_cost: 0.00025 sec, avg_batch_cost: 0.85905 sec, avg_samples: 1.00000, ips: 1.16407 sequences/sec,  [0m
[32m[2023-12-21 12:54:46,237] [    INFO][0m - global step 113 / 266800, loss: 2.240932, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82684 sec, avg_samples: 1.00000, ips: 1.20942 sequences/sec,  [0m
[32m[2023-12-21 12:54:47,069] [    INFO][0m - global step 114 / 266800, loss: 2.504665, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.83161 sec, avg_samples: 1.00000, ips: 1.20248 sequences/sec,  [0m
[32m[2023-12-21 12:54:47,935] [    INFO][0m - global step 115 / 266800, loss: 2.740645, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.86582 sec, avg_samples: 1.00000, ips: 1.15498 sequences/sec,  [0m
[32m[2023-12-21 12:54:49,008] [    INFO][0m - global step 116 / 266800, loss: 2.150469, avg_reader_cost: 0.00016 sec, avg_batch_cost: 1.07192 sec, avg_samples: 1.00000, ips: 0.93291 sequences/sec,  [0m
[32m[2023-12-21 12:54:49,920] [    INFO][0m - global step 117 / 266800, loss: 2.610155, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.91136 sec, avg_samples: 1.00000, ips: 1.09726 sequences/sec,  [0m
[32m[2023-12-21 12:54:50,830] [    INFO][0m - global step 118 / 266800, loss: 1.903424, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90993 sec, avg_samples: 1.00000, ips: 1.09899 sequences/sec,  [0m
[32m[2023-12-21 12:54:51,709] [    INFO][0m - global step 119 / 266800, loss: 2.981430, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.87897 sec, avg_samples: 1.00000, ips: 1.13769 sequences/sec,  [0m
[32m[2023-12-21 12:54:52,587] [    INFO][0m - global step 120 / 266800, loss: 2.056500, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.87791 sec, avg_samples: 1.00000, ips: 1.13907 sequences/sec,  [0m
[32m[2023-12-21 12:54:53,514] [    INFO][0m - global step 121 / 266800, loss: 2.837184, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.92594 sec, avg_samples: 1.00000, ips: 1.07998 sequences/sec,  [0m
[32m[2023-12-21 12:54:54,411] [    INFO][0m - global step 122 / 266800, loss: 2.466178, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.89667 sec, avg_samples: 1.00000, ips: 1.11524 sequences/sec,  [0m
[32m[2023-12-21 12:54:55,338] [    INFO][0m - global step 123 / 266800, loss: 2.578913, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92641 sec, avg_samples: 1.00000, ips: 1.07944 sequences/sec,  [0m
[32m[2023-12-21 12:54:56,255] [    INFO][0m - global step 124 / 266800, loss: 2.906804, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.91736 sec, avg_samples: 1.00000, ips: 1.09009 sequences/sec,  [0m
[32m[2023-12-21 12:54:57,105] [    INFO][0m - global step 125 / 266800, loss: 2.511492, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.84951 sec, avg_samples: 1.00000, ips: 1.17715 sequences/sec,  [0m
[32m[2023-12-21 12:54:57,999] [    INFO][0m - global step 126 / 266800, loss: 2.435269, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.89298 sec, avg_samples: 1.00000, ips: 1.11984 sequences/sec,  [0m
[32m[2023-12-21 12:54:58,849] [    INFO][0m - global step 127 / 266800, loss: 2.848229, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.84971 sec, avg_samples: 1.00000, ips: 1.17687 sequences/sec,  [0m
[32m[2023-12-21 12:54:59,787] [    INFO][0m - global step 128 / 266800, loss: 3.303391, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.93823 sec, avg_samples: 1.00000, ips: 1.06584 sequences/sec,  [0m
[32m[2023-12-21 12:55:00,640] [    INFO][0m - global step 129 / 266800, loss: 2.604638, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.85249 sec, avg_samples: 1.00000, ips: 1.17303 sequences/sec,  [0m
[32m[2023-12-21 12:55:01,595] [    INFO][0m - global step 130 / 266800, loss: 2.308934, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.95487 sec, avg_samples: 1.00000, ips: 1.04726 sequences/sec,  [0m
[32m[2023-12-21 12:55:02,472] [    INFO][0m - global step 131 / 266800, loss: 1.772274, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.87631 sec, avg_samples: 1.00000, ips: 1.14115 sequences/sec,  [0m
[32m[2023-12-21 12:55:03,368] [    INFO][0m - global step 132 / 266800, loss: 2.264315, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.89529 sec, avg_samples: 1.00000, ips: 1.11696 sequences/sec,  [0m
[32m[2023-12-21 12:55:04,208] [    INFO][0m - global step 133 / 266800, loss: 2.714500, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.84029 sec, avg_samples: 1.00000, ips: 1.19007 sequences/sec,  [0m
[32m[2023-12-21 12:55:05,123] [    INFO][0m - global step 134 / 266800, loss: 2.812063, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.91440 sec, avg_samples: 1.00000, ips: 1.09361 sequences/sec,  [0m
[32m[2023-12-21 12:55:05,985] [    INFO][0m - global step 135 / 266800, loss: 4.126012, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.86149 sec, avg_samples: 1.00000, ips: 1.16078 sequences/sec,  [0m
[32m[2023-12-21 12:55:06,953] [    INFO][0m - global step 136 / 266800, loss: 2.003836, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.96785 sec, avg_samples: 1.00000, ips: 1.03322 sequences/sec,  [0m
[32m[2023-12-21 12:55:07,867] [    INFO][0m - global step 137 / 266800, loss: 2.398470, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.91364 sec, avg_samples: 1.00000, ips: 1.09452 sequences/sec,  [0m
[32m[2023-12-21 12:55:08,790] [    INFO][0m - global step 138 / 266800, loss: 2.572164, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.92203 sec, avg_samples: 1.00000, ips: 1.08456 sequences/sec,  [0m
[32m[2023-12-21 12:55:09,657] [    INFO][0m - global step 139 / 266800, loss: 2.338915, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86691 sec, avg_samples: 1.00000, ips: 1.15352 sequences/sec,  [0m
[32m[2023-12-21 12:55:10,617] [    INFO][0m - global step 140 / 266800, loss: 1.970525, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.95983 sec, avg_samples: 1.00000, ips: 1.04186 sequences/sec,  [0m
[32m[2023-12-21 12:55:11,510] [    INFO][0m - global step 141 / 266800, loss: 1.856880, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.89205 sec, avg_samples: 1.00000, ips: 1.12101 sequences/sec,  [0m
[32m[2023-12-21 12:55:12,374] [    INFO][0m - global step 142 / 266800, loss: 2.485156, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.86345 sec, avg_samples: 1.00000, ips: 1.15814 sequences/sec,  [0m
[32m[2023-12-21 12:55:13,243] [    INFO][0m - global step 143 / 266800, loss: 2.306528, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86925 sec, avg_samples: 1.00000, ips: 1.15041 sequences/sec,  [0m
[32m[2023-12-21 12:55:14,094] [    INFO][0m - global step 144 / 266800, loss: 2.971000, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.85063 sec, avg_samples: 1.00000, ips: 1.17560 sequences/sec,  [0m
[32m[2023-12-21 12:55:14,917] [    INFO][0m - global step 145 / 266800, loss: 2.285067, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.82265 sec, avg_samples: 1.00000, ips: 1.21559 sequences/sec,  [0m
[32m[2023-12-21 12:55:15,804] [    INFO][0m - global step 146 / 266800, loss: 2.943288, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.88666 sec, avg_samples: 1.00000, ips: 1.12783 sequences/sec,  [0m
[32m[2023-12-21 12:55:16,714] [    INFO][0m - global step 147 / 266800, loss: 3.046579, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.90963 sec, avg_samples: 1.00000, ips: 1.09935 sequences/sec,  [0m
[32m[2023-12-21 12:55:17,576] [    INFO][0m - global step 148 / 266800, loss: 2.143323, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.86101 sec, avg_samples: 1.00000, ips: 1.16142 sequences/sec,  [0m
[32m[2023-12-21 12:55:18,390] [    INFO][0m - global step 149 / 266800, loss: 2.377167, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.81352 sec, avg_samples: 1.00000, ips: 1.22923 sequences/sec,  [0m
No XPU Memory Leak
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 12:59:24,752 Pod completed
LAUNCH INFO 2023-12-21 12:59:24,753 Exit code 0
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='seq2seq', logging_steps=10, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=1, max_seq_len=50, data_dir=None, pad_to_max_seq_len=False, optimizer='adam', learning_rate=0.001, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=5.0, num_layers=2, hidden_size=512, dropout=0.2, init_scale=0.1, max_len=50)
[32m[2023-12-21 12:58:00,622] [    INFO][0m - global step 10 / 1332070, loss: 228.321655, avg_reader_cost: 0.00233 sec, avg_batch_cost: 0.44001 sec, avg_samples: 26.00000, ips: 33.86273 words/sec,  [0m
[32m[2023-12-21 12:58:08,069] [    INFO][0m - global step 20 / 1332070, loss: 386.595825, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.74455 sec, avg_samples: 51.00000, ips: 44.99340 words/sec,  [0m
[32m[2023-12-21 12:58:12,479] [    INFO][0m - global step 30 / 1332070, loss: 150.166443, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.44090 sec, avg_samples: 22.00000, ips: 36.74287 words/sec,  [0m
[32m[2023-12-21 12:58:19,865] [    INFO][0m - global step 40 / 1332070, loss: 332.282257, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.73851 sec, avg_samples: 51.00000, ips: 44.27848 words/sec,  [0m
[32m[2023-12-21 12:58:24,215] [    INFO][0m - global step 50 / 1332070, loss: 200.645782, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.43481 sec, avg_samples: 26.00000, ips: 35.64803 words/sec,  [0m
[32m[2023-12-21 12:58:31,018] [    INFO][0m - global step 60 / 1332070, loss: 312.869110, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.68025 sec, avg_samples: 46.00000, ips: 40.27947 words/sec,  [0m
[32m[2023-12-21 12:58:35,212] [    INFO][0m - global step 70 / 1332070, loss: 120.087723, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.41928 sec, avg_samples: 18.00000, ips: 35.06046 words/sec,  [0m
[32m[2023-12-21 12:58:43,601] [    INFO][0m - global step 80 / 1332070, loss: 358.197357, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.83869 sec, avg_samples: 51.00000, ips: 46.38186 words/sec,  [0m
[32m[2023-12-21 12:58:47,334] [    INFO][0m - global step 90 / 1332070, loss: 103.245361, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.37326 sec, avg_samples: 17.00000, ips: 31.61318 words/sec,  [0m
[32m[2023-12-21 12:58:53,977] [    INFO][0m - global step 100 / 1332070, loss: 332.505707, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.66421 sec, avg_samples: 51.00000, ips: 44.56394 words/sec,  [0m
[32m[2023-12-21 12:58:58,553] [    INFO][0m - global step 110 / 1332070, loss: 130.306122, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.45744 sec, avg_samples: 26.00000, ips: 37.16299 words/sec,  [0m
[32m[2023-12-21 12:58:59,427] [    INFO][0m - global step 90 / 2668000, loss: 3.099251, avg_reader_cost: 0.00041 sec, avg_batch_cost: 36.44627 sec, avg_samples: 512.00000, ips: 14.04808 words/sec,  [0m
[32m[2023-12-21 12:59:05,515] [    INFO][0m - global step 120 / 1332070, loss: 364.233826, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.69605 sec, avg_samples: 51.00000, ips: 42.66954 words/sec,  [0m
[32m[2023-12-21 12:59:09,504] [    INFO][0m - global step 130 / 1332070, loss: 90.717468, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.39886 sec, avg_samples: 18.00000, ips: 33.84686 words/sec,  [0m
[32m[2023-12-21 12:59:15,772] [    INFO][0m - global step 140 / 1332070, loss: 277.708832, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.62664 sec, avg_samples: 43.00000, ips: 44.52308 words/sec,  [0m
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - seq2seq - python3.9 -m paddle.distributed.launch --gpus=0,1 test_tipc/train.py --model seq2seq --optimizer adam --max_seq_len 50 --learning_rate 0.001 --max_grad_norm 5.0 --max_steps 150 --device=xpu --save_model=/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1             - /workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log [0m
+ watchcat=5099
+ kill -9 5405
+ sleep 10
==END==test_tipc/configs/seq2seq/train_infer_python.txt
run.sh: line 334:  5405 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/seq2seq/train_infer_python.txt
++ date +%s
+ end=1703134774
++ echo 1703134552 1703134774
++ awk '{print $2-$1-2}'
+ time=220
+ echo 'test_tipc/configs/seq2seq/train_infer_python.txt spend time seconds 220'
+ read config_file
test_tipc/configs/seq2seq/train_infer_python.txt spend time seconds 220
+ [[ PaddleNLP == \P\a\d\d\l\e\C\l\a\s ]]
++ date +%s
+ start=1703134774
==START==test_tipc/configs/xlnet/train_infer_python.txt
+ echo ==START==test_tipc/configs/xlnet/train_infer_python.txt
++ awk 'NR==1, NR==32{print}' test_tipc/configs/xlnet/train_infer_python.txt
+ dataline='===========================train_params===========================
model_name:xlnet
python:python3.7
gpu_list:0|0,1
--device:gpu|gpu
null:null
--max_steps:null
--save_model:./test_tipc/output/
--batch_size:null
null:null
null:null
null:null
null:null
##
trainer:norm_train
norm_train:test_tipc/train.py --model xlnet --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path xlnet-base-cased --pad_to_max_seq_len --max_seq_len 128 --logging_steps 1 --task_name SST-2 --max_steps 150
null:null
null:null
null:null
null:null
null:null
##
===========================eval_params=========================== 
null:null
null:null
##
===========================infer_params===========================
null:null
null:null
null:null
null:null
null:null'
+ IFS='
'
+ lines=(${dataline})
++ func_parser_value model_name:xlnet
++ strs=model_name:xlnet
++ IFS=:
++ array=(${strs})
++ tmp=xlnet
++ echo xlnet
+ model_name=xlnet
+ sleep 10
+ run run_model test_tipc/configs/xlnet/train_infer_python.txt lite_train_lite_infer 3600 xlnet
+ ps -ef
+ grep test_tipc
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ ps -ef
+ grep python
+ grep -v grep
+ cut -c 9-15
+ xargs kill -9
kill: (247): No such process
kill: (249): No such process
kill: (267): No such process
kill: (272): No such process
kill: (272): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (276): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
kill: (278): No such process
+ waitfor=7200
+ command='run_model
test_tipc/configs/xlnet/train_infer_python.txt
lite_train_lite_infer
3600
xlnet'
+ commandpid=5707
+ run_model test_tipc/configs/xlnet/train_infer_python.txt lite_train_lite_infer 3600 xlnet
+ config_file=test_tipc/configs/xlnet/train_infer_python.txt
+ mode=lite_train_lite_infer
+ bash test_tipc/prepare.sh test_tipc/configs/xlnet/train_infer_python.txt lite_train_lite_infer
+ watchdog=5708
+ wait 5707
+ sleep 7200
+ last_status=0
+ [[ 0 -ne 0 ]]
+ bash test_tipc/test_train_inference_python_xpu.sh test_tipc/configs/xlnet/train_infer_python.txt lite_train_lite_infer
bash test_tipc/test_train_inference_python.sh test_tipc/configs/xlnet/train_infer_python.txt lite_train_lite_infer
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[32m[2023-12-21 12:59:48,571] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/xlnet/xlnet-base-cased-spiece.model and saved to /root/.paddlenlp/models/xlnet-base-cased[0m
[32m[2023-12-21 12:59:53,346] [    INFO][0m - Downloading xlnet-base-cased-spiece.model from https://bj.bcebos.com/paddlenlp/models/transformers/xlnet/xlnet-base-cased-spiece.model[0m
Namespace(device='xpu', model='xlnet', logging_steps=1, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1', batch_size=1, max_seq_len=128, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='xlnet-base-cased', task_name='SST-2', max_seq_length=128, warmup_steps=0, warmup_proportion=0.1)
  0%|          | 0.00/779k [00:00<?, ?B/s]  4%|▍         | 35.0k/779k [00:00<00:02, 278kB/s] 11%|█         | 83.0k/779k [00:00<00:02, 340kB/s] 19%|█▉        | 147k/779k [00:00<00:01, 414kB/s]  27%|██▋       | 211k/779k [00:00<00:01, 455kB/s] 35%|███▌      | 275k/779k [00:00<00:01, 468kB/s] 41%|████▏     | 323k/779k [00:00<00:01, 456kB/s] 48%|████▊     | 371k/779k [00:00<00:00, 459kB/s] 54%|█████▍    | 419k/779k [00:00<00:00, 441kB/s] 60%|█████▉    | 467k/779k [00:01<00:00, 420kB/s] 68%|██████▊   | 531k/779k [00:01<00:00, 471kB/s] 76%|███████▋  | 595k/779k [00:01<00:00, 518kB/s] 87%|████████▋ | 675k/779k [00:01<00:00, 582kB/s] 94%|█████████▍| 733k/779k [00:01<00:00, 560kB/s]100%|██████████| 779k/779k [00:01<00:00, 484kB/s]
[32m[2023-12-21 13:00:10,800] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/xlnet-base-cased/tokenizer_config.json[0m
[32m[2023-12-21 13:00:10,800] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/xlnet-base-cased/special_tokens_map.json[0m
[32m[2023-12-21 13:00:11,043] [    INFO][0m - Configuration saved in /root/.paddlenlp/models/xlnet-base-cased/config.json[0m
[32m[2023-12-21 13:00:11,482] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/xlnet/xlnet-base-cased.pdparams[0m
[32m[2023-12-21 13:00:11,482] [    INFO][0m - Downloading xlnet-base-cased.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/xlnet/xlnet-base-cased.pdparams[0m
  0%|          | 0.00/671M [00:00<?, ?B/s]  0%|          | 3.00k/671M [00:00<8:37:41, 22.6kB/s]  0%|          | 19.0k/671M [00:00<2:14:12, 87.3kB/s]  0%|          | 51.0k/671M [00:00<1:22:03, 143kB/s]   0%|          | 67.0k/671M [00:00<1:32:53, 126kB/s]  0%|          | 99.0k/671M [00:00<1:14:49, 157kB/s]  0%|          | 131k/671M [00:00<1:07:06, 175kB/s]   0%|          | 163k/671M [00:01<1:01:37, 190kB/s]  0%|          | 195k/671M [00:01<54:28, 215kB/s]    0%|          | 217k/671M [00:01<55:28, 211kB/s]  0%|          | 243k/671M [00:01<1:01:12, 191kB/s]  0%|          | 291k/671M [00:01<51:29, 228kB/s]    0%|          | 339k/671M [00:01<45:39, 257kB/s]  0%|          | 387k/671M [00:01<42:39, 275kB/s]  0%|          | 435k/671M [00:02<41:29, 282kB/s]  0%|          | 483k/671M [00:02<40:32, 289kB/s]  0%|          | 547k/671M [00:02<34:55, 335kB/s]  0%|          | 595k/671M [00:02<35:35, 329kB/s]  0%|          | 675k/671M [00:02<30:21, 386kB/s]  0%|          | 739k/671M [00:02<29:24, 398kB/s]  0%|          | 803k/671M [00:03<29:01, 403kB/s]  0%|          | 867k/671M [00:03<27:27, 426kB/s]  0%|          | 947k/671M [00:03<23:53, 490kB/s]  0%|          | 1.00M/671M [00:03<21:06, 554kB/s]  0%|          | 1.08M/671M [00:03<19:05, 613kB/s]  0%|          | 1.21M/671M [00:03<15:19, 764kB/s]  0%|          | 1.30M/671M [00:03<14:32, 805kB/s]  0%|          | 1.42M/671M [00:03<12:56, 904kB/s]  0%|          | 1.55M/671M [00:03<11:34, 1.01MB/s]  0%|          | 1.71M/671M [00:04<09:58, 1.17MB/s]  0%|          | 1.86M/671M [00:04<09:20, 1.25MB/s]  0%|          | 1.98M/671M [00:04<09:37, 1.21MB/s]  0%|          | 2.10M/671M [00:04<09:44, 1.20MB/s]  0%|          | 2.22M/671M [00:04<09:58, 1.17MB/s]  0%|          | 2.35M/671M [00:04<10:01, 1.16MB/s]  0%|          | 2.53M/671M [00:04<09:07, 1.28MB/s]  0%|          | 2.72M/671M [00:04<08:01, 1.45MB/s]  0%|          | 2.91M/671M [00:04<07:29, 1.56MB/s]  0%|          | 3.10M/671M [00:05<07:24, 1.57MB/s]  0%|          | 3.25M/671M [00:05<07:53, 1.48MB/s]  1%|          | 3.39M/671M [00:05<09:10, 1.27MB/s]  1%|          | 3.52M/671M [00:05<09:05, 1.28MB/s]  1%|          | 3.64M/671M [00:05<10:35, 1.10MB/s]  1%|          | 3.75M/671M [00:05<12:38, 921kB/s]   1%|          | 3.85M/671M [00:05<12:33, 928kB/s]  1%|          | 3.96M/671M [00:06<14:06, 826kB/s]  1%|          | 4.04M/671M [00:06<13:57, 835kB/s]  1%|          | 4.17M/671M [00:06<12:05, 963kB/s]  1%|          | 4.27M/671M [00:06<12:15, 950kB/s]  1%|          | 4.42M/671M [00:06<10:51, 1.07MB/s]  1%|          | 4.58M/671M [00:06<09:34, 1.22MB/s]  1%|          | 4.74M/671M [00:06<08:53, 1.31MB/s]  1%|          | 4.87M/671M [00:06<09:25, 1.23MB/s]  1%|          | 5.00M/671M [00:07<10:41, 1.09MB/s]  1%|          | 5.13M/671M [00:07<10:12, 1.14MB/s]  1%|          | 5.28M/671M [00:07<09:14, 1.26MB/s]  1%|          | 5.41M/671M [00:07<09:43, 1.19MB/s]  1%|          | 5.60M/671M [00:07<08:35, 1.35MB/s]  1%|          | 5.73M/671M [00:07<09:05, 1.28MB/s]  1%|          | 5.86M/671M [00:07<09:02, 1.29MB/s]  1%|          | 6.00M/671M [00:07<09:04, 1.28MB/s]  1%|          | 6.25M/671M [00:07<07:46, 1.49MB/s]  1%|          | 6.41M/671M [00:08<07:49, 1.48MB/s]  1%|          | 6.57M/671M [00:08<07:53, 1.47MB/s]  1%|          | 6.86M/671M [00:08<06:34, 1.76MB/s]  1%|          | 7.07M/671M [00:08<06:15, 1.85MB/s]  1%|          | 7.38M/671M [00:08<05:45, 2.01MB/s]  1%|          | 7.68M/671M [00:08<05:02, 2.30MB/s]  1%|          | 7.94M/671M [00:08<04:50, 2.39MB/s]  1%|          | 8.17M/671M [00:08<04:57, 2.33MB/s]  1%|▏         | 8.57M/671M [00:08<04:19, 2.67MB/s]  1%|▏         | 8.82M/671M [00:09<04:23, 2.63MB/s]  1%|▏         | 9.25M/671M [00:09<04:20, 2.66MB/s]  1%|▏         | 9.74M/671M [00:09<03:54, 2.96MB/s]  2%|▏         | 10.3M/671M [00:09<03:33, 3.24MB/s]  2%|▏         | 10.8M/671M [00:09<03:21, 3.43MB/s]  2%|▏         | 11.2M/671M [00:09<03:11, 3.61MB/s]  2%|▏         | 11.7M/671M [00:09<03:10, 3.63MB/s]  2%|▏         | 12.2M/671M [00:10<02:47, 4.12MB/s]  2%|▏         | 12.6M/671M [00:10<02:53, 3.98MB/s]  2%|▏         | 13.3M/671M [00:10<02:32, 4.51MB/s]  2%|▏         | 14.0M/671M [00:10<02:14, 5.11MB/s]  2%|▏         | 14.6M/671M [00:10<02:06, 5.45MB/s]  2%|▏         | 15.2M/671M [00:10<02:27, 4.67MB/s]  2%|▏         | 15.8M/671M [00:10<02:11, 5.23MB/s]  2%|▏         | 16.4M/671M [00:10<02:24, 4.73MB/s]  3%|▎         | 16.8M/671M [00:11<02:29, 4.57MB/s]  3%|▎         | 17.3M/671M [00:11<02:30, 4.55MB/s]  3%|▎         | 17.7M/671M [00:11<02:35, 4.39MB/s]  3%|▎         | 18.2M/671M [00:11<02:44, 4.16MB/s]  3%|▎         | 18.7M/671M [00:11<02:46, 4.12MB/s]  3%|▎         | 19.2M/671M [00:11<02:49, 4.03MB/s]  3%|▎         | 19.7M/671M [00:11<02:48, 4.05MB/s]  3%|▎         | 20.2M/671M [00:12<03:47, 3.00MB/s]  3%|▎         | 21.2M/671M [00:12<02:42, 4.18MB/s]  3%|▎         | 21.7M/671M [00:12<02:56, 3.86MB/s]  3%|▎         | 22.1M/671M [00:12<03:09, 3.59MB/s]  3%|▎         | 22.5M/671M [00:12<03:25, 3.31MB/s]  3%|▎         | 22.9M/671M [00:12<03:26, 3.29MB/s]  3%|▎         | 23.4M/671M [00:12<03:24, 3.32MB/s]  4%|▎         | 23.8M/671M [00:13<03:14, 3.49MB/s]  4%|▎         | 24.3M/671M [00:13<03:03, 3.70MB/s]  4%|▎         | 24.8M/671M [00:13<03:03, 3.70MB/s]  4%|▍         | 25.2M/671M [00:13<02:50, 3.96MB/s]  4%|▍         | 25.7M/671M [00:13<03:01, 3.73MB/s]  4%|▍         | 26.2M/671M [00:13<04:40, 2.41MB/s]  4%|▍         | 27.1M/671M [00:14<03:15, 3.46MB/s]  4%|▍         | 27.5M/671M [00:14<03:17, 3.41MB/s]  4%|▍         | 27.9M/671M [00:14<03:18, 3.39MB/s]  4%|▍         | 28.2M/671M [00:14<03:19, 3.37MB/s]  4%|▍         | 28.6M/671M [00:14<03:25, 3.28MB/s]  4%|▍         | 28.9M/671M [00:14<03:23, 3.30MB/s]  4%|▍         | 29.3M/671M [00:14<03:25, 3.28MB/s]  4%|▍         | 29.7M/671M [00:14<03:23, 3.31MB/s]  4%|▍         | 30.0M/671M [00:14<03:18, 3.38MB/s]  5%|▍         | 30.4M/671M [00:15<03:19, 3.37MB/s]  5%|▍         | 30.8M/671M [00:15<03:24, 3.28MB/s]  5%|▍         | 31.2M/671M [00:15<03:31, 3.16MB/s]  5%|▍         | 31.6M/671M [00:15<03:47, 2.94MB/s]  5%|▍         | 32.0M/671M [00:15<03:51, 2.89MB/s]  5%|▍         | 32.4M/671M [00:15<03:53, 2.87MB/s]  5%|▍         | 32.8M/671M [00:15<03:50, 2.90MB/s]  5%|▍         | 33.2M/671M [00:16<03:56, 2.83MB/s]  5%|▌         | 33.6M/671M [00:16<03:36, 3.09MB/s]  5%|▌         | 34.0M/671M [00:16<04:00, 2.77MB/s]  5%|▌         | 34.5M/671M [00:16<03:38, 3.05MB/s]  5%|▌         | 34.9M/671M [00:16<03:30, 3.17MB/s]  5%|▌         | 35.3M/671M [00:16<03:19, 3.34MB/s]  5%|▌         | 36.1M/671M [00:16<02:29, 4.46MB/s]  5%|▌         | 36.9M/671M [00:17<02:05, 5.30MB/s]  6%|▌         | 37.8M/671M [00:17<01:43, 6.42MB/s]  6%|▌         | 38.6M/671M [00:17<01:35, 6.95MB/s]  6%|▌         | 39.3M/671M [00:17<01:36, 6.89MB/s]  6%|▌         | 40.2M/671M [00:17<01:28, 7.43MB/s]  6%|▌         | 40.9M/671M [00:17<01:28, 7.44MB/s]  6%|▌         | 41.7M/671M [00:17<01:27, 7.50MB/s]  6%|▋         | 42.4M/671M [00:17<01:27, 7.50MB/s]  6%|▋         | 43.2M/671M [00:17<01:25, 7.68MB/s]  7%|▋         | 44.0M/671M [00:17<01:22, 7.97MB/s]  7%|▋         | 44.8M/671M [00:18<01:27, 7.52MB/s]  7%|▋         | 45.7M/671M [00:18<01:29, 7.31MB/s]  7%|▋         | 46.4M/671M [00:18<01:33, 7.00MB/s]  7%|▋         | 47.0M/671M [00:18<01:33, 6.98MB/s]  7%|▋         | 47.7M/671M [00:18<01:38, 6.63MB/s]  7%|▋         | 48.3M/671M [00:18<01:39, 6.56MB/s]  7%|▋         | 49.0M/671M [00:18<01:46, 6.12MB/s]  7%|▋         | 49.6M/671M [00:18<01:45, 6.16MB/s]  7%|▋         | 50.2M/671M [00:18<01:47, 6.06MB/s]  8%|▊         | 51.1M/671M [00:19<01:32, 7.01MB/s]  8%|▊         | 52.1M/671M [00:19<01:22, 7.88MB/s]  8%|▊         | 52.8M/671M [00:19<01:24, 7.69MB/s]  8%|▊         | 53.6M/671M [00:19<01:22, 7.89MB/s]  8%|▊         | 54.4M/671M [00:19<01:41, 6.39MB/s]  8%|▊         | 55.1M/671M [00:19<01:48, 5.95MB/s]  8%|▊         | 55.7M/671M [00:19<01:59, 5.41MB/s]  8%|▊         | 56.2M/671M [00:19<02:03, 5.21MB/s]  8%|▊         | 56.7M/671M [00:20<02:01, 5.31MB/s]  9%|▊         | 57.5M/671M [00:20<02:03, 5.20MB/s]  9%|▊         | 58.0M/671M [00:20<02:02, 5.24MB/s]  9%|▊         | 58.5M/671M [00:20<02:13, 4.81MB/s]  9%|▉         | 59.1M/671M [00:20<02:00, 5.32MB/s]  9%|▉         | 59.7M/671M [00:20<02:16, 4.68MB/s]  9%|▉         | 60.1M/671M [00:20<02:47, 3.81MB/s]  9%|▉         | 60.5M/671M [00:21<02:51, 3.74MB/s]  9%|▉         | 61.0M/671M [00:21<03:01, 3.52MB/s]  9%|▉         | 61.4M/671M [00:21<03:00, 3.54MB/s]  9%|▉         | 61.7M/671M [00:21<03:25, 3.11MB/s]  9%|▉         | 62.1M/671M [00:21<03:46, 2.81MB/s]  9%|▉         | 62.3M/671M [00:21<04:12, 2.52MB/s]  9%|▉         | 62.6M/671M [00:21<04:58, 2.13MB/s]  9%|▉         | 62.8M/671M [00:22<04:57, 2.14MB/s]  9%|▉         | 63.0M/671M [00:22<05:14, 2.03MB/s]  9%|▉         | 63.2M/671M [00:22<05:43, 1.85MB/s]  9%|▉         | 63.4M/671M [00:22<05:38, 1.88MB/s]  9%|▉         | 63.6M/671M [00:22<05:48, 1.83MB/s] 10%|▉         | 63.8M/671M [00:22<06:10, 1.72MB/s] 10%|▉         | 63.9M/671M [00:22<06:37, 1.60MB/s] 10%|▉         | 64.1M/671M [00:22<06:50, 1.55MB/s] 10%|▉         | 64.3M/671M [00:23<06:53, 1.54MB/s] 10%|▉         | 64.5M/671M [00:23<06:38, 1.60MB/s] 10%|▉         | 64.6M/671M [00:23<06:39, 1.59MB/s] 10%|▉         | 64.9M/671M [00:23<06:37, 1.60MB/s] 10%|▉         | 65.1M/671M [00:23<06:12, 1.70MB/s] 10%|▉         | 65.4M/671M [00:23<05:41, 1.86MB/s] 10%|▉         | 65.5M/671M [00:23<05:37, 1.88MB/s] 10%|▉         | 65.7M/671M [00:23<06:05, 1.74MB/s] 10%|▉         | 65.9M/671M [00:24<05:45, 1.84MB/s] 10%|▉         | 66.1M/671M [00:24<06:44, 1.57MB/s] 10%|▉         | 66.3M/671M [00:24<06:11, 1.70MB/s] 10%|▉         | 66.5M/671M [00:24<06:00, 1.76MB/s] 10%|▉         | 66.7M/671M [00:24<06:27, 1.64MB/s] 10%|▉         | 66.9M/671M [00:24<05:53, 1.79MB/s] 10%|█         | 67.2M/671M [00:24<05:17, 1.99MB/s] 10%|█         | 67.4M/671M [00:24<05:31, 1.91MB/s] 10%|█         | 67.6M/671M [00:25<05:49, 1.81MB/s] 10%|█         | 67.9M/671M [00:25<05:02, 2.09MB/s] 10%|█         | 68.1M/671M [00:25<05:07, 2.05MB/s] 10%|█         | 68.3M/671M [00:25<05:07, 2.05MB/s] 10%|█         | 68.5M/671M [00:25<05:36, 1.88MB/s] 10%|█         | 68.8M/671M [00:25<05:47, 1.82MB/s] 10%|█         | 69.0M/671M [00:25<05:45, 1.82MB/s] 10%|█         | 69.2M/671M [00:25<06:15, 1.68MB/s] 10%|█         | 69.5M/671M [00:26<05:36, 1.87MB/s] 10%|█         | 69.7M/671M [00:26<05:43, 1.83MB/s] 10%|█         | 69.9M/671M [00:26<05:10, 2.03MB/s] 10%|█         | 70.1M/671M [00:26<05:16, 1.99MB/s] 10%|█         | 70.4M/671M [00:26<04:58, 2.11MB/s] 11%|█         | 70.6M/671M [00:26<04:59, 2.10MB/s] 11%|█         | 70.8M/671M [00:26<05:01, 2.09MB/s] 11%|█         | 71.0M/671M [00:26<05:16, 1.99MB/s] 11%|█         | 71.2M/671M [00:26<06:02, 1.74MB/s] 11%|█         | 71.3M/671M [00:27<06:12, 1.69MB/s] 11%|█         | 71.5M/671M [00:27<07:42, 1.36MB/s] 11%|█         | 71.7M/671M [00:27<07:29, 1.40MB/s] 11%|█         | 71.9M/671M [00:27<06:39, 1.57MB/s] 11%|█         | 72.1M/671M [00:27<06:05, 1.72MB/s] 11%|█         | 72.3M/671M [00:27<06:01, 1.73MB/s] 11%|█         | 72.5M/671M [00:27<05:41, 1.83MB/s] 11%|█         | 72.7M/671M [00:27<05:22, 1.94MB/s] 11%|█         | 72.9M/671M [00:28<05:35, 1.87MB/s] 11%|█         | 73.1M/671M [00:28<05:53, 1.77MB/s] 11%|█         | 73.3M/671M [00:28<05:31, 1.89MB/s] 11%|█         | 73.6M/671M [00:28<05:07, 2.04MB/s] 11%|█         | 73.8M/671M [00:28<06:15, 1.67MB/s] 11%|█         | 74.0M/671M [00:28<06:17, 1.66MB/s] 11%|█         | 74.1M/671M [00:28<06:22, 1.63MB/s] 11%|█         | 74.5M/671M [00:28<05:39, 1.84MB/s] 11%|█         | 74.8M/671M [00:29<05:01, 2.07MB/s] 11%|█         | 75.2M/671M [00:29<04:29, 2.32MB/s] 11%|█▏        | 75.5M/671M [00:29<04:25, 2.35MB/s] 11%|█▏        | 75.9M/671M [00:29<04:24, 2.35MB/s] 11%|█▏        | 76.2M/671M [00:29<04:20, 2.39MB/s] 11%|█▏        | 76.5M/671M [00:29<04:00, 2.59MB/s] 11%|█▏        | 76.8M/671M [00:29<04:00, 2.59MB/s] 11%|█▏        | 77.1M/671M [00:30<04:57, 2.09MB/s] 12%|█▏        | 77.4M/671M [00:30<04:13, 2.45MB/s] 12%|█▏        | 77.7M/671M [00:30<04:29, 2.31MB/s] 12%|█▏        | 78.0M/671M [00:30<04:13, 2.45MB/s] 12%|█▏        | 78.2M/671M [00:30<04:05, 2.53MB/s] 12%|█▏        | 78.5M/671M [00:30<03:47, 2.73MB/s] 12%|█▏        | 78.9M/671M [00:30<03:59, 2.60MB/s] 12%|█▏        | 79.4M/671M [00:30<03:23, 3.05MB/s] 12%|█▏        | 79.9M/671M [00:31<02:54, 3.56MB/s] 12%|█▏        | 80.5M/671M [00:31<02:21, 4.37MB/s] 12%|█▏        | 81.3M/671M [00:31<01:58, 5.20MB/s] 12%|█▏        | 81.9M/671M [00:31<01:48, 5.68MB/s] 12%|█▏        | 82.7M/671M [00:31<01:39, 6.23MB/s] 12%|█▏        | 83.5M/671M [00:31<01:27, 7.03MB/s] 13%|█▎        | 84.4M/671M [00:31<01:19, 7.69MB/s] 13%|█▎        | 85.2M/671M [00:31<01:28, 6.96MB/s] 13%|█▎        | 85.9M/671M [00:32<02:04, 4.92MB/s] 13%|█▎        | 86.4M/671M [00:32<02:14, 4.54MB/s] 13%|█▎        | 86.9M/671M [00:32<02:22, 4.30MB/s] 13%|█▎        | 87.4M/671M [00:32<02:30, 4.07MB/s] 13%|█▎        | 87.8M/671M [00:32<02:51, 3.56MB/s] 13%|█▎        | 88.2M/671M [00:32<02:48, 3.63MB/s] 13%|█▎        | 88.5M/671M [00:32<02:49, 3.60MB/s] 13%|█▎        | 89.0M/671M [00:32<02:31, 4.03MB/s] 13%|█▎        | 89.6M/671M [00:33<02:16, 4.47MB/s] 13%|█▎        | 90.4M/671M [00:33<01:53, 5.37MB/s] 14%|█▎        | 91.1M/671M [00:33<01:50, 5.51MB/s] 14%|█▎        | 91.8M/671M [00:33<01:45, 5.78MB/s] 14%|█▍        | 92.3M/671M [00:33<01:59, 5.07MB/s] 14%|█▍        | 92.9M/671M [00:33<02:08, 4.70MB/s] 14%|█▍        | 93.4M/671M [00:33<02:11, 4.61MB/s] 14%|█▍        | 94.2M/671M [00:33<01:49, 5.54MB/s] 14%|█▍        | 94.9M/671M [00:34<01:48, 5.56MB/s] 14%|█▍        | 95.8M/671M [00:34<01:36, 6.23MB/s] 14%|█▍        | 96.5M/671M [00:34<01:35, 6.27MB/s] 14%|█▍        | 97.1M/671M [00:34<01:37, 6.16MB/s] 15%|█▍        | 97.7M/671M [00:34<01:46, 5.62MB/s] 15%|█▍        | 98.2M/671M [00:34<01:47, 5.58MB/s] 15%|█▍        | 98.7M/671M [00:34<02:08, 4.67MB/s] 15%|█▍        | 99.2M/671M [00:34<02:16, 4.39MB/s] 15%|█▍        | 99.7M/671M [00:35<02:32, 3.93MB/s] 15%|█▍        | 100M/671M [00:35<02:44, 3.64MB/s]  15%|█▍        | 100M/671M [00:35<03:06, 3.20MB/s] 15%|█▌        | 101M/671M [00:35<03:26, 2.89MB/s] 15%|█▌        | 101M/671M [00:35<03:38, 2.73MB/s] 15%|█▌        | 101M/671M [00:35<03:33, 2.80MB/s] 15%|█▌        | 102M/671M [00:35<03:35, 2.76MB/s] 15%|█▌        | 102M/671M [00:36<03:33, 2.79MB/s] 15%|█▌        | 102M/671M [00:36<03:45, 2.64MB/s] 15%|█▌        | 102M/671M [00:36<04:00, 2.48MB/s] 15%|█▌        | 103M/671M [00:36<04:16, 2.32MB/s] 15%|█▌        | 103M/671M [00:36<04:23, 2.26MB/s] 15%|█▌        | 103M/671M [00:36<04:25, 2.24MB/s] 15%|█▌        | 103M/671M [00:36<04:22, 2.27MB/s] 15%|█▌        | 104M/671M [00:36<04:29, 2.21MB/s] 15%|█▌        | 104M/671M [00:36<04:16, 2.31MB/s] 16%|█▌        | 104M/671M [00:37<04:29, 2.20MB/s] 16%|█▌        | 104M/671M [00:37<04:42, 2.10MB/s] 16%|█▌        | 105M/671M [00:37<05:06, 1.94MB/s] 16%|█▌        | 105M/671M [00:37<05:32, 1.79MB/s] 16%|█▌        | 105M/671M [00:37<05:11, 1.90MB/s] 16%|█▌        | 105M/671M [00:37<05:05, 1.94MB/s] 16%|█▌        | 106M/671M [00:37<04:41, 2.11MB/s] 16%|█▌        | 106M/671M [00:37<04:55, 2.01MB/s] 16%|█▌        | 106M/671M [00:38<04:54, 2.01MB/s] 16%|█▌        | 106M/671M [00:38<04:39, 2.12MB/s] 16%|█▌        | 106M/671M [00:38<04:23, 2.24MB/s] 16%|█▌        | 107M/671M [00:38<03:47, 2.60MB/s] 16%|█▌        | 107M/671M [00:38<03:02, 3.23MB/s] 16%|█▌        | 108M/671M [00:38<02:40, 3.69MB/s] 16%|█▌        | 108M/671M [00:38<02:24, 4.09MB/s] 16%|█▌        | 109M/671M [00:38<02:20, 4.18MB/s] 16%|█▋        | 109M/671M [00:38<02:13, 4.42MB/s] 16%|█▋        | 110M/671M [00:39<02:11, 4.47MB/s] 16%|█▋        | 111M/671M [00:39<01:50, 5.32MB/s] 17%|█▋        | 111M/671M [00:39<01:42, 5.72MB/s] 17%|█▋        | 112M/671M [00:39<01:43, 5.64MB/s] 17%|█▋        | 112M/671M [00:39<01:49, 5.35MB/s] 17%|█▋        | 113M/671M [00:39<02:02, 4.78MB/s] 17%|█▋        | 113M/671M [00:39<02:05, 4.65MB/s] 17%|█▋        | 114M/671M [00:39<02:08, 4.54MB/s] 17%|█▋        | 115M/671M [00:40<02:01, 4.80MB/s] 17%|█▋        | 115M/671M [00:40<01:53, 5.13MB/s] 17%|█▋        | 116M/671M [00:40<01:46, 5.49MB/s] 17%|█▋        | 116M/671M [00:40<01:43, 5.61MB/s] 17%|█▋        | 117M/671M [00:40<01:39, 5.86MB/s] 18%|█▊        | 118M/671M [00:40<01:38, 5.87MB/s] 18%|█▊        | 118M/671M [00:40<01:38, 5.90MB/s] 18%|█▊        | 119M/671M [00:40<01:38, 5.86MB/s] 18%|█▊        | 119M/671M [00:40<01:59, 4.84MB/s] 18%|█▊        | 120M/671M [00:41<02:02, 4.71MB/s] 18%|█▊        | 120M/671M [00:41<01:55, 5.02MB/s] 18%|█▊        | 121M/671M [00:41<01:44, 5.51MB/s] 18%|█▊        | 122M/671M [00:41<01:37, 5.91MB/s] 18%|█▊        | 123M/671M [00:41<01:31, 6.29MB/s] 18%|█▊        | 123M/671M [00:41<01:28, 6.50MB/s] 18%|█▊        | 124M/671M [00:41<01:24, 6.77MB/s] 19%|█▊        | 125M/671M [00:41<01:25, 6.68MB/s] 19%|█▊        | 125M/671M [00:41<01:32, 6.18MB/s] 19%|█▉        | 126M/671M [00:42<01:41, 5.62MB/s] 19%|█▉        | 126M/671M [00:42<01:42, 5.59MB/s] 19%|█▉        | 127M/671M [00:42<01:44, 5.43MB/s] 19%|█▉        | 128M/671M [00:42<01:43, 5.51MB/s] 19%|█▉        | 128M/671M [00:42<01:44, 5.44MB/s] 19%|█▉        | 129M/671M [00:42<01:50, 5.12MB/s] 19%|█▉        | 129M/671M [00:42<01:53, 5.01MB/s] 19%|█▉        | 130M/671M [00:42<01:55, 4.91MB/s] 19%|█▉        | 130M/671M [00:42<01:53, 5.01MB/s] 20%|█▉        | 131M/671M [00:43<01:46, 5.33MB/s] 20%|█▉        | 132M/671M [00:43<01:39, 5.67MB/s] 20%|█▉        | 132M/671M [00:43<01:45, 5.35MB/s] 20%|█▉        | 133M/671M [00:43<01:56, 4.84MB/s] 20%|█▉        | 133M/671M [00:43<01:56, 4.82MB/s] 20%|█▉        | 134M/671M [00:43<02:25, 3.87MB/s] 20%|██        | 134M/671M [00:43<02:49, 3.32MB/s] 20%|██        | 135M/671M [00:44<02:40, 3.50MB/s] 20%|██        | 135M/671M [00:44<02:51, 3.28MB/s] 20%|██        | 136M/671M [00:44<02:08, 4.36MB/s] 20%|██        | 137M/671M [00:44<02:13, 4.18MB/s] 20%|██        | 137M/671M [00:44<02:13, 4.18MB/s] 20%|██        | 137M/671M [00:44<02:30, 3.73MB/s] 21%|██        | 138M/671M [00:44<02:53, 3.22MB/s] 21%|██        | 138M/671M [00:45<03:10, 2.93MB/s] 21%|██        | 138M/671M [00:45<03:31, 2.64MB/s] 21%|██        | 139M/671M [00:45<03:40, 2.53MB/s] 21%|██        | 139M/671M [00:45<03:23, 2.74MB/s] 21%|██        | 139M/671M [00:45<03:26, 2.70MB/s] 21%|██        | 140M/671M [00:45<03:36, 2.58MB/s] 21%|██        | 140M/671M [00:45<03:39, 2.53MB/s] 21%|██        | 140M/671M [00:45<03:57, 2.34MB/s] 21%|██        | 140M/671M [00:46<04:02, 2.29MB/s] 21%|██        | 140M/671M [00:46<04:14, 2.19MB/s] 21%|██        | 141M/671M [00:46<04:27, 2.07MB/s] 21%|██        | 141M/671M [00:46<04:09, 2.23MB/s] 21%|██        | 141M/671M [00:46<04:37, 2.00MB/s] 21%|██        | 141M/671M [00:46<04:23, 2.11MB/s] 21%|██        | 142M/671M [00:46<05:11, 1.78MB/s] 21%|██        | 142M/671M [00:47<04:56, 1.87MB/s] 21%|██        | 142M/671M [00:47<05:17, 1.75MB/s] 21%|██        | 142M/671M [00:47<05:09, 1.79MB/s] 21%|██▏       | 143M/671M [00:47<05:14, 1.76MB/s] 21%|██▏       | 143M/671M [00:47<05:01, 1.83MB/s] 21%|██▏       | 143M/671M [00:47<05:15, 1.76MB/s] 21%|██▏       | 143M/671M [00:47<05:28, 1.69MB/s] 21%|██▏       | 143M/671M [00:47<05:44, 1.61MB/s] 21%|██▏       | 143M/671M [00:47<05:52, 1.57MB/s] 21%|██▏       | 144M/671M [00:48<05:56, 1.55MB/s] 21%|██▏       | 144M/671M [00:48<06:02, 1.52MB/s] 21%|██▏       | 144M/671M [00:48<06:13, 1.48MB/s] 21%|██▏       | 144M/671M [00:48<06:34, 1.40MB/s] 21%|██▏       | 144M/671M [00:48<06:33, 1.40MB/s] 22%|██▏       | 144M/671M [00:48<06:22, 1.44MB/s] 22%|██▏       | 145M/671M [00:48<05:10, 1.78MB/s] 22%|██▏       | 145M/671M [00:48<05:18, 1.73MB/s] 22%|██▏       | 145M/671M [00:48<04:32, 2.02MB/s] 22%|██▏       | 145M/671M [00:49<04:41, 1.96MB/s] 22%|██▏       | 146M/671M [00:49<03:57, 2.32MB/s] 22%|██▏       | 146M/671M [00:49<03:21, 2.74MB/s] 22%|██▏       | 146M/671M [00:49<03:17, 2.79MB/s] 22%|██▏       | 147M/671M [00:49<03:05, 2.97MB/s] 22%|██▏       | 147M/671M [00:49<03:40, 2.50MB/s] 22%|██▏       | 147M/671M [00:49<04:04, 2.25MB/s] 22%|██▏       | 148M/671M [00:50<04:04, 2.25MB/s] 22%|██▏       | 148M/671M [00:50<04:07, 2.22MB/s] 22%|██▏       | 148M/671M [00:50<04:10, 2.19MB/s] 22%|██▏       | 148M/671M [00:50<04:13, 2.16MB/s] 22%|██▏       | 148M/671M [00:50<04:24, 2.07MB/s] 22%|██▏       | 149M/671M [00:50<04:30, 2.02MB/s] 22%|██▏       | 149M/671M [00:50<04:43, 1.93MB/s] 22%|██▏       | 149M/671M [00:50<05:05, 1.79MB/s] 22%|██▏       | 149M/671M [00:50<05:27, 1.67MB/s] 22%|██▏       | 149M/671M [00:51<05:35, 1.63MB/s] 22%|██▏       | 149M/671M [00:51<06:11, 1.47MB/s] 22%|██▏       | 150M/671M [00:51<06:44, 1.35MB/s] 22%|██▏       | 150M/671M [00:51<06:42, 1.36MB/s] 22%|██▏       | 150M/671M [00:51<06:12, 1.47MB/s] 22%|██▏       | 150M/671M [00:51<05:57, 1.52MB/s] 22%|██▏       | 150M/671M [00:51<05:31, 1.65MB/s] 22%|██▏       | 151M/671M [00:51<04:44, 1.92MB/s] 23%|██▎       | 151M/671M [00:52<04:43, 1.92MB/s] 23%|██▎       | 151M/671M [00:52<04:47, 1.89MB/s] 23%|██▎       | 151M/671M [00:52<04:40, 1.94MB/s] 23%|██▎       | 152M/671M [00:52<03:53, 2.33MB/s] 23%|██▎       | 152M/671M [00:52<04:10, 2.17MB/s] 23%|██▎       | 152M/671M [00:52<04:08, 2.19MB/s] 23%|██▎       | 152M/671M [00:52<03:43, 2.43MB/s] 23%|██▎       | 153M/671M [00:52<04:21, 2.08MB/s] 23%|██▎       | 153M/671M [00:52<03:41, 2.45MB/s] 23%|██▎       | 153M/671M [00:53<04:33, 1.98MB/s] 23%|██▎       | 153M/671M [00:53<04:41, 1.92MB/s] 23%|██▎       | 154M/671M [00:53<05:38, 1.60MB/s] 23%|██▎       | 154M/671M [00:53<05:32, 1.63MB/s] 23%|██▎       | 154M/671M [00:53<05:01, 1.79MB/s] 23%|██▎       | 155M/671M [00:53<04:28, 2.02MB/s] 23%|██▎       | 155M/671M [00:54<04:25, 2.03MB/s] 23%|██▎       | 155M/671M [00:54<04:12, 2.14MB/s] 23%|██▎       | 155M/671M [00:54<03:40, 2.45MB/s] 23%|██▎       | 156M/671M [00:54<03:55, 2.29MB/s] 23%|██▎       | 156M/671M [00:54<03:58, 2.26MB/s] 23%|██▎       | 156M/671M [00:54<04:05, 2.20MB/s] 23%|██▎       | 157M/671M [00:54<03:41, 2.43MB/s] 23%|██▎       | 157M/671M [00:54<03:42, 2.42MB/s] 23%|██▎       | 157M/671M [00:55<03:20, 2.68MB/s] 23%|██▎       | 157M/671M [00:55<03:36, 2.48MB/s] 24%|██▎       | 158M/671M [00:55<03:27, 2.59MB/s] 24%|██▎       | 158M/671M [00:55<03:49, 2.34MB/s] 24%|██▎       | 158M/671M [00:55<03:56, 2.27MB/s] 24%|██▎       | 158M/671M [00:55<04:02, 2.21MB/s] 24%|██▎       | 159M/671M [00:55<03:53, 2.30MB/s] 24%|██▎       | 159M/671M [00:55<03:50, 2.33MB/s] 24%|██▎       | 159M/671M [00:55<03:56, 2.27MB/s] 24%|██▍       | 159M/671M [00:56<04:44, 1.89MB/s] 24%|██▍       | 160M/671M [00:56<04:47, 1.86MB/s] 24%|██▍       | 160M/671M [00:56<04:39, 1.92MB/s] 24%|██▍       | 160M/671M [00:56<04:05, 2.18MB/s] 24%|██▍       | 160M/671M [00:56<03:48, 2.34MB/s] 24%|██▍       | 161M/671M [00:56<03:46, 2.36MB/s] 24%|██▍       | 161M/671M [00:56<04:11, 2.12MB/s] 24%|██▍       | 161M/671M [00:56<04:18, 2.07MB/s] 24%|██▍       | 161M/671M [00:57<04:27, 2.00MB/s] 24%|██▍       | 162M/671M [00:57<04:32, 1.96MB/s] 24%|██▍       | 162M/671M [00:57<04:48, 1.85MB/s] 24%|██▍       | 162M/671M [00:57<04:31, 1.96MB/s] 24%|██▍       | 162M/671M [00:57<04:16, 2.08MB/s] 24%|██▍       | 162M/671M [00:57<04:23, 2.02MB/s] 24%|██▍       | 163M/671M [00:57<04:32, 1.95MB/s] 24%|██▍       | 163M/671M [00:57<04:50, 1.83MB/s] 24%|██▍       | 163M/671M [00:58<04:48, 1.85MB/s] 24%|██▍       | 163M/671M [00:58<04:47, 1.85MB/s] 24%|██▍       | 163M/671M [00:58<04:29, 1.97MB/s] 24%|██▍       | 164M/671M [00:58<05:35, 1.59MB/s] 24%|██▍       | 164M/671M [00:58<04:18, 2.05MB/s] 24%|██▍       | 164M/671M [00:58<04:39, 1.90MB/s] 25%|██▍       | 164M/671M [00:58<04:42, 1.88MB/s] 25%|██▍       | 165M/671M [00:58<04:55, 1.80MB/s] 25%|██▍       | 165M/671M [00:59<05:51, 1.51MB/s] 25%|██▍       | 165M/671M [00:59<06:18, 1.40MB/s] 25%|██▍       | 165M/671M [00:59<06:23, 1.38MB/s] 25%|██▍       | 165M/671M [00:59<06:19, 1.40MB/s] 25%|██▍       | 165M/671M [00:59<06:17, 1.40MB/s] 25%|██▍       | 166M/671M [00:59<06:18, 1.40MB/s] 25%|██▍       | 166M/671M [00:59<06:26, 1.37MB/s] 25%|██▍       | 166M/671M [01:00<06:38, 1.33MB/s] 25%|██▍       | 166M/671M [01:00<06:44, 1.31MB/s] 25%|██▍       | 166M/671M [01:00<06:33, 1.34MB/s] 25%|██▍       | 167M/671M [01:00<06:31, 1.35MB/s] 25%|██▍       | 167M/671M [01:00<05:57, 1.48MB/s] 25%|██▍       | 167M/671M [01:00<05:42, 1.54MB/s] 25%|██▍       | 167M/671M [01:00<05:36, 1.57MB/s] 25%|██▍       | 167M/671M [01:01<05:35, 1.57MB/s] 25%|██▍       | 167M/671M [01:01<06:14, 1.41MB/s] 25%|██▍       | 168M/671M [01:01<06:40, 1.32MB/s] 25%|██▌       | 168M/671M [01:01<06:30, 1.35MB/s] 25%|██▌       | 168M/671M [01:01<05:56, 1.48MB/s] 25%|██▌       | 168M/671M [01:01<05:33, 1.58MB/s] 25%|██▌       | 168M/671M [01:01<05:13, 1.68MB/s] 25%|██▌       | 169M/671M [01:01<04:58, 1.76MB/s] 25%|██▌       | 169M/671M [01:02<04:01, 2.17MB/s] 25%|██▌       | 170M/671M [01:02<03:28, 2.52MB/s] 25%|██▌       | 170M/671M [01:02<02:37, 3.32MB/s] 25%|██▌       | 171M/671M [01:02<02:40, 3.27MB/s] 25%|██▌       | 171M/671M [01:02<02:41, 3.24MB/s] 26%|██▌       | 171M/671M [01:02<02:38, 3.30MB/s] 26%|██▌       | 172M/671M [01:02<02:26, 3.56MB/s] 26%|██▌       | 172M/671M [01:02<02:28, 3.52MB/s] 26%|██▌       | 173M/671M [01:03<02:27, 3.55MB/s] 26%|██▌       | 173M/671M [01:03<02:34, 3.37MB/s] 26%|██▌       | 173M/671M [01:03<02:51, 3.05MB/s] 26%|██▌       | 174M/671M [01:03<02:52, 3.02MB/s] 26%|██▌       | 174M/671M [01:03<02:56, 2.95MB/s] 26%|██▌       | 175M/671M [01:03<02:48, 3.08MB/s] 26%|██▌       | 175M/671M [01:03<02:54, 2.98MB/s] 26%|██▌       | 176M/671M [01:04<02:55, 2.97MB/s] 26%|██▌       | 176M/671M [01:04<02:57, 2.91MB/s] 26%|██▋       | 176M/671M [01:04<02:45, 3.13MB/s] 26%|██▋       | 177M/671M [01:04<03:03, 2.83MB/s] 26%|██▋       | 177M/671M [01:04<02:53, 2.98MB/s] 26%|██▋       | 177M/671M [01:04<02:39, 3.24MB/s] 27%|██▋       | 178M/671M [01:04<03:05, 2.79MB/s] 27%|██▋       | 178M/671M [01:05<04:25, 1.95MB/s] 27%|██▋       | 178M/671M [01:05<04:23, 1.96MB/s] 27%|██▋       | 178M/671M [01:05<05:22, 1.60MB/s] 27%|██▋       | 179M/671M [01:05<06:33, 1.31MB/s] 27%|██▋       | 179M/671M [01:05<06:29, 1.32MB/s] 27%|██▋       | 179M/671M [01:06<06:22, 1.35MB/s] 27%|██▋       | 179M/671M [01:06<07:08, 1.20MB/s] 27%|██▋       | 179M/671M [01:06<07:28, 1.15MB/s] 27%|██▋       | 179M/671M [01:06<07:38, 1.12MB/s] 27%|██▋       | 179M/671M [01:06<07:43, 1.11MB/s] 27%|██▋       | 180M/671M [01:06<07:40, 1.12MB/s] 27%|██▋       | 180M/671M [01:06<07:32, 1.14MB/s] 27%|██▋       | 180M/671M [01:06<07:19, 1.17MB/s] 27%|██▋       | 180M/671M [01:07<07:04, 1.21MB/s] 27%|██▋       | 180M/671M [01:07<07:08, 1.20MB/s] 27%|██▋       | 180M/671M [01:07<07:27, 1.15MB/s] 27%|██▋       | 180M/671M [01:07<07:29, 1.14MB/s] 27%|██▋       | 181M/671M [01:07<07:31, 1.14MB/s] 27%|██▋       | 181M/671M [01:07<06:30, 1.32MB/s] 27%|██▋       | 181M/671M [01:07<05:29, 1.56MB/s] 27%|██▋       | 181M/671M [01:07<04:15, 2.01MB/s] 27%|██▋       | 182M/671M [01:07<04:26, 1.92MB/s] 27%|██▋       | 182M/671M [01:08<03:59, 2.14MB/s] 27%|██▋       | 182M/671M [01:08<04:15, 2.00MB/s] 27%|██▋       | 182M/671M [01:08<04:18, 1.98MB/s] 27%|██▋       | 182M/671M [01:08<04:11, 2.04MB/s] 27%|██▋       | 183M/671M [01:08<03:47, 2.25MB/s] 27%|██▋       | 183M/671M [01:08<03:42, 2.30MB/s] 27%|██▋       | 183M/671M [01:08<03:35, 2.37MB/s] 27%|██▋       | 183M/671M [01:08<03:44, 2.28MB/s] 27%|██▋       | 184M/671M [01:09<03:59, 2.14MB/s] 27%|██▋       | 184M/671M [01:09<04:18, 1.97MB/s] 27%|██▋       | 184M/671M [01:09<04:39, 1.82MB/s] 27%|██▋       | 184M/671M [01:09<04:37, 1.83MB/s] 28%|██▊       | 185M/671M [01:09<04:33, 1.86MB/s] 28%|██▊       | 185M/671M [01:09<04:34, 1.86MB/s] 28%|██▊       | 185M/671M [01:09<04:26, 1.91MB/s] 28%|██▊       | 185M/671M [01:09<04:55, 1.73MB/s] 28%|██▊       | 185M/671M [01:10<04:44, 1.79MB/s] 28%|██▊       | 186M/671M [01:10<03:52, 2.19MB/s] 28%|██▊       | 186M/671M [01:10<03:24, 2.49MB/s] 28%|██▊       | 186M/671M [01:10<03:34, 2.36MB/s] 28%|██▊       | 187M/671M [01:10<03:41, 2.30MB/s] 28%|██▊       | 187M/671M [01:10<03:52, 2.18MB/s] 28%|██▊       | 187M/671M [01:10<03:54, 2.16MB/s] 28%|██▊       | 187M/671M [01:10<03:49, 2.21MB/s] 28%|██▊       | 188M/671M [01:11<03:42, 2.27MB/s] 28%|██▊       | 188M/671M [01:11<03:54, 2.16MB/s] 28%|██▊       | 188M/671M [01:11<04:13, 2.00MB/s] 28%|██▊       | 188M/671M [01:11<04:19, 1.95MB/s] 28%|██▊       | 189M/671M [01:11<04:05, 2.05MB/s] 28%|██▊       | 189M/671M [01:11<04:16, 1.97MB/s] 28%|██▊       | 189M/671M [01:11<04:07, 2.04MB/s] 28%|██▊       | 189M/671M [01:12<05:03, 1.66MB/s] 28%|██▊       | 190M/671M [01:12<05:02, 1.67MB/s] 28%|██▊       | 190M/671M [01:12<05:02, 1.67MB/s] 28%|██▊       | 190M/671M [01:12<05:00, 1.68MB/s] 28%|██▊       | 190M/671M [01:12<05:00, 1.68MB/s] 28%|██▊       | 190M/671M [01:12<05:38, 1.49MB/s] 28%|██▊       | 190M/671M [01:12<05:04, 1.65MB/s] 28%|██▊       | 191M/671M [01:12<04:56, 1.70MB/s] 28%|██▊       | 191M/671M [01:12<04:39, 1.80MB/s] 28%|██▊       | 191M/671M [01:13<04:48, 1.74MB/s] 29%|██▊       | 191M/671M [01:13<04:27, 1.88MB/s] 29%|██▊       | 191M/671M [01:13<04:28, 1.87MB/s] 29%|██▊       | 192M/671M [01:13<04:28, 1.87MB/s] 29%|██▊       | 192M/671M [01:13<04:40, 1.79MB/s] 29%|██▊       | 192M/671M [01:13<04:28, 1.87MB/s] 29%|██▊       | 192M/671M [01:13<05:14, 1.59MB/s] 29%|██▊       | 192M/671M [01:13<05:18, 1.58MB/s] 29%|██▊       | 193M/671M [01:13<04:54, 1.70MB/s] 29%|██▉       | 193M/671M [01:14<04:23, 1.90MB/s] 29%|██▉       | 193M/671M [01:14<04:15, 1.96MB/s] 29%|██▉       | 193M/671M [01:14<04:05, 2.04MB/s] 29%|██▉       | 194M/671M [01:14<04:02, 2.06MB/s] 29%|██▉       | 194M/671M [01:14<04:13, 1.97MB/s] 29%|██▉       | 194M/671M [01:14<04:13, 1.97MB/s] 29%|██▉       | 194M/671M [01:14<03:58, 2.10MB/s] 29%|██▉       | 194M/671M [01:14<03:37, 2.30MB/s] 29%|██▉       | 195M/671M [01:14<03:41, 2.26MB/s] 29%|██▉       | 195M/671M [01:15<03:12, 2.58MB/s] 29%|██▉       | 195M/671M [01:15<02:32, 3.26MB/s] 29%|██▉       | 196M/671M [01:15<02:13, 3.73MB/s] 29%|██▉       | 196M/671M [01:15<02:01, 4.11MB/s] 29%|██▉       | 197M/671M [01:15<02:11, 3.77MB/s] 29%|██▉       | 197M/671M [01:15<02:25, 3.41MB/s] 29%|██▉       | 198M/671M [01:15<02:43, 3.03MB/s] 29%|██▉       | 198M/671M [01:15<02:50, 2.91MB/s] 30%|██▉       | 198M/671M [01:16<02:52, 2.87MB/s] 30%|██▉       | 198M/671M [01:16<02:49, 2.93MB/s] 30%|██▉       | 199M/671M [01:16<02:56, 2.81MB/s] 30%|██▉       | 199M/671M [01:16<02:58, 2.76MB/s] 30%|██▉       | 199M/671M [01:16<02:48, 2.93MB/s] 30%|██▉       | 200M/671M [01:16<02:21, 3.49MB/s] 30%|██▉       | 200M/671M [01:16<02:12, 3.73MB/s] 30%|██▉       | 201M/671M [01:16<02:10, 3.77MB/s] 30%|██▉       | 201M/671M [01:16<02:13, 3.70MB/s] 30%|███       | 201M/671M [01:17<02:25, 3.37MB/s] 30%|███       | 202M/671M [01:17<02:30, 3.28MB/s] 30%|███       | 202M/671M [01:17<02:30, 3.27MB/s] 30%|███       | 202M/671M [01:17<02:38, 3.10MB/s] 30%|███       | 203M/671M [01:17<02:55, 2.80MB/s] 30%|███       | 203M/671M [01:17<03:05, 2.64MB/s] 30%|███       | 203M/671M [01:17<03:18, 2.46MB/s] 30%|███       | 203M/671M [01:17<03:10, 2.58MB/s] 30%|███       | 204M/671M [01:17<03:07, 2.61MB/s] 30%|███       | 204M/671M [01:18<04:12, 1.94MB/s] 30%|███       | 204M/671M [01:18<03:32, 2.31MB/s] 31%|███       | 205M/671M [01:18<03:38, 2.23MB/s] 31%|███       | 205M/671M [01:18<04:04, 1.99MB/s] 31%|███       | 205M/671M [01:18<03:51, 2.11MB/s] 31%|███       | 205M/671M [01:18<03:44, 2.17MB/s] 31%|███       | 206M/671M [01:18<03:42, 2.19MB/s] 31%|███       | 206M/671M [01:19<03:32, 2.30MB/s] 31%|███       | 206M/671M [01:19<03:32, 2.29MB/s] 31%|███       | 206M/671M [01:19<03:33, 2.28MB/s] 31%|███       | 206M/671M [01:19<03:37, 2.24MB/s] 31%|███       | 207M/671M [01:19<03:44, 2.16MB/s] 31%|███       | 207M/671M [01:19<04:04, 1.99MB/s] 31%|███       | 207M/671M [01:19<04:19, 1.87MB/s] 31%|███       | 207M/671M [01:19<04:23, 1.84MB/s] 31%|███       | 207M/671M [01:19<04:39, 1.74MB/s] 31%|███       | 208M/671M [01:20<04:43, 1.71MB/s] 31%|███       | 208M/671M [01:20<04:26, 1.82MB/s] 31%|███       | 208M/671M [01:20<04:22, 1.85MB/s] 31%|███       | 208M/671M [01:20<04:21, 1.85MB/s] 31%|███       | 209M/671M [01:20<04:22, 1.85MB/s] 31%|███       | 209M/671M [01:20<04:15, 1.90MB/s] 31%|███       | 209M/671M [01:20<04:04, 1.98MB/s] 31%|███       | 209M/671M [01:20<04:00, 2.01MB/s] 31%|███       | 210M/671M [01:21<03:52, 2.08MB/s] 31%|███▏      | 210M/671M [01:21<03:55, 2.06MB/s] 31%|███▏      | 210M/671M [01:21<04:02, 1.99MB/s] 31%|███▏      | 210M/671M [01:21<04:28, 1.80MB/s] 31%|███▏      | 210M/671M [01:21<04:43, 1.70MB/s] 31%|███▏      | 210M/671M [01:21<04:42, 1.71MB/s] 31%|███▏      | 211M/671M [01:21<04:54, 1.64MB/s] 31%|███▏      | 211M/671M [01:21<04:39, 1.73MB/s] 31%|███▏      | 211M/671M [01:21<04:22, 1.84MB/s] 32%|███▏      | 211M/671M [01:22<04:20, 1.85MB/s] 32%|███▏      | 212M/671M [01:22<04:13, 1.90MB/s] 32%|███▏      | 212M/671M [01:22<04:29, 1.78MB/s] 32%|███▏      | 212M/671M [01:22<04:33, 1.76MB/s] 32%|███▏      | 212M/671M [01:22<04:35, 1.75MB/s] 32%|███▏      | 212M/671M [01:22<04:50, 1.65MB/s] 32%|███▏      | 213M/671M [01:22<04:27, 1.79MB/s] 32%|███▏      | 213M/671M [01:22<04:30, 1.77MB/s] 32%|███▏      | 213M/671M [01:23<04:32, 1.76MB/s] 32%|███▏      | 213M/671M [01:23<04:14, 1.88MB/s] 32%|███▏      | 213M/671M [01:23<04:15, 1.88MB/s] 32%|███▏      | 214M/671M [01:23<04:16, 1.87MB/s] 32%|███▏      | 214M/671M [01:23<04:17, 1.86MB/s] 32%|███▏      | 214M/671M [01:23<03:58, 2.01MB/s] 32%|███▏      | 214M/671M [01:23<04:23, 1.82MB/s] 32%|███▏      | 214M/671M [01:23<03:58, 2.01MB/s] 32%|███▏      | 215M/671M [01:23<03:31, 2.26MB/s] 32%|███▏      | 215M/671M [01:24<02:57, 2.69MB/s] 32%|███▏      | 216M/671M [01:24<02:34, 3.09MB/s] 32%|███▏      | 216M/671M [01:24<02:08, 3.71MB/s] 32%|███▏      | 217M/671M [01:24<01:51, 4.28MB/s] 32%|███▏      | 217M/671M [01:24<01:39, 4.76MB/s] 32%|███▏      | 218M/671M [01:24<01:36, 4.93MB/s] 33%|███▎      | 218M/671M [01:24<01:43, 4.58MB/s] 33%|███▎      | 219M/671M [01:24<01:48, 4.37MB/s] 33%|███▎      | 219M/671M [01:25<02:11, 3.59MB/s] 33%|███▎      | 219M/671M [01:25<02:28, 3.19MB/s] 33%|███▎      | 220M/671M [01:25<02:28, 3.17MB/s] 33%|███▎      | 220M/671M [01:25<02:19, 3.39MB/s] 33%|███▎      | 221M/671M [01:25<02:46, 2.83MB/s] 33%|███▎      | 221M/671M [01:25<02:52, 2.74MB/s] 33%|███▎      | 221M/671M [01:25<02:51, 2.74MB/s] 33%|███▎      | 221M/671M [01:25<03:00, 2.61MB/s] 33%|███▎      | 222M/671M [01:26<03:09, 2.48MB/s] 33%|███▎      | 222M/671M [01:26<03:14, 2.41MB/s] 33%|███▎      | 222M/671M [01:26<02:56, 2.66MB/s] 33%|███▎      | 223M/671M [01:26<02:49, 2.77MB/s] 33%|███▎      | 223M/671M [01:26<02:49, 2.76MB/s] 33%|███▎      | 223M/671M [01:26<02:47, 2.79MB/s] 33%|███▎      | 223M/671M [01:26<02:39, 2.94MB/s] 33%|███▎      | 224M/671M [01:26<02:14, 3.47MB/s] 33%|███▎      | 224M/671M [01:26<02:02, 3.80MB/s] 34%|███▎      | 225M/671M [01:27<01:52, 4.16MB/s] 34%|███▎      | 225M/671M [01:27<02:20, 3.31MB/s] 34%|███▎      | 226M/671M [01:27<02:36, 2.99MB/s] 34%|███▎      | 226M/671M [01:27<02:34, 3.01MB/s] 34%|███▍      | 227M/671M [01:27<02:17, 3.39MB/s] 34%|███▍      | 227M/671M [01:27<02:34, 3.01MB/s] 34%|███▍      | 227M/671M [01:27<02:32, 3.04MB/s] 34%|███▍      | 228M/671M [01:28<02:38, 2.93MB/s] 34%|███▍      | 228M/671M [01:28<02:42, 2.85MB/s] 34%|███▍      | 228M/671M [01:28<02:44, 2.83MB/s] 34%|███▍      | 228M/671M [01:28<02:38, 2.92MB/s] 34%|███▍      | 229M/671M [01:28<02:33, 3.01MB/s] 34%|███▍      | 229M/671M [01:28<02:12, 3.49MB/s] 34%|███▍      | 230M/671M [01:28<02:15, 3.41MB/s] 34%|███▍      | 230M/671M [01:28<02:19, 3.31MB/s] 34%|███▍      | 230M/671M [01:28<02:26, 3.15MB/s] 34%|███▍      | 231M/671M [01:29<02:31, 3.04MB/s] 34%|███▍      | 231M/671M [01:29<02:33, 3.01MB/s] 35%|███▍      | 231M/671M [01:29<02:34, 2.98MB/s] 35%|███▍      | 232M/671M [01:29<02:25, 3.17MB/s] 35%|███▍      | 232M/671M [01:29<02:56, 2.60MB/s] 35%|███▍      | 233M/671M [01:29<02:05, 3.67MB/s] 35%|███▍      | 233M/671M [01:29<02:04, 3.69MB/s] 35%|███▍      | 234M/671M [01:29<02:08, 3.56MB/s] 35%|███▍      | 234M/671M [01:30<02:10, 3.52MB/s] 35%|███▍      | 234M/671M [01:30<02:05, 3.65MB/s] 35%|███▌      | 235M/671M [01:30<02:09, 3.53MB/s] 35%|███▌      | 235M/671M [01:30<01:56, 3.90MB/s] 35%|███▌      | 236M/671M [01:30<02:00, 3.79MB/s] 35%|███▌      | 236M/671M [01:30<01:48, 4.19MB/s] 35%|███▌      | 237M/671M [01:30<01:34, 4.82MB/s] 35%|███▌      | 237M/671M [01:30<01:40, 4.50MB/s] 35%|███▌      | 238M/671M [01:31<01:32, 4.93MB/s] 36%|███▌      | 239M/671M [01:31<01:27, 5.20MB/s] 36%|███▌      | 239M/671M [01:31<01:28, 5.11MB/s] 36%|███▌      | 240M/671M [01:31<01:26, 5.25MB/s] 36%|███▌      | 240M/671M [01:31<01:29, 5.02MB/s] 36%|███▌      | 241M/671M [01:31<01:20, 5.63MB/s] 36%|███▌      | 242M/671M [01:31<01:14, 6.03MB/s] 36%|███▌      | 242M/671M [01:31<01:11, 6.28MB/s] 36%|███▋      | 243M/671M [01:31<01:08, 6.56MB/s] 36%|███▋      | 244M/671M [01:31<01:05, 6.82MB/s] 36%|███▋      | 245M/671M [01:32<01:03, 7.05MB/s] 37%|███▋      | 245M/671M [01:32<01:01, 7.26MB/s] 37%|███▋      | 246M/671M [01:32<01:00, 7.35MB/s] 37%|███▋      | 247M/671M [01:32<00:59, 7.46MB/s] 37%|███▋      | 248M/671M [01:32<00:58, 7.56MB/s] 37%|███▋      | 248M/671M [01:32<01:01, 7.18MB/s] 37%|███▋      | 249M/671M [01:32<01:23, 5.29MB/s] 37%|███▋      | 250M/671M [01:33<01:41, 4.37MB/s] 37%|███▋      | 250M/671M [01:33<01:51, 3.97MB/s] 37%|███▋      | 250M/671M [01:33<01:54, 3.86MB/s] 37%|███▋      | 251M/671M [01:33<01:58, 3.70MB/s] 37%|███▋      | 251M/671M [01:33<01:59, 3.68MB/s] 38%|███▊      | 252M/671M [01:33<02:03, 3.55MB/s] 38%|███▊      | 252M/671M [01:33<02:12, 3.32MB/s] 38%|███▊      | 252M/671M [01:33<02:02, 3.57MB/s] 38%|███▊      | 253M/671M [01:34<02:24, 3.04MB/s] 38%|███▊      | 253M/671M [01:34<02:12, 3.30MB/s] 38%|███▊      | 254M/671M [01:34<02:11, 3.32MB/s] 38%|███▊      | 254M/671M [01:34<02:18, 3.15MB/s] 38%|███▊      | 254M/671M [01:34<02:31, 2.88MB/s] 38%|███▊      | 255M/671M [01:34<02:15, 3.22MB/s] 38%|███▊      | 255M/671M [01:34<02:15, 3.22MB/s] 38%|███▊      | 255M/671M [01:34<02:14, 3.25MB/s] 38%|███▊      | 256M/671M [01:35<02:05, 3.47MB/s] 38%|███▊      | 256M/671M [01:35<01:54, 3.81MB/s] 38%|███▊      | 257M/671M [01:35<01:44, 4.15MB/s] 38%|███▊      | 257M/671M [01:35<01:42, 4.25MB/s] 38%|███▊      | 258M/671M [01:35<01:40, 4.29MB/s] 39%|███▊      | 258M/671M [01:35<01:43, 4.19MB/s] 39%|███▊      | 259M/671M [01:35<01:46, 4.04MB/s] 39%|███▊      | 260M/671M [01:36<01:51, 3.85MB/s] 39%|███▉      | 260M/671M [01:36<01:42, 4.19MB/s] 39%|███▉      | 261M/671M [01:36<01:44, 4.10MB/s] 39%|███▉      | 261M/671M [01:36<01:46, 4.03MB/s] 39%|███▉      | 262M/671M [01:36<01:47, 4.00MB/s] 39%|███▉      | 262M/671M [01:36<01:46, 4.01MB/s] 39%|███▉      | 262M/671M [01:36<01:47, 3.98MB/s] 39%|███▉      | 263M/671M [01:36<01:50, 3.88MB/s] 39%|███▉      | 263M/671M [01:37<02:16, 3.13MB/s] 39%|███▉      | 263M/671M [01:37<02:18, 3.09MB/s] 39%|███▉      | 264M/671M [01:37<02:07, 3.34MB/s] 39%|███▉      | 264M/671M [01:37<02:16, 3.13MB/s] 39%|███▉      | 265M/671M [01:37<01:59, 3.57MB/s] 40%|███▉      | 265M/671M [01:37<01:59, 3.55MB/s] 40%|███▉      | 265M/671M [01:37<02:09, 3.27MB/s] 40%|███▉      | 266M/671M [01:37<01:50, 3.85MB/s] 40%|███▉      | 266M/671M [01:37<02:01, 3.50MB/s] 40%|███▉      | 267M/671M [01:38<01:50, 3.84MB/s] 40%|███▉      | 267M/671M [01:38<01:34, 4.47MB/s] 40%|███▉      | 268M/671M [01:38<01:29, 4.71MB/s] 40%|████      | 269M/671M [01:38<01:10, 5.95MB/s] 40%|████      | 270M/671M [01:38<01:04, 6.51MB/s] 40%|████      | 270M/671M [01:38<01:16, 5.50MB/s] 40%|████      | 271M/671M [01:38<01:17, 5.41MB/s] 41%|████      | 272M/671M [01:39<01:36, 4.34MB/s] 41%|████      | 272M/671M [01:39<01:43, 4.03MB/s] 41%|████      | 272M/671M [01:39<02:08, 3.24MB/s] 41%|████      | 273M/671M [01:39<02:18, 3.01MB/s] 41%|████      | 273M/671M [01:39<02:14, 3.09MB/s] 41%|████      | 273M/671M [01:39<02:34, 2.70MB/s] 41%|████      | 274M/671M [01:39<02:40, 2.60MB/s] 41%|████      | 274M/671M [01:40<02:56, 2.35MB/s] 41%|████      | 274M/671M [01:40<03:09, 2.19MB/s] 41%|████      | 275M/671M [01:40<03:16, 2.11MB/s] 41%|████      | 275M/671M [01:40<03:17, 2.10MB/s] 41%|████      | 275M/671M [01:40<03:23, 2.04MB/s] 41%|████      | 275M/671M [01:40<03:21, 2.06MB/s] 41%|████      | 276M/671M [01:40<03:12, 2.16MB/s] 41%|████      | 276M/671M [01:41<03:02, 2.27MB/s] 41%|████      | 276M/671M [01:41<02:50, 2.43MB/s] 41%|████      | 277M/671M [01:41<02:48, 2.45MB/s] 41%|████▏     | 277M/671M [01:41<02:43, 2.53MB/s] 41%|████▏     | 277M/671M [01:41<02:42, 2.54MB/s] 41%|████▏     | 277M/671M [01:41<02:41, 2.55MB/s] 41%|████▏     | 278M/671M [01:41<02:43, 2.51MB/s] 41%|████▏     | 278M/671M [01:41<02:43, 2.52MB/s] 41%|████▏     | 278M/671M [01:42<02:39, 2.57MB/s] 42%|████▏     | 279M/671M [01:42<02:41, 2.55MB/s] 42%|████▏     | 279M/671M [01:42<02:46, 2.47MB/s] 42%|████▏     | 279M/671M [01:42<02:51, 2.39MB/s] 42%|████▏     | 280M/671M [01:42<02:52, 2.38MB/s] 42%|████▏     | 280M/671M [01:42<02:44, 2.49MB/s] 42%|████▏     | 280M/671M [01:42<02:41, 2.53MB/s] 42%|████▏     | 280M/671M [01:42<02:48, 2.42MB/s] 42%|████▏     | 281M/671M [01:43<03:00, 2.26MB/s] 42%|████▏     | 281M/671M [01:43<02:54, 2.35MB/s] 42%|████▏     | 281M/671M [01:43<02:50, 2.39MB/s] 42%|████▏     | 282M/671M [01:43<02:39, 2.55MB/s] 42%|████▏     | 282M/671M [01:43<02:38, 2.56MB/s] 42%|████▏     | 282M/671M [01:43<02:37, 2.58MB/s] 42%|████▏     | 283M/671M [01:43<02:42, 2.51MB/s] 42%|████▏     | 283M/671M [01:43<02:33, 2.65MB/s] 42%|████▏     | 283M/671M [01:44<02:44, 2.47MB/s] 42%|████▏     | 283M/671M [01:44<02:39, 2.54MB/s] 42%|████▏     | 284M/671M [01:44<02:38, 2.56MB/s] 42%|████▏     | 284M/671M [01:44<02:40, 2.53MB/s] 42%|████▏     | 284M/671M [01:44<02:34, 2.63MB/s] 42%|████▏     | 285M/671M [01:44<02:32, 2.65MB/s] 43%|████▎     | 285M/671M [01:44<02:38, 2.56MB/s] 43%|████▎     | 285M/671M [01:44<02:35, 2.60MB/s] 43%|████▎     | 286M/671M [01:45<02:48, 2.39MB/s] 43%|████▎     | 286M/671M [01:45<02:55, 2.30MB/s] 43%|████▎     | 286M/671M [01:45<02:58, 2.25MB/s] 43%|████▎     | 287M/671M [01:45<03:02, 2.21MB/s] 43%|████▎     | 287M/671M [01:45<03:05, 2.17MB/s] 43%|████▎     | 287M/671M [01:45<02:58, 2.25MB/s] 43%|████▎     | 287M/671M [01:45<03:02, 2.20MB/s] 43%|████▎     | 288M/671M [01:46<03:20, 2.00MB/s] 43%|████▎     | 288M/671M [01:46<03:22, 1.99MB/s] 43%|████▎     | 288M/671M [01:46<02:57, 2.26MB/s] 43%|████▎     | 288M/671M [01:46<03:04, 2.17MB/s] 43%|████▎     | 289M/671M [01:46<03:07, 2.14MB/s] 43%|████▎     | 289M/671M [01:46<03:05, 2.16MB/s] 43%|████▎     | 289M/671M [01:46<02:49, 2.36MB/s] 43%|████▎     | 289M/671M [01:46<02:33, 2.60MB/s] 43%|████▎     | 290M/671M [01:46<02:27, 2.71MB/s] 43%|████▎     | 290M/671M [01:47<02:15, 2.95MB/s] 43%|████▎     | 290M/671M [01:47<01:55, 3.46MB/s] 43%|████▎     | 291M/671M [01:47<01:46, 3.74MB/s] 44%|████▎     | 292M/671M [01:47<01:13, 5.43MB/s] 44%|████▎     | 293M/671M [01:47<01:07, 5.84MB/s] 44%|████▎     | 293M/671M [01:47<01:08, 5.80MB/s] 44%|████▍     | 294M/671M [01:47<01:06, 5.91MB/s] 44%|████▍     | 294M/671M [01:47<01:07, 5.85MB/s] 44%|████▍     | 295M/671M [01:47<01:06, 5.95MB/s] 44%|████▍     | 296M/671M [01:48<01:10, 5.56MB/s] 44%|████▍     | 296M/671M [01:48<01:08, 5.72MB/s] 44%|████▍     | 297M/671M [01:48<01:20, 4.88MB/s] 44%|████▍     | 297M/671M [01:48<01:24, 4.61MB/s] 44%|████▍     | 298M/671M [01:48<01:24, 4.61MB/s] 45%|████▍     | 298M/671M [01:48<01:22, 4.71MB/s] 45%|████▍     | 299M/671M [01:48<01:26, 4.53MB/s] 45%|████▍     | 299M/671M [01:48<01:23, 4.66MB/s] 45%|████▍     | 300M/671M [01:49<01:22, 4.73MB/s] 45%|████▍     | 300M/671M [01:49<01:38, 3.95MB/s] 45%|████▍     | 301M/671M [01:49<01:28, 4.38MB/s] 45%|████▍     | 302M/671M [01:49<01:18, 4.94MB/s] 45%|████▌     | 302M/671M [01:49<01:18, 4.91MB/s] 45%|████▌     | 303M/671M [01:49<01:07, 5.74MB/s] 45%|████▌     | 304M/671M [01:49<00:54, 7.07MB/s] 45%|████▌     | 305M/671M [01:49<00:55, 6.86MB/s] 46%|████▌     | 305M/671M [01:49<00:56, 6.73MB/s] 46%|████▌     | 306M/671M [01:50<01:00, 6.32MB/s] 46%|████▌     | 307M/671M [01:50<01:04, 5.89MB/s] 46%|████▌     | 307M/671M [01:50<01:06, 5.77MB/s] 46%|████▌     | 308M/671M [01:50<01:03, 5.95MB/s] 46%|████▌     | 309M/671M [01:50<00:59, 6.36MB/s] 46%|████▌     | 309M/671M [01:50<01:00, 6.30MB/s] 46%|████▌     | 310M/671M [01:50<01:07, 5.59MB/s] 46%|████▋     | 311M/671M [01:50<01:05, 5.77MB/s] 46%|████▋     | 311M/671M [01:51<01:04, 5.86MB/s] 47%|████▋     | 312M/671M [01:51<00:52, 7.12MB/s] 47%|████▋     | 314M/671M [01:51<00:41, 9.04MB/s] 47%|████▋     | 315M/671M [01:51<00:36, 10.1MB/s] 47%|████▋     | 316M/671M [01:51<00:43, 8.55MB/s] 47%|████▋     | 317M/671M [01:51<00:53, 6.98MB/s] 47%|████▋     | 318M/671M [01:51<00:59, 6.23MB/s] 47%|████▋     | 318M/671M [01:52<01:06, 5.57MB/s] 48%|████▊     | 319M/671M [01:52<01:05, 5.66MB/s] 48%|████▊     | 319M/671M [01:52<01:16, 4.84MB/s] 48%|████▊     | 320M/671M [01:52<01:32, 3.99MB/s] 48%|████▊     | 321M/671M [01:52<01:13, 4.99MB/s] 48%|████▊     | 322M/671M [01:52<01:19, 4.59MB/s] 48%|████▊     | 322M/671M [01:53<01:24, 4.34MB/s] 48%|████▊     | 323M/671M [01:53<01:33, 3.92MB/s] 48%|████▊     | 323M/671M [01:53<01:12, 5.03MB/s] 48%|████▊     | 324M/671M [01:53<01:12, 5.03MB/s] 48%|████▊     | 325M/671M [01:53<01:16, 4.75MB/s] 48%|████▊     | 325M/671M [01:53<01:19, 4.58MB/s] 49%|████▊     | 326M/671M [01:53<01:26, 4.19MB/s] 49%|████▊     | 326M/671M [01:53<01:25, 4.23MB/s] 49%|████▊     | 326M/671M [01:54<01:26, 4.18MB/s] 49%|████▊     | 327M/671M [01:54<01:28, 4.05MB/s] 49%|████▉     | 327M/671M [01:54<01:49, 3.28MB/s] 49%|████▉     | 328M/671M [01:54<01:49, 3.29MB/s] 49%|████▉     | 328M/671M [01:54<01:49, 3.28MB/s] 49%|████▉     | 328M/671M [01:54<02:16, 2.62MB/s] 49%|████▉     | 328M/671M [01:54<02:21, 2.54MB/s] 49%|████▉     | 329M/671M [01:54<02:22, 2.51MB/s] 49%|████▉     | 329M/671M [01:55<02:18, 2.58MB/s] 49%|████▉     | 330M/671M [01:55<02:02, 2.91MB/s] 49%|████▉     | 330M/671M [01:55<02:02, 2.91MB/s] 49%|████▉     | 330M/671M [01:55<02:11, 2.71MB/s] 49%|████▉     | 330M/671M [01:55<02:17, 2.59MB/s] 49%|████▉     | 331M/671M [01:55<02:27, 2.42MB/s] 49%|████▉     | 331M/671M [01:55<02:33, 2.32MB/s] 49%|████▉     | 331M/671M [01:56<02:44, 2.16MB/s] 49%|████▉     | 331M/671M [01:56<02:45, 2.15MB/s] 49%|████▉     | 332M/671M [01:56<02:41, 2.20MB/s] 50%|████▉     | 332M/671M [01:56<02:19, 2.54MB/s] 50%|████▉     | 333M/671M [01:56<02:06, 2.79MB/s] 50%|████▉     | 333M/671M [01:56<01:44, 3.39MB/s] 50%|████▉     | 334M/671M [01:56<01:34, 3.74MB/s] 50%|████▉     | 334M/671M [01:56<01:36, 3.67MB/s] 50%|████▉     | 334M/671M [01:57<01:47, 3.28MB/s] 50%|████▉     | 335M/671M [01:57<01:47, 3.29MB/s] 50%|████▉     | 335M/671M [01:57<01:48, 3.23MB/s] 50%|█████     | 335M/671M [01:57<01:50, 3.19MB/s] 50%|█████     | 336M/671M [01:57<01:54, 3.08MB/s] 50%|█████     | 336M/671M [01:57<01:55, 3.03MB/s] 50%|█████     | 336M/671M [01:57<02:04, 2.81MB/s] 50%|█████     | 337M/671M [01:57<01:49, 3.20MB/s] 50%|█████     | 337M/671M [01:57<01:35, 3.66MB/s] 50%|█████     | 338M/671M [01:58<01:29, 3.89MB/s] 50%|█████     | 338M/671M [01:58<01:50, 3.15MB/s] 50%|█████     | 338M/671M [01:58<02:09, 2.68MB/s] 51%|█████     | 339M/671M [01:58<02:08, 2.72MB/s] 51%|█████     | 339M/671M [01:58<02:19, 2.49MB/s] 51%|█████     | 339M/671M [01:58<02:27, 2.36MB/s] 51%|█████     | 340M/671M [01:59<02:23, 2.42MB/s] 51%|█████     | 340M/671M [01:59<02:18, 2.51MB/s] 51%|█████     | 340M/671M [01:59<01:55, 3.01MB/s] 51%|█████     | 341M/671M [01:59<01:41, 3.39MB/s] 51%|█████     | 341M/671M [01:59<01:47, 3.22MB/s] 51%|█████     | 342M/671M [01:59<01:56, 2.95MB/s] 51%|█████     | 342M/671M [01:59<02:14, 2.56MB/s] 51%|█████     | 342M/671M [01:59<02:17, 2.50MB/s] 51%|█████     | 342M/671M [02:00<02:37, 2.19MB/s] 51%|█████     | 343M/671M [02:00<02:38, 2.17MB/s] 51%|█████     | 343M/671M [02:00<02:34, 2.23MB/s] 51%|█████     | 343M/671M [02:00<02:29, 2.30MB/s] 51%|█████     | 344M/671M [02:00<02:54, 1.96MB/s] 51%|█████▏    | 344M/671M [02:00<02:48, 2.04MB/s] 51%|█████▏    | 344M/671M [02:00<02:43, 2.09MB/s] 51%|█████▏    | 344M/671M [02:01<02:34, 2.22MB/s] 51%|█████▏    | 345M/671M [02:01<02:34, 2.21MB/s] 51%|█████▏    | 345M/671M [02:01<02:46, 2.05MB/s] 51%|█████▏    | 345M/671M [02:01<02:50, 2.00MB/s] 52%|█████▏    | 345M/671M [02:01<02:20, 2.42MB/s] 52%|█████▏    | 346M/671M [02:01<01:43, 3.29MB/s] 52%|█████▏    | 346M/671M [02:01<01:27, 3.89MB/s] 52%|█████▏    | 347M/671M [02:01<01:19, 4.26MB/s] 52%|█████▏    | 347M/671M [02:01<01:26, 3.90MB/s] 52%|█████▏    | 348M/671M [02:02<01:46, 3.17MB/s] 52%|█████▏    | 348M/671M [02:02<02:02, 2.76MB/s] 52%|█████▏    | 348M/671M [02:02<02:14, 2.52MB/s] 52%|█████▏    | 349M/671M [02:02<02:18, 2.45MB/s] 52%|█████▏    | 349M/671M [02:02<02:17, 2.45MB/s] 52%|█████▏    | 349M/671M [02:02<02:37, 2.14MB/s] 52%|█████▏    | 350M/671M [02:02<02:14, 2.50MB/s] 52%|█████▏    | 350M/671M [02:03<02:21, 2.37MB/s] 52%|█████▏    | 350M/671M [02:03<02:31, 2.22MB/s] 52%|█████▏    | 350M/671M [02:03<02:25, 2.31MB/s] 52%|█████▏    | 351M/671M [02:03<02:54, 1.92MB/s] 52%|█████▏    | 351M/671M [02:03<03:00, 1.85MB/s] 52%|█████▏    | 351M/671M [02:03<03:20, 1.67MB/s] 52%|█████▏    | 351M/671M [02:03<03:29, 1.60MB/s] 52%|█████▏    | 351M/671M [02:04<03:43, 1.50MB/s] 52%|█████▏    | 351M/671M [02:04<03:42, 1.50MB/s] 52%|█████▏    | 352M/671M [02:04<03:51, 1.45MB/s] 52%|█████▏    | 352M/671M [02:04<03:52, 1.44MB/s] 52%|█████▏    | 352M/671M [02:04<03:34, 1.56MB/s] 52%|█████▏    | 352M/671M [02:04<03:31, 1.58MB/s] 53%|█████▎    | 352M/671M [02:04<03:00, 1.84MB/s] 53%|█████▎    | 353M/671M [02:04<03:09, 1.76MB/s] 53%|█████▎    | 353M/671M [02:04<02:48, 1.98MB/s] 53%|█████▎    | 353M/671M [02:05<02:38, 2.10MB/s] 53%|█████▎    | 353M/671M [02:05<03:01, 1.83MB/s] 53%|█████▎    | 353M/671M [02:05<02:54, 1.91MB/s] 53%|█████▎    | 354M/671M [02:05<02:29, 2.22MB/s] 53%|█████▎    | 354M/671M [02:05<02:37, 2.11MB/s] 53%|█████▎    | 354M/671M [02:05<02:26, 2.27MB/s] 53%|█████▎    | 355M/671M [02:05<02:26, 2.27MB/s] 53%|█████▎    | 355M/671M [02:05<02:32, 2.18MB/s] 53%|█████▎    | 355M/671M [02:06<02:31, 2.19MB/s] 53%|█████▎    | 355M/671M [02:06<02:29, 2.22MB/s] 53%|█████▎    | 355M/671M [02:06<02:22, 2.33MB/s] 53%|█████▎    | 356M/671M [02:06<02:43, 2.02MB/s] 53%|█████▎    | 356M/671M [02:06<02:44, 2.00MB/s] 53%|█████▎    | 356M/671M [02:06<02:45, 1.99MB/s] 53%|█████▎    | 356M/671M [02:06<02:50, 1.93MB/s] 53%|█████▎    | 356M/671M [02:06<02:50, 1.93MB/s] 53%|█████▎    | 357M/671M [02:06<02:59, 1.84MB/s] 53%|█████▎    | 357M/671M [02:07<03:02, 1.81MB/s] 53%|█████▎    | 357M/671M [02:07<03:02, 1.81MB/s] 53%|█████▎    | 357M/671M [02:07<03:02, 1.80MB/s] 53%|█████▎    | 357M/671M [02:07<03:07, 1.76MB/s] 53%|█████▎    | 358M/671M [02:07<03:04, 1.78MB/s] 53%|█████▎    | 358M/671M [02:07<02:56, 1.86MB/s] 53%|█████▎    | 358M/671M [02:07<02:53, 1.89MB/s] 53%|█████▎    | 358M/671M [02:07<02:55, 1.87MB/s] 53%|█████▎    | 358M/671M [02:07<02:58, 1.83MB/s] 53%|█████▎    | 359M/671M [02:08<03:01, 1.80MB/s] 53%|█████▎    | 359M/671M [02:08<03:11, 1.70MB/s] 54%|█████▎    | 359M/671M [02:08<03:10, 1.71MB/s] 54%|█████▎    | 359M/671M [02:08<03:01, 1.80MB/s] 54%|█████▎    | 359M/671M [02:08<02:25, 2.24MB/s] 54%|█████▎    | 360M/671M [02:08<02:23, 2.28MB/s] 54%|█████▎    | 360M/671M [02:08<02:09, 2.52MB/s] 54%|█████▎    | 360M/671M [02:08<01:59, 2.73MB/s] 54%|█████▍    | 361M/671M [02:08<01:56, 2.78MB/s] 54%|█████▍    | 361M/671M [02:09<01:51, 2.92MB/s] 54%|█████▍    | 361M/671M [02:09<01:44, 3.10MB/s] 54%|█████▍    | 362M/671M [02:09<01:40, 3.23MB/s] 54%|█████▍    | 362M/671M [02:09<01:34, 3.41MB/s] 54%|█████▍    | 363M/671M [02:09<01:33, 3.45MB/s] 54%|█████▍    | 363M/671M [02:09<01:24, 3.81MB/s] 54%|█████▍    | 364M/671M [02:09<01:13, 4.40MB/s] 54%|█████▍    | 364M/671M [02:09<01:10, 4.55MB/s] 54%|█████▍    | 365M/671M [02:09<01:06, 4.83MB/s] 54%|█████▍    | 365M/671M [02:09<01:05, 4.89MB/s] 55%|█████▍    | 366M/671M [02:10<01:10, 4.53MB/s] 55%|█████▍    | 366M/671M [02:10<01:11, 4.46MB/s] 55%|█████▍    | 366M/671M [02:10<01:15, 4.22MB/s] 55%|█████▍    | 367M/671M [02:10<01:26, 3.67MB/s] 55%|█████▍    | 367M/671M [02:10<01:35, 3.35MB/s] 55%|█████▍    | 368M/671M [02:10<01:47, 2.97MB/s] 55%|█████▍    | 368M/671M [02:11<01:53, 2.80MB/s] 55%|█████▍    | 368M/671M [02:11<01:53, 2.80MB/s] 55%|█████▌    | 369M/671M [02:11<01:48, 2.92MB/s] 55%|█████▌    | 369M/671M [02:11<01:46, 2.96MB/s] 55%|█████▌    | 370M/671M [02:11<01:47, 2.93MB/s] 55%|█████▌    | 370M/671M [02:11<01:39, 3.18MB/s] 55%|█████▌    | 371M/671M [02:11<01:31, 3.44MB/s] 55%|█████▌    | 371M/671M [02:11<01:37, 3.22MB/s] 55%|█████▌    | 371M/671M [02:12<01:31, 3.44MB/s] 55%|█████▌    | 372M/671M [02:12<01:32, 3.40MB/s] 55%|█████▌    | 372M/671M [02:12<01:39, 3.15MB/s] 56%|█████▌    | 372M/671M [02:12<01:36, 3.24MB/s] 56%|█████▌    | 373M/671M [02:12<01:39, 3.14MB/s] 56%|█████▌    | 373M/671M [02:12<01:33, 3.32MB/s] 56%|█████▌    | 374M/671M [02:12<01:29, 3.48MB/s] 56%|█████▌    | 374M/671M [02:12<01:22, 3.78MB/s] 56%|█████▌    | 375M/671M [02:13<01:21, 3.82MB/s] 56%|█████▌    | 375M/671M [02:13<01:25, 3.62MB/s] 56%|█████▌    | 375M/671M [02:13<01:32, 3.36MB/s] 56%|█████▌    | 376M/671M [02:13<01:31, 3.38MB/s] 56%|█████▌    | 376M/671M [02:13<01:27, 3.51MB/s] 56%|█████▌    | 377M/671M [02:13<01:24, 3.66MB/s] 56%|█████▌    | 377M/671M [02:13<01:19, 3.88MB/s] 56%|█████▋    | 378M/671M [02:13<01:08, 4.47MB/s] 56%|█████▋    | 378M/671M [02:13<01:08, 4.46MB/s] 56%|█████▋    | 379M/671M [02:14<01:07, 4.56MB/s] 57%|█████▋    | 379M/671M [02:14<01:11, 4.30MB/s] 57%|█████▋    | 380M/671M [02:14<01:06, 4.62MB/s] 57%|█████▋    | 380M/671M [02:14<01:11, 4.26MB/s] 57%|█████▋    | 380M/671M [02:14<01:16, 3.95MB/s] 57%|█████▋    | 381M/671M [02:14<01:19, 3.82MB/s] 57%|█████▋    | 381M/671M [02:14<01:28, 3.44MB/s] 57%|█████▋    | 382M/671M [02:14<01:25, 3.54MB/s] 57%|█████▋    | 382M/671M [02:15<01:29, 3.36MB/s] 57%|█████▋    | 382M/671M [02:15<01:26, 3.50MB/s] 57%|█████▋    | 383M/671M [02:15<01:26, 3.50MB/s] 57%|█████▋    | 383M/671M [02:15<01:11, 4.21MB/s] 57%|█████▋    | 384M/671M [02:15<01:00, 4.94MB/s] 57%|█████▋    | 384M/671M [02:15<00:59, 5.03MB/s] 57%|█████▋    | 385M/671M [02:15<00:53, 5.62MB/s] 58%|█████▊    | 386M/671M [02:15<00:49, 5.99MB/s] 58%|█████▊    | 386M/671M [02:15<00:47, 6.31MB/s] 58%|█████▊    | 387M/671M [02:15<00:45, 6.48MB/s] 58%|█████▊    | 388M/671M [02:16<00:44, 6.61MB/s] 58%|█████▊    | 389M/671M [02:16<00:44, 6.71MB/s] 58%|█████▊    | 389M/671M [02:16<00:43, 6.78MB/s] 58%|█████▊    | 390M/671M [02:16<00:43, 6.82MB/s] 58%|█████▊    | 391M/671M [02:16<00:43, 6.80MB/s] 58%|█████▊    | 391M/671M [02:16<00:45, 6.47MB/s] 58%|█████▊    | 392M/671M [02:16<00:51, 5.62MB/s] 59%|█████▊    | 392M/671M [02:16<01:02, 4.65MB/s] 59%|█████▊    | 393M/671M [02:17<01:07, 4.32MB/s] 59%|█████▊    | 393M/671M [02:17<01:07, 4.28MB/s] 59%|█████▊    | 394M/671M [02:17<01:08, 4.25MB/s] 59%|█████▉    | 394M/671M [02:17<01:08, 4.21MB/s] 59%|█████▉    | 395M/671M [02:17<01:24, 3.44MB/s] 59%|█████▉    | 395M/671M [02:17<01:26, 3.33MB/s] 59%|█████▉    | 395M/671M [02:17<01:34, 3.05MB/s] 59%|█████▉    | 396M/671M [02:17<01:33, 3.08MB/s] 59%|█████▉    | 396M/671M [02:18<01:36, 2.98MB/s] 59%|█████▉    | 396M/671M [02:18<01:47, 2.67MB/s] 59%|█████▉    | 396M/671M [02:18<01:54, 2.51MB/s] 59%|█████▉    | 397M/671M [02:18<01:55, 2.48MB/s] 59%|█████▉    | 397M/671M [02:18<01:56, 2.46MB/s] 59%|█████▉    | 397M/671M [02:18<01:50, 2.58MB/s] 59%|█████▉    | 397M/671M [02:18<02:04, 2.29MB/s] 59%|█████▉    | 398M/671M [02:18<02:01, 2.36MB/s] 59%|█████▉    | 398M/671M [02:19<02:01, 2.35MB/s] 59%|█████▉    | 398M/671M [02:19<01:45, 2.72MB/s] 59%|█████▉    | 399M/671M [02:19<01:46, 2.69MB/s] 59%|█████▉    | 399M/671M [02:19<01:46, 2.66MB/s] 60%|█████▉    | 399M/671M [02:19<01:45, 2.69MB/s] 60%|█████▉    | 399M/671M [02:19<01:49, 2.59MB/s] 60%|█████▉    | 400M/671M [02:19<01:41, 2.80MB/s] 60%|█████▉    | 400M/671M [02:19<01:34, 3.01MB/s] 60%|█████▉    | 401M/671M [02:19<01:25, 3.32MB/s] 60%|█████▉    | 401M/671M [02:20<01:15, 3.72MB/s] 60%|█████▉    | 402M/671M [02:20<01:09, 4.07MB/s] 60%|█████▉    | 402M/671M [02:20<01:05, 4.27MB/s] 60%|██████    | 403M/671M [02:20<01:01, 4.55MB/s] 60%|██████    | 403M/671M [02:20<01:04, 4.38MB/s] 60%|██████    | 404M/671M [02:20<01:04, 4.35MB/s] 60%|██████    | 404M/671M [02:20<01:03, 4.38MB/s] 60%|██████    | 405M/671M [02:20<01:00, 4.58MB/s] 60%|██████    | 405M/671M [02:21<01:03, 4.36MB/s] 61%|██████    | 406M/671M [02:21<01:07, 4.12MB/s] 61%|██████    | 406M/671M [02:21<01:07, 4.10MB/s] 61%|██████    | 407M/671M [02:21<01:08, 4.04MB/s] 61%|██████    | 407M/671M [02:21<01:06, 4.16MB/s] 61%|██████    | 408M/671M [02:21<01:05, 4.20MB/s] 61%|██████    | 408M/671M [02:21<00:57, 4.78MB/s] 61%|██████    | 409M/671M [02:21<00:58, 4.72MB/s] 61%|██████    | 409M/671M [02:21<00:59, 4.60MB/s] 61%|██████    | 410M/671M [02:22<01:01, 4.48MB/s] 61%|██████    | 410M/671M [02:22<01:01, 4.47MB/s] 61%|██████    | 411M/671M [02:22<01:07, 4.04MB/s] 61%|██████▏   | 411M/671M [02:22<01:08, 4.00MB/s] 61%|██████▏   | 411M/671M [02:22<01:10, 3.86MB/s] 61%|██████▏   | 412M/671M [02:22<01:29, 3.02MB/s] 61%|██████▏   | 412M/671M [02:22<01:37, 2.78MB/s] 61%|██████▏   | 412M/671M [02:23<01:36, 2.82MB/s] 62%|██████▏   | 413M/671M [02:23<01:46, 2.55MB/s] 62%|██████▏   | 413M/671M [02:23<01:49, 2.46MB/s] 62%|██████▏   | 413M/671M [02:23<01:43, 2.60MB/s] 62%|██████▏   | 414M/671M [02:23<01:38, 2.72MB/s] 62%|██████▏   | 414M/671M [02:23<01:28, 3.03MB/s] 62%|██████▏   | 414M/671M [02:23<01:21, 3.30MB/s] 62%|██████▏   | 415M/671M [02:23<01:20, 3.33MB/s] 62%|██████▏   | 415M/671M [02:23<01:17, 3.47MB/s] 62%|██████▏   | 416M/671M [02:24<01:18, 3.40MB/s] 62%|██████▏   | 416M/671M [02:24<01:20, 3.31MB/s] 62%|██████▏   | 416M/671M [02:24<01:22, 3.24MB/s] 62%|██████▏   | 417M/671M [02:24<01:41, 2.63MB/s] 62%|██████▏   | 417M/671M [02:24<01:47, 2.47MB/s] 62%|██████▏   | 417M/671M [02:24<01:48, 2.45MB/s] 62%|██████▏   | 417M/671M [02:24<01:46, 2.50MB/s] 62%|██████▏   | 418M/671M [02:24<01:46, 2.49MB/s] 62%|██████▏   | 418M/671M [02:25<02:06, 2.09MB/s] 62%|██████▏   | 418M/671M [02:25<01:55, 2.29MB/s] 62%|██████▏   | 419M/671M [02:25<01:43, 2.56MB/s] 62%|██████▏   | 419M/671M [02:25<01:39, 2.66MB/s] 63%|██████▎   | 419M/671M [02:25<01:37, 2.71MB/s] 63%|██████▎   | 420M/671M [02:25<01:39, 2.64MB/s] 63%|██████▎   | 420M/671M [02:25<01:40, 2.62MB/s] 63%|██████▎   | 420M/671M [02:26<01:46, 2.47MB/s] 63%|██████▎   | 420M/671M [02:26<01:56, 2.24MB/s] 63%|██████▎   | 421M/671M [02:26<02:01, 2.16MB/s] 63%|██████▎   | 421M/671M [02:26<02:06, 2.07MB/s] 63%|██████▎   | 421M/671M [02:26<02:08, 2.03MB/s] 63%|██████▎   | 421M/671M [02:26<02:13, 1.96MB/s] 63%|██████▎   | 421M/671M [02:26<02:14, 1.95MB/s] 63%|██████▎   | 422M/671M [02:26<02:12, 1.98MB/s] 63%|██████▎   | 422M/671M [02:26<02:14, 1.94MB/s] 63%|██████▎   | 422M/671M [02:27<02:13, 1.96MB/s] 63%|██████▎   | 422M/671M [02:27<02:17, 1.89MB/s] 63%|██████▎   | 422M/671M [02:27<02:20, 1.86MB/s] 63%|██████▎   | 423M/671M [02:27<02:16, 1.90MB/s] 63%|██████▎   | 423M/671M [02:27<02:00, 2.15MB/s] 63%|██████▎   | 423M/671M [02:27<01:56, 2.23MB/s] 63%|██████▎   | 424M/671M [02:27<01:50, 2.34MB/s] 63%|██████▎   | 424M/671M [02:27<01:52, 2.30MB/s] 63%|██████▎   | 424M/671M [02:28<01:54, 2.25MB/s] 63%|██████▎   | 424M/671M [02:28<02:00, 2.14MB/s] 63%|██████▎   | 424M/671M [02:28<02:05, 2.06MB/s] 63%|██████▎   | 425M/671M [02:28<02:08, 2.01MB/s] 63%|██████▎   | 425M/671M [02:28<02:06, 2.04MB/s] 63%|██████▎   | 425M/671M [02:28<02:02, 2.10MB/s] 63%|██████▎   | 425M/671M [02:28<01:59, 2.15MB/s] 63%|██████▎   | 426M/671M [02:28<01:55, 2.23MB/s] 63%|██████▎   | 426M/671M [02:28<01:58, 2.16MB/s] 64%|██████▎   | 426M/671M [02:28<02:01, 2.10MB/s] 64%|██████▎   | 426M/671M [02:29<02:17, 1.87MB/s] 64%|██████▎   | 426M/671M [02:29<02:22, 1.79MB/s] 64%|██████▎   | 427M/671M [02:29<02:21, 1.81MB/s] 64%|██████▎   | 427M/671M [02:29<02:20, 1.82MB/s] 64%|██████▎   | 427M/671M [02:29<02:24, 1.77MB/s] 64%|██████▎   | 427M/671M [02:29<02:24, 1.76MB/s] 64%|██████▎   | 427M/671M [02:29<02:13, 1.91MB/s] 64%|██████▍   | 428M/671M [02:30<01:45, 2.41MB/s] 64%|██████▍   | 428M/671M [02:30<01:23, 3.04MB/s] 64%|██████▍   | 429M/671M [02:30<01:11, 3.54MB/s] 64%|██████▍   | 429M/671M [02:30<01:05, 3.88MB/s] 64%|██████▍   | 430M/671M [02:30<01:17, 3.26MB/s] 64%|██████▍   | 430M/671M [02:30<01:15, 3.34MB/s] 64%|██████▍   | 430M/671M [02:30<01:19, 3.18MB/s] 64%|██████▍   | 431M/671M [02:30<01:27, 2.88MB/s] 64%|██████▍   | 431M/671M [02:30<01:29, 2.79MB/s] 64%|██████▍   | 431M/671M [02:31<01:26, 2.91MB/s] 64%|██████▍   | 432M/671M [02:31<01:17, 3.24MB/s] 64%|██████▍   | 432M/671M [02:31<01:14, 3.34MB/s] 65%|██████▍   | 433M/671M [02:31<01:09, 3.57MB/s] 65%|██████▍   | 433M/671M [02:31<01:08, 3.63MB/s] 65%|██████▍   | 434M/671M [02:31<00:58, 4.26MB/s] 65%|██████▍   | 434M/671M [02:31<00:51, 4.79MB/s] 65%|██████▍   | 435M/671M [02:31<00:46, 5.27MB/s] 65%|██████▍   | 435M/671M [02:31<00:44, 5.57MB/s] 65%|██████▌   | 436M/671M [02:32<00:41, 5.90MB/s] 65%|██████▌   | 437M/671M [02:32<00:39, 6.18MB/s] 65%|██████▌   | 437M/671M [02:32<00:38, 6.35MB/s] 65%|██████▌   | 438M/671M [02:32<00:37, 6.54MB/s] 65%|██████▌   | 439M/671M [02:32<00:36, 6.65MB/s] 66%|██████▌   | 439M/671M [02:32<00:36, 6.72MB/s] 66%|██████▌   | 440M/671M [02:32<00:35, 6.81MB/s] 66%|██████▌   | 441M/671M [02:32<00:42, 5.62MB/s] 66%|██████▌   | 441M/671M [02:33<00:50, 4.80MB/s] 66%|██████▌   | 442M/671M [02:33<00:53, 4.44MB/s] 66%|██████▌   | 442M/671M [02:33<00:56, 4.22MB/s] 66%|██████▌   | 443M/671M [02:33<00:56, 4.22MB/s] 66%|██████▌   | 443M/671M [02:33<00:57, 4.13MB/s] 66%|██████▌   | 444M/671M [02:33<01:01, 3.90MB/s] 66%|██████▌   | 444M/671M [02:33<01:04, 3.68MB/s] 66%|██████▋   | 444M/671M [02:33<01:07, 3.54MB/s] 66%|██████▋   | 445M/671M [02:34<01:10, 3.35MB/s] 66%|██████▋   | 445M/671M [02:34<01:16, 3.10MB/s] 66%|██████▋   | 445M/671M [02:34<01:15, 3.13MB/s] 66%|██████▋   | 446M/671M [02:34<01:20, 2.93MB/s] 66%|██████▋   | 446M/671M [02:34<01:23, 2.82MB/s] 67%|██████▋   | 446M/671M [02:34<01:12, 3.23MB/s] 67%|██████▋   | 447M/671M [02:34<01:06, 3.55MB/s] 67%|██████▋   | 447M/671M [02:34<01:08, 3.42MB/s] 67%|██████▋   | 448M/671M [02:34<01:05, 3.59MB/s] 67%|██████▋   | 448M/671M [02:35<01:05, 3.56MB/s] 67%|██████▋   | 448M/671M [02:35<01:07, 3.47MB/s] 67%|██████▋   | 449M/671M [02:35<01:07, 3.45MB/s] 67%|██████▋   | 449M/671M [02:35<01:04, 3.58MB/s] 67%|██████▋   | 449M/671M [02:35<01:08, 3.39MB/s] 67%|██████▋   | 450M/671M [02:35<01:07, 3.46MB/s] 67%|██████▋   | 450M/671M [02:35<01:05, 3.51MB/s] 67%|██████▋   | 450M/671M [02:35<01:01, 3.74MB/s] 67%|██████▋   | 451M/671M [02:35<01:02, 3.68MB/s] 67%|██████▋   | 451M/671M [02:36<00:53, 4.33MB/s] 67%|██████▋   | 452M/671M [02:36<00:45, 5.03MB/s] 68%|██████▊   | 453M/671M [02:36<00:43, 5.20MB/s] 68%|██████▊   | 453M/671M [02:36<00:42, 5.31MB/s] 68%|██████▊   | 454M/671M [02:36<00:44, 5.11MB/s] 68%|██████▊   | 454M/671M [02:36<00:47, 4.77MB/s] 68%|██████▊   | 455M/671M [02:36<00:49, 4.56MB/s] 68%|██████▊   | 455M/671M [02:36<00:53, 4.20MB/s] 68%|██████▊   | 456M/671M [02:36<00:54, 4.16MB/s] 68%|██████▊   | 456M/671M [02:37<00:50, 4.42MB/s] 68%|██████▊   | 457M/671M [02:37<00:43, 5.19MB/s] 68%|██████▊   | 457M/671M [02:37<00:46, 4.85MB/s] 68%|██████▊   | 458M/671M [02:37<00:40, 5.46MB/s] 68%|██████▊   | 459M/671M [02:37<00:40, 5.49MB/s] 68%|██████▊   | 459M/671M [02:37<00:39, 5.54MB/s] 69%|██████▊   | 460M/671M [02:37<00:39, 5.65MB/s] 69%|██████▊   | 460M/671M [02:37<00:38, 5.76MB/s] 69%|██████▉   | 461M/671M [02:37<00:37, 5.86MB/s] 69%|██████▉   | 462M/671M [02:38<00:37, 5.83MB/s] 69%|██████▉   | 462M/671M [02:38<00:37, 5.82MB/s] 69%|██████▉   | 463M/671M [02:38<00:38, 5.70MB/s] 69%|██████▉   | 463M/671M [02:38<00:38, 5.58MB/s] 69%|██████▉   | 464M/671M [02:38<00:38, 5.70MB/s] 69%|██████▉   | 465M/671M [02:38<00:42, 5.09MB/s] 69%|██████▉   | 465M/671M [02:38<00:42, 5.11MB/s] 69%|██████▉   | 466M/671M [02:38<00:40, 5.30MB/s] 70%|██████▉   | 466M/671M [02:38<00:44, 4.84MB/s] 70%|██████▉   | 467M/671M [02:39<00:45, 4.74MB/s] 70%|██████▉   | 467M/671M [02:39<00:49, 4.35MB/s] 70%|██████▉   | 468M/671M [02:39<00:53, 3.98MB/s] 70%|██████▉   | 468M/671M [02:39<00:50, 4.21MB/s] 70%|██████▉   | 468M/671M [02:39<00:50, 4.21MB/s] 70%|██████▉   | 469M/671M [02:39<00:53, 3.96MB/s] 70%|███████   | 469M/671M [02:39<00:52, 3.99MB/s] 70%|███████   | 470M/671M [02:39<00:49, 4.28MB/s] 70%|███████   | 470M/671M [02:40<00:53, 3.94MB/s] 70%|███████   | 471M/671M [02:40<00:43, 4.85MB/s] 70%|███████   | 472M/671M [02:40<00:45, 4.55MB/s] 70%|███████   | 472M/671M [02:40<00:44, 4.72MB/s] 70%|███████   | 473M/671M [02:40<00:47, 4.36MB/s] 71%|███████   | 473M/671M [02:40<00:55, 3.73MB/s] 71%|███████   | 473M/671M [02:40<01:04, 3.19MB/s] 71%|███████   | 474M/671M [02:41<01:08, 3.04MB/s] 71%|███████   | 474M/671M [02:41<01:07, 3.07MB/s] 71%|███████   | 474M/671M [02:41<01:05, 3.15MB/s] 71%|███████   | 475M/671M [02:41<01:03, 3.25MB/s] 71%|███████   | 475M/671M [02:41<01:04, 3.20MB/s] 71%|███████   | 475M/671M [02:41<01:05, 3.12MB/s] 71%|███████   | 476M/671M [02:41<01:02, 3.26MB/s] 71%|███████   | 476M/671M [02:41<00:58, 3.51MB/s] 71%|███████   | 477M/671M [02:41<00:59, 3.41MB/s] 71%|███████   | 477M/671M [02:42<01:01, 3.32MB/s] 71%|███████   | 477M/671M [02:42<01:03, 3.17MB/s] 71%|███████   | 478M/671M [02:42<01:00, 3.33MB/s] 71%|███████▏  | 478M/671M [02:42<01:01, 3.30MB/s] 71%|███████▏  | 478M/671M [02:42<01:01, 3.28MB/s] 71%|███████▏  | 479M/671M [02:42<01:10, 2.84MB/s] 71%|███████▏  | 479M/671M [02:42<01:12, 2.76MB/s] 71%|███████▏  | 479M/671M [02:42<01:18, 2.56MB/s] 71%|███████▏  | 479M/671M [02:42<01:20, 2.48MB/s] 72%|███████▏  | 480M/671M [02:43<01:19, 2.53MB/s] 72%|███████▏  | 480M/671M [02:43<01:28, 2.26MB/s] 72%|███████▏  | 480M/671M [02:43<01:24, 2.35MB/s] 72%|███████▏  | 480M/671M [02:43<01:15, 2.66MB/s] 72%|███████▏  | 481M/671M [02:43<01:09, 2.86MB/s] 72%|███████▏  | 481M/671M [02:43<01:03, 3.10MB/s] 72%|███████▏  | 482M/671M [02:43<01:02, 3.16MB/s] 72%|███████▏  | 482M/671M [02:43<01:03, 3.13MB/s] 72%|███████▏  | 482M/671M [02:44<01:04, 3.07MB/s] 72%|███████▏  | 482M/671M [02:44<01:04, 3.06MB/s] 72%|███████▏  | 483M/671M [02:44<01:02, 3.17MB/s] 72%|███████▏  | 483M/671M [02:44<00:59, 3.30MB/s] 72%|███████▏  | 484M/671M [02:44<00:55, 3.55MB/s] 72%|███████▏  | 484M/671M [02:44<00:54, 3.60MB/s] 72%|███████▏  | 485M/671M [02:44<00:51, 3.76MB/s] 72%|███████▏  | 485M/671M [02:44<00:45, 4.29MB/s] 72%|███████▏  | 486M/671M [02:44<00:42, 4.57MB/s] 73%|███████▎  | 486M/671M [02:45<00:44, 4.36MB/s] 73%|███████▎  | 487M/671M [02:45<00:51, 3.75MB/s] 73%|███████▎  | 487M/671M [02:45<00:53, 3.62MB/s] 73%|███████▎  | 487M/671M [02:45<01:02, 3.05MB/s] 73%|███████▎  | 488M/671M [02:45<01:03, 3.02MB/s] 73%|███████▎  | 488M/671M [02:45<01:04, 2.95MB/s] 73%|███████▎  | 489M/671M [02:45<01:03, 3.03MB/s] 73%|███████▎  | 489M/671M [02:46<00:47, 4.00MB/s] 73%|███████▎  | 490M/671M [02:46<00:40, 4.64MB/s] 73%|███████▎  | 491M/671M [02:46<00:38, 4.94MB/s] 73%|███████▎  | 491M/671M [02:46<00:35, 5.35MB/s] 73%|███████▎  | 492M/671M [02:46<00:32, 5.81MB/s] 73%|███████▎  | 493M/671M [02:46<00:34, 5.43MB/s] 74%|███████▎  | 493M/671M [02:46<00:38, 4.89MB/s] 74%|███████▎  | 494M/671M [02:46<00:43, 4.31MB/s] 74%|███████▎  | 494M/671M [02:47<00:46, 4.00MB/s] 74%|███████▎  | 494M/671M [02:47<00:49, 3.71MB/s] 74%|███████▍  | 495M/671M [02:47<00:50, 3.64MB/s] 74%|███████▍  | 495M/671M [02:47<00:54, 3.35MB/s] 74%|███████▍  | 496M/671M [02:47<00:54, 3.34MB/s] 74%|███████▍  | 496M/671M [02:47<00:57, 3.18MB/s] 74%|███████▍  | 496M/671M [02:47<01:04, 2.82MB/s] 74%|███████▍  | 496M/671M [02:47<01:06, 2.76MB/s] 74%|███████▍  | 497M/671M [02:48<01:06, 2.74MB/s] 74%|███████▍  | 497M/671M [02:48<01:08, 2.66MB/s] 74%|███████▍  | 497M/671M [02:48<01:08, 2.66MB/s] 74%|███████▍  | 497M/671M [02:48<01:13, 2.48MB/s] 74%|███████▍  | 498M/671M [02:48<01:01, 2.97MB/s] 74%|███████▍  | 498M/671M [02:48<01:02, 2.90MB/s] 74%|███████▍  | 499M/671M [02:48<01:06, 2.70MB/s] 74%|███████▍  | 499M/671M [02:48<01:13, 2.44MB/s] 74%|███████▍  | 499M/671M [02:49<01:08, 2.62MB/s] 74%|███████▍  | 500M/671M [02:49<01:08, 2.61MB/s] 75%|███████▍  | 500M/671M [02:49<01:08, 2.60MB/s] 75%|███████▍  | 500M/671M [02:49<01:21, 2.19MB/s] 75%|███████▍  | 500M/671M [02:49<01:27, 2.03MB/s] 75%|███████▍  | 500M/671M [02:49<01:32, 1.92MB/s] 75%|███████▍  | 501M/671M [02:49<01:34, 1.88MB/s] 75%|███████▍  | 501M/671M [02:49<01:32, 1.93MB/s] 75%|███████▍  | 501M/671M [02:50<01:28, 2.00MB/s] 75%|███████▍  | 501M/671M [02:50<01:27, 2.02MB/s] 75%|███████▍  | 502M/671M [02:50<01:30, 1.96MB/s] 75%|███████▍  | 502M/671M [02:50<01:29, 1.98MB/s] 75%|███████▍  | 502M/671M [02:50<01:26, 2.04MB/s] 75%|███████▍  | 502M/671M [02:50<01:28, 1.99MB/s] 75%|███████▍  | 503M/671M [02:50<01:28, 1.99MB/s] 75%|███████▍  | 503M/671M [02:50<01:24, 2.09MB/s] 75%|███████▌  | 503M/671M [02:51<01:30, 1.94MB/s] 75%|███████▌  | 503M/671M [02:51<01:37, 1.81MB/s] 75%|███████▌  | 504M/671M [02:51<01:29, 1.97MB/s] 75%|███████▌  | 504M/671M [02:51<01:46, 1.65MB/s] 75%|███████▌  | 504M/671M [02:51<01:57, 1.49MB/s] 75%|███████▌  | 504M/671M [02:51<01:55, 1.52MB/s] 75%|███████▌  | 504M/671M [02:51<01:46, 1.64MB/s] 75%|███████▌  | 505M/671M [02:52<01:33, 1.86MB/s] 75%|███████▌  | 505M/671M [02:52<01:22, 2.10MB/s] 75%|███████▌  | 505M/671M [02:52<01:03, 2.71MB/s] 75%|███████▌  | 506M/671M [02:52<00:59, 2.88MB/s] 75%|███████▌  | 506M/671M [02:52<00:59, 2.92MB/s] 76%|███████▌  | 506M/671M [02:52<00:50, 3.40MB/s] 76%|███████▌  | 507M/671M [02:52<00:51, 3.37MB/s] 76%|███████▌  | 507M/671M [02:52<00:45, 3.77MB/s] 76%|███████▌  | 508M/671M [02:52<00:48, 3.52MB/s] 76%|███████▌  | 508M/671M [02:53<00:45, 3.78MB/s] 76%|███████▌  | 509M/671M [02:53<00:34, 4.94MB/s] 76%|███████▌  | 510M/671M [02:53<00:28, 5.87MB/s] 76%|███████▌  | 510M/671M [02:53<00:27, 6.22MB/s] 76%|███████▌  | 511M/671M [02:53<00:28, 5.92MB/s] 76%|███████▋  | 512M/671M [02:53<00:28, 5.91MB/s] 76%|███████▋  | 512M/671M [02:53<00:28, 5.77MB/s] 76%|███████▋  | 513M/671M [02:53<00:33, 4.88MB/s] 77%|███████▋  | 513M/671M [02:54<00:43, 3.80MB/s] 77%|███████▋  | 514M/671M [02:54<00:49, 3.34MB/s] 77%|███████▋  | 514M/671M [02:54<01:03, 2.59MB/s] 77%|███████▋  | 514M/671M [02:54<01:03, 2.57MB/s] 77%|███████▋  | 515M/671M [02:54<01:08, 2.40MB/s] 77%|███████▋  | 515M/671M [02:54<01:09, 2.35MB/s] 77%|███████▋  | 515M/671M [02:54<00:56, 2.88MB/s] 77%|███████▋  | 516M/671M [02:55<01:00, 2.71MB/s] 77%|███████▋  | 516M/671M [02:55<01:08, 2.36MB/s] 77%|███████▋  | 516M/671M [02:55<01:08, 2.36MB/s] 77%|███████▋  | 516M/671M [02:55<01:18, 2.05MB/s] 77%|███████▋  | 517M/671M [02:55<01:11, 2.27MB/s] 77%|███████▋  | 517M/671M [02:55<01:20, 2.01MB/s] 77%|███████▋  | 517M/671M [02:55<01:20, 1.99MB/s] 77%|███████▋  | 517M/671M [02:56<01:20, 2.01MB/s] 77%|███████▋  | 517M/671M [02:56<01:24, 1.91MB/s] 77%|███████▋  | 518M/671M [02:56<01:26, 1.86MB/s] 77%|███████▋  | 518M/671M [02:56<01:30, 1.78MB/s] 77%|███████▋  | 518M/671M [02:56<01:19, 2.01MB/s] 77%|███████▋  | 518M/671M [02:56<01:27, 1.84MB/s] 77%|███████▋  | 519M/671M [02:56<01:16, 2.09MB/s] 77%|███████▋  | 519M/671M [02:56<01:07, 2.36MB/s] 77%|███████▋  | 519M/671M [02:57<01:13, 2.15MB/s] 77%|███████▋  | 519M/671M [02:57<01:20, 1.96MB/s] 77%|███████▋  | 520M/671M [02:57<01:22, 1.92MB/s] 78%|███████▊  | 520M/671M [02:57<01:27, 1.81MB/s] 78%|███████▊  | 520M/671M [02:57<01:15, 2.10MB/s] 78%|███████▊  | 520M/671M [02:57<01:22, 1.91MB/s] 78%|███████▊  | 520M/671M [02:57<01:20, 1.96MB/s] 78%|███████▊  | 521M/671M [02:57<01:14, 2.11MB/s] 78%|███████▊  | 521M/671M [02:57<01:07, 2.31MB/s] 78%|███████▊  | 521M/671M [02:58<01:12, 2.17MB/s] 78%|███████▊  | 522M/671M [02:58<01:07, 2.32MB/s] 78%|███████▊  | 522M/671M [02:58<01:07, 2.31MB/s] 78%|███████▊  | 522M/671M [02:58<01:09, 2.24MB/s] 78%|███████▊  | 522M/671M [02:58<01:09, 2.23MB/s] 78%|███████▊  | 522M/671M [02:58<01:15, 2.05MB/s] 78%|███████▊  | 523M/671M [02:58<01:09, 2.22MB/s] 78%|███████▊  | 523M/671M [02:59<01:01, 2.53MB/s] 78%|███████▊  | 524M/671M [02:59<00:59, 2.58MB/s] 78%|███████▊  | 524M/671M [02:59<00:57, 2.67MB/s] 78%|███████▊  | 524M/671M [02:59<00:55, 2.77MB/s] 78%|███████▊  | 525M/671M [02:59<01:02, 2.44MB/s] 78%|███████▊  | 525M/671M [02:59<01:07, 2.26MB/s] 78%|███████▊  | 525M/671M [02:59<01:31, 1.67MB/s] 78%|███████▊  | 525M/671M [03:00<01:28, 1.73MB/s] 78%|███████▊  | 525M/671M [03:00<01:32, 1.64MB/s] 78%|███████▊  | 526M/671M [03:00<01:35, 1.59MB/s] 78%|███████▊  | 526M/671M [03:00<01:29, 1.70MB/s] 78%|███████▊  | 526M/671M [03:00<01:31, 1.66MB/s] 78%|███████▊  | 526M/671M [03:00<01:21, 1.85MB/s] 79%|███████▊  | 527M/671M [03:00<01:08, 2.21MB/s] 79%|███████▊  | 527M/671M [03:00<01:07, 2.24MB/s] 79%|███████▊  | 527M/671M [03:01<01:09, 2.16MB/s] 79%|███████▊  | 528M/671M [03:01<01:06, 2.26MB/s] 79%|███████▊  | 528M/671M [03:01<01:13, 2.03MB/s] 79%|███████▉  | 528M/671M [03:01<01:18, 1.89MB/s] 79%|███████▉  | 528M/671M [03:01<01:13, 2.03MB/s] 79%|███████▉  | 529M/671M [03:01<01:13, 2.02MB/s] 79%|███████▉  | 529M/671M [03:01<01:21, 1.81MB/s] 79%|███████▉  | 529M/671M [03:02<01:18, 1.88MB/s] 79%|███████▉  | 529M/671M [03:02<01:13, 2.01MB/s] 79%|███████▉  | 530M/671M [03:02<01:19, 1.86MB/s] 79%|███████▉  | 530M/671M [03:02<01:16, 1.94MB/s] 79%|███████▉  | 530M/671M [03:02<01:14, 1.96MB/s] 79%|███████▉  | 530M/671M [03:02<01:12, 2.04MB/s] 79%|███████▉  | 531M/671M [03:02<01:08, 2.14MB/s] 79%|███████▉  | 531M/671M [03:02<00:56, 2.61MB/s] 79%|███████▉  | 531M/671M [03:03<00:51, 2.82MB/s] 79%|███████▉  | 532M/671M [03:03<00:51, 2.85MB/s] 79%|███████▉  | 532M/671M [03:03<00:52, 2.74MB/s] 79%|███████▉  | 532M/671M [03:03<00:56, 2.56MB/s] 79%|███████▉  | 533M/671M [03:03<00:54, 2.66MB/s] 79%|███████▉  | 533M/671M [03:03<00:50, 2.86MB/s] 80%|███████▉  | 533M/671M [03:03<00:48, 2.96MB/s] 80%|███████▉  | 533M/671M [03:03<00:51, 2.81MB/s] 80%|███████▉  | 534M/671M [03:04<00:54, 2.65MB/s] 80%|███████▉  | 534M/671M [03:04<00:56, 2.52MB/s] 80%|███████▉  | 534M/671M [03:04<00:57, 2.50MB/s] 80%|███████▉  | 535M/671M [03:04<00:54, 2.61MB/s] 80%|███████▉  | 535M/671M [03:04<00:57, 2.49MB/s] 80%|███████▉  | 535M/671M [03:04<00:58, 2.41MB/s] 80%|███████▉  | 535M/671M [03:04<01:01, 2.30MB/s] 80%|███████▉  | 536M/671M [03:04<00:56, 2.50MB/s] 80%|███████▉  | 536M/671M [03:04<00:57, 2.45MB/s] 80%|███████▉  | 536M/671M [03:05<00:49, 2.84MB/s] 80%|████████  | 536M/671M [03:05<00:49, 2.82MB/s] 80%|████████  | 537M/671M [03:05<00:50, 2.77MB/s] 80%|████████  | 537M/671M [03:05<00:55, 2.53MB/s] 80%|████████  | 537M/671M [03:05<01:06, 2.09MB/s] 80%|████████  | 537M/671M [03:05<01:08, 2.05MB/s] 80%|████████  | 538M/671M [03:05<01:25, 1.63MB/s] 80%|████████  | 538M/671M [03:06<01:30, 1.54MB/s] 80%|████████  | 538M/671M [03:06<01:30, 1.54MB/s] 80%|████████  | 538M/671M [03:06<01:30, 1.54MB/s] 80%|████████  | 538M/671M [03:06<01:33, 1.48MB/s] 80%|████████  | 538M/671M [03:06<01:36, 1.43MB/s] 80%|████████  | 539M/671M [03:06<01:19, 1.74MB/s] 80%|████████  | 539M/671M [03:06<01:16, 1.81MB/s] 80%|████████  | 539M/671M [03:06<01:11, 1.93MB/s] 80%|████████  | 539M/671M [03:06<01:08, 2.00MB/s] 80%|████████  | 540M/671M [03:07<01:10, 1.95MB/s] 81%|████████  | 540M/671M [03:07<01:09, 1.97MB/s] 81%|████████  | 540M/671M [03:07<01:12, 1.90MB/s] 81%|████████  | 540M/671M [03:07<01:07, 2.03MB/s] 81%|████████  | 541M/671M [03:07<01:07, 2.03MB/s] 81%|████████  | 541M/671M [03:07<01:03, 2.14MB/s] 81%|████████  | 541M/671M [03:07<01:05, 2.08MB/s] 81%|████████  | 541M/671M [03:07<01:07, 2.00MB/s] 81%|████████  | 542M/671M [03:07<01:06, 2.05MB/s] 81%|████████  | 542M/671M [03:08<01:09, 1.95MB/s] 81%|████████  | 542M/671M [03:08<01:11, 1.90MB/s] 81%|████████  | 542M/671M [03:08<01:22, 1.63MB/s] 81%|████████  | 542M/671M [03:08<01:23, 1.60MB/s] 81%|████████  | 542M/671M [03:08<01:22, 1.63MB/s] 81%|████████  | 543M/671M [03:08<01:22, 1.63MB/s] 81%|████████  | 543M/671M [03:08<01:08, 1.96MB/s] 81%|████████  | 543M/671M [03:08<01:11, 1.88MB/s] 81%|████████  | 543M/671M [03:09<01:33, 1.43MB/s] 81%|████████  | 543M/671M [03:09<01:37, 1.37MB/s] 81%|████████  | 544M/671M [03:09<01:36, 1.39MB/s] 81%|████████  | 544M/671M [03:09<01:31, 1.45MB/s] 81%|████████  | 544M/671M [03:09<01:29, 1.49MB/s] 81%|████████  | 544M/671M [03:09<01:36, 1.37MB/s] 81%|████████  | 544M/671M [03:09<01:31, 1.45MB/s] 81%|████████  | 544M/671M [03:09<01:12, 1.81MB/s] 81%|████████  | 545M/671M [03:10<01:09, 1.90MB/s] 81%|████████▏ | 545M/671M [03:10<01:15, 1.74MB/s] 81%|████████▏ | 545M/671M [03:10<01:05, 2.01MB/s] 81%|████████▏ | 545M/671M [03:10<01:12, 1.82MB/s] 81%|████████▏ | 546M/671M [03:10<01:13, 1.77MB/s] 81%|████████▏ | 546M/671M [03:10<01:16, 1.70MB/s] 81%|████████▏ | 546M/671M [03:10<01:19, 1.65MB/s] 81%|████████▏ | 546M/671M [03:10<01:17, 1.68MB/s] 81%|████████▏ | 546M/671M [03:11<01:18, 1.66MB/s] 81%|████████▏ | 546M/671M [03:11<01:22, 1.57MB/s] 82%|████████▏ | 547M/671M [03:11<01:20, 1.61MB/s] 82%|████████▏ | 547M/671M [03:11<01:21, 1.60MB/s] 82%|████████▏ | 547M/671M [03:11<01:19, 1.62MB/s] 82%|████████▏ | 547M/671M [03:11<01:13, 1.76MB/s] 82%|████████▏ | 547M/671M [03:11<01:11, 1.80MB/s] 82%|████████▏ | 548M/671M [03:11<01:08, 1.89MB/s] 82%|████████▏ | 548M/671M [03:11<01:09, 1.86MB/s] 82%|████████▏ | 548M/671M [03:12<01:09, 1.85MB/s] 82%|████████▏ | 548M/671M [03:12<01:08, 1.87MB/s] 82%|████████▏ | 548M/671M [03:12<01:08, 1.88MB/s] 82%|████████▏ | 549M/671M [03:12<01:08, 1.88MB/s] 82%|████████▏ | 549M/671M [03:12<01:03, 2.01MB/s] 82%|████████▏ | 549M/671M [03:12<01:00, 2.12MB/s] 82%|████████▏ | 549M/671M [03:12<00:57, 2.21MB/s] 82%|████████▏ | 550M/671M [03:12<00:50, 2.49MB/s] 82%|████████▏ | 550M/671M [03:12<00:51, 2.46MB/s] 82%|████████▏ | 550M/671M [03:13<00:52, 2.38MB/s] 82%|████████▏ | 550M/671M [03:13<00:56, 2.24MB/s] 82%|████████▏ | 551M/671M [03:13<00:56, 2.22MB/s] 82%|████████▏ | 551M/671M [03:13<00:53, 2.36MB/s] 82%|████████▏ | 551M/671M [03:13<00:51, 2.45MB/s] 82%|████████▏ | 551M/671M [03:13<00:47, 2.62MB/s] 82%|████████▏ | 552M/671M [03:13<00:37, 3.36MB/s] 82%|████████▏ | 553M/671M [03:13<00:24, 5.06MB/s] 83%|████████▎ | 554M/671M [03:13<00:19, 6.41MB/s] 83%|████████▎ | 555M/671M [03:14<00:16, 7.41MB/s] 83%|████████▎ | 556M/671M [03:14<00:14, 8.06MB/s] 83%|████████▎ | 557M/671M [03:14<00:13, 8.85MB/s] 83%|████████▎ | 558M/671M [03:14<00:12, 9.47MB/s] 83%|████████▎ | 559M/671M [03:14<00:11, 10.0MB/s] 84%|████████▎ | 560M/671M [03:14<00:11, 10.5MB/s] 84%|████████▎ | 561M/671M [03:14<00:10, 10.8MB/s] 84%|████████▍ | 563M/671M [03:14<00:10, 11.0MB/s] 84%|████████▍ | 564M/671M [03:14<00:09, 11.5MB/s] 84%|████████▍ | 565M/671M [03:14<00:09, 11.9MB/s] 84%|████████▍ | 566M/671M [03:15<00:08, 12.3MB/s] 85%|████████▍ | 568M/671M [03:15<00:08, 12.7MB/s] 85%|████████▍ | 569M/671M [03:15<00:08, 13.0MB/s] 85%|████████▌ | 570M/671M [03:15<00:07, 13.3MB/s] 85%|████████▌ | 572M/671M [03:15<00:09, 11.3MB/s] 85%|████████▌ | 573M/671M [03:15<00:10, 9.85MB/s] 86%|████████▌ | 574M/671M [03:15<00:13, 7.58MB/s] 86%|████████▌ | 575M/671M [03:16<00:16, 6.00MB/s] 86%|████████▌ | 575M/671M [03:16<00:18, 5.47MB/s] 86%|████████▌ | 576M/671M [03:16<00:20, 4.86MB/s] 86%|████████▌ | 576M/671M [03:16<00:23, 4.13MB/s] 86%|████████▌ | 577M/671M [03:16<00:27, 3.54MB/s] 86%|████████▌ | 577M/671M [03:17<00:25, 3.80MB/s] 86%|████████▌ | 578M/671M [03:17<00:28, 3.37MB/s] 86%|████████▌ | 578M/671M [03:17<00:25, 3.78MB/s] 86%|████████▋ | 579M/671M [03:17<00:26, 3.66MB/s] 86%|████████▋ | 579M/671M [03:17<00:24, 3.96MB/s] 86%|████████▋ | 580M/671M [03:17<00:21, 4.48MB/s] 87%|████████▋ | 580M/671M [03:17<00:19, 4.82MB/s] 87%|████████▋ | 581M/671M [03:17<00:19, 4.87MB/s] 87%|████████▋ | 581M/671M [03:18<00:21, 4.45MB/s] 87%|████████▋ | 582M/671M [03:18<00:20, 4.48MB/s] 87%|████████▋ | 582M/671M [03:18<00:21, 4.30MB/s] 87%|████████▋ | 583M/671M [03:18<00:20, 4.50MB/s] 87%|████████▋ | 583M/671M [03:18<00:20, 4.50MB/s] 87%|████████▋ | 584M/671M [03:18<00:19, 4.57MB/s] 87%|████████▋ | 584M/671M [03:18<00:21, 4.25MB/s] 87%|████████▋ | 585M/671M [03:18<00:21, 4.25MB/s] 87%|████████▋ | 585M/671M [03:18<00:19, 4.56MB/s] 87%|████████▋ | 586M/671M [03:19<00:17, 5.05MB/s] 87%|████████▋ | 587M/671M [03:19<00:15, 5.51MB/s] 88%|████████▊ | 587M/671M [03:19<00:18, 4.78MB/s] 88%|████████▊ | 588M/671M [03:19<00:21, 3.97MB/s] 88%|████████▊ | 588M/671M [03:19<00:25, 3.34MB/s] 88%|████████▊ | 588M/671M [03:19<00:26, 3.19MB/s] 88%|████████▊ | 589M/671M [03:19<00:27, 3.10MB/s] 88%|████████▊ | 589M/671M [03:20<00:25, 3.32MB/s] 88%|████████▊ | 589M/671M [03:20<00:26, 3.25MB/s] 88%|████████▊ | 590M/671M [03:20<00:24, 3.48MB/s] 88%|████████▊ | 590M/671M [03:20<00:25, 3.31MB/s] 88%|████████▊ | 591M/671M [03:20<00:25, 3.31MB/s] 88%|████████▊ | 591M/671M [03:20<00:26, 3.16MB/s] 88%|████████▊ | 591M/671M [03:20<00:30, 2.73MB/s] 88%|████████▊ | 591M/671M [03:20<00:30, 2.72MB/s] 88%|████████▊ | 592M/671M [03:21<00:35, 2.35MB/s] 88%|████████▊ | 592M/671M [03:21<00:32, 2.53MB/s] 88%|████████▊ | 592M/671M [03:21<00:37, 2.17MB/s] 88%|████████▊ | 593M/671M [03:21<00:36, 2.22MB/s] 88%|████████▊ | 593M/671M [03:21<00:46, 1.76MB/s] 88%|████████▊ | 593M/671M [03:21<00:37, 2.15MB/s] 89%|████████▊ | 594M/671M [03:22<00:37, 2.16MB/s] 89%|████████▊ | 594M/671M [03:22<00:44, 1.79MB/s] 89%|████████▊ | 594M/671M [03:22<00:53, 1.51MB/s] 89%|████████▊ | 594M/671M [03:22<00:51, 1.55MB/s] 89%|████████▊ | 594M/671M [03:22<00:54, 1.46MB/s] 89%|████████▊ | 595M/671M [03:22<00:54, 1.46MB/s] 89%|████████▊ | 595M/671M [03:23<01:06, 1.20MB/s] 89%|████████▊ | 595M/671M [03:23<01:05, 1.21MB/s] 89%|████████▊ | 595M/671M [03:23<01:06, 1.20MB/s] 89%|████████▉ | 595M/671M [03:23<00:55, 1.41MB/s] 89%|████████▉ | 596M/671M [03:23<00:47, 1.67MB/s] 89%|████████▉ | 596M/671M [03:23<00:43, 1.81MB/s] 89%|████████▉ | 596M/671M [03:23<00:43, 1.78MB/s] 89%|████████▉ | 596M/671M [03:23<00:41, 1.88MB/s] 89%|████████▉ | 597M/671M [03:24<00:38, 2.02MB/s] 89%|████████▉ | 597M/671M [03:24<00:37, 2.06MB/s] 89%|████████▉ | 597M/671M [03:24<00:37, 2.08MB/s] 89%|████████▉ | 597M/671M [03:24<00:37, 2.05MB/s] 89%|████████▉ | 597M/671M [03:24<00:42, 1.81MB/s] 89%|████████▉ | 598M/671M [03:24<00:42, 1.81MB/s] 89%|████████▉ | 598M/671M [03:24<00:38, 1.99MB/s] 89%|████████▉ | 598M/671M [03:24<00:36, 2.09MB/s] 89%|████████▉ | 599M/671M [03:25<00:34, 2.16MB/s] 89%|████████▉ | 599M/671M [03:25<00:32, 2.29MB/s] 89%|████████▉ | 599M/671M [03:25<00:32, 2.33MB/s] 89%|████████▉ | 599M/671M [03:25<00:30, 2.44MB/s] 89%|████████▉ | 600M/671M [03:25<00:29, 2.49MB/s] 89%|████████▉ | 600M/671M [03:25<00:31, 2.33MB/s] 89%|████████▉ | 600M/671M [03:25<00:34, 2.16MB/s] 90%|████████▉ | 600M/671M [03:25<00:33, 2.22MB/s] 90%|████████▉ | 601M/671M [03:25<00:31, 2.32MB/s] 90%|████████▉ | 601M/671M [03:26<00:28, 2.54MB/s] 90%|████████▉ | 601M/671M [03:26<00:29, 2.48MB/s] 90%|████████▉ | 601M/671M [03:26<00:32, 2.25MB/s] 90%|████████▉ | 602M/671M [03:26<00:35, 2.05MB/s] 90%|████████▉ | 602M/671M [03:26<00:33, 2.15MB/s] 90%|████████▉ | 602M/671M [03:26<00:34, 2.08MB/s] 90%|████████▉ | 602M/671M [03:26<00:32, 2.21MB/s] 90%|████████▉ | 603M/671M [03:26<00:30, 2.30MB/s] 90%|████████▉ | 603M/671M [03:27<00:33, 2.10MB/s] 90%|████████▉ | 603M/671M [03:27<00:30, 2.30MB/s] 90%|████████▉ | 603M/671M [03:27<00:32, 2.15MB/s] 90%|█████████ | 604M/671M [03:27<00:31, 2.24MB/s] 90%|█████████ | 604M/671M [03:27<00:30, 2.33MB/s] 90%|█████████ | 604M/671M [03:27<00:30, 2.28MB/s] 90%|█████████ | 604M/671M [03:27<00:28, 2.42MB/s] 90%|█████████ | 605M/671M [03:27<00:34, 2.01MB/s] 90%|█████████ | 605M/671M [03:28<00:35, 1.97MB/s] 90%|█████████ | 605M/671M [03:28<00:36, 1.90MB/s] 90%|█████████ | 605M/671M [03:28<00:36, 1.89MB/s] 90%|█████████ | 605M/671M [03:28<00:40, 1.68MB/s] 90%|█████████ | 606M/671M [03:28<00:34, 1.99MB/s] 90%|█████████ | 606M/671M [03:28<00:36, 1.88MB/s] 90%|█████████ | 606M/671M [03:28<00:32, 2.07MB/s] 90%|█████████ | 606M/671M [03:28<00:32, 2.08MB/s] 90%|█████████ | 607M/671M [03:28<00:33, 1.99MB/s] 90%|█████████ | 607M/671M [03:29<00:35, 1.88MB/s] 91%|█████████ | 607M/671M [03:29<00:30, 2.17MB/s] 91%|█████████ | 607M/671M [03:29<00:32, 2.06MB/s] 91%|█████████ | 608M/671M [03:29<00:28, 2.28MB/s] 91%|█████████ | 608M/671M [03:29<00:26, 2.49MB/s] 91%|█████████ | 608M/671M [03:29<00:24, 2.64MB/s] 91%|█████████ | 609M/671M [03:29<00:31, 2.04MB/s] 91%|█████████ | 609M/671M [03:29<00:32, 1.97MB/s] 91%|█████████ | 609M/671M [03:30<00:33, 1.95MB/s] 91%|█████████ | 609M/671M [03:30<00:33, 1.93MB/s] 91%|█████████ | 609M/671M [03:30<00:33, 1.95MB/s] 91%|█████████ | 610M/671M [03:30<00:33, 1.94MB/s] 91%|█████████ | 610M/671M [03:30<00:35, 1.82MB/s] 91%|█████████ | 610M/671M [03:30<00:36, 1.75MB/s] 91%|█████████ | 610M/671M [03:30<00:37, 1.69MB/s] 91%|█████████ | 610M/671M [03:30<00:33, 1.87MB/s] 91%|█████████ | 611M/671M [03:31<00:32, 1.91MB/s] 91%|█████████ | 611M/671M [03:31<00:32, 1.94MB/s] 91%|█████████ | 611M/671M [03:31<00:31, 1.96MB/s] 91%|█████████ | 611M/671M [03:31<00:34, 1.81MB/s] 91%|█████████ | 611M/671M [03:31<00:33, 1.86MB/s] 91%|█████████ | 612M/671M [03:31<00:32, 1.92MB/s] 91%|█████████ | 612M/671M [03:31<00:32, 1.91MB/s] 91%|█████████▏| 612M/671M [03:31<00:31, 1.93MB/s] 91%|█████████▏| 612M/671M [03:31<00:30, 1.99MB/s] 91%|█████████▏| 613M/671M [03:32<00:32, 1.88MB/s] 91%|█████████▏| 613M/671M [03:32<00:32, 1.87MB/s] 91%|█████████▏| 613M/671M [03:32<00:31, 1.90MB/s] 91%|█████████▏| 613M/671M [03:32<00:31, 1.91MB/s] 92%|█████████▏| 614M/671M [03:32<00:31, 1.91MB/s] 92%|█████████▏| 614M/671M [03:32<00:32, 1.85MB/s] 92%|█████████▏| 614M/671M [03:33<00:30, 1.92MB/s] 92%|█████████▏| 615M/671M [03:33<00:29, 2.00MB/s] 92%|█████████▏| 615M/671M [03:33<00:28, 2.08MB/s] 92%|█████████▏| 615M/671M [03:33<00:26, 2.15MB/s] 92%|█████████▏| 616M/671M [03:33<00:23, 2.42MB/s] 92%|█████████▏| 616M/671M [03:33<00:21, 2.71MB/s] 92%|█████████▏| 616M/671M [03:33<00:19, 2.92MB/s] 92%|█████████▏| 617M/671M [03:34<00:22, 2.47MB/s] 92%|█████████▏| 617M/671M [03:34<00:18, 2.95MB/s] 92%|█████████▏| 617M/671M [03:34<00:20, 2.65MB/s] 92%|█████████▏| 618M/671M [03:34<00:20, 2.66MB/s] 92%|█████████▏| 618M/671M [03:34<00:22, 2.47MB/s] 92%|█████████▏| 618M/671M [03:34<00:23, 2.35MB/s] 92%|█████████▏| 619M/671M [03:34<00:23, 2.28MB/s] 92%|█████████▏| 619M/671M [03:35<00:24, 2.23MB/s] 92%|█████████▏| 619M/671M [03:35<00:23, 2.33MB/s] 92%|█████████▏| 620M/671M [03:35<00:23, 2.32MB/s] 92%|█████████▏| 620M/671M [03:35<00:21, 2.52MB/s] 93%|█████████▎| 620M/671M [03:35<00:19, 2.67MB/s] 93%|█████████▎| 621M/671M [03:35<00:18, 2.82MB/s] 93%|█████████▎| 621M/671M [03:35<00:19, 2.72MB/s] 93%|█████████▎| 621M/671M [03:35<00:18, 2.72MB/s] 93%|█████████▎| 622M/671M [03:36<00:18, 2.71MB/s] 93%|█████████▎| 622M/671M [03:36<00:19, 2.56MB/s] 93%|█████████▎| 622M/671M [03:36<00:18, 2.79MB/s] 93%|█████████▎| 623M/671M [03:36<00:18, 2.71MB/s] 93%|█████████▎| 623M/671M [03:36<00:20, 2.44MB/s] 93%|█████████▎| 623M/671M [03:36<00:21, 2.29MB/s] 93%|█████████▎| 624M/671M [03:36<00:19, 2.56MB/s] 93%|█████████▎| 624M/671M [03:37<00:17, 2.80MB/s] 93%|█████████▎| 624M/671M [03:37<00:17, 2.73MB/s] 93%|█████████▎| 625M/671M [03:37<00:19, 2.47MB/s] 93%|█████████▎| 625M/671M [03:37<00:18, 2.54MB/s] 93%|█████████▎| 625M/671M [03:37<00:17, 2.74MB/s] 93%|█████████▎| 626M/671M [03:37<00:16, 2.91MB/s] 93%|█████████▎| 626M/671M [03:37<00:15, 3.06MB/s] 93%|█████████▎| 626M/671M [03:37<00:18, 2.49MB/s] 93%|█████████▎| 627M/671M [03:38<00:15, 3.05MB/s] 94%|█████████▎| 627M/671M [03:38<00:15, 3.04MB/s] 94%|█████████▎| 627M/671M [03:38<00:14, 3.06MB/s] 94%|█████████▎| 628M/671M [03:38<00:14, 3.05MB/s] 94%|█████████▎| 628M/671M [03:38<00:12, 3.43MB/s] 94%|█████████▎| 629M/671M [03:38<00:12, 3.61MB/s] 94%|█████████▍| 629M/671M [03:38<00:10, 4.25MB/s] 94%|█████████▍| 630M/671M [03:38<00:08, 5.06MB/s] 94%|█████████▍| 631M/671M [03:38<00:06, 6.24MB/s] 94%|█████████▍| 632M/671M [03:38<00:06, 6.66MB/s] 94%|█████████▍| 632M/671M [03:39<00:05, 7.39MB/s] 94%|█████████▍| 633M/671M [03:39<00:05, 7.28MB/s] 95%|█████████▍| 634M/671M [03:39<00:05, 6.93MB/s] 95%|█████████▍| 635M/671M [03:39<00:05, 6.85MB/s] 95%|█████████▍| 635M/671M [03:39<00:05, 6.40MB/s] 95%|█████████▍| 636M/671M [03:39<00:06, 5.47MB/s] 95%|█████████▍| 636M/671M [03:39<00:06, 5.41MB/s] 95%|█████████▍| 637M/671M [03:39<00:07, 4.99MB/s] 95%|█████████▌| 637M/671M [03:40<00:07, 4.76MB/s] 95%|█████████▌| 638M/671M [03:40<00:07, 4.38MB/s] 95%|█████████▌| 638M/671M [03:40<00:10, 3.37MB/s] 95%|█████████▌| 639M/671M [03:40<00:10, 3.19MB/s] 95%|█████████▌| 639M/671M [03:40<00:11, 2.98MB/s] 95%|█████████▌| 639M/671M [03:40<00:11, 2.86MB/s] 95%|█████████▌| 640M/671M [03:40<00:11, 2.84MB/s] 95%|█████████▌| 640M/671M [03:41<00:11, 2.79MB/s] 95%|█████████▌| 640M/671M [03:41<00:11, 2.79MB/s] 95%|█████████▌| 640M/671M [03:41<00:11, 2.77MB/s] 96%|█████████▌| 641M/671M [03:41<00:11, 2.81MB/s] 96%|█████████▌| 641M/671M [03:41<00:10, 2.89MB/s] 96%|█████████▌| 641M/671M [03:41<00:10, 2.90MB/s] 96%|█████████▌| 642M/671M [03:41<00:10, 3.02MB/s] 96%|█████████▌| 642M/671M [03:41<00:08, 3.65MB/s] 96%|█████████▌| 643M/671M [03:41<00:06, 4.34MB/s] 96%|█████████▌| 643M/671M [03:41<00:05, 4.85MB/s] 96%|█████████▌| 644M/671M [03:42<00:05, 5.19MB/s] 96%|█████████▌| 644M/671M [03:42<00:05, 5.30MB/s] 96%|█████████▌| 645M/671M [03:42<00:05, 5.07MB/s] 96%|█████████▋| 645M/671M [03:42<00:05, 4.62MB/s] 96%|█████████▋| 646M/671M [03:42<00:06, 4.17MB/s] 96%|█████████▋| 646M/671M [03:42<00:08, 3.17MB/s] 96%|█████████▋| 647M/671M [03:43<00:09, 2.66MB/s] 96%|█████████▋| 647M/671M [03:43<00:10, 2.34MB/s] 97%|█████████▋| 647M/671M [03:43<00:11, 2.14MB/s] 97%|█████████▋| 647M/671M [03:43<00:11, 2.08MB/s] 97%|█████████▋| 648M/671M [03:43<00:11, 2.01MB/s] 97%|█████████▋| 648M/671M [03:43<00:11, 2.00MB/s] 97%|█████████▋| 648M/671M [03:43<00:10, 2.20MB/s] 97%|█████████▋| 648M/671M [03:44<00:11, 1.95MB/s] 97%|█████████▋| 649M/671M [03:44<00:13, 1.76MB/s] 97%|█████████▋| 649M/671M [03:44<00:12, 1.82MB/s] 97%|█████████▋| 649M/671M [03:44<00:11, 1.93MB/s] 97%|█████████▋| 649M/671M [03:44<00:10, 2.18MB/s] 97%|█████████▋| 650M/671M [03:44<00:09, 2.33MB/s] 97%|█████████▋| 650M/671M [03:44<00:07, 2.97MB/s] 97%|█████████▋| 651M/671M [03:44<00:05, 3.58MB/s] 97%|█████████▋| 651M/671M [03:44<00:05, 4.04MB/s] 97%|█████████▋| 651M/671M [03:45<00:05, 3.56MB/s] 97%|█████████▋| 652M/671M [03:45<00:06, 2.94MB/s] 97%|█████████▋| 652M/671M [03:45<00:07, 2.44MB/s] 97%|█████████▋| 652M/671M [03:45<00:07, 2.42MB/s] 97%|█████████▋| 653M/671M [03:45<00:08, 2.18MB/s] 97%|█████████▋| 653M/671M [03:45<00:08, 2.15MB/s] 97%|█████████▋| 653M/671M [03:45<00:08, 2.11MB/s] 97%|█████████▋| 653M/671M [03:46<00:08, 2.18MB/s] 97%|█████████▋| 654M/671M [03:46<00:07, 2.39MB/s] 98%|█████████▊| 654M/671M [03:46<00:06, 2.51MB/s] 98%|█████████▊| 654M/671M [03:46<00:06, 2.74MB/s] 98%|█████████▊| 655M/671M [03:46<00:06, 2.60MB/s] 98%|█████████▊| 655M/671M [03:46<00:06, 2.42MB/s] 98%|█████████▊| 655M/671M [03:46<00:06, 2.49MB/s] 98%|█████████▊| 655M/671M [03:46<00:06, 2.47MB/s] 98%|█████████▊| 656M/671M [03:46<00:06, 2.38MB/s] 98%|█████████▊| 656M/671M [03:47<00:06, 2.35MB/s] 98%|█████████▊| 656M/671M [03:47<00:06, 2.42MB/s] 98%|█████████▊| 656M/671M [03:47<00:06, 2.44MB/s] 98%|█████████▊| 657M/671M [03:47<00:06, 2.39MB/s] 98%|█████████▊| 657M/671M [03:47<00:06, 2.38MB/s] 98%|█████████▊| 657M/671M [03:47<00:05, 2.45MB/s] 98%|█████████▊| 658M/671M [03:47<00:05, 2.49MB/s] 98%|█████████▊| 658M/671M [03:47<00:05, 2.61MB/s] 98%|█████████▊| 658M/671M [03:48<00:04, 2.64MB/s] 98%|█████████▊| 659M/671M [03:48<00:04, 2.75MB/s] 98%|█████████▊| 659M/671M [03:48<00:04, 2.67MB/s] 98%|█████████▊| 659M/671M [03:48<00:04, 2.76MB/s] 98%|█████████▊| 659M/671M [03:48<00:04, 2.59MB/s] 98%|█████████▊| 660M/671M [03:48<00:04, 2.52MB/s] 98%|█████████▊| 660M/671M [03:48<00:04, 2.51MB/s] 98%|█████████▊| 660M/671M [03:48<00:04, 2.52MB/s] 98%|█████████▊| 660M/671M [03:48<00:04, 2.47MB/s] 99%|█████████▊| 661M/671M [03:49<00:04, 2.47MB/s] 99%|█████████▊| 661M/671M [03:49<00:04, 2.41MB/s] 99%|█████████▊| 661M/671M [03:49<00:04, 2.29MB/s] 99%|█████████▊| 661M/671M [03:49<00:04, 2.26MB/s] 99%|█████████▊| 662M/671M [03:49<00:04, 2.05MB/s] 99%|█████████▊| 662M/671M [03:49<00:04, 2.12MB/s] 99%|█████████▉| 662M/671M [03:49<00:03, 2.43MB/s] 99%|█████████▉| 663M/671M [03:49<00:03, 2.36MB/s] 99%|█████████▉| 663M/671M [03:50<00:02, 2.90MB/s] 99%|█████████▉| 664M/671M [03:50<00:02, 3.57MB/s] 99%|█████████▉| 664M/671M [03:50<00:01, 4.11MB/s] 99%|█████████▉| 665M/671M [03:50<00:01, 4.50MB/s] 99%|█████████▉| 665M/671M [03:50<00:01, 4.77MB/s] 99%|█████████▉| 666M/671M [03:50<00:01, 4.89MB/s] 99%|█████████▉| 666M/671M [03:50<00:00, 4.82MB/s] 99%|█████████▉| 667M/671M [03:50<00:01, 3.89MB/s] 99%|█████████▉| 667M/671M [03:51<00:01, 3.49MB/s]100%|█████████▉| 667M/671M [03:51<00:01, 3.16MB/s]100%|█████████▉| 668M/671M [03:51<00:01, 2.70MB/s]100%|█████████▉| 668M/671M [03:51<00:00, 2.81MB/s]100%|█████████▉| 668M/671M [03:51<00:00, 2.45MB/s]100%|█████████▉| 669M/671M [03:51<00:00, 2.47MB/s]100%|█████████▉| 669M/671M [03:51<00:00, 3.15MB/s]100%|█████████▉| 670M/671M [03:52<00:00, 3.76MB/s]100%|█████████▉| 670M/671M [03:52<00:00, 4.29MB/s]100%|██████████| 671M/671M [03:52<00:00, 3.03MB/s]
[32m[2023-12-21 13:04:05,324] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/xlnet-base-cased/model_state.pdparams[0m
[32m[2023-12-21 13:04:09,212] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-12-21 13:04:14,052] [    INFO][0m - All model checkpoint weights were used when initializing XLNetForSequenceClassification.
[0m
[32m[2023-12-21 13:04:14,053] [    INFO][0m - All the weights of XLNetForSequenceClassification were initialized from the model checkpoint at xlnet-base-cased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLNetForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
[32m[2023-12-21 13:04:19,423] [    INFO][0m - global step 1 / 673490, loss: 0.466471, avg_reader_cost: 0.01024 sec, avg_batch_cost: 5.36216 sec, avg_samples: 1.00000, ips: 0.18649 sequences/sec,  [0m
[32m[2023-12-21 13:04:23,330] [    INFO][0m - global step 2 / 673490, loss: 0.720343, avg_reader_cost: 0.00035 sec, avg_batch_cost: 3.90707 sec, avg_samples: 1.00000, ips: 0.25595 sequences/sec,  [0m
[32m[2023-12-21 13:04:27,470] [    INFO][0m - global step 3 / 673490, loss: 0.684998, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.13929 sec, avg_samples: 1.00000, ips: 0.24159 sequences/sec,  [0m
[32m[2023-12-21 13:04:31,378] [    INFO][0m - global step 4 / 673490, loss: 0.365081, avg_reader_cost: 0.00048 sec, avg_batch_cost: 3.90671 sec, avg_samples: 1.00000, ips: 0.25597 sequences/sec,  [0m
[32m[2023-12-21 13:04:35,457] [    INFO][0m - global step 5 / 673490, loss: 0.630877, avg_reader_cost: 0.00026 sec, avg_batch_cost: 4.07803 sec, avg_samples: 1.00000, ips: 0.24522 sequences/sec,  [0m
[32m[2023-12-21 13:04:39,358] [    INFO][0m - global step 6 / 673490, loss: 0.927675, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.90046 sec, avg_samples: 1.00000, ips: 0.25638 sequences/sec,  [0m
[32m[2023-12-21 13:04:43,266] [    INFO][0m - global step 7 / 673490, loss: 0.746464, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.90806 sec, avg_samples: 1.00000, ips: 0.25588 sequences/sec,  [0m
[32m[2023-12-21 13:04:47,050] [    INFO][0m - global step 8 / 673490, loss: 0.591161, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.78358 sec, avg_samples: 1.00000, ips: 0.26430 sequences/sec,  [0m
[32m[2023-12-21 13:04:51,113] [    INFO][0m - global step 9 / 673490, loss: 0.511496, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.06248 sec, avg_samples: 1.00000, ips: 0.24616 sequences/sec,  [0m
[32m[2023-12-21 13:04:55,090] [    INFO][0m - global step 10 / 673490, loss: 0.301121, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.97637 sec, avg_samples: 1.00000, ips: 0.25149 sequences/sec,  [0m
[32m[2023-12-21 13:04:58,967] [    INFO][0m - global step 11 / 673490, loss: 0.559020, avg_reader_cost: 0.00036 sec, avg_batch_cost: 3.87679 sec, avg_samples: 1.00000, ips: 0.25795 sequences/sec,  [0m
[32m[2023-12-21 13:05:02,797] [    INFO][0m - global step 12 / 673490, loss: 0.985691, avg_reader_cost: 0.00025 sec, avg_batch_cost: 3.82951 sec, avg_samples: 1.00000, ips: 0.26113 sequences/sec,  [0m
[32m[2023-12-21 13:05:06,571] [    INFO][0m - global step 13 / 673490, loss: 1.473255, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.77379 sec, avg_samples: 1.00000, ips: 0.26499 sequences/sec,  [0m
[32m[2023-12-21 13:05:10,392] [    INFO][0m - global step 14 / 673490, loss: 1.284654, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.82031 sec, avg_samples: 1.00000, ips: 0.26176 sequences/sec,  [0m
[32m[2023-12-21 13:05:14,031] [    INFO][0m - global step 15 / 673490, loss: 0.411587, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.63817 sec, avg_samples: 1.00000, ips: 0.27486 sequences/sec,  [0m
[32m[2023-12-21 13:05:17,723] [    INFO][0m - global step 16 / 673490, loss: 1.163261, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.69184 sec, avg_samples: 1.00000, ips: 0.27087 sequences/sec,  [0m
[32m[2023-12-21 13:05:21,628] [    INFO][0m - global step 17 / 673490, loss: 0.984612, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.90519 sec, avg_samples: 1.00000, ips: 0.25607 sequences/sec,  [0m
[32m[2023-12-21 13:05:25,408] [    INFO][0m - global step 18 / 673490, loss: 0.882260, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.77870 sec, avg_samples: 1.00000, ips: 0.26464 sequences/sec,  [0m
[32m[2023-12-21 13:05:29,252] [    INFO][0m - global step 19 / 673490, loss: 0.332970, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.84369 sec, avg_samples: 1.00000, ips: 0.26017 sequences/sec,  [0m
[32m[2023-12-21 13:05:32,835] [    INFO][0m - global step 20 / 673490, loss: 0.279697, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.58338 sec, avg_samples: 1.00000, ips: 0.27907 sequences/sec,  [0m
[32m[2023-12-21 13:05:36,434] [    INFO][0m - global step 21 / 673490, loss: 0.334568, avg_reader_cost: 0.00028 sec, avg_batch_cost: 3.59831 sec, avg_samples: 1.00000, ips: 0.27791 sequences/sec,  [0m
[32m[2023-12-21 13:05:40,566] [    INFO][0m - global step 22 / 673490, loss: 0.265013, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.13166 sec, avg_samples: 1.00000, ips: 0.24203 sequences/sec,  [0m
[32m[2023-12-21 13:05:44,514] [    INFO][0m - global step 23 / 673490, loss: 0.156683, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.94691 sec, avg_samples: 1.00000, ips: 0.25336 sequences/sec,  [0m
[32m[2023-12-21 13:05:48,266] [    INFO][0m - global step 24 / 673490, loss: 0.361235, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.75192 sec, avg_samples: 1.00000, ips: 0.26653 sequences/sec,  [0m
[32m[2023-12-21 13:05:51,978] [    INFO][0m - global step 25 / 673490, loss: 0.260157, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.71160 sec, avg_samples: 1.00000, ips: 0.26943 sequences/sec,  [0m
[32m[2023-12-21 13:05:55,923] [    INFO][0m - global step 26 / 673490, loss: 0.298937, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.94438 sec, avg_samples: 1.00000, ips: 0.25353 sequences/sec,  [0m
[32m[2023-12-21 13:05:59,703] [    INFO][0m - global step 27 / 673490, loss: 1.651342, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.78006 sec, avg_samples: 1.00000, ips: 0.26455 sequences/sec,  [0m
[32m[2023-12-21 13:06:03,462] [    INFO][0m - global step 28 / 673490, loss: 1.330504, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.75802 sec, avg_samples: 1.00000, ips: 0.26610 sequences/sec,  [0m
[32m[2023-12-21 13:06:07,114] [    INFO][0m - global step 29 / 673490, loss: 0.157397, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.65172 sec, avg_samples: 1.00000, ips: 0.27384 sequences/sec,  [0m
[32m[2023-12-21 13:06:11,315] [    INFO][0m - global step 30 / 673490, loss: 0.227325, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.20083 sec, avg_samples: 1.00000, ips: 0.23805 sequences/sec,  [0m
[32m[2023-12-21 13:06:15,084] [    INFO][0m - global step 31 / 673490, loss: 2.177230, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.76918 sec, avg_samples: 1.00000, ips: 0.26531 sequences/sec,  [0m
[32m[2023-12-21 13:06:19,177] [    INFO][0m - global step 32 / 673490, loss: 0.089072, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.09248 sec, avg_samples: 1.00000, ips: 0.24435 sequences/sec,  [0m
[32m[2023-12-21 13:06:22,920] [    INFO][0m - global step 33 / 673490, loss: 1.659066, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.74230 sec, avg_samples: 1.00000, ips: 0.26722 sequences/sec,  [0m
[32m[2023-12-21 13:06:26,745] [    INFO][0m - global step 34 / 673490, loss: 1.534951, avg_reader_cost: 0.00031 sec, avg_batch_cost: 3.82487 sec, avg_samples: 1.00000, ips: 0.26145 sequences/sec,  [0m
[32m[2023-12-21 13:06:30,615] [    INFO][0m - global step 35 / 673490, loss: 0.136632, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.86942 sec, avg_samples: 1.00000, ips: 0.25844 sequences/sec,  [0m
[32m[2023-12-21 13:06:34,685] [    INFO][0m - global step 36 / 673490, loss: 0.205293, avg_reader_cost: 0.00021 sec, avg_batch_cost: 4.06935 sec, avg_samples: 1.00000, ips: 0.24574 sequences/sec,  [0m
[32m[2023-12-21 13:06:38,380] [    INFO][0m - global step 37 / 673490, loss: 2.385578, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.69456 sec, avg_samples: 1.00000, ips: 0.27067 sequences/sec,  [0m
[32m[2023-12-21 13:06:42,323] [    INFO][0m - global step 38 / 673490, loss: 0.333670, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.94310 sec, avg_samples: 1.00000, ips: 0.25361 sequences/sec,  [0m
[32m[2023-12-21 13:06:46,558] [    INFO][0m - global step 39 / 673490, loss: 0.207218, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.23446 sec, avg_samples: 1.00000, ips: 0.23616 sequences/sec,  [0m
[32m[2023-12-21 13:06:50,492] [    INFO][0m - global step 40 / 673490, loss: 0.220793, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.93372 sec, avg_samples: 1.00000, ips: 0.25421 sequences/sec,  [0m
[32m[2023-12-21 13:06:54,502] [    INFO][0m - global step 41 / 673490, loss: 1.322298, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.00903 sec, avg_samples: 1.00000, ips: 0.24944 sequences/sec,  [0m
[32m[2023-12-21 13:06:58,607] [    INFO][0m - global step 42 / 673490, loss: 0.161961, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.10467 sec, avg_samples: 1.00000, ips: 0.24363 sequences/sec,  [0m
[32m[2023-12-21 13:07:02,861] [    INFO][0m - global step 43 / 673490, loss: 0.098297, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.25358 sec, avg_samples: 1.00000, ips: 0.23510 sequences/sec,  [0m
[32m[2023-12-21 13:07:06,742] [    INFO][0m - global step 44 / 673490, loss: 2.413285, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.88076 sec, avg_samples: 1.00000, ips: 0.25768 sequences/sec,  [0m
[32m[2023-12-21 13:07:10,751] [    INFO][0m - global step 45 / 673490, loss: 1.948689, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.00915 sec, avg_samples: 1.00000, ips: 0.24943 sequences/sec,  [0m
[32m[2023-12-21 13:07:14,887] [    INFO][0m - global step 46 / 673490, loss: 0.156909, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.13550 sec, avg_samples: 1.00000, ips: 0.24181 sequences/sec,  [0m
[32m[2023-12-21 13:07:18,735] [    INFO][0m - global step 47 / 673490, loss: 2.048822, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.84719 sec, avg_samples: 1.00000, ips: 0.25993 sequences/sec,  [0m
[32m[2023-12-21 13:07:22,817] [    INFO][0m - global step 48 / 673490, loss: 0.088301, avg_reader_cost: 0.00032 sec, avg_batch_cost: 4.08218 sec, avg_samples: 1.00000, ips: 0.24497 sequences/sec,  [0m
[32m[2023-12-21 13:07:26,631] [    INFO][0m - global step 49 / 673490, loss: 2.072734, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.81285 sec, avg_samples: 1.00000, ips: 0.26227 sequences/sec,  [0m
[32m[2023-12-21 13:07:30,484] [    INFO][0m - global step 50 / 673490, loss: 1.979367, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.85289 sec, avg_samples: 1.00000, ips: 0.25955 sequences/sec,  [0m
[32m[2023-12-21 13:07:34,573] [    INFO][0m - global step 51 / 673490, loss: 0.045765, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.08873 sec, avg_samples: 1.00000, ips: 0.24457 sequences/sec,  [0m
[32m[2023-12-21 13:07:38,436] [    INFO][0m - global step 52 / 673490, loss: 0.163692, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.86273 sec, avg_samples: 1.00000, ips: 0.25888 sequences/sec,  [0m
[32m[2023-12-21 13:07:42,204] [    INFO][0m - global step 53 / 673490, loss: 2.143962, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.76749 sec, avg_samples: 1.00000, ips: 0.26543 sequences/sec,  [0m
[32m[2023-12-21 13:07:46,064] [    INFO][0m - global step 54 / 673490, loss: 0.142862, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.85950 sec, avg_samples: 1.00000, ips: 0.25910 sequences/sec,  [0m
[32m[2023-12-21 13:07:49,845] [    INFO][0m - global step 55 / 673490, loss: 2.305823, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.78050 sec, avg_samples: 1.00000, ips: 0.26452 sequences/sec,  [0m
[32m[2023-12-21 13:07:53,756] [    INFO][0m - global step 56 / 673490, loss: 0.151449, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.91054 sec, avg_samples: 1.00000, ips: 0.25572 sequences/sec,  [0m
[32m[2023-12-21 13:07:57,674] [    INFO][0m - global step 57 / 673490, loss: 0.121963, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.91819 sec, avg_samples: 1.00000, ips: 0.25522 sequences/sec,  [0m
[32m[2023-12-21 13:08:01,817] [    INFO][0m - global step 58 / 673490, loss: 0.086427, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.14217 sec, avg_samples: 1.00000, ips: 0.24142 sequences/sec,  [0m
[32m[2023-12-21 13:08:05,972] [    INFO][0m - global step 59 / 673490, loss: 0.071800, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.15509 sec, avg_samples: 1.00000, ips: 0.24067 sequences/sec,  [0m
[32m[2023-12-21 13:08:10,029] [    INFO][0m - global step 60 / 673490, loss: 0.134914, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.05613 sec, avg_samples: 1.00000, ips: 0.24654 sequences/sec,  [0m
[32m[2023-12-21 13:08:14,206] [    INFO][0m - global step 61 / 673490, loss: 0.056795, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.17649 sec, avg_samples: 1.00000, ips: 0.23944 sequences/sec,  [0m
[32m[2023-12-21 13:08:18,673] [    INFO][0m - global step 62 / 673490, loss: 0.033514, avg_reader_cost: 0.00026 sec, avg_batch_cost: 4.46657 sec, avg_samples: 1.00000, ips: 0.22389 sequences/sec,  [0m
[32m[2023-12-21 13:08:22,691] [    INFO][0m - global step 63 / 673490, loss: 0.044675, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.01810 sec, avg_samples: 1.00000, ips: 0.24887 sequences/sec,  [0m
[32m[2023-12-21 13:08:27,119] [    INFO][0m - global step 64 / 673490, loss: 0.012527, avg_reader_cost: 0.00018 sec, avg_batch_cost: 4.42738 sec, avg_samples: 1.00000, ips: 0.22587 sequences/sec,  [0m
[32m[2023-12-21 13:08:31,487] [    INFO][0m - global step 65 / 673490, loss: 0.094859, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.36712 sec, avg_samples: 1.00000, ips: 0.22898 sequences/sec,  [0m
[32m[2023-12-21 13:08:35,932] [    INFO][0m - global step 66 / 673490, loss: 0.027158, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.44495 sec, avg_samples: 1.00000, ips: 0.22497 sequences/sec,  [0m
[32m[2023-12-21 13:08:39,995] [    INFO][0m - global step 67 / 673490, loss: 0.064108, avg_reader_cost: 0.00017 sec, avg_batch_cost: 4.06300 sec, avg_samples: 1.00000, ips: 0.24612 sequences/sec,  [0m
[32m[2023-12-21 13:08:44,255] [    INFO][0m - global step 68 / 673490, loss: 0.034139, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.25921 sec, avg_samples: 1.00000, ips: 0.23479 sequences/sec,  [0m
[32m[2023-12-21 13:08:48,042] [    INFO][0m - global step 69 / 673490, loss: 4.398359, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.78665 sec, avg_samples: 1.00000, ips: 0.26409 sequences/sec,  [0m
[32m[2023-12-21 13:08:51,902] [    INFO][0m - global step 70 / 673490, loss: 5.110398, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.85934 sec, avg_samples: 1.00000, ips: 0.25911 sequences/sec,  [0m
[32m[2023-12-21 13:08:55,932] [    INFO][0m - global step 71 / 673490, loss: 5.003105, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.02986 sec, avg_samples: 1.00000, ips: 0.24815 sequences/sec,  [0m
[32m[2023-12-21 13:08:59,809] [    INFO][0m - global step 72 / 673490, loss: 5.212582, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.87646 sec, avg_samples: 1.00000, ips: 0.25797 sequences/sec,  [0m
[32m[2023-12-21 13:09:03,787] [    INFO][0m - global step 73 / 673490, loss: 5.071333, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.97756 sec, avg_samples: 1.00000, ips: 0.25141 sequences/sec,  [0m
[32m[2023-12-21 13:09:07,806] [    INFO][0m - global step 74 / 673490, loss: 0.105574, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.01922 sec, avg_samples: 1.00000, ips: 0.24880 sequences/sec,  [0m
[32m[2023-12-21 13:09:11,586] [    INFO][0m - global step 75 / 673490, loss: 4.272330, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.77906 sec, avg_samples: 1.00000, ips: 0.26462 sequences/sec,  [0m
[32m[2023-12-21 13:09:15,369] [    INFO][0m - global step 76 / 673490, loss: 4.113063, avg_reader_cost: 0.00027 sec, avg_batch_cost: 3.78259 sec, avg_samples: 1.00000, ips: 0.26437 sequences/sec,  [0m
[32m[2023-12-21 13:09:19,704] [    INFO][0m - global step 77 / 673490, loss: 0.037038, avg_reader_cost: 0.00019 sec, avg_batch_cost: 4.33422 sec, avg_samples: 1.00000, ips: 0.23072 sequences/sec,  [0m
[32m[2023-12-21 13:09:24,135] [    INFO][0m - global step 78 / 673490, loss: 0.016936, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.43057 sec, avg_samples: 1.00000, ips: 0.22570 sequences/sec,  [0m
[32m[2023-12-21 13:09:28,048] [    INFO][0m - global step 79 / 673490, loss: 2.321078, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.91292 sec, avg_samples: 1.00000, ips: 0.25556 sequences/sec,  [0m
[32m[2023-12-21 13:09:31,917] [    INFO][0m - global step 80 / 673490, loss: 3.921530, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.86861 sec, avg_samples: 1.00000, ips: 0.25849 sequences/sec,  [0m
[32m[2023-12-21 13:09:36,057] [    INFO][0m - global step 81 / 673490, loss: 4.326421, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.14022 sec, avg_samples: 1.00000, ips: 0.24153 sequences/sec,  [0m
[32m[2023-12-21 13:09:40,515] [    INFO][0m - global step 82 / 673490, loss: 0.048018, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.45685 sec, avg_samples: 1.00000, ips: 0.22437 sequences/sec,  [0m
[32m[2023-12-21 13:09:44,101] [    INFO][0m - global step 83 / 673490, loss: 3.182341, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.58630 sec, avg_samples: 1.00000, ips: 0.27884 sequences/sec,  [0m
[32m[2023-12-21 13:09:48,026] [    INFO][0m - global step 84 / 673490, loss: 3.203467, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.92416 sec, avg_samples: 1.00000, ips: 0.25483 sequences/sec,  [0m
[32m[2023-12-21 13:09:52,077] [    INFO][0m - global step 85 / 673490, loss: 0.126848, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.05087 sec, avg_samples: 1.00000, ips: 0.24686 sequences/sec,  [0m
[32m[2023-12-21 13:09:55,895] [    INFO][0m - global step 86 / 673490, loss: 2.360058, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.81755 sec, avg_samples: 1.00000, ips: 0.26195 sequences/sec,  [0m
[32m[2023-12-21 13:09:59,777] [    INFO][0m - global step 87 / 673490, loss: 2.460705, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.88143 sec, avg_samples: 1.00000, ips: 0.25764 sequences/sec,  [0m
[32m[2023-12-21 13:10:03,753] [    INFO][0m - global step 88 / 673490, loss: 2.707981, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.97542 sec, avg_samples: 1.00000, ips: 0.25155 sequences/sec,  [0m
[32m[2023-12-21 13:10:07,528] [    INFO][0m - global step 89 / 673490, loss: 1.360416, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.77458 sec, avg_samples: 1.00000, ips: 0.26493 sequences/sec,  [0m
[32m[2023-12-21 13:10:11,604] [    INFO][0m - global step 90 / 673490, loss: 0.268151, avg_reader_cost: 0.00029 sec, avg_batch_cost: 4.07559 sec, avg_samples: 1.00000, ips: 0.24536 sequences/sec,  [0m
[32m[2023-12-21 13:10:15,738] [    INFO][0m - global step 91 / 673490, loss: 0.228371, avg_reader_cost: 0.00019 sec, avg_batch_cost: 4.13356 sec, avg_samples: 1.00000, ips: 0.24192 sequences/sec,  [0m
[32m[2023-12-21 13:10:19,775] [    INFO][0m - global step 92 / 673490, loss: 0.188835, avg_reader_cost: 0.00023 sec, avg_batch_cost: 4.03644 sec, avg_samples: 1.00000, ips: 0.24774 sequences/sec,  [0m
[32m[2023-12-21 13:10:23,696] [    INFO][0m - global step 93 / 673490, loss: 1.928336, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.92082 sec, avg_samples: 1.00000, ips: 0.25505 sequences/sec,  [0m
[32m[2023-12-21 13:10:27,921] [    INFO][0m - global step 94 / 673490, loss: 0.367226, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.22509 sec, avg_samples: 1.00000, ips: 0.23668 sequences/sec,  [0m
[32m[2023-12-21 13:10:31,782] [    INFO][0m - global step 95 / 673490, loss: 1.651051, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.86072 sec, avg_samples: 1.00000, ips: 0.25902 sequences/sec,  [0m
[32m[2023-12-21 13:10:35,749] [    INFO][0m - global step 96 / 673490, loss: 1.774799, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.96667 sec, avg_samples: 1.00000, ips: 0.25210 sequences/sec,  [0m
[32m[2023-12-21 13:10:40,042] [    INFO][0m - global step 97 / 673490, loss: 0.177920, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.29209 sec, avg_samples: 1.00000, ips: 0.23299 sequences/sec,  [0m
[32m[2023-12-21 13:10:44,068] [    INFO][0m - global step 98 / 673490, loss: 0.201547, avg_reader_cost: 0.00022 sec, avg_batch_cost: 4.02617 sec, avg_samples: 1.00000, ips: 0.24837 sequences/sec,  [0m
[32m[2023-12-21 13:10:48,244] [    INFO][0m - global step 99 / 673490, loss: 0.194889, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.17553 sec, avg_samples: 1.00000, ips: 0.23949 sequences/sec,  [0m
[32m[2023-12-21 13:10:52,358] [    INFO][0m - global step 100 / 673490, loss: 1.441352, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.11376 sec, avg_samples: 1.00000, ips: 0.24309 sequences/sec,  [0m
[32m[2023-12-21 13:10:56,184] [    INFO][0m - global step 101 / 673490, loss: 2.284238, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.82503 sec, avg_samples: 1.00000, ips: 0.26144 sequences/sec,  [0m
[32m[2023-12-21 13:10:59,988] [    INFO][0m - global step 102 / 673490, loss: 0.309621, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.80382 sec, avg_samples: 1.00000, ips: 0.26289 sequences/sec,  [0m
[32m[2023-12-21 13:11:03,949] [    INFO][0m - global step 103 / 673490, loss: 1.530324, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.96073 sec, avg_samples: 1.00000, ips: 0.25248 sequences/sec,  [0m
[32m[2023-12-21 13:11:07,926] [    INFO][0m - global step 104 / 673490, loss: 1.707521, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.97594 sec, avg_samples: 1.00000, ips: 0.25151 sequences/sec,  [0m
[32m[2023-12-21 13:11:12,201] [    INFO][0m - global step 105 / 673490, loss: 0.323374, avg_reader_cost: 0.00018 sec, avg_batch_cost: 4.27513 sec, avg_samples: 1.00000, ips: 0.23391 sequences/sec,  [0m
[32m[2023-12-21 13:11:16,052] [    INFO][0m - global step 106 / 673490, loss: 0.487563, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.85045 sec, avg_samples: 1.00000, ips: 0.25971 sequences/sec,  [0m
[32m[2023-12-21 13:11:20,035] [    INFO][0m - global step 107 / 673490, loss: 0.243124, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.98285 sec, avg_samples: 1.00000, ips: 0.25108 sequences/sec,  [0m
[32m[2023-12-21 13:11:24,154] [    INFO][0m - global step 108 / 673490, loss: 0.293691, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.11864 sec, avg_samples: 1.00000, ips: 0.24280 sequences/sec,  [0m
[32m[2023-12-21 13:11:28,269] [    INFO][0m - global step 109 / 673490, loss: 1.495553, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.11427 sec, avg_samples: 1.00000, ips: 0.24306 sequences/sec,  [0m
[32m[2023-12-21 13:11:32,174] [    INFO][0m - global step 110 / 673490, loss: 0.449407, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.90484 sec, avg_samples: 1.00000, ips: 0.25609 sequences/sec,  [0m
[32m[2023-12-21 13:11:36,015] [    INFO][0m - global step 111 / 673490, loss: 1.920977, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.84066 sec, avg_samples: 1.00000, ips: 0.26037 sequences/sec,  [0m
[32m[2023-12-21 13:11:40,124] [    INFO][0m - global step 112 / 673490, loss: 1.136146, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.10841 sec, avg_samples: 1.00000, ips: 0.24340 sequences/sec,  [0m
[32m[2023-12-21 13:11:43,972] [    INFO][0m - global step 113 / 673490, loss: 2.452014, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.84782 sec, avg_samples: 1.00000, ips: 0.25989 sequences/sec,  [0m
[32m[2023-12-21 13:11:47,732] [    INFO][0m - global step 114 / 673490, loss: 2.188278, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.75893 sec, avg_samples: 1.00000, ips: 0.26603 sequences/sec,  [0m
[32m[2023-12-21 13:11:51,558] [    INFO][0m - global step 115 / 673490, loss: 1.475914, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.82632 sec, avg_samples: 1.00000, ips: 0.26135 sequences/sec,  [0m
[32m[2023-12-21 13:11:55,630] [    INFO][0m - global step 116 / 673490, loss: 0.319279, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.07154 sec, avg_samples: 1.00000, ips: 0.24561 sequences/sec,  [0m
[32m[2023-12-21 13:11:59,562] [    INFO][0m - global step 117 / 673490, loss: 0.418990, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.93112 sec, avg_samples: 1.00000, ips: 0.25438 sequences/sec,  [0m
[32m[2023-12-21 13:12:03,563] [    INFO][0m - global step 118 / 673490, loss: 1.342475, avg_reader_cost: 0.00030 sec, avg_batch_cost: 4.00034 sec, avg_samples: 1.00000, ips: 0.24998 sequences/sec,  [0m
[32m[2023-12-21 13:12:07,486] [    INFO][0m - global step 119 / 673490, loss: 1.605483, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.92253 sec, avg_samples: 1.00000, ips: 0.25494 sequences/sec,  [0m
[32m[2023-12-21 13:12:11,440] [    INFO][0m - global step 120 / 673490, loss: 0.478356, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.95366 sec, avg_samples: 1.00000, ips: 0.25293 sequences/sec,  [0m
[32m[2023-12-21 13:12:15,776] [    INFO][0m - global step 121 / 673490, loss: 0.155306, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.33630 sec, avg_samples: 1.00000, ips: 0.23061 sequences/sec,  [0m
[32m[2023-12-21 13:12:19,984] [    INFO][0m - global step 122 / 673490, loss: 0.300719, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.20739 sec, avg_samples: 1.00000, ips: 0.23768 sequences/sec,  [0m
[32m[2023-12-21 13:12:23,964] [    INFO][0m - global step 123 / 673490, loss: 0.294576, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.97957 sec, avg_samples: 1.00000, ips: 0.25128 sequences/sec,  [0m
[32m[2023-12-21 13:12:28,240] [    INFO][0m - global step 124 / 673490, loss: 0.266341, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.27602 sec, avg_samples: 1.00000, ips: 0.23386 sequences/sec,  [0m
[32m[2023-12-21 13:12:32,013] [    INFO][0m - global step 125 / 673490, loss: 0.400877, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.77197 sec, avg_samples: 1.00000, ips: 0.26511 sequences/sec,  [0m
[32m[2023-12-21 13:12:35,811] [    INFO][0m - global step 126 / 673490, loss: 1.698313, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.79740 sec, avg_samples: 1.00000, ips: 0.26334 sequences/sec,  [0m
[32m[2023-12-21 13:12:40,018] [    INFO][0m - global step 127 / 673490, loss: 0.247070, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.20730 sec, avg_samples: 1.00000, ips: 0.23768 sequences/sec,  [0m
[32m[2023-12-21 13:12:44,048] [    INFO][0m - global step 128 / 673490, loss: 1.790277, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.02948 sec, avg_samples: 1.00000, ips: 0.24817 sequences/sec,  [0m
[32m[2023-12-21 13:12:48,491] [    INFO][0m - global step 129 / 673490, loss: 0.165467, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.44263 sec, avg_samples: 1.00000, ips: 0.22509 sequences/sec,  [0m
[32m[2023-12-21 13:12:52,199] [    INFO][0m - global step 130 / 673490, loss: 2.031081, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.70791 sec, avg_samples: 1.00000, ips: 0.26969 sequences/sec,  [0m
[32m[2023-12-21 13:12:55,847] [    INFO][0m - global step 131 / 673490, loss: 1.519075, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.64773 sec, avg_samples: 1.00000, ips: 0.27414 sequences/sec,  [0m
[32m[2023-12-21 13:12:59,908] [    INFO][0m - global step 132 / 673490, loss: 1.514142, avg_reader_cost: 0.00023 sec, avg_batch_cost: 4.06031 sec, avg_samples: 1.00000, ips: 0.24629 sequences/sec,  [0m
[32m[2023-12-21 13:13:04,065] [    INFO][0m - global step 133 / 673490, loss: 1.295792, avg_reader_cost: 0.00023 sec, avg_batch_cost: 4.15642 sec, avg_samples: 1.00000, ips: 0.24059 sequences/sec,  [0m
[32m[2023-12-21 13:13:07,787] [    INFO][0m - global step 134 / 673490, loss: 1.842661, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.72141 sec, avg_samples: 1.00000, ips: 0.26872 sequences/sec,  [0m
[32m[2023-12-21 13:13:11,812] [    INFO][0m - global step 135 / 673490, loss: 0.247268, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.02479 sec, avg_samples: 1.00000, ips: 0.24846 sequences/sec,  [0m
[32m[2023-12-21 13:13:15,944] [    INFO][0m - global step 136 / 673490, loss: 0.295281, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.13127 sec, avg_samples: 1.00000, ips: 0.24206 sequences/sec,  [0m
[32m[2023-12-21 13:13:19,626] [    INFO][0m - global step 137 / 673490, loss: 1.686694, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.68215 sec, avg_samples: 1.00000, ips: 0.27158 sequences/sec,  [0m
[32m[2023-12-21 13:13:23,591] [    INFO][0m - global step 138 / 673490, loss: 1.621693, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.96422 sec, avg_samples: 1.00000, ips: 0.25226 sequences/sec,  [0m
[32m[2023-12-21 13:13:27,280] [    INFO][0m - global step 139 / 673490, loss: 1.969909, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.68850 sec, avg_samples: 1.00000, ips: 0.27111 sequences/sec,  [0m
[32m[2023-12-21 13:13:31,035] [    INFO][0m - global step 140 / 673490, loss: 1.390813, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.75469 sec, avg_samples: 1.00000, ips: 0.26633 sequences/sec,  [0m
[32m[2023-12-21 13:13:34,778] [    INFO][0m - global step 141 / 673490, loss: 1.202716, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.74246 sec, avg_samples: 1.00000, ips: 0.26720 sequences/sec,  [0m
[32m[2023-12-21 13:13:38,785] [    INFO][0m - global step 142 / 673490, loss: 0.336942, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.00716 sec, avg_samples: 1.00000, ips: 0.24955 sequences/sec,  [0m
[32m[2023-12-21 13:13:42,815] [    INFO][0m - global step 143 / 673490, loss: 1.201128, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.02921 sec, avg_samples: 1.00000, ips: 0.24819 sequences/sec,  [0m
[32m[2023-12-21 13:13:46,756] [    INFO][0m - global step 144 / 673490, loss: 1.433310, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.94043 sec, avg_samples: 1.00000, ips: 0.25378 sequences/sec,  [0m
[32m[2023-12-21 13:13:50,767] [    INFO][0m - global step 145 / 673490, loss: 0.391567, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.01103 sec, avg_samples: 1.00000, ips: 0.24931 sequences/sec,  [0m
[32m[2023-12-21 13:13:54,593] [    INFO][0m - global step 146 / 673490, loss: 0.457643, avg_reader_cost: 0.00032 sec, avg_batch_cost: 3.82552 sec, avg_samples: 1.00000, ips: 0.26140 sequences/sec,  [0m
[32m[2023-12-21 13:13:58,308] [    INFO][0m - global step 147 / 673490, loss: 0.842067, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.71413 sec, avg_samples: 1.00000, ips: 0.26924 sequences/sec,  [0m
[32m[2023-12-21 13:14:02,229] [    INFO][0m - global step 148 / 673490, loss: 0.263272, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.92081 sec, avg_samples: 1.00000, ips: 0.25505 sequences/sec,  [0m
[32m[2023-12-21 13:14:06,210] [    INFO][0m - global step 149 / 673490, loss: 1.524588, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.98125 sec, avg_samples: 1.00000, ips: 0.25118 sequences/sec,  [0m
No XPU Memory Leak
[33m Run successfully with command - xlnet - python3.9 test_tipc/train.py --model xlnet --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path xlnet-base-cased --pad_to_max_seq_len --max_seq_len 128 --logging_steps 1 --task_name SST-2 --max_steps 150 --device=xpu  --save_model=/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1             >/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log 2>&1 - /workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
LAUNCH INFO 2023-12-21 13:14:16,259 -----------  Configuration  ----------------------
LAUNCH INFO 2023-12-21 13:14:16,259 auto_parallel_config: None
LAUNCH INFO 2023-12-21 13:14:16,259 auto_tuner_json: None
LAUNCH INFO 2023-12-21 13:14:16,260 devices: 0,1
LAUNCH INFO 2023-12-21 13:14:16,260 elastic_level: -1
LAUNCH INFO 2023-12-21 13:14:16,260 elastic_timeout: 30
LAUNCH INFO 2023-12-21 13:14:16,260 enable_gpu_log: True
LAUNCH INFO 2023-12-21 13:14:16,260 gloo_port: 6767
LAUNCH INFO 2023-12-21 13:14:16,260 host: None
LAUNCH INFO 2023-12-21 13:14:16,260 ips: None
LAUNCH INFO 2023-12-21 13:14:16,260 job_id: default
LAUNCH INFO 2023-12-21 13:14:16,260 legacy: False
LAUNCH INFO 2023-12-21 13:14:16,260 log_dir: log
LAUNCH INFO 2023-12-21 13:14:16,260 log_level: INFO
LAUNCH INFO 2023-12-21 13:14:16,260 log_overwrite: False
LAUNCH INFO 2023-12-21 13:14:16,260 master: None
LAUNCH INFO 2023-12-21 13:14:16,260 max_restart: 3
LAUNCH INFO 2023-12-21 13:14:16,260 nnodes: 1
LAUNCH INFO 2023-12-21 13:14:16,260 nproc_per_node: None
LAUNCH INFO 2023-12-21 13:14:16,260 rank: -1
LAUNCH INFO 2023-12-21 13:14:16,260 run_mode: collective
LAUNCH INFO 2023-12-21 13:14:16,260 server_num: None
LAUNCH INFO 2023-12-21 13:14:16,260 servers: 
LAUNCH INFO 2023-12-21 13:14:16,260 sort_ip: False
LAUNCH INFO 2023-12-21 13:14:16,260 start_port: 6070
LAUNCH INFO 2023-12-21 13:14:16,260 trainer_num: None
LAUNCH INFO 2023-12-21 13:14:16,260 trainers: 
LAUNCH INFO 2023-12-21 13:14:16,260 training_script: test_tipc/train.py
LAUNCH INFO 2023-12-21 13:14:16,261 training_script_args: ['--model', 'xlnet', '--optimizer', 'adamw', '--lr_scheduler', 'linear_decay_with_warmup', '--learning_rate', '2e-5', '--max_grad_norm', '1.0', '--model_name_or_path', 'xlnet-base-cased', '--pad_to_max_seq_len', '--max_seq_len', '128', '--logging_steps', '1', '--task_name', 'SST-2', '--max_steps', '150', '--device=xpu', '--save_model=/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1']
LAUNCH INFO 2023-12-21 13:14:16,261 with_gloo: 1
LAUNCH INFO 2023-12-21 13:14:16,261 --------------------------------------------------
LAUNCH INFO 2023-12-21 13:14:16,261 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-12-21 13:14:16,262 Run Pod: mrpnke, replicas 2, status ready
LAUNCH INFO 2023-12-21 13:14:16,279 Watching Pod: mrpnke, replicas 2, status running
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='seq2seq', logging_steps=10, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=1, max_seq_len=50, data_dir=None, pad_to_max_seq_len=False, optimizer='adam', learning_rate=0.001, lr_scheduler=None, scheduler_update_by_epoch=False, max_grad_norm=5.0, num_layers=2, hidden_size=512, dropout=0.2, init_scale=0.1, max_len=50)
[32m[2023-12-21 12:58:00,622] [    INFO][0m - global step 10 / 1332070, loss: 228.321655, avg_reader_cost: 0.00233 sec, avg_batch_cost: 0.44001 sec, avg_samples: 26.00000, ips: 33.86273 words/sec,  [0m
[32m[2023-12-21 12:58:08,069] [    INFO][0m - global step 20 / 1332070, loss: 386.595825, avg_reader_cost: 0.00023 sec, avg_batch_cost: 0.74455 sec, avg_samples: 51.00000, ips: 44.99340 words/sec,  [0m
[32m[2023-12-21 12:58:12,479] [    INFO][0m - global step 30 / 1332070, loss: 150.166443, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.44090 sec, avg_samples: 22.00000, ips: 36.74287 words/sec,  [0m
[32m[2023-12-21 12:58:19,865] [    INFO][0m - global step 40 / 1332070, loss: 332.282257, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.73851 sec, avg_samples: 51.00000, ips: 44.27848 words/sec,  [0m
[32m[2023-12-21 12:58:24,215] [    INFO][0m - global step 50 / 1332070, loss: 200.645782, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.43481 sec, avg_samples: 26.00000, ips: 35.64803 words/sec,  [0m
[32m[2023-12-21 12:58:31,018] [    INFO][0m - global step 60 / 1332070, loss: 312.869110, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.68025 sec, avg_samples: 46.00000, ips: 40.27947 words/sec,  [0m
[32m[2023-12-21 12:58:35,212] [    INFO][0m - global step 70 / 1332070, loss: 120.087723, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.41928 sec, avg_samples: 18.00000, ips: 35.06046 words/sec,  [0m
[32m[2023-12-21 12:58:43,601] [    INFO][0m - global step 80 / 1332070, loss: 358.197357, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.83869 sec, avg_samples: 51.00000, ips: 46.38186 words/sec,  [0m
[32m[2023-12-21 12:58:47,334] [    INFO][0m - global step 90 / 1332070, loss: 103.245361, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.37326 sec, avg_samples: 17.00000, ips: 31.61318 words/sec,  [0m
[32m[2023-12-21 12:58:53,977] [    INFO][0m - global step 100 / 1332070, loss: 332.505707, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.66421 sec, avg_samples: 51.00000, ips: 44.56394 words/sec,  [0m
[32m[2023-12-21 12:58:58,553] [    INFO][0m - global step 110 / 1332070, loss: 130.306122, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.45744 sec, avg_samples: 26.00000, ips: 37.16299 words/sec,  [0m
[32m[2023-12-21 12:58:59,427] [    INFO][0m - global step 90 / 2668000, loss: 3.099251, avg_reader_cost: 0.00041 sec, avg_batch_cost: 36.44627 sec, avg_samples: 512.00000, ips: 14.04808 words/sec,  [0m
[32m[2023-12-21 12:59:05,515] [    INFO][0m - global step 120 / 1332070, loss: 364.233826, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.69605 sec, avg_samples: 51.00000, ips: 42.66954 words/sec,  [0m
[32m[2023-12-21 12:59:09,504] [    INFO][0m - global step 130 / 1332070, loss: 90.717468, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.39886 sec, avg_samples: 18.00000, ips: 33.84686 words/sec,  [0m
[32m[2023-12-21 12:59:15,772] [    INFO][0m - global step 140 / 1332070, loss: 277.708832, avg_reader_cost: 0.00020 sec, avg_batch_cost: 0.62664 sec, avg_samples: 43.00000, ips: 44.52308 words/sec,  [0m
No XPU Memory Leak
[32m[2023-12-21 13:04:59,236] [    INFO][0m - global step 100 / 2668000, loss: 3.585686, avg_reader_cost: 0.00037 sec, avg_batch_cost: 35.98087 sec, avg_samples: 512.00000, ips: 14.22978 words/sec,  [0m
[32m[2023-12-21 13:11:12,990] [    INFO][0m - global step 110 / 2668000, loss: 3.197540, avg_reader_cost: 0.00038 sec, avg_batch_cost: 37.37535 sec, avg_samples: 512.00000, ips: 13.69887 words/sec,  [0m
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='xlnet', logging_steps=1, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=1, max_seq_len=128, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='xlnet-base-cased', task_name='SST-2', max_seq_length=128, warmup_steps=0, warmup_proportion=0.1)
[32m[2023-12-21 13:14:19,659] [    INFO][0m - Already cached /root/.paddlenlp/models/xlnet-base-cased/xlnet-base-cased-spiece.model[0m
[32m[2023-12-21 13:14:19,708] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/xlnet-base-cased/tokenizer_config.json[0m
[32m[2023-12-21 13:14:19,709] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/xlnet-base-cased/special_tokens_map.json[0m
[32m[2023-12-21 13:14:19,962] [    INFO][0m - Already cached /root/.paddlenlp/models/xlnet-base-cased/model_state.pdparams[0m
[32m[2023-12-21 13:14:19,962] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/xlnet-base-cased/model_state.pdparams[0m
[32m[2023-12-21 13:14:23,543] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-12-21 13:14:28,198] [    INFO][0m - All model checkpoint weights were used when initializing XLNetForSequenceClassification.
[0m
[32m[2023-12-21 13:14:28,198] [    INFO][0m - All the weights of XLNetForSequenceClassification were initialized from the model checkpoint at xlnet-base-cased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLNetForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
XCCL /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libbkcl.so loaded
XPURT /opt/py39/lib/python3.9/site-packages/paddle/base/../libs/libxpurt.so.1 loaded
/opt/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
Namespace(device='xpu', model='xlnet', logging_steps=1, seed=None, use_amp=None, scale_loss=128, amp_level='O2', amp_use_promote=False, custom_black_list=None, to_static=False, max_steps=150, epoch=10, generated_inputs=False, num_workers=4, profiler_options=None, save_model='/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1', batch_size=1, max_seq_len=128, data_dir=None, pad_to_max_seq_len=True, optimizer='adamw', learning_rate=2e-05, lr_scheduler='linear_decay_with_warmup', scheduler_update_by_epoch=False, beta1=0.9, beta2=0.999, epsilon=1e-08, max_grad_norm=1.0, weight_decay=0.0, model_name_or_path='xlnet-base-cased', task_name='SST-2', max_seq_length=128, warmup_steps=0, warmup_proportion=0.1)
[32m[2023-12-21 13:14:19,659] [    INFO][0m - Already cached /root/.paddlenlp/models/xlnet-base-cased/xlnet-base-cased-spiece.model[0m
[32m[2023-12-21 13:14:19,708] [    INFO][0m - tokenizer config file saved in /root/.paddlenlp/models/xlnet-base-cased/tokenizer_config.json[0m
[32m[2023-12-21 13:14:19,709] [    INFO][0m - Special tokens file saved in /root/.paddlenlp/models/xlnet-base-cased/special_tokens_map.json[0m
[32m[2023-12-21 13:14:19,962] [    INFO][0m - Already cached /root/.paddlenlp/models/xlnet-base-cased/model_state.pdparams[0m
[32m[2023-12-21 13:14:19,962] [    INFO][0m - Loading weights file model_state.pdparams from cache at /root/.paddlenlp/models/xlnet-base-cased/model_state.pdparams[0m
[32m[2023-12-21 13:14:23,543] [    INFO][0m - Loaded weights file from disk, setting weights to model.[0m
[32m[2023-12-21 13:14:28,198] [    INFO][0m - All model checkpoint weights were used when initializing XLNetForSequenceClassification.
[0m
[32m[2023-12-21 13:14:28,198] [    INFO][0m - All the weights of XLNetForSequenceClassification were initialized from the model checkpoint at xlnet-base-cased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use XLNetForSequenceClassification for predictions without further training.[0m
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:2353: FutureWarning: The `max_seq_len` argument is deprecated and will be removed in a future version, please use `max_length` instead.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1925: UserWarning: Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  warnings.warn(
/opt/py39/lib/python3.9/site-packages/paddlenlp-2.6.1.post0-py3.9.egg/paddlenlp/transformers/tokenizer_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
[32m[2023-12-21 13:14:32,604] [    INFO][0m - global step 1 / 336750, loss: 1.069450, avg_reader_cost: 0.01889 sec, avg_batch_cost: 4.40287 sec, avg_samples: 1.00000, ips: 0.22712 sequences/sec,  [0m
[32m[2023-12-21 13:14:36,573] [    INFO][0m - global step 2 / 336750, loss: 0.593711, avg_reader_cost: 0.00027 sec, avg_batch_cost: 3.96776 sec, avg_samples: 1.00000, ips: 0.25203 sequences/sec,  [0m
[32m[2023-12-21 13:14:40,660] [    INFO][0m - global step 3 / 336750, loss: 0.342273, avg_reader_cost: 0.00027 sec, avg_batch_cost: 4.08694 sec, avg_samples: 1.00000, ips: 0.24468 sequences/sec,  [0m
[32m[2023-12-21 13:14:44,819] [    INFO][0m - global step 4 / 336750, loss: 0.409784, avg_reader_cost: 0.00017 sec, avg_batch_cost: 4.15790 sec, avg_samples: 1.00000, ips: 0.24051 sequences/sec,  [0m
[32m[2023-12-21 13:14:48,561] [    INFO][0m - global step 5 / 336750, loss: 0.294520, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.74187 sec, avg_samples: 1.00000, ips: 0.26725 sequences/sec,  [0m
[32m[2023-12-21 13:14:52,384] [    INFO][0m - global step 6 / 336750, loss: 1.002215, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.82200 sec, avg_samples: 1.00000, ips: 0.26164 sequences/sec,  [0m
[32m[2023-12-21 13:14:56,074] [    INFO][0m - global step 7 / 336750, loss: 1.096548, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.68973 sec, avg_samples: 1.00000, ips: 0.27102 sequences/sec,  [0m
[32m[2023-12-21 13:14:59,788] [    INFO][0m - global step 8 / 336750, loss: 0.640906, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.71396 sec, avg_samples: 1.00000, ips: 0.26925 sequences/sec,  [0m
[32m[2023-12-21 13:15:03,544] [    INFO][0m - global step 9 / 336750, loss: 0.476790, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.75503 sec, avg_samples: 1.00000, ips: 0.26631 sequences/sec,  [0m
[32m[2023-12-21 13:15:07,355] [    INFO][0m - global step 10 / 336750, loss: 0.738077, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.81067 sec, avg_samples: 1.00000, ips: 0.26242 sequences/sec,  [0m
[32m[2023-12-21 13:15:10,937] [    INFO][0m - global step 11 / 336750, loss: 1.218149, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.58185 sec, avg_samples: 1.00000, ips: 0.27919 sequences/sec,  [0m
[32m[2023-12-21 13:15:14,988] [    INFO][0m - global step 12 / 336750, loss: 0.357612, avg_reader_cost: 0.00022 sec, avg_batch_cost: 4.05038 sec, avg_samples: 1.00000, ips: 0.24689 sequences/sec,  [0m
[32m[2023-12-21 13:15:18,516] [    INFO][0m - global step 13 / 336750, loss: 0.575731, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.52728 sec, avg_samples: 1.00000, ips: 0.28350 sequences/sec,  [0m
[32m[2023-12-21 13:15:21,993] [    INFO][0m - global step 14 / 336750, loss: 0.398029, avg_reader_cost: 0.00029 sec, avg_batch_cost: 3.47613 sec, avg_samples: 1.00000, ips: 0.28768 sequences/sec,  [0m
[32m[2023-12-21 13:15:25,747] [    INFO][0m - global step 15 / 336750, loss: 1.242640, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.75426 sec, avg_samples: 1.00000, ips: 0.26636 sequences/sec,  [0m
[32m[2023-12-21 13:15:29,480] [    INFO][0m - global step 16 / 336750, loss: 0.515400, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.73292 sec, avg_samples: 1.00000, ips: 0.26789 sequences/sec,  [0m
[32m[2023-12-21 13:15:33,118] [    INFO][0m - global step 17 / 336750, loss: 0.335483, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.63738 sec, avg_samples: 1.00000, ips: 0.27492 sequences/sec,  [0m
[32m[2023-12-21 13:15:36,733] [    INFO][0m - global step 18 / 336750, loss: 0.281049, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.61413 sec, avg_samples: 1.00000, ips: 0.27669 sequences/sec,  [0m
[32m[2023-12-21 13:15:40,441] [    INFO][0m - global step 19 / 336750, loss: 1.404827, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.70774 sec, avg_samples: 1.00000, ips: 0.26971 sequences/sec,  [0m
[32m[2023-12-21 13:15:43,962] [    INFO][0m - global step 20 / 336750, loss: 0.246064, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.52059 sec, avg_samples: 1.00000, ips: 0.28404 sequences/sec,  [0m
[32m[2023-12-21 13:15:47,549] [    INFO][0m - global step 21 / 336750, loss: 0.118135, avg_reader_cost: 0.00025 sec, avg_batch_cost: 3.58668 sec, avg_samples: 1.00000, ips: 0.27881 sequences/sec,  [0m
[32m[2023-12-21 13:14:32,604] [    INFO][0m - global step 1 / 336750, loss: 1.069450, avg_reader_cost: 0.01889 sec, avg_batch_cost: 4.40287 sec, avg_samples: 1.00000, ips: 0.22712 sequences/sec,  [0m
[32m[2023-12-21 13:14:36,573] [    INFO][0m - global step 2 / 336750, loss: 0.593711, avg_reader_cost: 0.00027 sec, avg_batch_cost: 3.96776 sec, avg_samples: 1.00000, ips: 0.25203 sequences/sec,  [0m
[32m[2023-12-21 13:14:40,660] [    INFO][0m - global step 3 / 336750, loss: 0.342273, avg_reader_cost: 0.00027 sec, avg_batch_cost: 4.08694 sec, avg_samples: 1.00000, ips: 0.24468 sequences/sec,  [0m
[32m[2023-12-21 13:14:44,819] [    INFO][0m - global step 4 / 336750, loss: 0.409784, avg_reader_cost: 0.00017 sec, avg_batch_cost: 4.15790 sec, avg_samples: 1.00000, ips: 0.24051 sequences/sec,  [0m
[32m[2023-12-21 13:14:48,561] [    INFO][0m - global step 5 / 336750, loss: 0.294520, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.74187 sec, avg_samples: 1.00000, ips: 0.26725 sequences/sec,  [0m
[32m[2023-12-21 13:14:52,384] [    INFO][0m - global step 6 / 336750, loss: 1.002215, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.82200 sec, avg_samples: 1.00000, ips: 0.26164 sequences/sec,  [0m
[32m[2023-12-21 13:14:56,074] [    INFO][0m - global step 7 / 336750, loss: 1.096548, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.68973 sec, avg_samples: 1.00000, ips: 0.27102 sequences/sec,  [0m
[32m[2023-12-21 13:14:59,788] [    INFO][0m - global step 8 / 336750, loss: 0.640906, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.71396 sec, avg_samples: 1.00000, ips: 0.26925 sequences/sec,  [0m
[32m[2023-12-21 13:15:03,544] [    INFO][0m - global step 9 / 336750, loss: 0.476790, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.75503 sec, avg_samples: 1.00000, ips: 0.26631 sequences/sec,  [0m
[32m[2023-12-21 13:15:07,355] [    INFO][0m - global step 10 / 336750, loss: 0.738077, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.81067 sec, avg_samples: 1.00000, ips: 0.26242 sequences/sec,  [0m
[32m[2023-12-21 13:15:10,937] [    INFO][0m - global step 11 / 336750, loss: 1.218149, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.58185 sec, avg_samples: 1.00000, ips: 0.27919 sequences/sec,  [0m
[32m[2023-12-21 13:15:14,988] [    INFO][0m - global step 12 / 336750, loss: 0.357612, avg_reader_cost: 0.00022 sec, avg_batch_cost: 4.05038 sec, avg_samples: 1.00000, ips: 0.24689 sequences/sec,  [0m
[32m[2023-12-21 13:15:18,516] [    INFO][0m - global step 13 / 336750, loss: 0.575731, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.52728 sec, avg_samples: 1.00000, ips: 0.28350 sequences/sec,  [0m
[32m[2023-12-21 13:15:21,993] [    INFO][0m - global step 14 / 336750, loss: 0.398029, avg_reader_cost: 0.00029 sec, avg_batch_cost: 3.47613 sec, avg_samples: 1.00000, ips: 0.28768 sequences/sec,  [0m
[32m[2023-12-21 13:15:25,747] [    INFO][0m - global step 15 / 336750, loss: 1.242640, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.75426 sec, avg_samples: 1.00000, ips: 0.26636 sequences/sec,  [0m
[32m[2023-12-21 13:15:29,480] [    INFO][0m - global step 16 / 336750, loss: 0.515400, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.73292 sec, avg_samples: 1.00000, ips: 0.26789 sequences/sec,  [0m
[32m[2023-12-21 13:15:33,118] [    INFO][0m - global step 17 / 336750, loss: 0.335483, avg_reader_cost: 0.00026 sec, avg_batch_cost: 3.63738 sec, avg_samples: 1.00000, ips: 0.27492 sequences/sec,  [0m
[32m[2023-12-21 13:15:36,733] [    INFO][0m - global step 18 / 336750, loss: 0.281049, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.61413 sec, avg_samples: 1.00000, ips: 0.27669 sequences/sec,  [0m
[32m[2023-12-21 13:15:40,441] [    INFO][0m - global step 19 / 336750, loss: 1.404827, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.70774 sec, avg_samples: 1.00000, ips: 0.26971 sequences/sec,  [0m
[32m[2023-12-21 13:15:43,962] [    INFO][0m - global step 20 / 336750, loss: 0.246064, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.52059 sec, avg_samples: 1.00000, ips: 0.28404 sequences/sec,  [0m
[32m[2023-12-21 13:15:47,549] [    INFO][0m - global step 21 / 336750, loss: 0.118135, avg_reader_cost: 0.00025 sec, avg_batch_cost: 3.58668 sec, avg_samples: 1.00000, ips: 0.27881 sequences/sec,  [0m
[32m[2023-12-21 13:15:51,151] [    INFO][0m - global step 22 / 336750, loss: 0.177526, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.60121 sec, avg_samples: 1.00000, ips: 0.27768 sequences/sec,  [0m
[32m[2023-12-21 13:15:54,854] [    INFO][0m - global step 23 / 336750, loss: 1.407634, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.70261 sec, avg_samples: 1.00000, ips: 0.27008 sequences/sec,  [0m
[32m[2023-12-21 13:15:58,415] [    INFO][0m - global step 24 / 336750, loss: 1.361824, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.56106 sec, avg_samples: 1.00000, ips: 0.28081 sequences/sec,  [0m
[32m[2023-12-21 13:16:02,094] [    INFO][0m - global step 25 / 336750, loss: 0.087778, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.67838 sec, avg_samples: 1.00000, ips: 0.27186 sequences/sec,  [0m
[32m[2023-12-21 13:16:05,680] [    INFO][0m - global step 26 / 336750, loss: 0.131258, avg_reader_cost: 0.00022 sec, avg_batch_cost: 3.58527 sec, avg_samples: 1.00000, ips: 0.27892 sequences/sec,  [0m
[32m[2023-12-21 13:16:09,443] [    INFO][0m - global step 27 / 336750, loss: 0.132699, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.76326 sec, avg_samples: 1.00000, ips: 0.26573 sequences/sec,  [0m
[32m[2023-12-21 13:16:13,051] [    INFO][0m - global step 28 / 336750, loss: 1.756169, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.60710 sec, avg_samples: 1.00000, ips: 0.27723 sequences/sec,  [0m
[32m[2023-12-21 13:16:16,589] [    INFO][0m - global step 29 / 336750, loss: 0.060998, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.53810 sec, avg_samples: 1.00000, ips: 0.28264 sequences/sec,  [0m
[32m[2023-12-21 13:16:20,088] [    INFO][0m - global step 30 / 336750, loss: 3.599509, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.49811 sec, avg_samples: 1.00000, ips: 0.28587 sequences/sec,  [0m
[32m[2023-12-21 13:16:23,850] [    INFO][0m - global step 31 / 336750, loss: 0.058486, avg_reader_cost: 0.00031 sec, avg_batch_cost: 3.76204 sec, avg_samples: 1.00000, ips: 0.26581 sequences/sec,  [0m
[32m[2023-12-21 13:16:27,365] [    INFO][0m - global step 32 / 336750, loss: 4.470854, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.51392 sec, avg_samples: 1.00000, ips: 0.28458 sequences/sec,  [0m
[32m[2023-12-21 13:16:31,014] [    INFO][0m - global step 33 / 336750, loss: 0.029995, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.64904 sec, avg_samples: 1.00000, ips: 0.27404 sequences/sec,  [0m
[32m[2023-12-21 13:16:34,536] [    INFO][0m - global step 34 / 336750, loss: 2.737659, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.52166 sec, avg_samples: 1.00000, ips: 0.28396 sequences/sec,  [0m
[32m[2023-12-21 13:16:38,070] [    INFO][0m - global step 35 / 336750, loss: 2.170859, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.53407 sec, avg_samples: 1.00000, ips: 0.28296 sequences/sec,  [0m
[32m[2023-12-21 13:16:41,695] [    INFO][0m - global step 36 / 336750, loss: 0.136145, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.62385 sec, avg_samples: 1.00000, ips: 0.27595 sequences/sec,  [0m
[32m[2023-12-21 13:16:45,239] [    INFO][0m - global step 37 / 336750, loss: 0.142717, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.54431 sec, avg_samples: 1.00000, ips: 0.28214 sequences/sec,  [0m
[32m[2023-12-21 13:16:48,759] [    INFO][0m - global step 38 / 336750, loss: 3.870332, avg_reader_cost: 0.00022 sec, avg_batch_cost: 3.51926 sec, avg_samples: 1.00000, ips: 0.28415 sequences/sec,  [0m
[32m[2023-12-21 13:16:52,373] [    INFO][0m - global step 39 / 336750, loss: 0.032957, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.61313 sec, avg_samples: 1.00000, ips: 0.27677 sequences/sec,  [0m
[32m[2023-12-21 13:16:56,089] [    INFO][0m - global step 40 / 336750, loss: 0.013545, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.71631 sec, avg_samples: 1.00000, ips: 0.26908 sequences/sec,  [0m
[32m[2023-12-21 13:15:51,151] [    INFO][0m - global step 22 / 336750, loss: 0.177526, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.60121 sec, avg_samples: 1.00000, ips: 0.27768 sequences/sec,  [0m
[32m[2023-12-21 13:15:54,854] [    INFO][0m - global step 23 / 336750, loss: 1.407634, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.70261 sec, avg_samples: 1.00000, ips: 0.27008 sequences/sec,  [0m
[32m[2023-12-21 13:15:58,415] [    INFO][0m - global step 24 / 336750, loss: 1.361824, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.56106 sec, avg_samples: 1.00000, ips: 0.28081 sequences/sec,  [0m
[32m[2023-12-21 13:16:02,094] [    INFO][0m - global step 25 / 336750, loss: 0.087778, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.67838 sec, avg_samples: 1.00000, ips: 0.27186 sequences/sec,  [0m
[32m[2023-12-21 13:16:05,680] [    INFO][0m - global step 26 / 336750, loss: 0.131258, avg_reader_cost: 0.00022 sec, avg_batch_cost: 3.58527 sec, avg_samples: 1.00000, ips: 0.27892 sequences/sec,  [0m
[32m[2023-12-21 13:16:09,443] [    INFO][0m - global step 27 / 336750, loss: 0.132699, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.76326 sec, avg_samples: 1.00000, ips: 0.26573 sequences/sec,  [0m
[32m[2023-12-21 13:16:13,051] [    INFO][0m - global step 28 / 336750, loss: 1.756169, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.60710 sec, avg_samples: 1.00000, ips: 0.27723 sequences/sec,  [0m
[32m[2023-12-21 13:16:16,589] [    INFO][0m - global step 29 / 336750, loss: 0.060998, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.53810 sec, avg_samples: 1.00000, ips: 0.28264 sequences/sec,  [0m
[32m[2023-12-21 13:16:20,088] [    INFO][0m - global step 30 / 336750, loss: 3.599509, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.49811 sec, avg_samples: 1.00000, ips: 0.28587 sequences/sec,  [0m
[32m[2023-12-21 13:16:23,850] [    INFO][0m - global step 31 / 336750, loss: 0.058486, avg_reader_cost: 0.00031 sec, avg_batch_cost: 3.76204 sec, avg_samples: 1.00000, ips: 0.26581 sequences/sec,  [0m
[32m[2023-12-21 13:16:27,365] [    INFO][0m - global step 32 / 336750, loss: 4.470854, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.51392 sec, avg_samples: 1.00000, ips: 0.28458 sequences/sec,  [0m
[32m[2023-12-21 13:16:31,014] [    INFO][0m - global step 33 / 336750, loss: 0.029995, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.64904 sec, avg_samples: 1.00000, ips: 0.27404 sequences/sec,  [0m
[32m[2023-12-21 13:16:34,536] [    INFO][0m - global step 34 / 336750, loss: 2.737659, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.52166 sec, avg_samples: 1.00000, ips: 0.28396 sequences/sec,  [0m
[32m[2023-12-21 13:16:38,070] [    INFO][0m - global step 35 / 336750, loss: 2.170859, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.53407 sec, avg_samples: 1.00000, ips: 0.28296 sequences/sec,  [0m
[32m[2023-12-21 13:16:41,695] [    INFO][0m - global step 36 / 336750, loss: 0.136145, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.62385 sec, avg_samples: 1.00000, ips: 0.27595 sequences/sec,  [0m
[32m[2023-12-21 13:16:45,239] [    INFO][0m - global step 37 / 336750, loss: 0.142717, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.54431 sec, avg_samples: 1.00000, ips: 0.28214 sequences/sec,  [0m
[32m[2023-12-21 13:16:48,759] [    INFO][0m - global step 38 / 336750, loss: 3.870332, avg_reader_cost: 0.00022 sec, avg_batch_cost: 3.51926 sec, avg_samples: 1.00000, ips: 0.28415 sequences/sec,  [0m
[32m[2023-12-21 13:16:52,373] [    INFO][0m - global step 39 / 336750, loss: 0.032957, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.61313 sec, avg_samples: 1.00000, ips: 0.27677 sequences/sec,  [0m
[32m[2023-12-21 13:16:56,089] [    INFO][0m - global step 40 / 336750, loss: 0.013545, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.71631 sec, avg_samples: 1.00000, ips: 0.26908 sequences/sec,  [0m
[32m[2023-12-21 13:16:59,584] [    INFO][0m - global step 41 / 336750, loss: 3.016236, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.49466 sec, avg_samples: 1.00000, ips: 0.28615 sequences/sec,  [0m
[32m[2023-12-21 13:17:03,046] [    INFO][0m - global step 42 / 336750, loss: 2.582963, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.46152 sec, avg_samples: 1.00000, ips: 0.28889 sequences/sec,  [0m
[32m[2023-12-21 13:17:06,554] [    INFO][0m - global step 43 / 336750, loss: 3.128652, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.50802 sec, avg_samples: 1.00000, ips: 0.28506 sequences/sec,  [0m
[32m[2023-12-21 13:17:08,373] [    INFO][0m - global step 120 / 2668000, loss: 2.269362, avg_reader_cost: 0.00036 sec, avg_batch_cost: 35.53819 sec, avg_samples: 512.00000, ips: 14.40704 words/sec,  [0m
[32m[2023-12-21 13:17:10,277] [    INFO][0m - global step 44 / 336750, loss: 0.049134, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.72180 sec, avg_samples: 1.00000, ips: 0.26869 sequences/sec,  [0m
[32m[2023-12-21 13:17:13,921] [    INFO][0m - global step 45 / 336750, loss: 0.053676, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.64378 sec, avg_samples: 1.00000, ips: 0.27444 sequences/sec,  [0m
[32m[2023-12-21 13:17:17,482] [    INFO][0m - global step 46 / 336750, loss: 0.101370, avg_reader_cost: 0.00021 sec, avg_batch_cost: 3.56070 sec, avg_samples: 1.00000, ips: 0.28084 sequences/sec,  [0m
[32m[2023-12-21 13:17:21,157] [    INFO][0m - global step 47 / 336750, loss: 0.131301, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.67441 sec, avg_samples: 1.00000, ips: 0.27215 sequences/sec,  [0m
[32m[2023-12-21 13:17:24,938] [    INFO][0m - global step 48 / 336750, loss: 0.042522, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.78074 sec, avg_samples: 1.00000, ips: 0.26450 sequences/sec,  [0m
[32m[2023-12-21 13:17:28,430] [    INFO][0m - global step 49 / 336750, loss: 0.057502, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.49134 sec, avg_samples: 1.00000, ips: 0.28642 sequences/sec,  [0m
[32m[2023-12-21 13:17:31,911] [    INFO][0m - global step 50 / 336750, loss: 3.040954, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.48134 sec, avg_samples: 1.00000, ips: 0.28725 sequences/sec,  [0m
[32m[2023-12-21 13:17:35,397] [    INFO][0m - global step 51 / 336750, loss: 2.504508, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.48583 sec, avg_samples: 1.00000, ips: 0.28688 sequences/sec,  [0m
[32m[2023-12-21 13:17:38,966] [    INFO][0m - global step 52 / 336750, loss: 1.264978, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.56843 sec, avg_samples: 1.00000, ips: 0.28024 sequences/sec,  [0m
[32m[2023-12-21 13:17:42,589] [    INFO][0m - global step 53 / 336750, loss: 0.118258, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.62223 sec, avg_samples: 1.00000, ips: 0.27607 sequences/sec,  [0m
[32m[2023-12-21 13:17:46,014] [    INFO][0m - global step 54 / 336750, loss: 2.222987, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.42535 sec, avg_samples: 1.00000, ips: 0.29194 sequences/sec,  [0m
[32m[2023-12-21 13:17:49,697] [    INFO][0m - global step 55 / 336750, loss: 0.081615, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.68236 sec, avg_samples: 1.00000, ips: 0.27157 sequences/sec,  [0m
[32m[2023-12-21 13:17:53,285] [    INFO][0m - global step 56 / 336750, loss: 0.038737, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.58700 sec, avg_samples: 1.00000, ips: 0.27878 sequences/sec,  [0m
[32m[2023-12-21 13:17:56,903] [    INFO][0m - global step 57 / 336750, loss: 0.058624, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.61817 sec, avg_samples: 1.00000, ips: 0.27638 sequences/sec,  [0m
[32m[2023-12-21 13:18:00,371] [    INFO][0m - global step 58 / 336750, loss: 3.813503, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.46789 sec, avg_samples: 1.00000, ips: 0.28836 sequences/sec,  [0m
[32m[2023-12-21 13:18:03,968] [    INFO][0m - global step 59 / 336750, loss: 0.053927, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.59598 sec, avg_samples: 1.00000, ips: 0.27809 sequences/sec,  [0m
[32m[2023-12-21 13:18:07,728] [    INFO][0m - global step 60 / 336750, loss: 0.042642, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.76026 sec, avg_samples: 1.00000, ips: 0.26594 sequences/sec,  [0m
[32m[2023-12-21 13:16:59,584] [    INFO][0m - global step 41 / 336750, loss: 3.016236, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.49466 sec, avg_samples: 1.00000, ips: 0.28615 sequences/sec,  [0m
[32m[2023-12-21 13:17:03,046] [    INFO][0m - global step 42 / 336750, loss: 2.582963, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.46152 sec, avg_samples: 1.00000, ips: 0.28889 sequences/sec,  [0m
[32m[2023-12-21 13:17:06,554] [    INFO][0m - global step 43 / 336750, loss: 3.128652, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.50802 sec, avg_samples: 1.00000, ips: 0.28506 sequences/sec,  [0m
[32m[2023-12-21 13:17:08,373] [    INFO][0m - global step 120 / 2668000, loss: 2.269362, avg_reader_cost: 0.00036 sec, avg_batch_cost: 35.53819 sec, avg_samples: 512.00000, ips: 14.40704 words/sec,  [0m
[32m[2023-12-21 13:17:10,277] [    INFO][0m - global step 44 / 336750, loss: 0.049134, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.72180 sec, avg_samples: 1.00000, ips: 0.26869 sequences/sec,  [0m
[32m[2023-12-21 13:17:13,921] [    INFO][0m - global step 45 / 336750, loss: 0.053676, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.64378 sec, avg_samples: 1.00000, ips: 0.27444 sequences/sec,  [0m
[32m[2023-12-21 13:17:17,482] [    INFO][0m - global step 46 / 336750, loss: 0.101370, avg_reader_cost: 0.00021 sec, avg_batch_cost: 3.56070 sec, avg_samples: 1.00000, ips: 0.28084 sequences/sec,  [0m
[32m[2023-12-21 13:17:21,157] [    INFO][0m - global step 47 / 336750, loss: 0.131301, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.67441 sec, avg_samples: 1.00000, ips: 0.27215 sequences/sec,  [0m
[32m[2023-12-21 13:17:24,938] [    INFO][0m - global step 48 / 336750, loss: 0.042522, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.78074 sec, avg_samples: 1.00000, ips: 0.26450 sequences/sec,  [0m
[32m[2023-12-21 13:17:28,430] [    INFO][0m - global step 49 / 336750, loss: 0.057502, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.49134 sec, avg_samples: 1.00000, ips: 0.28642 sequences/sec,  [0m
[32m[2023-12-21 13:17:31,911] [    INFO][0m - global step 50 / 336750, loss: 3.040954, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.48134 sec, avg_samples: 1.00000, ips: 0.28725 sequences/sec,  [0m
[32m[2023-12-21 13:17:35,397] [    INFO][0m - global step 51 / 336750, loss: 2.504508, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.48583 sec, avg_samples: 1.00000, ips: 0.28688 sequences/sec,  [0m
[32m[2023-12-21 13:17:38,966] [    INFO][0m - global step 52 / 336750, loss: 1.264978, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.56843 sec, avg_samples: 1.00000, ips: 0.28024 sequences/sec,  [0m
[32m[2023-12-21 13:17:42,589] [    INFO][0m - global step 53 / 336750, loss: 0.118258, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.62223 sec, avg_samples: 1.00000, ips: 0.27607 sequences/sec,  [0m
[32m[2023-12-21 13:17:46,014] [    INFO][0m - global step 54 / 336750, loss: 2.222987, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.42535 sec, avg_samples: 1.00000, ips: 0.29194 sequences/sec,  [0m
[32m[2023-12-21 13:17:49,697] [    INFO][0m - global step 55 / 336750, loss: 0.081615, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.68236 sec, avg_samples: 1.00000, ips: 0.27157 sequences/sec,  [0m
[32m[2023-12-21 13:17:53,285] [    INFO][0m - global step 56 / 336750, loss: 0.038737, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.58700 sec, avg_samples: 1.00000, ips: 0.27878 sequences/sec,  [0m
[32m[2023-12-21 13:17:56,903] [    INFO][0m - global step 57 / 336750, loss: 0.058624, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.61817 sec, avg_samples: 1.00000, ips: 0.27638 sequences/sec,  [0m
[32m[2023-12-21 13:18:00,371] [    INFO][0m - global step 58 / 336750, loss: 3.813503, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.46789 sec, avg_samples: 1.00000, ips: 0.28836 sequences/sec,  [0m
[32m[2023-12-21 13:18:03,968] [    INFO][0m - global step 59 / 336750, loss: 0.053927, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.59598 sec, avg_samples: 1.00000, ips: 0.27809 sequences/sec,  [0m
[32m[2023-12-21 13:18:07,728] [    INFO][0m - global step 60 / 336750, loss: 0.042642, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.76026 sec, avg_samples: 1.00000, ips: 0.26594 sequences/sec,  [0m
[32m[2023-12-21 13:18:11,565] [    INFO][0m - global step 61 / 336750, loss: 0.039112, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.83588 sec, avg_samples: 1.00000, ips: 0.26070 sequences/sec,  [0m
[32m[2023-12-21 13:18:15,176] [    INFO][0m - global step 62 / 336750, loss: 2.682165, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.61090 sec, avg_samples: 1.00000, ips: 0.27694 sequences/sec,  [0m
[32m[2023-12-21 13:18:18,730] [    INFO][0m - global step 63 / 336750, loss: 2.887510, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.55417 sec, avg_samples: 1.00000, ips: 0.28136 sequences/sec,  [0m
[32m[2023-12-21 13:18:22,651] [    INFO][0m - global step 64 / 336750, loss: 0.033731, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.91988 sec, avg_samples: 1.00000, ips: 0.25511 sequences/sec,  [0m
[32m[2023-12-21 13:18:26,515] [    INFO][0m - global step 65 / 336750, loss: 0.004205, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.86358 sec, avg_samples: 1.00000, ips: 0.25883 sequences/sec,  [0m
[32m[2023-12-21 13:18:30,091] [    INFO][0m - global step 66 / 336750, loss: 0.028810, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.57552 sec, avg_samples: 1.00000, ips: 0.27968 sequences/sec,  [0m
[32m[2023-12-21 13:18:33,488] [    INFO][0m - global step 67 / 336750, loss: 3.624273, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.39730 sec, avg_samples: 1.00000, ips: 0.29435 sequences/sec,  [0m
[32m[2023-12-21 13:18:37,083] [    INFO][0m - global step 68 / 336750, loss: 0.015324, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.59486 sec, avg_samples: 1.00000, ips: 0.27818 sequences/sec,  [0m
[32m[2023-12-21 13:18:40,560] [    INFO][0m - global step 69 / 336750, loss: 3.524530, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.47583 sec, avg_samples: 1.00000, ips: 0.28770 sequences/sec,  [0m
[32m[2023-12-21 13:18:44,268] [    INFO][0m - global step 70 / 336750, loss: 0.066355, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.70780 sec, avg_samples: 1.00000, ips: 0.26970 sequences/sec,  [0m
[32m[2023-12-21 13:18:47,778] [    INFO][0m - global step 71 / 336750, loss: 3.754437, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.50948 sec, avg_samples: 1.00000, ips: 0.28494 sequences/sec,  [0m
[32m[2023-12-21 13:18:51,360] [    INFO][0m - global step 72 / 336750, loss: 0.185898, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.58227 sec, avg_samples: 1.00000, ips: 0.27915 sequences/sec,  [0m
[32m[2023-12-21 13:18:54,849] [    INFO][0m - global step 73 / 336750, loss: 2.857381, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.48877 sec, avg_samples: 1.00000, ips: 0.28663 sequences/sec,  [0m
[32m[2023-12-21 13:18:58,584] [    INFO][0m - global step 74 / 336750, loss: 0.130091, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.73405 sec, avg_samples: 1.00000, ips: 0.26781 sequences/sec,  [0m
[32m[2023-12-21 13:19:02,223] [    INFO][0m - global step 75 / 336750, loss: 2.349725, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.63947 sec, avg_samples: 1.00000, ips: 0.27477 sequences/sec,  [0m
[32m[2023-12-21 13:19:06,379] [    INFO][0m - global step 76 / 336750, loss: 0.007230, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.15517 sec, avg_samples: 1.00000, ips: 0.24066 sequences/sec,  [0m
[32m[2023-12-21 13:19:09,878] [    INFO][0m - global step 77 / 336750, loss: 2.450840, avg_reader_cost: 0.00021 sec, avg_batch_cost: 3.49889 sec, avg_samples: 1.00000, ips: 0.28580 sequences/sec,  [0m
[32m[2023-12-21 13:19:13,487] [    INFO][0m - global step 78 / 336750, loss: 0.063866, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.60810 sec, avg_samples: 1.00000, ips: 0.27715 sequences/sec,  [0m
[32m[2023-12-21 13:19:17,043] [    INFO][0m - global step 79 / 336750, loss: 2.595309, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.55585 sec, avg_samples: 1.00000, ips: 0.28123 sequences/sec,  [0m
[32m[2023-12-21 13:18:11,565] [    INFO][0m - global step 61 / 336750, loss: 0.039112, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.83588 sec, avg_samples: 1.00000, ips: 0.26070 sequences/sec,  [0m
[32m[2023-12-21 13:18:15,176] [    INFO][0m - global step 62 / 336750, loss: 2.682165, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.61090 sec, avg_samples: 1.00000, ips: 0.27694 sequences/sec,  [0m
[32m[2023-12-21 13:18:18,730] [    INFO][0m - global step 63 / 336750, loss: 2.887510, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.55417 sec, avg_samples: 1.00000, ips: 0.28136 sequences/sec,  [0m
[32m[2023-12-21 13:18:22,651] [    INFO][0m - global step 64 / 336750, loss: 0.033731, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.91988 sec, avg_samples: 1.00000, ips: 0.25511 sequences/sec,  [0m
[32m[2023-12-21 13:18:26,515] [    INFO][0m - global step 65 / 336750, loss: 0.004205, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.86358 sec, avg_samples: 1.00000, ips: 0.25883 sequences/sec,  [0m
[32m[2023-12-21 13:18:30,091] [    INFO][0m - global step 66 / 336750, loss: 0.028810, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.57552 sec, avg_samples: 1.00000, ips: 0.27968 sequences/sec,  [0m
[32m[2023-12-21 13:18:33,488] [    INFO][0m - global step 67 / 336750, loss: 3.624273, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.39730 sec, avg_samples: 1.00000, ips: 0.29435 sequences/sec,  [0m
[32m[2023-12-21 13:18:37,083] [    INFO][0m - global step 68 / 336750, loss: 0.015324, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.59486 sec, avg_samples: 1.00000, ips: 0.27818 sequences/sec,  [0m
[32m[2023-12-21 13:18:40,560] [    INFO][0m - global step 69 / 336750, loss: 3.524530, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.47583 sec, avg_samples: 1.00000, ips: 0.28770 sequences/sec,  [0m
[32m[2023-12-21 13:18:44,268] [    INFO][0m - global step 70 / 336750, loss: 0.066355, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.70780 sec, avg_samples: 1.00000, ips: 0.26970 sequences/sec,  [0m
[32m[2023-12-21 13:18:47,778] [    INFO][0m - global step 71 / 336750, loss: 3.754437, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.50948 sec, avg_samples: 1.00000, ips: 0.28494 sequences/sec,  [0m
[32m[2023-12-21 13:18:51,360] [    INFO][0m - global step 72 / 336750, loss: 0.185898, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.58227 sec, avg_samples: 1.00000, ips: 0.27915 sequences/sec,  [0m
[32m[2023-12-21 13:18:54,849] [    INFO][0m - global step 73 / 336750, loss: 2.857381, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.48877 sec, avg_samples: 1.00000, ips: 0.28663 sequences/sec,  [0m
[32m[2023-12-21 13:18:58,584] [    INFO][0m - global step 74 / 336750, loss: 0.130091, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.73405 sec, avg_samples: 1.00000, ips: 0.26781 sequences/sec,  [0m
[32m[2023-12-21 13:19:02,223] [    INFO][0m - global step 75 / 336750, loss: 2.349725, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.63947 sec, avg_samples: 1.00000, ips: 0.27477 sequences/sec,  [0m
[32m[2023-12-21 13:19:06,379] [    INFO][0m - global step 76 / 336750, loss: 0.007230, avg_reader_cost: 0.00016 sec, avg_batch_cost: 4.15517 sec, avg_samples: 1.00000, ips: 0.24066 sequences/sec,  [0m
[32m[2023-12-21 13:19:09,878] [    INFO][0m - global step 77 / 336750, loss: 2.450840, avg_reader_cost: 0.00021 sec, avg_batch_cost: 3.49889 sec, avg_samples: 1.00000, ips: 0.28580 sequences/sec,  [0m
[32m[2023-12-21 13:19:13,487] [    INFO][0m - global step 78 / 336750, loss: 0.063866, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.60810 sec, avg_samples: 1.00000, ips: 0.27715 sequences/sec,  [0m
[32m[2023-12-21 13:19:17,043] [    INFO][0m - global step 79 / 336750, loss: 2.595309, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.55585 sec, avg_samples: 1.00000, ips: 0.28123 sequences/sec,  [0m
[32m[2023-12-21 13:19:20,980] [    INFO][0m - global step 80 / 336750, loss: 0.049805, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.93642 sec, avg_samples: 1.00000, ips: 0.25404 sequences/sec,  [0m
[32m[2023-12-21 13:19:24,525] [    INFO][0m - global step 81 / 336750, loss: 2.470005, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.54489 sec, avg_samples: 1.00000, ips: 0.28210 sequences/sec,  [0m
[32m[2023-12-21 13:19:28,340] [    INFO][0m - global step 82 / 336750, loss: 0.135904, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.81456 sec, avg_samples: 1.00000, ips: 0.26215 sequences/sec,  [0m
[32m[2023-12-21 13:19:31,914] [    INFO][0m - global step 83 / 336750, loss: 2.635353, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.57386 sec, avg_samples: 1.00000, ips: 0.27981 sequences/sec,  [0m
[32m[2023-12-21 13:19:35,382] [    INFO][0m - global step 84 / 336750, loss: 0.094124, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.46730 sec, avg_samples: 1.00000, ips: 0.28841 sequences/sec,  [0m
[32m[2023-12-21 13:19:39,000] [    INFO][0m - global step 85 / 336750, loss: 2.209023, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.61787 sec, avg_samples: 1.00000, ips: 0.27641 sequences/sec,  [0m
[32m[2023-12-21 13:19:42,724] [    INFO][0m - global step 86 / 336750, loss: 0.092366, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.72412 sec, avg_samples: 1.00000, ips: 0.26852 sequences/sec,  [0m
[32m[2023-12-21 13:19:46,724] [    INFO][0m - global step 87 / 336750, loss: 0.010033, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.99884 sec, avg_samples: 1.00000, ips: 0.25007 sequences/sec,  [0m
[32m[2023-12-21 13:19:50,265] [    INFO][0m - global step 88 / 336750, loss: 5.098188, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.54111 sec, avg_samples: 1.00000, ips: 0.28240 sequences/sec,  [0m
[32m[2023-12-21 13:19:53,786] [    INFO][0m - global step 89 / 336750, loss: 2.422416, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.52098 sec, avg_samples: 1.00000, ips: 0.28401 sequences/sec,  [0m
[32m[2023-12-21 13:19:57,546] [    INFO][0m - global step 90 / 336750, loss: 1.884473, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.75901 sec, avg_samples: 1.00000, ips: 0.26603 sequences/sec,  [0m
[32m[2023-12-21 13:20:01,718] [    INFO][0m - global step 91 / 336750, loss: 0.004992, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.17162 sec, avg_samples: 1.00000, ips: 0.23972 sequences/sec,  [0m
[32m[2023-12-21 13:20:05,706] [    INFO][0m - global step 92 / 336750, loss: 0.012987, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.98783 sec, avg_samples: 1.00000, ips: 0.25076 sequences/sec,  [0m
[32m[2023-12-21 13:20:09,437] [    INFO][0m - global step 93 / 336750, loss: 0.006328, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.73117 sec, avg_samples: 1.00000, ips: 0.26801 sequences/sec,  [0m
[32m[2023-12-21 13:20:13,034] [    INFO][0m - global step 94 / 336750, loss: 1.921977, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.59632 sec, avg_samples: 1.00000, ips: 0.27806 sequences/sec,  [0m
[32m[2023-12-21 13:20:16,739] [    INFO][0m - global step 95 / 336750, loss: 0.065925, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.70431 sec, avg_samples: 1.00000, ips: 0.26996 sequences/sec,  [0m
[32m[2023-12-21 13:20:20,435] [    INFO][0m - global step 96 / 336750, loss: 0.126908, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.69610 sec, avg_samples: 1.00000, ips: 0.27056 sequences/sec,  [0m
[32m[2023-12-21 13:20:24,030] [    INFO][0m - global step 97 / 336750, loss: 4.541228, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.59471 sec, avg_samples: 1.00000, ips: 0.27819 sequences/sec,  [0m
[32m[2023-12-21 13:20:27,833] [    INFO][0m - global step 98 / 336750, loss: 0.164743, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.80238 sec, avg_samples: 1.00000, ips: 0.26299 sequences/sec,  [0m
[32m[2023-12-21 13:20:31,521] [    INFO][0m - global step 99 / 336750, loss: 2.275252, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.68802 sec, avg_samples: 1.00000, ips: 0.27115 sequences/sec,  [0m
[32m[2023-12-21 13:20:35,021] [    INFO][0m - global step 100 / 336750, loss: 1.538025, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.49895 sec, avg_samples: 1.00000, ips: 0.28580 sequences/sec,  [0m
[32m[2023-12-21 13:19:20,980] [    INFO][0m - global step 80 / 336750, loss: 0.049805, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.93642 sec, avg_samples: 1.00000, ips: 0.25404 sequences/sec,  [0m
[32m[2023-12-21 13:19:24,525] [    INFO][0m - global step 81 / 336750, loss: 2.470005, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.54489 sec, avg_samples: 1.00000, ips: 0.28210 sequences/sec,  [0m
[32m[2023-12-21 13:19:28,340] [    INFO][0m - global step 82 / 336750, loss: 0.135904, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.81456 sec, avg_samples: 1.00000, ips: 0.26215 sequences/sec,  [0m
[32m[2023-12-21 13:19:31,914] [    INFO][0m - global step 83 / 336750, loss: 2.635353, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.57386 sec, avg_samples: 1.00000, ips: 0.27981 sequences/sec,  [0m
[32m[2023-12-21 13:19:35,382] [    INFO][0m - global step 84 / 336750, loss: 0.094124, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.46730 sec, avg_samples: 1.00000, ips: 0.28841 sequences/sec,  [0m
[32m[2023-12-21 13:19:39,000] [    INFO][0m - global step 85 / 336750, loss: 2.209023, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.61787 sec, avg_samples: 1.00000, ips: 0.27641 sequences/sec,  [0m
[32m[2023-12-21 13:19:42,724] [    INFO][0m - global step 86 / 336750, loss: 0.092366, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.72412 sec, avg_samples: 1.00000, ips: 0.26852 sequences/sec,  [0m
[32m[2023-12-21 13:19:46,724] [    INFO][0m - global step 87 / 336750, loss: 0.010033, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.99884 sec, avg_samples: 1.00000, ips: 0.25007 sequences/sec,  [0m
[32m[2023-12-21 13:19:50,265] [    INFO][0m - global step 88 / 336750, loss: 5.098188, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.54111 sec, avg_samples: 1.00000, ips: 0.28240 sequences/sec,  [0m
[32m[2023-12-21 13:19:53,786] [    INFO][0m - global step 89 / 336750, loss: 2.422416, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.52098 sec, avg_samples: 1.00000, ips: 0.28401 sequences/sec,  [0m
[32m[2023-12-21 13:19:57,546] [    INFO][0m - global step 90 / 336750, loss: 1.884473, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.75901 sec, avg_samples: 1.00000, ips: 0.26603 sequences/sec,  [0m
[32m[2023-12-21 13:20:01,718] [    INFO][0m - global step 91 / 336750, loss: 0.004992, avg_reader_cost: 0.00014 sec, avg_batch_cost: 4.17162 sec, avg_samples: 1.00000, ips: 0.23972 sequences/sec,  [0m
[32m[2023-12-21 13:20:05,706] [    INFO][0m - global step 92 / 336750, loss: 0.012987, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.98783 sec, avg_samples: 1.00000, ips: 0.25076 sequences/sec,  [0m
[32m[2023-12-21 13:20:09,437] [    INFO][0m - global step 93 / 336750, loss: 0.006328, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.73117 sec, avg_samples: 1.00000, ips: 0.26801 sequences/sec,  [0m
[32m[2023-12-21 13:20:13,034] [    INFO][0m - global step 94 / 336750, loss: 1.921977, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.59632 sec, avg_samples: 1.00000, ips: 0.27806 sequences/sec,  [0m
[32m[2023-12-21 13:20:16,739] [    INFO][0m - global step 95 / 336750, loss: 0.065925, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.70431 sec, avg_samples: 1.00000, ips: 0.26996 sequences/sec,  [0m
[32m[2023-12-21 13:20:20,435] [    INFO][0m - global step 96 / 336750, loss: 0.126908, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.69610 sec, avg_samples: 1.00000, ips: 0.27056 sequences/sec,  [0m
[32m[2023-12-21 13:20:24,030] [    INFO][0m - global step 97 / 336750, loss: 4.541228, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.59471 sec, avg_samples: 1.00000, ips: 0.27819 sequences/sec,  [0m
[32m[2023-12-21 13:20:27,833] [    INFO][0m - global step 98 / 336750, loss: 0.164743, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.80238 sec, avg_samples: 1.00000, ips: 0.26299 sequences/sec,  [0m
[32m[2023-12-21 13:20:31,521] [    INFO][0m - global step 99 / 336750, loss: 2.275252, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.68802 sec, avg_samples: 1.00000, ips: 0.27115 sequences/sec,  [0m
[32m[2023-12-21 13:20:35,021] [    INFO][0m - global step 100 / 336750, loss: 1.538025, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.49895 sec, avg_samples: 1.00000, ips: 0.28580 sequences/sec,  [0m
[32m[2023-12-21 13:20:38,825] [    INFO][0m - global step 101 / 336750, loss: 0.108693, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.80395 sec, avg_samples: 1.00000, ips: 0.26288 sequences/sec,  [0m
[32m[2023-12-21 13:20:42,637] [    INFO][0m - global step 102 / 336750, loss: 0.070371, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.81121 sec, avg_samples: 1.00000, ips: 0.26238 sequences/sec,  [0m
[32m[2023-12-21 13:20:46,206] [    INFO][0m - global step 103 / 336750, loss: 2.129917, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.56901 sec, avg_samples: 1.00000, ips: 0.28019 sequences/sec,  [0m
[32m[2023-12-21 13:20:49,702] [    INFO][0m - global step 104 / 336750, loss: 2.089877, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.49607 sec, avg_samples: 1.00000, ips: 0.28604 sequences/sec,  [0m
[32m[2023-12-21 13:20:53,465] [    INFO][0m - global step 105 / 336750, loss: 0.153717, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.76247 sec, avg_samples: 1.00000, ips: 0.26578 sequences/sec,  [0m
[32m[2023-12-21 13:20:57,110] [    INFO][0m - global step 106 / 336750, loss: 2.127915, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.64472 sec, avg_samples: 1.00000, ips: 0.27437 sequences/sec,  [0m
[32m[2023-12-21 13:21:00,938] [    INFO][0m - global step 107 / 336750, loss: 0.117116, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.82719 sec, avg_samples: 1.00000, ips: 0.26129 sequences/sec,  [0m
[32m[2023-12-21 13:21:04,716] [    INFO][0m - global step 108 / 336750, loss: 0.025183, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.77822 sec, avg_samples: 1.00000, ips: 0.26468 sequences/sec,  [0m
[32m[2023-12-21 13:21:08,475] [    INFO][0m - global step 109 / 336750, loss: 0.006963, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.75862 sec, avg_samples: 1.00000, ips: 0.26605 sequences/sec,  [0m
[32m[2023-12-21 13:21:12,206] [    INFO][0m - global step 110 / 336750, loss: 0.197824, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.73069 sec, avg_samples: 1.00000, ips: 0.26805 sequences/sec,  [0m
[32m[2023-12-21 13:21:15,646] [    INFO][0m - global step 111 / 336750, loss: 0.063914, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.43912 sec, avg_samples: 1.00000, ips: 0.29077 sequences/sec,  [0m
[32m[2023-12-21 13:21:19,581] [    INFO][0m - global step 112 / 336750, loss: 0.066870, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.93446 sec, avg_samples: 1.00000, ips: 0.25416 sequences/sec,  [0m
[32m[2023-12-21 13:21:23,196] [    INFO][0m - global step 113 / 336750, loss: 0.085356, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.61452 sec, avg_samples: 1.00000, ips: 0.27666 sequences/sec,  [0m
[32m[2023-12-21 13:21:26,989] [    INFO][0m - global step 114 / 336750, loss: 0.080441, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.79255 sec, avg_samples: 1.00000, ips: 0.26367 sequences/sec,  [0m
[32m[2023-12-21 13:21:30,775] [    INFO][0m - global step 115 / 336750, loss: 0.049588, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.78563 sec, avg_samples: 1.00000, ips: 0.26416 sequences/sec,  [0m
[32m[2023-12-21 13:21:34,404] [    INFO][0m - global step 116 / 336750, loss: 2.761782, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.62955 sec, avg_samples: 1.00000, ips: 0.27552 sequences/sec,  [0m
[32m[2023-12-21 13:21:38,150] [    INFO][0m - global step 117 / 336750, loss: 0.057670, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.74515 sec, avg_samples: 1.00000, ips: 0.26701 sequences/sec,  [0m
[32m[2023-12-21 13:21:41,730] [    INFO][0m - global step 118 / 336750, loss: 2.811325, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.58004 sec, avg_samples: 1.00000, ips: 0.27933 sequences/sec,  [0m
[32m[2023-12-21 13:21:45,313] [    INFO][0m - global step 119 / 336750, loss: 0.110329, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.58273 sec, avg_samples: 1.00000, ips: 0.27912 sequences/sec,  [0m
[32m[2023-12-21 13:20:38,825] [    INFO][0m - global step 101 / 336750, loss: 0.108693, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.80395 sec, avg_samples: 1.00000, ips: 0.26288 sequences/sec,  [0m
[32m[2023-12-21 13:20:42,637] [    INFO][0m - global step 102 / 336750, loss: 0.070371, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.81121 sec, avg_samples: 1.00000, ips: 0.26238 sequences/sec,  [0m
[32m[2023-12-21 13:20:46,206] [    INFO][0m - global step 103 / 336750, loss: 2.129917, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.56901 sec, avg_samples: 1.00000, ips: 0.28019 sequences/sec,  [0m
[32m[2023-12-21 13:20:49,702] [    INFO][0m - global step 104 / 336750, loss: 2.089877, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.49607 sec, avg_samples: 1.00000, ips: 0.28604 sequences/sec,  [0m
[32m[2023-12-21 13:20:53,465] [    INFO][0m - global step 105 / 336750, loss: 0.153717, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.76247 sec, avg_samples: 1.00000, ips: 0.26578 sequences/sec,  [0m
[32m[2023-12-21 13:20:57,110] [    INFO][0m - global step 106 / 336750, loss: 2.127915, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.64472 sec, avg_samples: 1.00000, ips: 0.27437 sequences/sec,  [0m
[32m[2023-12-21 13:21:00,938] [    INFO][0m - global step 107 / 336750, loss: 0.117116, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.82719 sec, avg_samples: 1.00000, ips: 0.26129 sequences/sec,  [0m
[32m[2023-12-21 13:21:04,716] [    INFO][0m - global step 108 / 336750, loss: 0.025183, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.77822 sec, avg_samples: 1.00000, ips: 0.26468 sequences/sec,  [0m
[32m[2023-12-21 13:21:08,475] [    INFO][0m - global step 109 / 336750, loss: 0.006963, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.75862 sec, avg_samples: 1.00000, ips: 0.26605 sequences/sec,  [0m
[32m[2023-12-21 13:21:12,206] [    INFO][0m - global step 110 / 336750, loss: 0.197824, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.73069 sec, avg_samples: 1.00000, ips: 0.26805 sequences/sec,  [0m
[32m[2023-12-21 13:21:15,646] [    INFO][0m - global step 111 / 336750, loss: 0.063914, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.43912 sec, avg_samples: 1.00000, ips: 0.29077 sequences/sec,  [0m
[32m[2023-12-21 13:21:19,581] [    INFO][0m - global step 112 / 336750, loss: 0.066870, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.93446 sec, avg_samples: 1.00000, ips: 0.25416 sequences/sec,  [0m
[32m[2023-12-21 13:21:23,196] [    INFO][0m - global step 113 / 336750, loss: 0.085356, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.61452 sec, avg_samples: 1.00000, ips: 0.27666 sequences/sec,  [0m
[32m[2023-12-21 13:21:26,989] [    INFO][0m - global step 114 / 336750, loss: 0.080441, avg_reader_cost: 0.00023 sec, avg_batch_cost: 3.79255 sec, avg_samples: 1.00000, ips: 0.26367 sequences/sec,  [0m
[32m[2023-12-21 13:21:30,775] [    INFO][0m - global step 115 / 336750, loss: 0.049588, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.78563 sec, avg_samples: 1.00000, ips: 0.26416 sequences/sec,  [0m
[32m[2023-12-21 13:21:34,404] [    INFO][0m - global step 116 / 336750, loss: 2.761782, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.62955 sec, avg_samples: 1.00000, ips: 0.27552 sequences/sec,  [0m
[32m[2023-12-21 13:21:38,150] [    INFO][0m - global step 117 / 336750, loss: 0.057670, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.74515 sec, avg_samples: 1.00000, ips: 0.26701 sequences/sec,  [0m
[32m[2023-12-21 13:21:41,730] [    INFO][0m - global step 118 / 336750, loss: 2.811325, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.58004 sec, avg_samples: 1.00000, ips: 0.27933 sequences/sec,  [0m
[32m[2023-12-21 13:21:45,313] [    INFO][0m - global step 119 / 336750, loss: 0.110329, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.58273 sec, avg_samples: 1.00000, ips: 0.27912 sequences/sec,  [0m
[32m[2023-12-21 13:21:48,975] [    INFO][0m - global step 120 / 336750, loss: 2.807212, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.66081 sec, avg_samples: 1.00000, ips: 0.27316 sequences/sec,  [0m
[32m[2023-12-21 13:21:52,804] [    INFO][0m - global step 121 / 336750, loss: 0.012429, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.82877 sec, avg_samples: 1.00000, ips: 0.26118 sequences/sec,  [0m
[32m[2023-12-21 13:21:56,442] [    INFO][0m - global step 122 / 336750, loss: 0.072155, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.63803 sec, avg_samples: 1.00000, ips: 0.27487 sequences/sec,  [0m
[32m[2023-12-21 13:22:01,587] [    INFO][0m - global step 123 / 336750, loss: 0.001088, avg_reader_cost: 0.00019 sec, avg_batch_cost: 5.14500 sec, avg_samples: 1.00000, ips: 0.19436 sequences/sec,  [0m
[32m[2023-12-21 13:22:05,158] [    INFO][0m - global step 124 / 336750, loss: 3.201504, avg_reader_cost: 0.00022 sec, avg_batch_cost: 3.57050 sec, avg_samples: 1.00000, ips: 0.28007 sequences/sec,  [0m
[32m[2023-12-21 13:22:08,613] [    INFO][0m - global step 125 / 336750, loss: 0.063731, avg_reader_cost: 0.00019 sec, avg_batch_cost: 3.45380 sec, avg_samples: 1.00000, ips: 0.28954 sequences/sec,  [0m
[32m[2023-12-21 13:22:12,068] [    INFO][0m - global step 126 / 336750, loss: 3.596202, avg_reader_cost: 0.00018 sec, avg_batch_cost: 3.45528 sec, avg_samples: 1.00000, ips: 0.28941 sequences/sec,  [0m
[32m[2023-12-21 13:22:15,558] [    INFO][0m - global step 127 / 336750, loss: 2.947735, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.48930 sec, avg_samples: 1.00000, ips: 0.28659 sequences/sec,  [0m
[32m[2023-12-21 13:22:19,122] [    INFO][0m - global step 128 / 336750, loss: 2.807056, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.56421 sec, avg_samples: 1.00000, ips: 0.28057 sequences/sec,  [0m
[32m[2023-12-21 13:22:22,625] [    INFO][0m - global step 129 / 336750, loss: 2.824594, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.50200 sec, avg_samples: 1.00000, ips: 0.28555 sequences/sec,  [0m
[32m[2023-12-21 13:22:25,993] [    INFO][0m - global step 130 / 336750, loss: 2.815471, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.36781 sec, avg_samples: 1.00000, ips: 0.29693 sequences/sec,  [0m
[32m[2023-12-21 13:22:29,609] [    INFO][0m - global step 131 / 336750, loss: 4.481309, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.61621 sec, avg_samples: 1.00000, ips: 0.27653 sequences/sec,  [0m
[32m[2023-12-21 13:22:33,498] [    INFO][0m - global step 132 / 336750, loss: 0.043173, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.88830 sec, avg_samples: 1.00000, ips: 0.25718 sequences/sec,  [0m
[32m[2023-12-21 13:22:37,066] [    INFO][0m - global step 133 / 336750, loss: 0.048456, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.56722 sec, avg_samples: 1.00000, ips: 0.28033 sequences/sec,  [0m
[32m[2023-12-21 13:22:40,555] [    INFO][0m - global step 134 / 336750, loss: 5.252215, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.48939 sec, avg_samples: 1.00000, ips: 0.28658 sequences/sec,  [0m
[32m[2023-12-21 13:22:44,232] [    INFO][0m - global step 135 / 336750, loss: 0.148820, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.67596 sec, avg_samples: 1.00000, ips: 0.27204 sequences/sec,  [0m
[32m[2023-12-21 13:22:47,775] [    INFO][0m - global step 136 / 336750, loss: 1.913263, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.54282 sec, avg_samples: 1.00000, ips: 0.28226 sequences/sec,  [0m
[32m[2023-12-21 13:22:51,445] [    INFO][0m - global step 137 / 336750, loss: 0.033410, avg_reader_cost: 0.00021 sec, avg_batch_cost: 3.66939 sec, avg_samples: 1.00000, ips: 0.27252 sequences/sec,  [0m
[32m[2023-12-21 13:22:55,138] [    INFO][0m - global step 138 / 336750, loss: 0.017986, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.69325 sec, avg_samples: 1.00000, ips: 0.27076 sequences/sec,  [0m
[32m[2023-12-21 13:22:59,364] [    INFO][0m - global step 139 / 336750, loss: 0.003238, avg_reader_cost: 0.00015 sec, avg_batch_cost: 4.22543 sec, avg_samples: 1.00000, ips: 0.23666 sequences/sec,  [0m
LAUNCH INFO 2023-12-21 13:24:21,004 Pod completed
LAUNCH INFO 2023-12-21 13:24:21,004 Exit code 0
[32m[2023-12-21 13:23:02,905] [    INFO][0m - global step 140 / 336750, loss: 3.254924, avg_reader_cost: 0.00020 sec, avg_batch_cost: 3.54041 sec, avg_samples: 1.00000, ips: 0.28245 sequences/sec,  [0m
[32m[2023-12-21 13:23:06,409] [    INFO][0m - global step 141 / 336750, loss: 2.886678, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.50351 sec, avg_samples: 1.00000, ips: 0.28543 sequences/sec,  [0m
[32m[2023-12-21 13:23:09,910] [    INFO][0m - global step 142 / 336750, loss: 2.761525, avg_reader_cost: 0.00017 sec, avg_batch_cost: 3.50071 sec, avg_samples: 1.00000, ips: 0.28566 sequences/sec,  [0m
[32m[2023-12-21 13:23:13,515] [    INFO][0m - global step 143 / 336750, loss: 0.052502, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.60520 sec, avg_samples: 1.00000, ips: 0.27738 sequences/sec,  [0m
[32m[2023-12-21 13:23:16,454] [    INFO][0m - global step 130 / 2668000, loss: 3.498029, avg_reader_cost: 0.00038 sec, avg_batch_cost: 36.80798 sec, avg_samples: 512.00000, ips: 13.91003 words/sec,  [0m
[32m[2023-12-21 13:23:17,493] [    INFO][0m - global step 144 / 336750, loss: 0.003862, avg_reader_cost: 0.00013 sec, avg_batch_cost: 3.97712 sec, avg_samples: 1.00000, ips: 0.25144 sequences/sec,  [0m
[32m[2023-12-21 13:23:21,201] [    INFO][0m - global step 145 / 336750, loss: 0.054728, avg_reader_cost: 0.00014 sec, avg_batch_cost: 3.70795 sec, avg_samples: 1.00000, ips: 0.26969 sequences/sec,  [0m
[32m[2023-12-21 13:23:25,115] [    INFO][0m - global step 146 / 336750, loss: 0.116749, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.91349 sec, avg_samples: 1.00000, ips: 0.25553 sequences/sec,  [0m
[32m[2023-12-21 13:23:28,732] [    INFO][0m - global step 147 / 336750, loss: 0.137144, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.61709 sec, avg_samples: 1.00000, ips: 0.27647 sequences/sec,  [0m
[32m[2023-12-21 13:23:32,380] [    INFO][0m - global step 148 / 336750, loss: 0.079413, avg_reader_cost: 0.00016 sec, avg_batch_cost: 3.64755 sec, avg_samples: 1.00000, ips: 0.27416 sequences/sec,  [0m
[32m[2023-12-21 13:23:35,843] [    INFO][0m - global step 149 / 336750, loss: 2.777231, avg_reader_cost: 0.00015 sec, avg_batch_cost: 3.46204 sec, avg_samples: 1.00000, ips: 0.28885 sequences/sec,  [0m
No XPU Memory Leak
No XPU Memory Leak
[33m Run successfully with command - xlnet - python3.9 -m paddle.distributed.launch --gpus=0,1 test_tipc/train.py --model xlnet --optimizer adamw --lr_scheduler linear_decay_with_warmup --learning_rate 2e-5 --max_grad_norm 1.0 --model_name_or_path xlnet-base-cased --pad_to_max_seq_len --max_seq_len 128 --logging_steps 1 --task_name SST-2 --max_steps 150 --device=xpu --save_model=/workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1             - /workspace/PaddleNLP/tests/test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log [0m
+ watchcat=5405
+ kill -9 5708
+ sleep 10
==END==test_tipc/configs/xlnet/train_infer_python.txt
run.sh: line 334:  5708 Killed                  ( sleep $waitfor; kill -9 ${commandpid} > /dev/null 2>&1 && printmsg $5 $2 )
+ echo ==END==test_tipc/configs/xlnet/train_infer_python.txt
++ date +%s
+ end=1703136271
++ echo 1703134774 1703136271
++ awk '{print $2-$1-2}'
+ time=1495
test_tipc/configs/xlnet/train_infer_python.txt spend time seconds 1495
+ echo 'test_tipc/configs/xlnet/train_infer_python.txt spend time seconds 1495'
+ read config_file
+ log_file=RESULT
++ pwd
+ current_path=/workspace/PaddleNLP/tests
/workspace/PaddleNLP/tests
PaddleNLP
+ echo /workspace/PaddleNLP/tests
+ echo PaddleNLP
+ [[ /workspace/PaddleNLP/tests == *PaddleNLP* ]]
++ find . -name '*.log'
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ grep 'with command'
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0_usetrt_False_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0_usetrt_False_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0_usetrt_False_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/results_python.log
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/results_python.log
+ grep 'with command'
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0,1_usetrt_False_precision_fp32_batchsize_32.log
./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0,1_usetrt_False_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/bert_base_text_cls/lite_train_lite_infer/python_infer_gpu_gpus_0,1_usetrt_False_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/results_python.log
./test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie3_for_sequence_classification/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
./test_tipc/output/xlnet/lite_train_lite_infer/results_python.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/xlnet/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/xlnet/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/xlnet/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/results_python.log
./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/bert_for_question_answering/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/results_python.log
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ grep 'with command'
./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_information_extraction/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
./test_tipc/output/ernie_tiny/lite_train_lite_infer/results_python.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_tiny/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/ernie_tiny/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_tiny/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/seq2seq/lite_train_lite_infer/results_python.log
./test_tipc/output/seq2seq/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/seq2seq/lite_train_lite_infer/results_python.log
+ grep 'with command'
./test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/seq2seq/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/results_python.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ grep 'with command'
./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_matching/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/results_python.log
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/results_python.log
+ grep 'with command'
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_32.log
+ grep 'with command'
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/norm_gpus_0,1_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ cat ./test_tipc/output/ernie_text_cls/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_32.log
+ grep 'with command'
./test_tipc/output/bigru_crf/lite_train_lite_infer/results_python.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/results_python.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/results_python.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1_export.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0,1_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log
+ grep 'with command'
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0,1_usemkldnn_False_threads_1_precision_fp32_batchsize_1.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1_export.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1_export.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/norm_train_gpus_0_autocast_null_nodes_1_export.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_6_precision_fp32_batchsize_8.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log
./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log
+ cat ./test_tipc/output/bigru_crf/lite_train_lite_infer/python_infer_cpu_gpus_0_usemkldnn_False_threads_1_precision_fp32_batchsize_8.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.bycbks.log
./log/default.bycbks.log
+ cat ./log/default.bycbks.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.ylituh.log
./log/default.ylituh.log
+ cat ./log/default.ylituh.log
+ grep 'with command'
./log/default.mrpnke.log
+ for f in `find . -name '*.log'`
+ echo ./log/default.mrpnke.log
+ cat ./log/default.mrpnke.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.bkwwts.log
./log/default.bkwwts.log
+ cat ./log/default.bkwwts.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.eduptu.log
./log/default.eduptu.log
+ cat ./log/default.eduptu.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.yiugbj.log
./log/default.yiugbj.log
+ cat ./log/default.yiugbj.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.lmexpj.log
./log/default.lmexpj.log
+ cat ./log/default.lmexpj.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.ewgwsj.log
./log/default.ewgwsj.log
+ cat ./log/default.ewgwsj.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.gpu.log
./log/default.gpu.log
+ cat ./log/default.gpu.log
+ grep 'with command'
+ for f in `find . -name '*.log'`
+ echo ./log/default.gentob.log
./log/default.gentob.log
+ cat ./log/default.gentob.log
+ grep 'with command'
./log/default.rceitv.log
+ for f in `find . -name '*.log'`
+ echo ./log/default.rceitv.log
+ cat ./log/default.rceitv.log
+ grep 'with command'
+ python report.py PaddleNLP chain_base xpu_test@baidu.com suijiaxin@baidu.com,songkai05@baidu.com,liqi27@baidu.com,yinxiaoting@baidu.com,wangkai65@baidu.com,isa_train@baidu.com proxy-in.baidu.com
